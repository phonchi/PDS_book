<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.306">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="phonchi">
<meta name="dcterms.date" content="2023-05-22">

<title>Practical and Innovative Analytics in Data Science - 12&nbsp; Representation learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./13_Hyperparameter.html" rel="next">
<link href="./11_Transfer_learning.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./12_Representation_learning.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Representation learning</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Practical and Innovative Analytics in Data Science</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_end_to_end_machine_learning_project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">End-to-end Machine Learning project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_Dataset.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Framing the problem and constructing the dataset</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_Relational_Database_and_data_wrangling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Relational Database and data wrangling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_Clean_feature_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Data cleaning and feature engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_Feature_selection_extraction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Feature selection and extraction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_XAI.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Explainable AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_Deploy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Deploy and monitoring</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_neural_nets_with_tensorflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Introduction to Artificial Neural Networks - Tensorflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_Convolutional_NeuralNetworks_tensorflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Image processing with Convolutional Neural Networks - Tensorflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_Recurrent_Neural_Networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sequence Processing with RNNs and Attention - Tensforflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_Transfer_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Transfer learning and self-supervised learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_Representation_learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Representation learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_Hyperparameter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./NumPy_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Numpy - multidimensional data arrays for python</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Colab_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Introduction to Colab</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./kaggle-explore.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Introduction to Kaggle</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_neural_nets_with_pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Introduction to Artificial Neural Networks - Pytorch</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_Convolutional_NeuralNetworks_pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Image processing with Convolutional Neural Networks - Pytorch</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#setup" id="toc-setup" class="nav-link active" data-scroll-target="#setup"><span class="header-section-number">12.1</span> Setup</a></li>
  <li><a href="#autoencoder" id="toc-autoencoder" class="nav-link" data-scroll-target="#autoencoder"><span class="header-section-number">12.2</span> Autoencoder</a>
  <ul class="collapse">
  <li><a href="#pca-with-a-linear-autoencoder" id="toc-pca-with-a-linear-autoencoder" class="nav-link" data-scroll-target="#pca-with-a-linear-autoencoder"><span class="header-section-number">12.2.1</span> PCA with a linear Autoencoder</a></li>
  <li><a href="#stacked-autoencoders" id="toc-stacked-autoencoders" class="nav-link" data-scroll-target="#stacked-autoencoders"><span class="header-section-number">12.2.2</span> Stacked Autoencoders</a></li>
  <li><a href="#stacked-denoising-autoencoder" id="toc-stacked-denoising-autoencoder" class="nav-link" data-scroll-target="#stacked-denoising-autoencoder"><span class="header-section-number">12.2.3</span> Stacked denoising Autoencoder</a></li>
  <li><a href="#sparse-autoencoder" id="toc-sparse-autoencoder" class="nav-link" data-scroll-target="#sparse-autoencoder"><span class="header-section-number">12.2.4</span> Sparse Autoencoder</a></li>
  </ul></li>
  <li><a href="#variational-autoencoder" id="toc-variational-autoencoder" class="nav-link" data-scroll-target="#variational-autoencoder"><span class="header-section-number">12.3</span> Variational Autoencoder</a>
  <ul class="collapse">
  <li><a href="#latent-space-sampling-layer" id="toc-latent-space-sampling-layer" class="nav-link" data-scroll-target="#latent-space-sampling-layer"><span class="header-section-number">12.3.1</span> Latent-space-sampling layer</a></li>
  <li><a href="#vae-implementation" id="toc-vae-implementation" class="nav-link" data-scroll-target="#vae-implementation"><span class="header-section-number">12.3.2</span> VAE implementation</a></li>
  <li><a href="#generating-fashion-mnist-images" id="toc-generating-fashion-mnist-images" class="nav-link" data-scroll-target="#generating-fashion-mnist-images"><span class="header-section-number">12.3.3</span> Generating Fashion MNIST Images</a></li>
  </ul></li>
  <li><a href="#generative-adversarial-networks" id="toc-generative-adversarial-networks" class="nav-link" data-scroll-target="#generative-adversarial-networks"><span class="header-section-number">12.4</span> Generative Adversarial Networks</a>
  <ul class="collapse">
  <li><a href="#simple-dcgan" id="toc-simple-dcgan" class="nav-link" data-scroll-target="#simple-dcgan"><span class="header-section-number">12.4.1</span> Simple DCGAN</a></li>
  </ul></li>
  <li><a href="#diffusion-models" id="toc-diffusion-models" class="nav-link" data-scroll-target="#diffusion-models"><span class="header-section-number">12.5</span> Diffusion Models</a></li>
  <li><a href="#stable-diffusion-with-diffuser" id="toc-stable-diffusion-with-diffuser" class="nav-link" data-scroll-target="#stable-diffusion-with-diffuser"><span class="header-section-number">12.6</span> Stable Diffusion with <code>diffuser</code></a>
  <ul class="collapse">
  <li><a href="#delve-into-stable-difussion" id="toc-delve-into-stable-difussion" class="nav-link" data-scroll-target="#delve-into-stable-difussion"><span class="header-section-number">12.6.1</span> Delve into stable difussion</a></li>
  <li><a href="#write-your-own-inference-pipeline-with-diffusers" id="toc-write-your-own-inference-pipeline-with-diffusers" class="nav-link" data-scroll-target="#write-your-own-inference-pipeline-with-diffusers"><span class="header-section-number">12.6.2</span> Write your own inference pipeline with <code>diffusers</code></a></li>
  <li><a href="#actual-inference" id="toc-actual-inference" class="nav-link" data-scroll-target="#actual-inference"><span class="header-section-number">12.6.3</span> Actual inference</a></li>
  <li><a href="#stable-diffusion-with-pipeline-of-difusser" id="toc-stable-diffusion-with-pipeline-of-difusser" class="nav-link" data-scroll-target="#stable-diffusion-with-pipeline-of-difusser"><span class="header-section-number">12.6.4</span> Stable diffusion with Pipeline of <code>difusser</code></a></li>
  </ul></li>
  <li><a href="#high-performance-image-generation-using-stable-diffusion-in-kerascv" id="toc-high-performance-image-generation-using-stable-diffusion-in-kerascv" class="nav-link" data-scroll-target="#high-performance-image-generation-using-stable-diffusion-in-kerascv"><span class="header-section-number">12.7</span> High-performance image generation using Stable Diffusion in <code>KerasCV</code></a>
  <ul class="collapse">
  <li><a href="#perks-of-kerascv" id="toc-perks-of-kerascv" class="nav-link" data-scroll-target="#perks-of-kerascv"><span class="header-section-number">12.7.1</span> Perks of KerasCV</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">13</span> References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Representation learning</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>phonchi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 22, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>


<table align="left">
<tbody><tr><td>
<a href="https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/12_Representation_learning.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>
</td>
<td>
<a target="_blank" href="https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/12_Representation_learning.ipynb"><img src="https://kaggle.com/static/images/open-in-kaggle.svg"></a>
</td>

</tr></tbody></table>
<p><br></p>
<section id="setup" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="setup"><span class="header-section-number">12.1</span> Setup</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="op">%</span>pip install <span class="op">--</span>upgrade keras<span class="op">-</span>cv <span class="op">-</span>qq</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="op">%</span>pip install diffusers <span class="op">-</span>qq</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="op">%</span>pip install transformers ftfy accelerate <span class="op">-</span>qq</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="op">%</span>pip install torchinfo <span class="op">-</span>qq</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="op">%</span>pip install opentsne <span class="op">-</span>qq</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:29:44.214043Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:29:44.213765Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:29:48.908610Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:29:48.906895Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:29:44.214015Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># Python ≥3.7 is recommended</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> sys</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="cf">assert</span> sys.version_info <span class="op">&gt;=</span> (<span class="dv">3</span>, <span class="dv">7</span>)</span>
<span id="cb2-4"><a href="#cb2-4"></a></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="co"># Scikit-Learn ≥1.01 is recommended</span></span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="im">from</span> packaging <span class="im">import</span> version</span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="im">import</span> sklearn</span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="cf">assert</span> version.parse(sklearn.__version__) <span class="op">&gt;=</span> version.parse(<span class="st">"1.0.1"</span>)</span>
<span id="cb2-9"><a href="#cb2-9"></a></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="co"># Tensorflow ≥2.8.0 is recommended</span></span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="cf">assert</span> version.parse(tf.__version__) <span class="op">&gt;=</span> version.parse(<span class="st">"2.8.0"</span>)</span>
<span id="cb2-13"><a href="#cb2-13"></a></span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="co"># Difussion model</span></span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="im">import</span> keras_cv</span>
<span id="cb2-16"><a href="#cb2-16"></a><span class="im">import</span> torch</span>
<span id="cb2-17"><a href="#cb2-17"></a><span class="im">from</span> torch <span class="im">import</span> autocast</span>
<span id="cb2-18"><a href="#cb2-18"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb2-19"><a href="#cb2-19"></a><span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb2-20"><a href="#cb2-20"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms <span class="im">as</span> tfms</span>
<span id="cb2-21"><a href="#cb2-21"></a><span class="im">from</span> diffusers <span class="im">import</span> StableDiffusionPipeline</span>
<span id="cb2-22"><a href="#cb2-22"></a><span class="im">from</span> transformers <span class="im">import</span> CLIPTextModel, CLIPTokenizer</span>
<span id="cb2-23"><a href="#cb2-23"></a><span class="im">from</span> diffusers <span class="im">import</span> AutoencoderKL, UNet2DConditionModel</span>
<span id="cb2-24"><a href="#cb2-24"></a><span class="im">from</span> diffusers <span class="im">import</span> LMSDiscreteScheduler, PNDMScheduler</span>
<span id="cb2-25"><a href="#cb2-25"></a></span>
<span id="cb2-26"><a href="#cb2-26"></a><span class="co"># Image related</span></span>
<span id="cb2-27"><a href="#cb2-27"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb2-28"><a href="#cb2-28"></a><span class="im">from</span> skimage.io <span class="im">import</span> imread, imshow, show</span>
<span id="cb2-29"><a href="#cb2-29"></a><span class="im">from</span> skimage.color <span class="im">import</span> rgba2rgb</span>
<span id="cb2-30"><a href="#cb2-30"></a><span class="im">from</span> skimage.transform <span class="im">import</span> resize</span>
<span id="cb2-31"><a href="#cb2-31"></a></span>
<span id="cb2-32"><a href="#cb2-32"></a><span class="co"># Common imports</span></span>
<span id="cb2-33"><a href="#cb2-33"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb2-34"><a href="#cb2-34"></a><span class="im">from</span> openTSNE <span class="im">import</span> TSNE</span>
<span id="cb2-35"><a href="#cb2-35"></a><span class="im">import</span> requests</span>
<span id="cb2-36"><a href="#cb2-36"></a><span class="im">import</span> shutil</span>
<span id="cb2-37"><a href="#cb2-37"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-38"><a href="#cb2-38"></a><span class="im">import</span> os</span>
<span id="cb2-39"><a href="#cb2-39"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb2-40"><a href="#cb2-40"></a><span class="im">import</span> time</span>
<span id="cb2-41"><a href="#cb2-41"></a></span>
<span id="cb2-42"><a href="#cb2-42"></a><span class="co"># To plot pretty figures</span></span>
<span id="cb2-43"><a href="#cb2-43"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-44"><a href="#cb2-44"></a><span class="im">import</span> matplotlib</span>
<span id="cb2-45"><a href="#cb2-45"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb2-46"><a href="#cb2-46"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb2-47"><a href="#cb2-47"></a></span>
<span id="cb2-48"><a href="#cb2-48"></a>plt.rc(<span class="st">'font'</span>, size<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb2-49"><a href="#cb2-49"></a>plt.rc(<span class="st">'axes'</span>, labelsize<span class="op">=</span><span class="dv">14</span>, titlesize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb2-50"><a href="#cb2-50"></a>plt.rc(<span class="st">'legend'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb2-51"><a href="#cb2-51"></a>plt.rc(<span class="st">'xtick'</span>, labelsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb2-52"><a href="#cb2-52"></a>plt.rc(<span class="st">'ytick'</span>, labelsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb2-53"><a href="#cb2-53"></a></span>
<span id="cb2-54"><a href="#cb2-54"></a><span class="co"># to make this notebook's output stable across runs</span></span>
<span id="cb2-55"><a href="#cb2-55"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb2-56"><a href="#cb2-56"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility on CPU </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="cf">if</span> <span class="kw">not</span> tf.config.list_physical_devices(<span class="st">'GPU'</span>):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="bu">print</span>(<span class="st">"No GPU was detected. Neural nets can be very slow without a GPU."</span>)</span>
<span id="cb3-3"><a href="#cb3-3"></a>    <span class="cf">if</span> <span class="st">"google.colab"</span> <span class="kw">in</span> sys.modules:</span>
<span id="cb3-4"><a href="#cb3-4"></a>        <span class="bu">print</span>(<span class="st">"Go to Runtime &gt; Change runtime and select a GPU hardware "</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>              <span class="st">"accelerator."</span>)</span>
<span id="cb3-6"><a href="#cb3-6"></a>    <span class="cf">if</span> <span class="st">"kaggle_secrets"</span> <span class="kw">in</span> sys.modules:</span>
<span id="cb3-7"><a href="#cb3-7"></a>        <span class="bu">print</span>(<span class="st">"Go to Settings &gt; Accelerator and select GPU."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A couple utility functions to plot grayscale <span class="math inline">\(28 \times 28\)</span> image:</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T07:01:03.206855Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T07:01:03.206142Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T07:01:03.219548Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T07:01:03.218810Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T07:01:03.206807Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="co"># Visualize 2d manifold from  encodings using tSNE</span></span>
<span id="cb4-2"><a href="#cb4-2"></a></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="kw">def</span> plot_embeddings_tsne(X_data, y_data, encodings):</span>
<span id="cb4-4"><a href="#cb4-4"></a>  np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-5"><a href="#cb4-5"></a>  tsne <span class="op">=</span> TSNE(negative_gradient_method<span class="op">=</span><span class="st">"fft"</span>)</span>
<span id="cb4-6"><a href="#cb4-6"></a>  X_data_2D <span class="op">=</span> tsne.fit(encodings)</span>
<span id="cb4-7"><a href="#cb4-7"></a>  X_data_2D <span class="op">=</span> (X_data_2D <span class="op">-</span> X_data_2D.<span class="bu">min</span>()) <span class="op">/</span> (X_data_2D.<span class="bu">max</span>() <span class="op">-</span> X_data_2D.<span class="bu">min</span>())</span>
<span id="cb4-8"><a href="#cb4-8"></a></span>
<span id="cb4-9"><a href="#cb4-9"></a>  <span class="co"># adapted from https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html</span></span>
<span id="cb4-10"><a href="#cb4-10"></a>  plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb4-11"><a href="#cb4-11"></a>  cmap <span class="op">=</span> plt.cm.tab10</span>
<span id="cb4-12"><a href="#cb4-12"></a>  plt.scatter(X_data_2D[:, <span class="dv">0</span>], X_data_2D[:, <span class="dv">1</span>], c<span class="op">=</span>y_data, s<span class="op">=</span><span class="dv">10</span>, cmap<span class="op">=</span>cmap)</span>
<span id="cb4-13"><a href="#cb4-13"></a>  image_positions <span class="op">=</span> np.array([[<span class="fl">1.</span>, <span class="fl">1.</span>]])</span>
<span id="cb4-14"><a href="#cb4-14"></a>  <span class="cf">for</span> index, position <span class="kw">in</span> <span class="bu">enumerate</span>(X_data_2D):</span>
<span id="cb4-15"><a href="#cb4-15"></a>      dist <span class="op">=</span> np.<span class="bu">sum</span>((position <span class="op">-</span> image_positions) <span class="op">**</span> <span class="dv">2</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-16"><a href="#cb4-16"></a>      <span class="cf">if</span> np.<span class="bu">min</span>(dist) <span class="op">&gt;</span> <span class="fl">0.02</span>: <span class="co"># if far enough from other images</span></span>
<span id="cb4-17"><a href="#cb4-17"></a>          image_positions <span class="op">=</span> np.r_[image_positions, [position]]</span>
<span id="cb4-18"><a href="#cb4-18"></a>          imagebox <span class="op">=</span> matplotlib.offsetbox.AnnotationBbox(</span>
<span id="cb4-19"><a href="#cb4-19"></a>              matplotlib.offsetbox.OffsetImage(X_data[index], cmap<span class="op">=</span><span class="st">"binary"</span>),</span>
<span id="cb4-20"><a href="#cb4-20"></a>              position, bboxprops<span class="op">=</span>{<span class="st">"edgecolor"</span>: cmap(y_data[index]), <span class="st">"lw"</span>: <span class="dv">2</span>})</span>
<span id="cb4-21"><a href="#cb4-21"></a>          plt.gca().add_artist(imagebox)</span>
<span id="cb4-22"><a href="#cb4-22"></a>  plt.axis(<span class="st">"off"</span>)<span class="op">;</span></span>
<span id="cb4-23"><a href="#cb4-23"></a></span>
<span id="cb4-24"><a href="#cb4-24"></a><span class="kw">def</span> plot_reconstructions(model, images, n_images<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb4-25"><a href="#cb4-25"></a>    reconstructions <span class="op">=</span> np.clip(model.predict(images[:n_images]), <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb4-26"><a href="#cb4-26"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(n_images <span class="op">*</span> <span class="fl">1.5</span>, <span class="dv">3</span>))</span>
<span id="cb4-27"><a href="#cb4-27"></a>    <span class="cf">for</span> image_index <span class="kw">in</span> <span class="bu">range</span>(n_images):</span>
<span id="cb4-28"><a href="#cb4-28"></a>        plt.subplot(<span class="dv">2</span>, n_images, <span class="dv">1</span> <span class="op">+</span> image_index)</span>
<span id="cb4-29"><a href="#cb4-29"></a>        plt.imshow(images[image_index], cmap<span class="op">=</span><span class="st">"binary"</span>)</span>
<span id="cb4-30"><a href="#cb4-30"></a>        plt.axis(<span class="st">"off"</span>)</span>
<span id="cb4-31"><a href="#cb4-31"></a>        plt.subplot(<span class="dv">2</span>, n_images, <span class="dv">1</span> <span class="op">+</span> n_images <span class="op">+</span> image_index)</span>
<span id="cb4-32"><a href="#cb4-32"></a>        plt.imshow(reconstructions[image_index], cmap<span class="op">=</span><span class="st">"binary"</span>)</span>
<span id="cb4-33"><a href="#cb4-33"></a>        plt.axis(<span class="st">"off"</span>)  </span>
<span id="cb4-34"><a href="#cb4-34"></a></span>
<span id="cb4-35"><a href="#cb4-35"></a><span class="kw">def</span> plot_multiple_images(images, n_cols<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-36"><a href="#cb4-36"></a>    n_cols <span class="op">=</span> n_cols <span class="kw">or</span> <span class="bu">len</span>(images)</span>
<span id="cb4-37"><a href="#cb4-37"></a>    n_rows <span class="op">=</span> (<span class="bu">len</span>(images) <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> n_cols <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb4-38"><a href="#cb4-38"></a>    <span class="cf">if</span> images.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb4-39"><a href="#cb4-39"></a>        images <span class="op">=</span> images.squeeze(axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-40"><a href="#cb4-40"></a>    plt.figure(figsize<span class="op">=</span>(n_cols, n_rows))</span>
<span id="cb4-41"><a href="#cb4-41"></a>    <span class="cf">for</span> index, image <span class="kw">in</span> <span class="bu">enumerate</span>(images):</span>
<span id="cb4-42"><a href="#cb4-42"></a>        plt.subplot(n_rows, n_cols, index <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-43"><a href="#cb4-43"></a>        plt.imshow(image, cmap<span class="op">=</span><span class="st">"binary"</span>)</span>
<span id="cb4-44"><a href="#cb4-44"></a>        plt.axis(<span class="st">"off"</span>) </span>
<span id="cb4-45"><a href="#cb4-45"></a></span>
<span id="cb4-46"><a href="#cb4-46"></a><span class="kw">def</span> plot_images(images):</span>
<span id="cb4-47"><a href="#cb4-47"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">20</span>))</span>
<span id="cb4-48"><a href="#cb4-48"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(images)):</span>
<span id="cb4-49"><a href="#cb4-49"></a>        ax <span class="op">=</span> plt.subplot(<span class="dv">1</span>, <span class="bu">len</span>(images), i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-50"><a href="#cb4-50"></a>        plt.imshow(images[i])</span>
<span id="cb4-51"><a href="#cb4-51"></a>        plt.axis(<span class="st">"off"</span>)</span>
<span id="cb4-52"><a href="#cb4-52"></a></span>
<span id="cb4-53"><a href="#cb4-53"></a><span class="co"># To visualize statistics of the hidden units adapted from https://github.com/probml/pyprobml/blob/master/notebooks/book1/20/ae_mnist_tf.ipynb</span></span>
<span id="cb4-54"><a href="#cb4-54"></a></span>
<span id="cb4-55"><a href="#cb4-55"></a><span class="kw">def</span> plot_percent_hist(ax, data, bins):</span>
<span id="cb4-56"><a href="#cb4-56"></a>    counts, _ <span class="op">=</span> np.histogram(data, bins<span class="op">=</span>bins)</span>
<span id="cb4-57"><a href="#cb4-57"></a>    widths <span class="op">=</span> bins[<span class="dv">1</span>:] <span class="op">-</span> bins[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb4-58"><a href="#cb4-58"></a>    x <span class="op">=</span> bins[:<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> widths <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb4-59"><a href="#cb4-59"></a>    ax.bar(x, counts <span class="op">/</span> <span class="bu">len</span>(data), width<span class="op">=</span>widths<span class="op">*</span><span class="fl">0.8</span>)</span>
<span id="cb4-60"><a href="#cb4-60"></a>    ax.xaxis.set_ticks(bins)</span>
<span id="cb4-61"><a href="#cb4-61"></a>    ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(</span>
<span id="cb4-62"><a href="#cb4-62"></a>        <span class="kw">lambda</span> y, position: <span class="st">"</span><span class="sc">{}</span><span class="st">%"</span>.<span class="bu">format</span>(<span class="bu">int</span>(np.<span class="bu">round</span>(<span class="dv">100</span> <span class="op">*</span> y)))))</span>
<span id="cb4-63"><a href="#cb4-63"></a>    ax.grid(<span class="va">True</span>)</span>
<span id="cb4-64"><a href="#cb4-64"></a></span>
<span id="cb4-65"><a href="#cb4-65"></a><span class="kw">def</span> plot_activations_histogram2(encoder, height<span class="op">=</span><span class="dv">1</span>, n_bins<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb4-66"><a href="#cb4-66"></a>    X_valid_codings <span class="op">=</span> encoder(X_valid).numpy()</span>
<span id="cb4-67"><a href="#cb4-67"></a>    activation_means <span class="op">=</span> X_valid_codings.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-68"><a href="#cb4-68"></a>    mean <span class="op">=</span> activation_means.mean()</span>
<span id="cb4-69"><a href="#cb4-69"></a>    bins <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, n_bins <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-70"><a href="#cb4-70"></a></span>
<span id="cb4-71"><a href="#cb4-71"></a>    fig, ax1 <span class="op">=</span> plt.subplots()</span>
<span id="cb4-72"><a href="#cb4-72"></a>    plot_percent_hist(ax1, X_valid_codings.ravel(), bins)</span>
<span id="cb4-73"><a href="#cb4-73"></a>    ax1.plot([mean, mean], [<span class="dv">0</span>, height], <span class="st">"k--"</span>, label<span class="op">=</span><span class="st">"Overall Mean = </span><span class="sc">{:.2f}</span><span class="st">"</span>.<span class="bu">format</span>(mean))</span>
<span id="cb4-74"><a href="#cb4-74"></a>    ax1.legend(loc<span class="op">=</span><span class="st">"upper center"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-75"><a href="#cb4-75"></a>    ax1.set_xlabel(<span class="st">"Activation"</span>)</span>
<span id="cb4-76"><a href="#cb4-76"></a>    ax1.set_ylabel(<span class="st">"% Activations"</span>)</span>
<span id="cb4-77"><a href="#cb4-77"></a>    ax1.axis([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, height])</span>
<span id="cb4-78"><a href="#cb4-78"></a>    plt.show()</span>
<span id="cb4-79"><a href="#cb4-79"></a>    </span>
<span id="cb4-80"><a href="#cb4-80"></a>    fig, ax2 <span class="op">=</span> plt.subplots()</span>
<span id="cb4-81"><a href="#cb4-81"></a>    plot_percent_hist(ax2, activation_means, bins)</span>
<span id="cb4-82"><a href="#cb4-82"></a>    ax2.plot([mean, mean], [<span class="dv">0</span>, height], <span class="st">"k--"</span>, label<span class="op">=</span><span class="st">"Overall Mean = </span><span class="sc">{:.2f}</span><span class="st">"</span>.<span class="bu">format</span>(mean))</span>
<span id="cb4-83"><a href="#cb4-83"></a>    ax2.set_xlabel(<span class="st">"Neuron Mean Activation"</span>)</span>
<span id="cb4-84"><a href="#cb4-84"></a>    ax2.set_ylabel(<span class="st">"% Neurons"</span>)</span>
<span id="cb4-85"><a href="#cb4-85"></a>    ax2.axis([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, height])</span>
<span id="cb4-86"><a href="#cb4-86"></a>    plt.show()</span>
<span id="cb4-87"><a href="#cb4-87"></a></span>
<span id="cb4-88"><a href="#cb4-88"></a><span class="kw">def</span> plot_activations_heatmap(encoder, N<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb4-89"><a href="#cb4-89"></a>    X <span class="op">=</span> encoder(X_valid).numpy()</span>
<span id="cb4-90"><a href="#cb4-90"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb4-91"><a href="#cb4-91"></a>    plt.imshow(X[:N,:])</span>
<span id="cb4-92"><a href="#cb4-92"></a>    plt.colorbar() </span>
<span id="cb4-93"><a href="#cb4-93"></a></span>
<span id="cb4-94"><a href="#cb4-94"></a><span class="co"># Set device</span></span>
<span id="cb4-95"><a href="#cb4-95"></a>torch_device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb4-96"><a href="#cb4-96"></a></span>
<span id="cb4-97"><a href="#cb4-97"></a><span class="kw">def</span> pil_to_latent(input_im):</span>
<span id="cb4-98"><a href="#cb4-98"></a>    <span class="co"># Single image -&gt; single latent in a batch (so size 1, 4, 64, 64)</span></span>
<span id="cb4-99"><a href="#cb4-99"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb4-100"><a href="#cb4-100"></a>        latent <span class="op">=</span> vae.encode(tfms.ToTensor()(input_im).unsqueeze(<span class="dv">0</span>).to(torch_device)<span class="op">*</span><span class="dv">2</span><span class="op">-</span><span class="dv">1</span>) <span class="co"># Note scaling</span></span>
<span id="cb4-101"><a href="#cb4-101"></a>    <span class="cf">return</span> <span class="fl">0.18215</span> <span class="op">*</span> latent.latent_dist.sample()</span>
<span id="cb4-102"><a href="#cb4-102"></a></span>
<span id="cb4-103"><a href="#cb4-103"></a><span class="kw">def</span> latents_to_pil(latents):</span>
<span id="cb4-104"><a href="#cb4-104"></a>    <span class="co"># bath of latents -&gt; list of images</span></span>
<span id="cb4-105"><a href="#cb4-105"></a>    latents <span class="op">=</span> (<span class="dv">1</span> <span class="op">/</span> <span class="fl">0.18215</span>) <span class="op">*</span> latents</span>
<span id="cb4-106"><a href="#cb4-106"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb4-107"><a href="#cb4-107"></a>        image <span class="op">=</span> vae.decode(latents).sample</span>
<span id="cb4-108"><a href="#cb4-108"></a>    image <span class="op">=</span> (image <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="fl">0.5</span>).clamp(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb4-109"><a href="#cb4-109"></a>    image <span class="op">=</span> image.detach().cpu().permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>).numpy()</span>
<span id="cb4-110"><a href="#cb4-110"></a>    images <span class="op">=</span> (image <span class="op">*</span> <span class="dv">255</span>).<span class="bu">round</span>().astype(<span class="st">"uint8"</span>)</span>
<span id="cb4-111"><a href="#cb4-111"></a>    pil_images <span class="op">=</span> [Image.fromarray(image) <span class="cf">for</span> image <span class="kw">in</span> images]</span>
<span id="cb4-112"><a href="#cb4-112"></a>    <span class="cf">return</span> pil_images</span>
<span id="cb4-113"><a href="#cb4-113"></a></span>
<span id="cb4-114"><a href="#cb4-114"></a><span class="kw">def</span> download_from_pokemondb(input_url, out_file):</span>
<span id="cb4-115"><a href="#cb4-115"></a>  r <span class="op">=</span> requests.get(input_url, stream<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-116"><a href="#cb4-116"></a>  <span class="cf">if</span> r.status_code <span class="op">==</span> <span class="dv">200</span>:</span>
<span id="cb4-117"><a href="#cb4-117"></a>    <span class="cf">with</span> <span class="bu">open</span>(out_file, <span class="st">'wb'</span>) <span class="im">as</span> f:</span>
<span id="cb4-118"><a href="#cb4-118"></a>        r.raw.decode_content <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-119"><a href="#cb4-119"></a>        shutil.copyfileobj(r.raw, f)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="autoencoder" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="autoencoder"><span class="header-section-number">12.2</span> Autoencoder</h2>
<section id="pca-with-a-linear-autoencoder" class="level3" data-number="12.2.1">
<h3 data-number="12.2.1" class="anchored" data-anchor-id="pca-with-a-linear-autoencoder"><span class="header-section-number">12.2.1</span> PCA with a linear Autoencoder</h3>
<p>If the autoencoder uses only linear activations and the cost function is the mean squared error (MSE), then it ends up performing Principal Component Analysis. The following code builds a simple linear autoencoder to perform PCA on a 3D dataset, projecting it to 2D</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:30:30.093539Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:30:30.092944Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:30:30.103638Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:30:30.102694Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:30:30.093498Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb5-2"><a href="#cb5-2"></a></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="kw">def</span> generate_3d_data(m, w1<span class="op">=</span><span class="fl">0.1</span>, w2<span class="op">=</span><span class="fl">0.3</span>, noise<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb5-4"><a href="#cb5-4"></a>    angles <span class="op">=</span> np.random.rand(m) <span class="op">*</span> <span class="dv">3</span> <span class="op">*</span> np.pi <span class="op">/</span> <span class="dv">2</span> <span class="op">-</span> <span class="fl">0.5</span></span>
<span id="cb5-5"><a href="#cb5-5"></a>    data <span class="op">=</span> np.empty((m, <span class="dv">3</span>))</span>
<span id="cb5-6"><a href="#cb5-6"></a>    data[:, <span class="dv">0</span>] <span class="op">=</span> np.cos(angles) <span class="op">+</span> np.sin(angles)<span class="op">/</span><span class="dv">2</span> <span class="op">+</span> noise <span class="op">*</span> np.random.randn(m) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb5-7"><a href="#cb5-7"></a>    data[:, <span class="dv">1</span>] <span class="op">=</span> np.sin(angles) <span class="op">*</span> <span class="fl">0.7</span> <span class="op">+</span> noise <span class="op">*</span> np.random.randn(m) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>    data[:, <span class="dv">2</span>] <span class="op">=</span> data[:, <span class="dv">0</span>] <span class="op">*</span> w1 <span class="op">+</span> data[:, <span class="dv">1</span>] <span class="op">*</span> w2 <span class="op">+</span> noise <span class="op">*</span> np.random.randn(m)</span>
<span id="cb5-9"><a href="#cb5-9"></a>    <span class="cf">return</span> data</span>
<span id="cb5-10"><a href="#cb5-10"></a></span>
<span id="cb5-11"><a href="#cb5-11"></a>X_train <span class="op">=</span> generate_3d_data(<span class="dv">60</span>)</span>
<span id="cb5-12"><a href="#cb5-12"></a>X_train <span class="op">=</span> X_train <span class="op">-</span> X_train.mean(axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:30:30.286584Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:30:30.286282Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:30:30.708882Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:30:30.708259Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:30:30.286545Z&quot;}" data-executioninfo="{&quot;elapsed&quot;:987,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684641635150,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="a77e83c8-c836-4da3-ee9e-8fdc586770ec" data-trusted="true">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="fl">3.8</span>))</span>
<span id="cb6-2"><a href="#cb6-2"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a>ax.plot(X_train[:, <span class="dv">0</span>], X_train[:, <span class="dv">1</span>], X_train[:, <span class="dv">2</span>], <span class="st">"k."</span>)</span>
<span id="cb6-5"><a href="#cb6-5"></a>ax.set_xlabel(<span class="st">"$x_1$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>, labelpad<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb6-6"><a href="#cb6-6"></a>ax.set_ylabel(<span class="st">"$x_2$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>, labelpad<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb6-7"><a href="#cb6-7"></a>ax.set_zlabel(<span class="st">"$x_3$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>, labelpad<span class="op">=</span><span class="dv">10</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>Text(0.5, 0, '$x_3$')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-7-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>If the autoencoder uses only linear activations and the cost function is the mean squared error (MSE), then it ends up performing principal component analysis:</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:30:30.909648Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:30:30.909274Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:30:34.166157Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:30:34.164355Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:30:30.909616Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>tf.keras.backend.clear_session()</span>
<span id="cb8-2"><a href="#cb8-2"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility on CPU</span></span>
<span id="cb8-3"><a href="#cb8-3"></a></span>
<span id="cb8-4"><a href="#cb8-4"></a>encoder <span class="op">=</span> tf.keras.Sequential([tf.keras.layers.Dense(<span class="dv">2</span>)])</span>
<span id="cb8-5"><a href="#cb8-5"></a>decoder <span class="op">=</span> tf.keras.Sequential([tf.keras.layers.Dense(<span class="dv">3</span>)])</span>
<span id="cb8-6"><a href="#cb8-6"></a>autoencoder <span class="op">=</span> tf.keras.Sequential([encoder, decoder])</span>
<span id="cb8-7"><a href="#cb8-7"></a></span>
<span id="cb8-8"><a href="#cb8-8"></a>optimizer <span class="op">=</span> tf.keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-9"><a href="#cb8-9"></a>autoencoder.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"mse"</span>, optimizer<span class="op">=</span>optimizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:30:34.168119Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:30:34.167887Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:30:35.486476Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:30:35.485746Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:30:34.168086Z&quot;}" data-executioninfo="{&quot;elapsed&quot;:5802,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684641664553,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="9984da8f-5449-4167-c605-059609c90f03" data-trusted="true">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>history <span class="op">=</span> autoencoder.fit(X_train, X_train, epochs<span class="op">=</span><span class="dv">500</span>, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-2"><a href="#cb9-2"></a>codings <span class="op">=</span> encoder.predict(X_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>2/2 [==============================] - 0s 6ms/step</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:30:35.590247Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:30:35.590007Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:30:35.979782Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:30:35.979111Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:30:35.590212Z&quot;}" data-executioninfo="{&quot;elapsed&quot;:10,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684641664553,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="a305216f-f53a-42f3-8f3f-178d3fb3d046" data-trusted="true">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">3</span>))</span>
<span id="cb11-2"><a href="#cb11-2"></a>plt.plot(codings[:,<span class="dv">0</span>], codings[:, <span class="dv">1</span>], <span class="st">"b."</span>)</span>
<span id="cb11-3"><a href="#cb11-3"></a>plt.xlabel(<span class="st">"$z_1$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb11-4"><a href="#cb11-4"></a>plt.ylabel(<span class="st">"$z_2$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>, rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb11-5"><a href="#cb11-5"></a>plt.grid(<span class="va">True</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>As you can see, the autoencoder found the best 2D plane to project the data onto, preserving as much variance in the data as it could (just like PCA). Note that we exchage the axis in this figure, since autoencoder will not order the data according to the eigenvalues.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:30:35.981610Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:30:35.981153Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:30:35.998413Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:30:35.997360Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:30:35.981573Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-2"><a href="#cb12-2"></a>codings2 <span class="op">=</span> pca.fit_transform(X_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:30:36.000126Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:30:35.999854Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:30:36.189139Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:30:36.188362Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:30:36.000089Z&quot;}" data-executioninfo="{&quot;elapsed&quot;:944,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684641683753,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="ea4910dc-5d50-4962-efaa-e2c77ee0e246" data-trusted="true">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">3</span>))</span>
<span id="cb13-2"><a href="#cb13-2"></a>plt.plot(codings2[:,<span class="dv">0</span>], <span class="op">-</span>codings2[:, <span class="dv">1</span>], <span class="st">"b."</span>)</span>
<span id="cb13-3"><a href="#cb13-3"></a>plt.xlabel(<span class="st">"$z_1$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb13-4"><a href="#cb13-4"></a>plt.ylabel(<span class="st">"$z_2$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb13-5"><a href="#cb13-5"></a>plt.grid(<span class="va">True</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="stacked-autoencoders" class="level3" data-number="12.2.2">
<h3 data-number="12.2.2" class="anchored" data-anchor-id="stacked-autoencoders"><span class="header-section-number">12.2.2</span> Stacked Autoencoders</h3>
<p>Let’s load the fashion MNIST dataset, scale it, and split it into a training set, a validation set, and a test set:</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:30:36.191811Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:30:36.191051Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:30:37.270030Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:30:37.269269Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:30:36.191728Z&quot;}" data-executioninfo="{&quot;elapsed&quot;:2922,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684654343966,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="060ca0c0-f7f7-4201-eda4-fc47048f674b" data-trusted="true">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>fashion_mnist <span class="op">=</span> tf.keras.datasets.fashion_mnist.load_data()</span>
<span id="cb14-2"><a href="#cb14-2"></a>(X_train_full, y_train_full), (X_test, y_test) <span class="op">=</span> fashion_mnist</span>
<span id="cb14-3"><a href="#cb14-3"></a>X_train_full <span class="op">=</span> X_train_full.astype(np.float32) <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb14-4"><a href="#cb14-4"></a>X_test <span class="op">=</span> X_test.astype(np.float32) <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb14-5"><a href="#cb14-5"></a>X_train, X_valid <span class="op">=</span> X_train_full[:<span class="op">-</span><span class="dv">5000</span>], X_train_full[<span class="op">-</span><span class="dv">5000</span>:]</span>
<span id="cb14-6"><a href="#cb14-6"></a>y_train, y_valid <span class="op">=</span> y_train_full[:<span class="op">-</span><span class="dv">5000</span>], y_train_full[<span class="op">-</span><span class="dv">5000</span>:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz
29515/29515 [==============================] - 0s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz
26421880/26421880 [==============================] - 1s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz
5148/5148 [==============================] - 0s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz
4422102/4422102 [==============================] - 1s 0us/step</code></pre>
</div>
</div>
<section id="mlp-layer" class="level4" data-number="12.2.2.1">
<h4 data-number="12.2.2.1" class="anchored" data-anchor-id="mlp-layer"><span class="header-section-number">12.2.2.1</span> MLP Layer</h4>
<p>You can implement a stacked autoencoder very much like a regular deep MLP. The following code builds a stacked autoencoder for Fashion MNIST. we split the autoencoder model into two submodels: the encoder and the decoder.</p>
<ul>
<li><p>The encoder takes 28 × 28–pixel grayscale images, flattens them so that each image is represented as a vector of size 784, then processes these vectors through two <code>Dense</code> layers of diminishing sizes (100 units then 30 units), both using the <code>ReLU</code> activation function. For each input image, the encoder outputs a vector of size 30.</p></li>
<li><p>The decoder takes codings of size 30 (output by the encoder) and processes them through two Dense layers of increasing sizes (100 units then 784 units), and it reshapes the final vectors into 28 × 28 arrays so the decoder’s outputs have the same shape as the encoder’s inputs.</p></li>
<li><p>When compiling the stacked autoencoder, we use MSE loss and <code>Nadam</code> optimization.</p></li>
<li><p>We train the model using <code>X_train</code> as <strong>both the inputs and the targets</strong> (and similarly, we use <code>X_valid</code> as both the validation inputs and targets)</p></li>
</ul>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:30:39.404282Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:30:39.404031Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:31:02.982755Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:31:02.981939Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:30:39.404253Z&quot;}" data-executioninfo="{&quot;elapsed&quot;:269284,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684643592074,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="ab50dc78-b12c-45f5-dc52-e9177473acdf" data-trusted="true">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>tf.keras.backend.clear_session()</span>
<span id="cb16-2"><a href="#cb16-2"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility on CPU</span></span>
<span id="cb16-3"><a href="#cb16-3"></a></span>
<span id="cb16-4"><a href="#cb16-4"></a>stacked_encoder <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb16-5"><a href="#cb16-5"></a>    tf.keras.layers.Flatten(),</span>
<span id="cb16-6"><a href="#cb16-6"></a>    tf.keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb16-7"><a href="#cb16-7"></a>    tf.keras.layers.Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb16-8"><a href="#cb16-8"></a>])</span>
<span id="cb16-9"><a href="#cb16-9"></a>stacked_decoder <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb16-10"><a href="#cb16-10"></a>    tf.keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb16-11"><a href="#cb16-11"></a>    tf.keras.layers.Dense(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>),</span>
<span id="cb16-12"><a href="#cb16-12"></a>    tf.keras.layers.Reshape([<span class="dv">28</span>, <span class="dv">28</span>])</span>
<span id="cb16-13"><a href="#cb16-13"></a>])</span>
<span id="cb16-14"><a href="#cb16-14"></a>stacked_ae <span class="op">=</span> tf.keras.Sequential([stacked_encoder, stacked_decoder])</span>
<span id="cb16-15"><a href="#cb16-15"></a></span>
<span id="cb16-16"><a href="#cb16-16"></a>stacked_ae.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"mse"</span>, optimizer<span class="op">=</span><span class="st">"nadam"</span>)                   </span>
<span id="cb16-17"><a href="#cb16-17"></a>history <span class="op">=</span> stacked_ae.fit(X_train, X_train, epochs<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb16-18"><a href="#cb16-18"></a>                         validation_data<span class="op">=</span>(X_valid, X_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
1719/1719 [==============================] - 22s 9ms/step - loss: 0.0241 - val_loss: 0.0187
Epoch 2/20
1719/1719 [==============================] - 15s 9ms/step - loss: 0.0173 - val_loss: 0.0166
Epoch 3/20
1719/1719 [==============================] - 14s 8ms/step - loss: 0.0161 - val_loss: 0.0159
Epoch 4/20
1719/1719 [==============================] - 14s 8ms/step - loss: 0.0156 - val_loss: 0.0155
Epoch 5/20
1719/1719 [==============================] - 16s 9ms/step - loss: 0.0152 - val_loss: 0.0152
Epoch 6/20
1719/1719 [==============================] - 15s 9ms/step - loss: 0.0149 - val_loss: 0.0148
Epoch 7/20
1719/1719 [==============================] - 8s 5ms/step - loss: 0.0146 - val_loss: 0.0147
Epoch 8/20
1719/1719 [==============================] - 13s 8ms/step - loss: 0.0144 - val_loss: 0.0146
Epoch 9/20
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0143 - val_loss: 0.0144
Epoch 10/20
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0142 - val_loss: 0.0143
Epoch 11/20
1719/1719 [==============================] - 10s 6ms/step - loss: 0.0141 - val_loss: 0.0142
Epoch 12/20
1719/1719 [==============================] - 8s 5ms/step - loss: 0.0140 - val_loss: 0.0142
Epoch 13/20
1719/1719 [==============================] - 10s 6ms/step - loss: 0.0139 - val_loss: 0.0141
Epoch 14/20
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0139 - val_loss: 0.0140
Epoch 15/20
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0138 - val_loss: 0.0139
Epoch 16/20
1719/1719 [==============================] - 10s 6ms/step - loss: 0.0138 - val_loss: 0.0141
Epoch 17/20
1719/1719 [==============================] - 11s 6ms/step - loss: 0.0137 - val_loss: 0.0139
Epoch 18/20
1719/1719 [==============================] - 17s 10ms/step - loss: 0.0137 - val_loss: 0.0139
Epoch 19/20
1719/1719 [==============================] - 12s 7ms/step - loss: 0.0136 - val_loss: 0.0138
Epoch 20/20
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0136 - val_loss: 0.0138</code></pre>
</div>
</div>
</section>
<section id="visualizing-the-reconstructions" class="level4" data-number="12.2.2.2">
<h4 data-number="12.2.2.2" class="anchored" data-anchor-id="visualizing-the-reconstructions"><span class="header-section-number">12.2.2.2</span> Visualizing the Reconstructions</h4>
<p>One way to ensure that an autoencoder is properly trained is to compare the inputs and the outputs: the differences should not be too significant. Let’s plot a few images from the validation set, as well as their reconstructions:</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:31:02.985114Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:31:02.984428Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:31:03.713545Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:31:03.712694Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:31:02.985075Z&quot;}" data-executioninfo="{&quot;elapsed&quot;:1071,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684643593135,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="69f97611-76e2-4c18-b3be-3c0ed3f42402" data-trusted="true">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>plot_reconstructions(stacked_ae, X_valid)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 134ms/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-15-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>The reconstructions are recognizable, but a bit too lossy. We may need to train the model for longer, or make the encoder and decoder deeper, or make the codings larger. But if we make the network too powerful, it will manage to make perfect reconstructions without having learned any useful patterns in the data. For now, let’s go with this model.</p>
</section>
<section id="visualizing-fashion-mnist" class="level4" data-number="12.2.2.3">
<h4 data-number="12.2.2.3" class="anchored" data-anchor-id="visualizing-fashion-mnist"><span class="header-section-number">12.2.2.3</span> Visualizing Fashion MNIST</h4>
<p>Now that we have trained a stacked autoencoder, we can use it to reduce the dataset’s dimensionality. For visualization, this does not give great results compared to other dimensionality reduction algorithms, but one big advantage of autoencoders is that they can handle large datasets, with many instances and many features. <strong>So one strategy is to use an autoencoder to reduce the dimensionality down to a reasonable level, then use another dimensionality reduction algorithm for visualization</strong>. Let’s use this strategy to visualize Fashion MNIST. First, we use the encoder from our stacked autoencoder to reduce the dimensionality down to 30, then we use Scikit-Learn’s implementation of the t-SNE algorithm to reduce the dimensionality down to 2 for visualization:</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:31:22.005037Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:31:22.004487Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:31:56.860700Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:31:56.859050Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:31:22.005000Z&quot;}" data-executioninfo="{&quot;elapsed&quot;:78979,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684643672108,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="d2691296-f2e9-4330-c6c9-dc02df0e3ad2" data-trusted="true">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>Z <span class="op">=</span> stacked_encoder.predict(X_valid)</span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="bu">print</span>(Z.shape)</span>
<span id="cb20-3"><a href="#cb20-3"></a>plot_embeddings_tsne(X_valid, y_valid, Z)</span>
<span id="cb20-4"><a href="#cb20-4"></a>plt.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>157/157 [==============================] - 0s 2ms/step
(5000, 30)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-16-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>The t-SNE algorithm identified several clusters which match the classes reasonably well (each class is represented with a different color).</p>
</section>
<section id="using-convolutional-layers-instead-of-dense-layers" class="level4" data-number="12.2.2.4">
<h4 data-number="12.2.2.4" class="anchored" data-anchor-id="using-convolutional-layers-instead-of-dense-layers"><span class="header-section-number">12.2.2.4</span> Using Convolutional Layers Instead of Dense Layers</h4>
<p>If you are dealing with images, then the autoencoders we have seen so far will not work well (unless the images are very small): convolutional neural networks are far better suited than dense networks to work with images. So if you want to build an autoencoder for images, you will need to build a convolutional autoencoder. <strong>The encoder is a regular CNN composed of convolutional layers and pooling layers.</strong> It typically reduces the spatial dimensionality of the inputs (i.e., height and width) while increasing the depth (i.e., the number of feature maps). <strong>The decoder must do the reverse (upscale the image and reduce its depth back to the original dimensions), and for this you can use transpose convolutional layers</strong> (alternatively, you could combine upsampling layers with convolutional layers).</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:31:56.862423Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:31:56.862112Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:32:41.685035Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:32:41.684352Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:31:56.862385Z&quot;}" data-executioninfo="{&quot;elapsed&quot;:203443,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684643875549,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="da393e5a-972e-46da-bfa9-52adb2cbeed1" data-trusted="true">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>tf.keras.backend.clear_session()</span>
<span id="cb22-2"><a href="#cb22-2"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility on CPU</span></span>
<span id="cb22-3"><a href="#cb22-3"></a></span>
<span id="cb22-4"><a href="#cb22-4"></a>conv_encoder <span class="op">=</span> tf.keras.models.Sequential([</span>
<span id="cb22-5"><a href="#cb22-5"></a>    tf.keras.layers.Reshape([<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>], input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb22-6"><a href="#cb22-6"></a>    tf.keras.layers.Conv2D(<span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">"SAME"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb22-7"><a href="#cb22-7"></a>    tf.keras.layers.MaxPool2D(pool_size<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb22-8"><a href="#cb22-8"></a>    tf.keras.layers.Conv2D(<span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">"SAME"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb22-9"><a href="#cb22-9"></a>    tf.keras.layers.MaxPool2D(pool_size<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb22-10"><a href="#cb22-10"></a>    tf.keras.layers.Conv2D(<span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">"SAME"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb22-11"><a href="#cb22-11"></a>    tf.keras.layers.MaxPool2D(pool_size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb22-12"><a href="#cb22-12"></a>])</span>
<span id="cb22-13"><a href="#cb22-13"></a>conv_decoder <span class="op">=</span> tf.keras.models.Sequential([</span>
<span id="cb22-14"><a href="#cb22-14"></a>    tf.keras.layers.Conv2DTranspose(<span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, strides<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="st">"VALID"</span>, activation<span class="op">=</span><span class="st">"relu"</span>,</span>
<span id="cb22-15"><a href="#cb22-15"></a>                                 input_shape<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">64</span>]),</span>
<span id="cb22-16"><a href="#cb22-16"></a>    tf.keras.layers.Conv2DTranspose(<span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, strides<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="st">"SAME"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb22-17"><a href="#cb22-17"></a>    tf.keras.layers.Conv2DTranspose(<span class="dv">1</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, strides<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="st">"SAME"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb22-18"><a href="#cb22-18"></a>    tf.keras.layers.Reshape([<span class="dv">28</span>, <span class="dv">28</span>])</span>
<span id="cb22-19"><a href="#cb22-19"></a>])</span>
<span id="cb22-20"><a href="#cb22-20"></a></span>
<span id="cb22-21"><a href="#cb22-21"></a>conv_ae <span class="op">=</span> tf.keras.Sequential([conv_encoder, conv_decoder])</span>
<span id="cb22-22"><a href="#cb22-22"></a></span>
<span id="cb22-23"><a href="#cb22-23"></a>conv_ae.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"mse"</span>, optimizer<span class="op">=</span><span class="st">"nadam"</span>)</span>
<span id="cb22-24"><a href="#cb22-24"></a>history <span class="op">=</span> conv_ae.fit(X_train, X_train, epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb22-25"><a href="#cb22-25"></a>                      validation_data<span class="op">=</span>(X_valid, X_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
1719/1719 [==============================] - 24s 8ms/step - loss: 0.0179 - val_loss: 0.0117
Epoch 2/10
1719/1719 [==============================] - 13s 8ms/step - loss: 0.0102 - val_loss: 0.0095
Epoch 3/10
1719/1719 [==============================] - 13s 8ms/step - loss: 0.0088 - val_loss: 0.0081
Epoch 4/10
1719/1719 [==============================] - 14s 8ms/step - loss: 0.0079 - val_loss: 0.0076
Epoch 5/10
1719/1719 [==============================] - 14s 8ms/step - loss: 0.0073 - val_loss: 0.0069
Epoch 6/10
1719/1719 [==============================] - 18s 10ms/step - loss: 0.0068 - val_loss: 0.0065
Epoch 7/10
1719/1719 [==============================] - 13s 8ms/step - loss: 0.0065 - val_loss: 0.0062
Epoch 8/10
1719/1719 [==============================] - 14s 8ms/step - loss: 0.0062 - val_loss: 0.0069
Epoch 9/10
1719/1719 [==============================] - 13s 8ms/step - loss: 0.0060 - val_loss: 0.0060
Epoch 10/10
1719/1719 [==============================] - 13s 8ms/step - loss: 0.0058 - val_loss: 0.0058</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:32:41.686847Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:32:41.686595Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:32:42.206027Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:32:42.205361Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:32:41.686812Z&quot;}" data-executioninfo="{&quot;elapsed&quot;:1639,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684644053615,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="31566a58-1513-4a47-c513-684bc93c3c52" data-trusted="true">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a>plot_reconstructions(conv_ae, X_valid)</span>
<span id="cb24-2"><a href="#cb24-2"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 370ms/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-18-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T06:32:42.208652Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T06:32:42.207921Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T06:33:18.535740Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T06:33:18.535073Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T06:32:42.208605Z&quot;}" data-executioninfo="{&quot;elapsed&quot;:72399,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684644126712,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="85ccdf9d-ad31-4019-f755-4b58876a07e5" data-trusted="true">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a>Z <span class="op">=</span> conv_encoder.predict(X_valid)</span>
<span id="cb26-2"><a href="#cb26-2"></a>N <span class="op">=</span> Z.shape[<span class="dv">0</span>]</span>
<span id="cb26-3"><a href="#cb26-3"></a>ZZ <span class="op">=</span> np.reshape(Z, (N,<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb26-4"><a href="#cb26-4"></a>plot_embeddings_tsne(X_valid, y_valid, ZZ)</span>
<span id="cb26-5"><a href="#cb26-5"></a>plt.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>157/157 [==============================] - 1s 3ms/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-19-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="stacked-denoising-autoencoder" class="level3" data-number="12.2.3">
<h3 data-number="12.2.3" class="anchored" data-anchor-id="stacked-denoising-autoencoder"><span class="header-section-number">12.2.3</span> Stacked denoising Autoencoder</h3>
<p>Another way to force the autoencoder to learn useful features is to add noise to its inputs, training it to recover the original, noise-free inputs. The noise can be pure Gaussian noise added to the inputs, or it can be randomly switched-off inputs, just like in dropout (Bernoulli)</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:143009,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684644269718,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="47022beb-7bad-4e74-f682-0eabf51ff3a8">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>tf.keras.backend.clear_session()</span>
<span id="cb28-2"><a href="#cb28-2"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility on CPU</span></span>
<span id="cb28-3"><a href="#cb28-3"></a></span>
<span id="cb28-4"><a href="#cb28-4"></a><span class="co"># If you want, you can try replacing the Dropout layer with tf.keras.layers.GaussianNoise(0.2)</span></span>
<span id="cb28-5"><a href="#cb28-5"></a>dropout_encoder <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb28-6"><a href="#cb28-6"></a>    tf.keras.layers.Flatten(),</span>
<span id="cb28-7"><a href="#cb28-7"></a>    tf.keras.layers.Dropout(<span class="fl">0.5</span>),</span>
<span id="cb28-8"><a href="#cb28-8"></a>    tf.keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb28-9"><a href="#cb28-9"></a>    tf.keras.layers.Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"relu"</span>)</span>
<span id="cb28-10"><a href="#cb28-10"></a>])</span>
<span id="cb28-11"><a href="#cb28-11"></a>dropout_decoder <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb28-12"><a href="#cb28-12"></a>    tf.keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb28-13"><a href="#cb28-13"></a>    tf.keras.layers.Dense(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>),</span>
<span id="cb28-14"><a href="#cb28-14"></a>    tf.keras.layers.Reshape([<span class="dv">28</span>, <span class="dv">28</span>])</span>
<span id="cb28-15"><a href="#cb28-15"></a>])</span>
<span id="cb28-16"><a href="#cb28-16"></a>dropout_ae <span class="op">=</span> tf.keras.Sequential([dropout_encoder, dropout_decoder])</span>
<span id="cb28-17"><a href="#cb28-17"></a></span>
<span id="cb28-18"><a href="#cb28-18"></a><span class="co"># extra code – compiles and fits the model</span></span>
<span id="cb28-19"><a href="#cb28-19"></a>dropout_ae.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"mse"</span>, optimizer<span class="op">=</span><span class="st">"nadam"</span>)</span>
<span id="cb28-20"><a href="#cb28-20"></a>history <span class="op">=</span> dropout_ae.fit(X_train, X_train, epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb28-21"><a href="#cb28-21"></a>                         validation_data<span class="op">=</span>(X_valid, X_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
1719/1719 [==============================] - 12s 6ms/step - loss: 0.0287 - val_loss: 0.0214
Epoch 2/10
1719/1719 [==============================] - 8s 5ms/step - loss: 0.0220 - val_loss: 0.0196
Epoch 3/10
1719/1719 [==============================] - 10s 6ms/step - loss: 0.0208 - val_loss: 0.0187
Epoch 4/10
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0201 - val_loss: 0.0183
Epoch 5/10
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0196 - val_loss: 0.0179
Epoch 6/10
1719/1719 [==============================] - 10s 6ms/step - loss: 0.0194 - val_loss: 0.0178
Epoch 7/10
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0191 - val_loss: 0.0176
Epoch 8/10
1719/1719 [==============================] - 10s 6ms/step - loss: 0.0190 - val_loss: 0.0174
Epoch 9/10
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0188 - val_loss: 0.0173
Epoch 10/10
1719/1719 [==============================] - 10s 6ms/step - loss: 0.0187 - val_loss: 0.0170</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:5,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684644270675,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="4c7e2ff0-e172-4632-a33a-d7465a2165e0">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a>dropout <span class="op">=</span> tf.keras.layers.Dropout(<span class="fl">0.5</span>)</span>
<span id="cb30-2"><a href="#cb30-2"></a>plot_reconstructions(dropout_ae, dropout(X_valid, training<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb30-3"><a href="#cb30-3"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 25ms/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-21-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="sparse-autoencoder" class="level3" data-number="12.2.4">
<h3 data-number="12.2.4" class="anchored" data-anchor-id="sparse-autoencoder"><span class="header-section-number">12.2.4</span> Sparse Autoencoder</h3>
<p>Another kind of constraint that often leads to good feature extraction is sparsity: by adding an appropriate term to the cost function, the autoencoder is pushed to reduce the number of active neurons in the coding layer. For example, it may be pushed to have on average only 5% significantly active neurons in the coding layer. This forces the autoencoder to represent each input as a combination of a small number of activations. As a result, each neuron in the coding layer typically ends up representing a useful feature.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:91284,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684644361956,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="97eb4f86-cb95-4382-a7b5-450449375527">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a>tf.keras.backend.clear_session()</span>
<span id="cb32-2"><a href="#cb32-2"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility on CPU</span></span>
<span id="cb32-3"><a href="#cb32-3"></a><span class="co"># Normal autoencoder</span></span>
<span id="cb32-4"><a href="#cb32-4"></a></span>
<span id="cb32-5"><a href="#cb32-5"></a>stacked_encoder <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb32-6"><a href="#cb32-6"></a>    tf.keras.layers.Flatten(),</span>
<span id="cb32-7"><a href="#cb32-7"></a>    tf.keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb32-8"><a href="#cb32-8"></a>    tf.keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb32-9"><a href="#cb32-9"></a>])</span>
<span id="cb32-10"><a href="#cb32-10"></a>stacked_decoder <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb32-11"><a href="#cb32-11"></a>    tf.keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb32-12"><a href="#cb32-12"></a>    tf.keras.layers.Dense(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>),</span>
<span id="cb32-13"><a href="#cb32-13"></a>    tf.keras.layers.Reshape([<span class="dv">28</span>, <span class="dv">28</span>])</span>
<span id="cb32-14"><a href="#cb32-14"></a>])</span>
<span id="cb32-15"><a href="#cb32-15"></a>stacked_ae <span class="op">=</span> tf.keras.Sequential([stacked_encoder, stacked_decoder])</span>
<span id="cb32-16"><a href="#cb32-16"></a></span>
<span id="cb32-17"><a href="#cb32-17"></a>stacked_ae.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"mse"</span>, optimizer<span class="op">=</span><span class="st">"nadam"</span>)                   </span>
<span id="cb32-18"><a href="#cb32-18"></a>history <span class="op">=</span> stacked_ae.fit(X_train, X_train, epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb32-19"><a href="#cb32-19"></a>                         validation_data<span class="op">=</span>(X_valid, X_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
1719/1719 [==============================] - 12s 6ms/step - loss: 0.0207 - val_loss: 0.0142
Epoch 2/10
1719/1719 [==============================] - 8s 5ms/step - loss: 0.0128 - val_loss: 0.0121
Epoch 3/10
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0114 - val_loss: 0.0117
Epoch 4/10
1719/1719 [==============================] - 8s 5ms/step - loss: 0.0107 - val_loss: 0.0109
Epoch 5/10
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0104 - val_loss: 0.0101
Epoch 6/10
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0103 - val_loss: 0.0102
Epoch 7/10
1719/1719 [==============================] - 8s 5ms/step - loss: 0.0103 - val_loss: 0.0102
Epoch 8/10
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0102 - val_loss: 0.0104
Epoch 9/10
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0102 - val_loss: 0.0102
Epoch 10/10
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0101 - val_loss: 0.0104</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:4254,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684644464684,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="76e13a0f-26f6-4bfb-8c23-8e1eb2fda771">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a>plot_reconstructions(stacked_ae, X_valid)</span>
<span id="cb34-2"><a href="#cb34-2"></a>plot_activations_heatmap(stacked_encoder)</span>
<span id="cb34-3"><a href="#cb34-3"></a>plot_activations_histogram2(stacked_encoder, height<span class="op">=</span><span class="fl">0.35</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 287ms/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-23-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-23-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-23-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-23-output-5.png" class="img-fluid"></p>
</div>
</div>
<p>Three figures show neuron activity (in the bottleneck layer) for an autoencoder applied to Fashion MNIST. Heatmap of 300 neuron activations (columns) across 100 examples (rows). Histogram of activation levels derived from this heatmap. Histogram of the mean activation per neuron, averaged over all examples in the validation set. You can see that some neurons fire almost all the time (right side of the histogram).</p>
<p>A simple approach is to use the sigmoid activation function in the coding layer (to constrain the codings to values between 0 and 1), use a large coding layer (e.g., with 300 units), and add some L1 regularization to the coding layer’s activations.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:104787,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684644581862,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="b803b851-05a6-4ff5-c801-9fda6cb601f5">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>tf.keras.backend.clear_session()</span>
<span id="cb36-2"><a href="#cb36-2"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility on CPU</span></span>
<span id="cb36-3"><a href="#cb36-3"></a></span>
<span id="cb36-4"><a href="#cb36-4"></a>sparse_l1_encoder <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb36-5"><a href="#cb36-5"></a>    tf.keras.layers.Flatten(),</span>
<span id="cb36-6"><a href="#cb36-6"></a>    tf.keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb36-7"><a href="#cb36-7"></a>    tf.keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>),</span>
<span id="cb36-8"><a href="#cb36-8"></a>    tf.keras.layers.ActivityRegularization(l1<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb36-9"><a href="#cb36-9"></a>])</span>
<span id="cb36-10"><a href="#cb36-10"></a>sparse_l1_decoder <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb36-11"><a href="#cb36-11"></a>    tf.keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb36-12"><a href="#cb36-12"></a>    tf.keras.layers.Dense(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>),</span>
<span id="cb36-13"><a href="#cb36-13"></a>    tf.keras.layers.Reshape([<span class="dv">28</span>, <span class="dv">28</span>])</span>
<span id="cb36-14"><a href="#cb36-14"></a>])</span>
<span id="cb36-15"><a href="#cb36-15"></a>sparse_l1_ae <span class="op">=</span> tf.keras.Sequential([sparse_l1_encoder, sparse_l1_decoder])</span>
<span id="cb36-16"><a href="#cb36-16"></a></span>
<span id="cb36-17"><a href="#cb36-17"></a><span class="co"># extra code – compiles and fits the model</span></span>
<span id="cb36-18"><a href="#cb36-18"></a>sparse_l1_ae.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"mse"</span>, optimizer<span class="op">=</span><span class="st">"nadam"</span>)</span>
<span id="cb36-19"><a href="#cb36-19"></a>history <span class="op">=</span> sparse_l1_ae.fit(X_train, X_train, epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb36-20"><a href="#cb36-20"></a>                           validation_data<span class="op">=</span>(X_valid, X_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
1719/1719 [==============================] - 17s 8ms/step - loss: 0.0275 - val_loss: 0.0191
Epoch 2/10
1719/1719 [==============================] - 12s 7ms/step - loss: 0.0170 - val_loss: 0.0160
Epoch 3/10
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0148 - val_loss: 0.0146
Epoch 4/10
1719/1719 [==============================] - 10s 6ms/step - loss: 0.0137 - val_loss: 0.0133
Epoch 5/10
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0129 - val_loss: 0.0128
Epoch 6/10
1719/1719 [==============================] - 10s 6ms/step - loss: 0.0124 - val_loss: 0.0122
Epoch 7/10
1719/1719 [==============================] - 10s 6ms/step - loss: 0.0119 - val_loss: 0.0118
Epoch 8/10
1719/1719 [==============================] - 8s 5ms/step - loss: 0.0116 - val_loss: 0.0115
Epoch 9/10
1719/1719 [==============================] - 10s 6ms/step - loss: 0.0113 - val_loss: 0.0113
Epoch 10/10
1719/1719 [==============================] - 9s 5ms/step - loss: 0.0111 - val_loss: 0.0110</code></pre>
</div>
</div>
<p>This <code>ActivityRegularization</code> layer just returns its inputs, but as a side effect it adds a training loss equal to the sum of the absolute values of its inputs. This only affects training. Equivalently, you could remove the <code>ActivityRegularization</code> layer and set <code>activity_regularizer=tf.keras.regularizers.l1(1e-4)</code> in the previous layer. This penalty will encourage the neural network to produce codings close to 0, but since it will also be penalized if it does not reconstruct the inputs correctly, it will have to output at least a few nonzero values. Using the L1 norm rather than the L2 norm will push the neural network to preserve the most important codings while eliminating the ones that are not needed for the input image (rather than just reducing all codings).</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:3806,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684644603474,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="f42b6554-c7ea-4325-ff77-b8fa1f208b77">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>plot_reconstructions(sparse_l1_ae, X_valid)</span>
<span id="cb38-2"><a href="#cb38-2"></a>plot_activations_heatmap(sparse_l1_encoder)</span>
<span id="cb38-3"><a href="#cb38-3"></a>plot_activations_histogram2(sparse_l1_encoder)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 364ms/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-25-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-25-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-25-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-25-output-5.png" class="img-fluid"></p>
</div>
</div>
<p>Another approach, which often yields better results, is to measure the actual sparsity of the coding layer at each training iteration, and penalize the model when the measured sparsity differs from a target sparsity. We do so by computing the average activation of each neuron in the coding layer, over the whole training batch. Once we have the mean activation per neuron, we want to penalize the neurons that are too active, or not active enough, by adding a sparsity loss to the cost function.</p>
<p>Once we have computed the sparsity loss for each neuron in the coding layer, we sum up these losses and add the result to the cost function. In order to control the relative importance of the sparsity loss and the reconstruction loss, we can multiply the sparsity loss by a sparsity weight hyperparameter. If this weight is too high, the model will stick closely to the target sparsity, but it may not reconstruct the inputs properly, making the model useless. Conversely, if it is too low, the model will mostly ignore the sparsity objective and will not learn any interesting features.</p>
<p>We now have all we need to implement a sparse autoencoder based on the KL divergence. First, let’s create a custom regularizer to apply KL divergence regularization:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a>kl_divergence <span class="op">=</span> tf.keras.losses.kullback_leibler_divergence</span>
<span id="cb40-2"><a href="#cb40-2"></a></span>
<span id="cb40-3"><a href="#cb40-3"></a><span class="kw">class</span> KLDivergenceRegularizer(tf.keras.regularizers.Regularizer):</span>
<span id="cb40-4"><a href="#cb40-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, weight, target):</span>
<span id="cb40-5"><a href="#cb40-5"></a>        <span class="va">self</span>.weight <span class="op">=</span> weight</span>
<span id="cb40-6"><a href="#cb40-6"></a>        <span class="va">self</span>.target <span class="op">=</span> target</span>
<span id="cb40-7"><a href="#cb40-7"></a></span>
<span id="cb40-8"><a href="#cb40-8"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, inputs):</span>
<span id="cb40-9"><a href="#cb40-9"></a>        mean_activities <span class="op">=</span> tf.reduce_mean(inputs, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb40-10"><a href="#cb40-10"></a>        <span class="cf">return</span> <span class="va">self</span>.weight <span class="op">*</span> (</span>
<span id="cb40-11"><a href="#cb40-11"></a>            kl_divergence(<span class="va">self</span>.target, mean_activities) <span class="op">+</span></span>
<span id="cb40-12"><a href="#cb40-12"></a>            kl_divergence(<span class="fl">1.</span> <span class="op">-</span> <span class="va">self</span>.target, <span class="fl">1.</span> <span class="op">-</span> mean_activities))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s use this regularizer to push the model to have about 10% sparsity in the coding layer:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:123093,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684644738197,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="53349f94-93c4-448b-865b-12b2ca33b3f1">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a>tf.keras.backend.clear_session()</span>
<span id="cb41-2"><a href="#cb41-2"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility on CPU</span></span>
<span id="cb41-3"><a href="#cb41-3"></a></span>
<span id="cb41-4"><a href="#cb41-4"></a>kld_reg <span class="op">=</span> KLDivergenceRegularizer(weight<span class="op">=</span><span class="fl">5e-3</span>, target<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb41-5"><a href="#cb41-5"></a>sparse_kl_encoder <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb41-6"><a href="#cb41-6"></a>    tf.keras.layers.Flatten(),</span>
<span id="cb41-7"><a href="#cb41-7"></a>    tf.keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb41-8"><a href="#cb41-8"></a>    tf.keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>,</span>
<span id="cb41-9"><a href="#cb41-9"></a>                          activity_regularizer<span class="op">=</span>kld_reg)</span>
<span id="cb41-10"><a href="#cb41-10"></a>])</span>
<span id="cb41-11"><a href="#cb41-11"></a>sparse_kl_decoder <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb41-12"><a href="#cb41-12"></a>    tf.keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb41-13"><a href="#cb41-13"></a>    tf.keras.layers.Dense(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>),</span>
<span id="cb41-14"><a href="#cb41-14"></a>    tf.keras.layers.Reshape([<span class="dv">28</span>, <span class="dv">28</span>])</span>
<span id="cb41-15"><a href="#cb41-15"></a>])</span>
<span id="cb41-16"><a href="#cb41-16"></a>sparse_kl_ae <span class="op">=</span> tf.keras.Sequential([sparse_kl_encoder, sparse_kl_decoder])</span>
<span id="cb41-17"><a href="#cb41-17"></a></span>
<span id="cb41-18"><a href="#cb41-18"></a><span class="co"># extra code – compiles and fits the model</span></span>
<span id="cb41-19"><a href="#cb41-19"></a>sparse_kl_ae.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"mse"</span>, optimizer<span class="op">=</span><span class="st">"nadam"</span>)</span>
<span id="cb41-20"><a href="#cb41-20"></a>history <span class="op">=</span> sparse_kl_ae.fit(X_train, X_train, epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb41-21"><a href="#cb41-21"></a>                           validation_data<span class="op">=</span>(X_valid, X_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
1719/1719 [==============================] - 20s 10ms/step - loss: 0.0258 - val_loss: 0.0172
Epoch 2/10
1719/1719 [==============================] - 14s 8ms/step - loss: 0.0149 - val_loss: 0.0134
Epoch 3/10
1719/1719 [==============================] - 12s 7ms/step - loss: 0.0125 - val_loss: 0.0143
Epoch 4/10
1719/1719 [==============================] - 11s 7ms/step - loss: 0.0112 - val_loss: 0.0108
Epoch 5/10
1719/1719 [==============================] - 10s 6ms/step - loss: 0.0105 - val_loss: 0.0102
Epoch 6/10
1719/1719 [==============================] - 11s 6ms/step - loss: 0.0100 - val_loss: 0.0098
Epoch 7/10
1719/1719 [==============================] - 11s 7ms/step - loss: 0.0097 - val_loss: 0.0096
Epoch 8/10
1719/1719 [==============================] - 10s 6ms/step - loss: 0.0095 - val_loss: 0.0099
Epoch 9/10
1719/1719 [==============================] - 11s 7ms/step - loss: 0.0094 - val_loss: 0.0094
Epoch 10/10
1719/1719 [==============================] - 11s 6ms/step - loss: 0.0092 - val_loss: 0.0092</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2284,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684644740476,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="5d467b98-b45a-41f0-a6a3-72ddf7a2a24b">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a>plot_reconstructions(sparse_kl_ae, X_valid)</span>
<span id="cb43-2"><a href="#cb43-2"></a>plot_activations_heatmap(sparse_kl_encoder)</span>
<span id="cb43-3"><a href="#cb43-3"></a>plot_activations_histogram2(sparse_kl_encoder)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:tensorflow:5 out of the last 162 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7f618ee73640&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 110ms/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-28-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-28-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-28-output-5.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-28-output-6.png" class="img-fluid"></p>
</div>
</div>
<p>After training this sparse autoencoder on Fashion MNIST, the activations of the neurons in the coding layer are mostly close to 0 (about 70% of all activations are lower than 0.1), and all neurons have a mean activation around 0.1 (about 80% of all neurons have a mean activation between 0.1 and 0.2)</p>
</section>
</section>
<section id="variational-autoencoder" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="variational-autoencoder"><span class="header-section-number">12.3</span> Variational Autoencoder</h2>
<p>We’re going to be implementing a VAE that can generate MNIST digits. It’s going to have three parts:</p>
<ul>
<li>An encoder network that turns a real image into a mean and a variance in the latent space</li>
<li>A sampling layer that takes such a mean and variance, and uses them to sample a random point from the latent space</li>
<li>A decoder network that turns points from the latent space back into images</li>
</ul>
<section id="latent-space-sampling-layer" class="level3" data-number="12.3.1">
<h3 data-number="12.3.1" class="anchored" data-anchor-id="latent-space-sampling-layer"><span class="header-section-number">12.3.1</span> Latent-space-sampling layer</h3>
<p>First, we will need a custom layer to sample the codings:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a><span class="kw">class</span> Sampling(tf.keras.layers.Layer):</span>
<span id="cb46-2"><a href="#cb46-2"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb46-3"><a href="#cb46-3"></a>        mean, log_var <span class="op">=</span> inputs</span>
<span id="cb46-4"><a href="#cb46-4"></a>        <span class="cf">return</span> tf.random.normal(tf.shape(log_var)) <span class="op">*</span> tf.exp(log_var <span class="op">/</span> <span class="dv">2</span>) <span class="op">+</span> mean </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="vae-implementation" class="level3" data-number="12.3.2">
<h3 data-number="12.3.2" class="anchored" data-anchor-id="vae-implementation"><span class="header-section-number">12.3.2</span> VAE implementation</h3>
<section id="vae-encoder" class="level4" data-number="12.3.2.1">
<h4 data-number="12.3.2.1" class="anchored" data-anchor-id="vae-encoder"><span class="header-section-number">12.3.2.1</span> VAE encoder</h4>
<p>Next, we can create the encoder, using the functional API because the model is not entirely sequential:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:447,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684645105151,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="724bc698-423c-40be-c8fb-63e682da84aa">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a>tf.keras.backend.clear_session()</span>
<span id="cb47-2"><a href="#cb47-2"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility on CPU</span></span>
<span id="cb47-3"><a href="#cb47-3"></a></span>
<span id="cb47-4"><a href="#cb47-4"></a>codings_size <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb47-5"><a href="#cb47-5"></a></span>
<span id="cb47-6"><a href="#cb47-6"></a>inputs <span class="op">=</span> tf.keras.layers.Input(shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>])</span>
<span id="cb47-7"><a href="#cb47-7"></a>Z <span class="op">=</span> tf.keras.layers.Flatten()(inputs)</span>
<span id="cb47-8"><a href="#cb47-8"></a>Z <span class="op">=</span> tf.keras.layers.Dense(<span class="dv">150</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(Z)</span>
<span id="cb47-9"><a href="#cb47-9"></a>Z <span class="op">=</span> tf.keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(Z)</span>
<span id="cb47-10"><a href="#cb47-10"></a>codings_mean <span class="op">=</span> tf.keras.layers.Dense(codings_size)(Z)  <span class="co"># μ</span></span>
<span id="cb47-11"><a href="#cb47-11"></a>codings_log_var <span class="op">=</span> tf.keras.layers.Dense(codings_size)(Z)  <span class="co"># γ</span></span>
<span id="cb47-12"><a href="#cb47-12"></a>codings <span class="op">=</span> Sampling()([codings_mean, codings_log_var])</span>
<span id="cb47-13"><a href="#cb47-13"></a>variational_encoder <span class="op">=</span> tf.keras.Model(</span>
<span id="cb47-14"><a href="#cb47-14"></a>    inputs<span class="op">=</span>[inputs], outputs<span class="op">=</span>[codings_mean, codings_log_var, codings])</span>
<span id="cb47-15"><a href="#cb47-15"></a></span>
<span id="cb47-16"><a href="#cb47-16"></a>variational_encoder.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 28, 28)]     0           []                               
                                                                                                  
 flatten (Flatten)              (None, 784)          0           ['input_1[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 150)          117750      ['flatten[0][0]']                
                                                                                                  
 dense_1 (Dense)                (None, 100)          15100       ['dense[0][0]']                  
                                                                                                  
 dense_2 (Dense)                (None, 10)           1010        ['dense_1[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 10)           1010        ['dense_1[0][0]']                
                                                                                                  
 sampling (Sampling)            (None, 10)           0           ['dense_2[0][0]',                
                                                                  'dense_3[0][0]']                
                                                                                                  
==================================================================================================
Total params: 134,870
Trainable params: 134,870
Non-trainable params: 0
__________________________________________________________________________________________________</code></pre>
</div>
</div>
<p>Note that the <code>Dense</code> layers that output <code>codings_mean</code> (<span class="math inline">\(\mu\)</span>) and <code>codings_log_var</code> (<span class="math inline">\(\gamma\)</span>) have the same inputs (i.e., the outputs of the second <code>Dense</code> layer). We then pass both <code>codings_mean</code> and <code>codings_log_var</code> to the <code>Sampling</code> layer. Finally, the variational_encoder model has three outputs. Only the codings are required, but we add <code>codings_mean</code> and <code>codings_log_var</code> as well, in case we want to inspect their values.</p>
</section>
<section id="vae-decoder" class="level4" data-number="12.3.2.2">
<h4 data-number="12.3.2.2" class="anchored" data-anchor-id="vae-decoder"><span class="header-section-number">12.3.2.2</span> VAE decoder</h4>
<p>Now let’s build the decoder:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:4,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684645106602,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="3c09d583-2da6-4f7a-8a88-4e51ffb69f4b">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1"></a>decoder_inputs <span class="op">=</span> tf.keras.layers.Input(shape<span class="op">=</span>[codings_size])</span>
<span id="cb49-2"><a href="#cb49-2"></a>x <span class="op">=</span> tf.keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(decoder_inputs)</span>
<span id="cb49-3"><a href="#cb49-3"></a>x <span class="op">=</span> tf.keras.layers.Dense(<span class="dv">150</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb49-4"><a href="#cb49-4"></a>x <span class="op">=</span> tf.keras.layers.Dense(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>)(x)</span>
<span id="cb49-5"><a href="#cb49-5"></a>outputs <span class="op">=</span> tf.keras.layers.Reshape([<span class="dv">28</span>, <span class="dv">28</span>])(x)</span>
<span id="cb49-6"><a href="#cb49-6"></a>variational_decoder <span class="op">=</span> tf.keras.Model(inputs<span class="op">=</span>[decoder_inputs], outputs<span class="op">=</span>[outputs])</span>
<span id="cb49-7"><a href="#cb49-7"></a>variational_decoder.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 10)]              0         
                                                                 
 dense_4 (Dense)             (None, 100)               1100      
                                                                 
 dense_5 (Dense)             (None, 150)               15150     
                                                                 
 dense_6 (Dense)             (None, 784)               118384    
                                                                 
 reshape (Reshape)           (None, 28, 28)            0         
                                                                 
=================================================================
Total params: 134,634
Trainable params: 134,634
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<p>For this decoder, we could have used the sequential API instead of the functional API, since it is really just a simple stack of layers, virtually identical to many of the decoders we have built so far.</p>
</section>
<section id="vae-model" class="level4" data-number="12.3.2.3">
<h4 data-number="12.3.2.3" class="anchored" data-anchor-id="vae-model"><span class="header-section-number">12.3.2.3</span> VAE model</h4>
<p>Finally, let’s build the variational autoencoder model:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1"></a>_, _, codings <span class="op">=</span> variational_encoder(inputs)</span>
<span id="cb51-2"><a href="#cb51-2"></a>reconstructions <span class="op">=</span> variational_decoder(codings)</span>
<span id="cb51-3"><a href="#cb51-3"></a>variational_ae <span class="op">=</span> tf.keras.Model(inputs<span class="op">=</span>[inputs], outputs<span class="op">=</span>[reconstructions])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We ignore the first two outputs of the encoder (we only want to feed the codings to the decoder). Lastly, we must add the latent loss and the reconstruction loss:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1"></a>latent_loss <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> tf.reduce_sum( <span class="dv">1</span> <span class="op">+</span> codings_log_var <span class="op">-</span> tf.exp(codings_log_var) <span class="op">-</span> tf.square(codings_mean), axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb52-2"><a href="#cb52-2"></a><span class="co"># Indeed, the variational autoencoder’s reconstruction loss  is  supposed  to  be  </span></span>
<span id="cb52-3"><a href="#cb52-3"></a><span class="co"># the  sum  of  the  pixel  reconstruction  errors,  but  when  Keras</span></span>
<span id="cb52-4"><a href="#cb52-4"></a><span class="co"># computes the  "mse"  loss it computes the mean over all 784 pixels, rather than the</span></span>
<span id="cb52-5"><a href="#cb52-5"></a><span class="co"># sum. So, the reconstruction loss is 784 times smaller than we need it to be.</span></span>
<span id="cb52-6"><a href="#cb52-6"></a>variational_ae.add_loss(tf.reduce_mean(latent_loss) <span class="op">/</span> <span class="fl">784.</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:144700,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684645258478,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="4b1cd7c3-f784-4131-f52b-cca60ae91ce9">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1"></a>variational_ae.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"mse"</span>, optimizer<span class="op">=</span><span class="st">"nadam"</span>)</span>
<span id="cb53-2"><a href="#cb53-2"></a>history <span class="op">=</span> variational_ae.fit(X_train, X_train, epochs<span class="op">=</span><span class="dv">25</span>, batch_size<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb53-3"><a href="#cb53-3"></a>                             validation_data<span class="op">=</span>(X_valid, X_valid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/25
430/430 [==============================] - 7s 8ms/step - loss: 0.0507 - val_loss: 0.0384
Epoch 2/25
430/430 [==============================] - 5s 11ms/step - loss: 0.0365 - val_loss: 0.0357
Epoch 3/25
430/430 [==============================] - 7s 17ms/step - loss: 0.0347 - val_loss: 0.0345
Epoch 4/25
430/430 [==============================] - 4s 9ms/step - loss: 0.0338 - val_loss: 0.0338
Epoch 5/25
430/430 [==============================] - 4s 9ms/step - loss: 0.0332 - val_loss: 0.0333
Epoch 6/25
430/430 [==============================] - 4s 10ms/step - loss: 0.0328 - val_loss: 0.0330
Epoch 7/25
430/430 [==============================] - 4s 8ms/step - loss: 0.0325 - val_loss: 0.0330
Epoch 8/25
430/430 [==============================] - 5s 12ms/step - loss: 0.0323 - val_loss: 0.0327
Epoch 9/25
430/430 [==============================] - 4s 10ms/step - loss: 0.0320 - val_loss: 0.0325
Epoch 10/25
430/430 [==============================] - 4s 9ms/step - loss: 0.0319 - val_loss: 0.0320
Epoch 11/25
430/430 [==============================] - 4s 10ms/step - loss: 0.0317 - val_loss: 0.0320
Epoch 12/25
430/430 [==============================] - 3s 8ms/step - loss: 0.0315 - val_loss: 0.0317
Epoch 13/25
430/430 [==============================] - 4s 10ms/step - loss: 0.0315 - val_loss: 0.0316
Epoch 14/25
430/430 [==============================] - 4s 9ms/step - loss: 0.0313 - val_loss: 0.0315
Epoch 15/25
430/430 [==============================] - 3s 8ms/step - loss: 0.0313 - val_loss: 0.0318
Epoch 16/25
430/430 [==============================] - 3s 8ms/step - loss: 0.0312 - val_loss: 0.0315
Epoch 17/25
430/430 [==============================] - 7s 17ms/step - loss: 0.0312 - val_loss: 0.0314
Epoch 18/25
430/430 [==============================] - 5s 10ms/step - loss: 0.0311 - val_loss: 0.0314
Epoch 19/25
430/430 [==============================] - 5s 12ms/step - loss: 0.0310 - val_loss: 0.0313
Epoch 20/25
430/430 [==============================] - 5s 11ms/step - loss: 0.0310 - val_loss: 0.0313
Epoch 21/25
430/430 [==============================] - 4s 9ms/step - loss: 0.0309 - val_loss: 0.0311
Epoch 22/25
430/430 [==============================] - 3s 8ms/step - loss: 0.0309 - val_loss: 0.0312
Epoch 23/25
430/430 [==============================] - 3s 8ms/step - loss: 0.0308 - val_loss: 0.0312
Epoch 24/25
430/430 [==============================] - 4s 10ms/step - loss: 0.0308 - val_loss: 0.0311
Epoch 25/25
430/430 [==============================] - 4s 9ms/step - loss: 0.0308 - val_loss: 0.0310</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:954,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684645401222,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="29112ece-d582-4527-9314-0b1b87fe166f">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1"></a>plot_reconstructions(variational_ae, X_valid)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 134ms/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-35-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="generating-fashion-mnist-images" class="level3" data-number="12.3.3">
<h3 data-number="12.3.3" class="anchored" data-anchor-id="generating-fashion-mnist-images"><span class="header-section-number">12.3.3</span> Generating Fashion MNIST Images</h3>
<p>Now let’s use this variational autoencoder to generate images that look like fashion items. All we need to do is sample random codings from a Gaussian distribution and decode them:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1831,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684645406364,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="38e75651-abc8-418d-9d1c-70106989fa66">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1"></a>codings <span class="op">=</span> tf.random.normal(shape<span class="op">=</span>[<span class="dv">3</span> <span class="op">*</span> <span class="dv">7</span>, codings_size])</span>
<span id="cb57-2"><a href="#cb57-2"></a>images <span class="op">=</span> variational_decoder(codings).numpy()</span>
<span id="cb57-3"><a href="#cb57-3"></a></span>
<span id="cb57-4"><a href="#cb57-4"></a>plot_multiple_images(images, <span class="dv">7</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-36-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Variational autoencoders make it possible to perform semantic interpolation: instead of interpolating between two images at the pixel level, which would look as if the two images were just overlaid, we can interpolate at the codings level. For example, let’s take a few codings along an arbitrary line in latent space and decode them. We get a sequence of images that gradually go from sweaters to pants:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1"></a>codings <span class="op">=</span> np.zeros([<span class="dv">7</span>, codings_size])</span>
<span id="cb58-2"><a href="#cb58-2"></a>codings[:, <span class="dv">4</span>] <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">0.8</span>, <span class="fl">0.8</span>, <span class="dv">7</span>)  <span class="co"># axis 4 looks best in this case</span></span>
<span id="cb58-3"><a href="#cb58-3"></a>images <span class="op">=</span> variational_decoder(codings).numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:723,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681532170991,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="7ca2adf5-fb80-4d4c-9610-a68db408bd90">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1"></a>plot_multiple_images(images)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-38-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="generative-adversarial-networks" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="generative-adversarial-networks"><span class="header-section-number">12.4</span> Generative Adversarial Networks</h2>
<p>Let’s go ahead and build a simple GAN for Fashion MNIST. First, we need to build the generator and the discriminator. The generator is similar to an autoencoder’s decoder, and the discriminator is a regular binary classifier: it takes an image as input and ends with a <code>Dense</code> layer containing a single unit and using the sigmoid activation function. For the second phase of each training iteration, we also need the full GAN model containing the generator followed by the discriminator:</p>
<section id="simple-dcgan" class="level3" data-number="12.4.1">
<h3 data-number="12.4.1" class="anchored" data-anchor-id="simple-dcgan"><span class="header-section-number">12.4.1</span> Simple DCGAN</h3>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T07:51:11.188028Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T07:51:11.187770Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T07:51:11.762441Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T07:51:11.761637Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T07:51:11.187999Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1"></a>tf.keras.backend.clear_session()</span>
<span id="cb60-2"><a href="#cb60-2"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility on CPU</span></span>
<span id="cb60-3"><a href="#cb60-3"></a></span>
<span id="cb60-4"><a href="#cb60-4"></a>codings_size <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb60-5"><a href="#cb60-5"></a></span>
<span id="cb60-6"><a href="#cb60-6"></a>generator <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb60-7"><a href="#cb60-7"></a>    tf.keras.layers.Dense(<span class="dv">7</span> <span class="op">*</span> <span class="dv">7</span> <span class="op">*</span> <span class="dv">128</span>),</span>
<span id="cb60-8"><a href="#cb60-8"></a>    tf.keras.layers.Reshape([<span class="dv">7</span>, <span class="dv">7</span>, <span class="dv">128</span>]),</span>
<span id="cb60-9"><a href="#cb60-9"></a>    tf.keras.layers.BatchNormalization(),</span>
<span id="cb60-10"><a href="#cb60-10"></a>    tf.keras.layers.Conv2DTranspose(<span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb60-11"><a href="#cb60-11"></a>                                    padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb60-12"><a href="#cb60-12"></a>    tf.keras.layers.BatchNormalization(),</span>
<span id="cb60-13"><a href="#cb60-13"></a>    tf.keras.layers.Conv2DTranspose(<span class="dv">1</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb60-14"><a href="#cb60-14"></a>                                    padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"tanh"</span>),</span>
<span id="cb60-15"><a href="#cb60-15"></a>])</span>
<span id="cb60-16"><a href="#cb60-16"></a>discriminator <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb60-17"><a href="#cb60-17"></a>    tf.keras.layers.Conv2D(<span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, strides<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="st">"same"</span>,</span>
<span id="cb60-18"><a href="#cb60-18"></a>                        activation<span class="op">=</span>tf.keras.layers.LeakyReLU(<span class="fl">0.2</span>)),</span>
<span id="cb60-19"><a href="#cb60-19"></a>    tf.keras.layers.Dropout(<span class="fl">0.4</span>),</span>
<span id="cb60-20"><a href="#cb60-20"></a>    tf.keras.layers.Conv2D(<span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, strides<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="st">"same"</span>,</span>
<span id="cb60-21"><a href="#cb60-21"></a>                        activation<span class="op">=</span>tf.keras.layers.LeakyReLU(<span class="fl">0.2</span>)),</span>
<span id="cb60-22"><a href="#cb60-22"></a>    tf.keras.layers.Dropout(<span class="fl">0.4</span>),</span>
<span id="cb60-23"><a href="#cb60-23"></a>    tf.keras.layers.Flatten(),</span>
<span id="cb60-24"><a href="#cb60-24"></a>    tf.keras.layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>)</span>
<span id="cb60-25"><a href="#cb60-25"></a>])</span>
<span id="cb60-26"><a href="#cb60-26"></a>gan <span class="op">=</span> tf.keras.Sequential([generator, discriminator])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The generator takes codings of size 100, projects them to 6,272 dimensions (7 * 7 * 128), and reshapes the result to get a 7 × 7 × 128 tensor. This tensor is batch normalized and fed to a transposed convolutional layer with a stride of 2, which upsamples it from 7 × 7 to 14 × 14 and reduces its depth from 128 to 64. The result is batch normalized again and fed to another transposed convolutional layer with a stride of 2, which upsamples it from 14 × 14 to 28 × 28 and reduces the depth from 64 to 1. This layer uses the tanh activation function, so the outputs will range from -1 to 1. For this reason, before training the GAN, we need to rescale the training set to that same range. We also need to reshape it to add the channel dimension:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1"></a>X_train_dcgan <span class="op">=</span> X_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>) <span class="op">*</span> <span class="fl">2.</span> <span class="op">-</span> <span class="fl">1.</span> <span class="co"># reshape and rescale</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The discriminator looks much like a regular CNN for binary classification, except instead of using max pooling layers to downsample the image, we use strided convo lutions ( <code>strides=2</code> ). Note that we use the leaky ReLU activation function. Overall, we respected the DCGAN guidelines, except we replaced the <code>BatchNormalization</code> layers in the discriminator with <code>Dropout</code> layers.</p>
<p>Next, we need to compile these models. As the discriminator is a binary classifier, we can naturally use the binary cross-entropy loss. The <code>gan</code> model is also a binary classifier, so it can use the binary cross-entropy loss as well. <strong>However, the generator will only be trained through the gan model, so we do not need to compile it at all.</strong> Importantly, the discriminator should not be trained during the second phase, so we make it non-trainable before compiling the gan model:</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-03-11T07:47:15.538322Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-03-11T07:47:15.537505Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-03-11T07:47:15.869790Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-03-11T07:47:15.869036Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-03-11T07:47:15.538250Z&quot;}" data-trusted="true">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1"></a>discriminator.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>, optimizer<span class="op">=</span><span class="st">"rmsprop"</span>)</span>
<span id="cb62-2"><a href="#cb62-2"></a></span>
<span id="cb62-3"><a href="#cb62-3"></a><span class="co"># In the gan model, we should not train the discriminator</span></span>
<span id="cb62-4"><a href="#cb62-4"></a>discriminator.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb62-5"><a href="#cb62-5"></a>gan.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>, optimizer<span class="op">=</span><span class="st">"rmsprop"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<p>The <code>trainable</code> attribute is taken into account by <code>Keras</code> only when compiling a model, so after running this code, the discriminator is trainable if we call its <code>fit()</code> method or its <code>train_on_batch()</code> method (which we will be using), while it is not trainable when we call these methods on the gan model.</p>
</blockquote>
<p>Since the training loop is unusual, we cannot use the regular <code>fit()</code> method. Instead, we will write a custom training loop. For this, we first need to create a <code>Dataset</code> to iterate through the images:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb63-2"><a href="#cb63-2"></a>dataset <span class="op">=</span> tf.data.Dataset.from_tensor_slices(X_train_dcgan)</span>
<span id="cb63-3"><a href="#cb63-3"></a>dataset <span class="op">=</span> dataset.shuffle(<span class="dv">1000</span>)</span>
<span id="cb63-4"><a href="#cb63-4"></a>dataset <span class="op">=</span> dataset.batch(batch_size, drop_remainder<span class="op">=</span><span class="va">True</span>).prefetch(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1"></a><span class="kw">def</span> train_gan(gan, dataset, batch_size, codings_size, n_epochs):</span>
<span id="cb64-2"><a href="#cb64-2"></a>    generator, discriminator <span class="op">=</span> gan.layers</span>
<span id="cb64-3"><a href="#cb64-3"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb64-4"><a href="#cb64-4"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n_epochs<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># extra code</span></span>
<span id="cb64-5"><a href="#cb64-5"></a>        <span class="cf">for</span> X_batch <span class="kw">in</span> dataset:</span>
<span id="cb64-6"><a href="#cb64-6"></a>            <span class="co"># phase 1 - training the discriminator</span></span>
<span id="cb64-7"><a href="#cb64-7"></a>            noise <span class="op">=</span> tf.random.normal(shape<span class="op">=</span>[batch_size, codings_size])</span>
<span id="cb64-8"><a href="#cb64-8"></a>            generated_images <span class="op">=</span> generator(noise)</span>
<span id="cb64-9"><a href="#cb64-9"></a>            X_fake_and_real <span class="op">=</span> tf.concat([generated_images, X_batch], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb64-10"><a href="#cb64-10"></a>            y1 <span class="op">=</span> tf.constant([[<span class="fl">0.</span>]] <span class="op">*</span> batch_size <span class="op">+</span> [[<span class="fl">1.</span>]] <span class="op">*</span> batch_size)</span>
<span id="cb64-11"><a href="#cb64-11"></a>            discriminator.train_on_batch(X_fake_and_real, y1)</span>
<span id="cb64-12"><a href="#cb64-12"></a>            <span class="co"># phase 2 - training the generator</span></span>
<span id="cb64-13"><a href="#cb64-13"></a>            noise <span class="op">=</span> tf.random.normal(shape<span class="op">=</span>[batch_size, codings_size])</span>
<span id="cb64-14"><a href="#cb64-14"></a>            y2 <span class="op">=</span> tf.constant([[<span class="fl">1.</span>]] <span class="op">*</span> batch_size)</span>
<span id="cb64-15"><a href="#cb64-15"></a>            gan.train_on_batch(noise, y2)</span>
<span id="cb64-16"><a href="#cb64-16"></a>        <span class="co"># extra code — plot images during training</span></span>
<span id="cb64-17"><a href="#cb64-17"></a>        plot_multiple_images(generated_images.numpy(), <span class="dv">8</span>)</span>
<span id="cb64-18"><a href="#cb64-18"></a>        plt.show()</span>
<span id="cb64-19"><a href="#cb64-19"></a>        plt.close()</span>
<span id="cb64-20"><a href="#cb64-20"></a></span>
<span id="cb64-21"><a href="#cb64-21"></a>train_gan(gan, dataset, batch_size, codings_size, n_epochs<span class="op">=</span><span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As discussed earlier, you can see the two phases at each iteration:</p>
<ul>
<li><p>In phase one we feed Gaussian noise to the generator to produce fake images, • and we complete this batch by concatenating an equal number of real images. The targets <code>y1</code> are set to 0 for fake images and 1 for real images. Then we train the discriminator on this batch. Remember that the discriminator is trainable in this phase, but we are not touching the generator.</p></li>
<li><p>In phase two, we feed the GAN some Gaussian noise. Its generator will start by producing fake images, then the discriminator will try to guess whether these images are fake or real. In this phase, we are trying to improve the generator, which means that we want the discriminator to fail: this is why the targets <code>y2</code> are all set to 1, although the images are fake. In this phase, the discriminator is not trainable, so the only part of the gan model that will improve is the generator.</p></li>
</ul>
<p>After training, you can randomly sample some codings from a Gaussian distribution, and feed them to the generator to produce new images:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1645,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684657150482,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="f7a72ed6-1319-455b-92cd-8622c78d0063">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1"></a>noise <span class="op">=</span> tf.random.normal(shape<span class="op">=</span>[batch_size, codings_size])</span>
<span id="cb65-2"><a href="#cb65-2"></a>generated_images <span class="op">=</span> generator.predict(noise)</span>
<span id="cb65-3"><a href="#cb65-3"></a>plot_multiple_images(generated_images, <span class="dv">8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 142ms/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-44-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>See <a href="https://github.com/lucidrains/stylegan2-pytorch">stylegan</a> if you are looking for state-of-the-art solutions.</p>
</section>
</section>
<section id="diffusion-models" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="diffusion-models"><span class="header-section-number">12.5</span> Diffusion Models</h2>
<p>Starting with an image from the dataset, at each time step <span class="math inline">\(t\)</span>, the diffusion process adds Gaussian noise with mean 0 and variance <span class="math inline">\(\beta_t\)</span>. The model is then trained to reverse that process. More specifically, given a noisy image produced by the forward process, and given the time <span class="math inline">\(t\)</span>, the model is trained to predict the total noise that was added to the original image, scaled to variance 1.</p>
<p>The <a href="https://arxiv.org/abs/2006.11239">DDPM paper</a> increased <span class="math inline">\(\beta_t\)</span> from <span class="math inline">\(\beta_1\)</span> = 0.0001 to $_T = <span class="math inline">\(0.02 (\)</span>T$ is the max step), but the <a href="https://arxiv.org/pdf/2102.09672.pdf">Improved DDPM paper</a> suggested using the following <span class="math inline">\(\cos^2(\ldots)\)</span> schedule instead, which gradually decreases <span class="math inline">\(\bar{\alpha_t} = \prod_{i=0}^{t} \alpha_i\)</span> from 1 to 0, where <span class="math inline">\(\alpha_t = 1 - \beta_t\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1"></a><span class="kw">def</span> variance_schedule(T, s<span class="op">=</span><span class="fl">0.008</span>, max_beta<span class="op">=</span><span class="fl">0.999</span>):</span>
<span id="cb67-2"><a href="#cb67-2"></a>    t <span class="op">=</span> np.arange(T <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb67-3"><a href="#cb67-3"></a>    f <span class="op">=</span> np.cos((t <span class="op">/</span> T <span class="op">+</span> s) <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> s) <span class="op">*</span> np.pi <span class="op">/</span> <span class="dv">2</span>) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb67-4"><a href="#cb67-4"></a>    alpha <span class="op">=</span> np.clip(f[<span class="dv">1</span>:] <span class="op">/</span> f[:<span class="op">-</span><span class="dv">1</span>], <span class="dv">1</span> <span class="op">-</span> max_beta, <span class="dv">1</span>)</span>
<span id="cb67-5"><a href="#cb67-5"></a>    alpha <span class="op">=</span> np.append(<span class="dv">1</span>, alpha).astype(np.float32)  <span class="co"># add α₀ = 1</span></span>
<span id="cb67-6"><a href="#cb67-6"></a>    beta <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> alpha</span>
<span id="cb67-7"><a href="#cb67-7"></a>    alpha_cumprod <span class="op">=</span> np.cumprod(alpha)</span>
<span id="cb67-8"><a href="#cb67-8"></a>    <span class="cf">return</span> alpha, alpha_cumprod, beta  <span class="co"># αₜ , α̅ₜ , βₜ for t = 0 to T</span></span>
<span id="cb67-9"><a href="#cb67-9"></a></span>
<span id="cb67-10"><a href="#cb67-10"></a>np.random.seed(<span class="dv">42</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb67-11"><a href="#cb67-11"></a>T <span class="op">=</span> <span class="dv">4000</span></span>
<span id="cb67-12"><a href="#cb67-12"></a>alpha, alpha_cumprod, beta <span class="op">=</span> variance_schedule(T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the DDPM paper, the authors used <span class="math inline">\(T = 1,000\)</span>, while in the Improved DDPM, they bumped this up to <span class="math inline">\(T = 4,000\)</span>, so we use this value. The variable <code>alpha</code> is a vector containing <span class="math inline">\(\alpha_0, \alpha_1, ..., \alpha_T\)</span>. The variable <code>alpha_cumprod</code> is a vector containing <span class="math inline">\(\bar{\alpha_0}, \bar{\alpha_1}, ..., \bar{\alpha_T}\)</span>.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1006,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684657167228,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="59146c6e-6355-4e11-dff0-8149a777a375">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">3</span>))</span>
<span id="cb68-2"><a href="#cb68-2"></a>plt.plot(beta, <span class="st">"r--"</span>, label<span class="op">=</span><span class="vs">r"$\beta_t$"</span>)</span>
<span id="cb68-3"><a href="#cb68-3"></a>plt.plot(alpha_cumprod, <span class="st">"b"</span>, label<span class="op">=</span><span class="vs">r"$\bar{\alpha}_t$"</span>)</span>
<span id="cb68-4"><a href="#cb68-4"></a>plt.axis([<span class="dv">0</span>, T, <span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb68-5"><a href="#cb68-5"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb68-6"><a href="#cb68-6"></a>plt.xlabel(<span class="vs">r"t"</span>)</span>
<span id="cb68-7"><a href="#cb68-7"></a>plt.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-46-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>To train our model to reverse the diffusion process, <strong>we will need noisy images from different time steps of the forward process.</strong> For this, let’s create a <code>prepare_batch()</code> function that will take a batch of clean images from the dataset and prepare them:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1"></a><span class="kw">def</span> prepare_batch(X):</span>
<span id="cb69-2"><a href="#cb69-2"></a>    X <span class="op">=</span> tf.cast(X[..., tf.newaxis], tf.float32) <span class="op">*</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>  <span class="co"># scale from -1 to +1</span></span>
<span id="cb69-3"><a href="#cb69-3"></a>    X_shape <span class="op">=</span> tf.shape(X)</span>
<span id="cb69-4"><a href="#cb69-4"></a>    t <span class="op">=</span> tf.random.uniform([X_shape[<span class="dv">0</span>]], minval<span class="op">=</span><span class="dv">1</span>, maxval<span class="op">=</span>T <span class="op">+</span> <span class="dv">1</span>, dtype<span class="op">=</span>tf.int32)</span>
<span id="cb69-5"><a href="#cb69-5"></a>    alpha_cm <span class="op">=</span> tf.gather(alpha_cumprod, t)</span>
<span id="cb69-6"><a href="#cb69-6"></a>    alpha_cm <span class="op">=</span> tf.reshape(alpha_cm, [X_shape[<span class="dv">0</span>]] <span class="op">+</span> [<span class="dv">1</span>] <span class="op">*</span> (<span class="bu">len</span>(X_shape) <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb69-7"><a href="#cb69-7"></a>    noise <span class="op">=</span> tf.random.normal(X_shape)</span>
<span id="cb69-8"><a href="#cb69-8"></a>    <span class="cf">return</span> {</span>
<span id="cb69-9"><a href="#cb69-9"></a>        <span class="st">"X_noisy"</span>: alpha_cm <span class="op">**</span> <span class="fl">0.5</span> <span class="op">*</span> X <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> alpha_cm) <span class="op">**</span> <span class="fl">0.5</span> <span class="op">*</span> noise,</span>
<span id="cb69-10"><a href="#cb69-10"></a>        <span class="st">"time"</span>: t,</span>
<span id="cb69-11"><a href="#cb69-11"></a>    }, noise</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s go through this code: - For simplicity we will use Fashion MNIST, so the function must first add a channel axis. It will also help to scale the pixel values from -1 to 1, so it’s closer to the final Gaussian distribution with mean 0 and variance 1.</p>
<ul>
<li><p>Next, the function creates <code>t</code>, a vector containing a random time step for each image in the batch, between <code>1</code> and <code>T</code>. Then it uses <code>tf.gather()</code> to get the value of <code>alpha_cumprod</code> for each of the time steps in the vector <code>t</code>. This gives us the vector <code>alpha_cm</code>, containing one value of <span class="math inline">\(\bar{\alpha_t}\)</span> for each image.</p></li>
<li><p>The next line reshapes the <code>alpha_cm</code> from <code>[batch size]</code> to <code>[batch size, 1, 1, 1]</code>. This is needed to ensure <code>alpha_cm</code> can be broadcasted with the batch <code>X</code> . Then we generate some Gaussian noise with mean 0 and variance 1.</p></li>
<li><p>Lastly, we use apply the diffusion process to the images. Note that <code>x  **  0.5</code> is equal to the square root of <code>x</code>. The function returns a tuple containing the inputs and the targets. <strong>The inputs are represented as a Python <code>dict</code> containing the noisy images and the time steps used to generate them. The targets are the Gaussian noise used to generate each image.</strong></p></li>
</ul>
<p>Next, we’ll create a training dataset and a validation set that will apply the <code>prepare_batch()</code> function to every batch. As earlier, <code>X_train</code> and <code>X_valid</code> contain the Fashion MNIST images with pixel values ranging from 0 to 1:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1"></a><span class="kw">def</span> prepare_dataset(X, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb70-2"><a href="#cb70-2"></a>    ds <span class="op">=</span> tf.data.Dataset.from_tensor_slices(X)</span>
<span id="cb70-3"><a href="#cb70-3"></a>    <span class="cf">if</span> shuffle:</span>
<span id="cb70-4"><a href="#cb70-4"></a>        ds <span class="op">=</span> ds.shuffle(<span class="dv">10_000</span>)</span>
<span id="cb70-5"><a href="#cb70-5"></a>    <span class="cf">return</span> ds.batch(batch_size).<span class="bu">map</span>(prepare_batch).prefetch(<span class="dv">1</span>)</span>
<span id="cb70-6"><a href="#cb70-6"></a></span>
<span id="cb70-7"><a href="#cb70-7"></a>tf.random.set_seed(<span class="dv">43</span>)  <span class="co"># ensures reproducibility on CPU</span></span>
<span id="cb70-8"><a href="#cb70-8"></a>train_set <span class="op">=</span> prepare_dataset(X_train, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb70-9"><a href="#cb70-9"></a>valid_set <span class="op">=</span> prepare_dataset(X_valid, batch_size<span class="op">=</span><span class="dv">32</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As a quick sanity check, let’s take a look at a few training samples, along with the corresponding noise to predict, and the original images (which we get by subtracting the appropriately scaled noise from the appropriately scaled noisy image):</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2175,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684657206436,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="71bbb845-2f16-4bb9-da78-893a928e43a0">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1"></a><span class="co"># Just a quick sanity check</span></span>
<span id="cb71-2"><a href="#cb71-2"></a></span>
<span id="cb71-3"><a href="#cb71-3"></a><span class="kw">def</span> subtract_noise(X_noisy, time, noise):</span>
<span id="cb71-4"><a href="#cb71-4"></a>    X_shape <span class="op">=</span> tf.shape(X_noisy)</span>
<span id="cb71-5"><a href="#cb71-5"></a>    alpha_cm <span class="op">=</span> tf.gather(alpha_cumprod, time)</span>
<span id="cb71-6"><a href="#cb71-6"></a>    alpha_cm <span class="op">=</span> tf.reshape(alpha_cm, [X_shape[<span class="dv">0</span>]] <span class="op">+</span> [<span class="dv">1</span>] <span class="op">*</span> (<span class="bu">len</span>(X_shape) <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb71-7"><a href="#cb71-7"></a>    <span class="cf">return</span> (X_noisy <span class="op">-</span> (<span class="dv">1</span> <span class="op">-</span> alpha_cm) <span class="op">**</span> <span class="fl">0.5</span> <span class="op">*</span> noise) <span class="op">/</span> alpha_cm <span class="op">**</span> <span class="fl">0.5</span></span>
<span id="cb71-8"><a href="#cb71-8"></a></span>
<span id="cb71-9"><a href="#cb71-9"></a>X_dict, Y_noise <span class="op">=</span> <span class="bu">list</span>(train_set.take(<span class="dv">1</span>))[<span class="dv">0</span>]  <span class="co"># get the first batch</span></span>
<span id="cb71-10"><a href="#cb71-10"></a>X_original <span class="op">=</span> subtract_noise(X_dict[<span class="st">"X_noisy"</span>], X_dict[<span class="st">"time"</span>], Y_noise)</span>
<span id="cb71-11"><a href="#cb71-11"></a></span>
<span id="cb71-12"><a href="#cb71-12"></a><span class="bu">print</span>(<span class="st">"Original images"</span>)</span>
<span id="cb71-13"><a href="#cb71-13"></a>plot_multiple_images(X_original[:<span class="dv">8</span>].numpy())</span>
<span id="cb71-14"><a href="#cb71-14"></a>plt.show()</span>
<span id="cb71-15"><a href="#cb71-15"></a><span class="bu">print</span>(<span class="st">"Time steps:"</span>, X_dict[<span class="st">"time"</span>].numpy()[:<span class="dv">8</span>])</span>
<span id="cb71-16"><a href="#cb71-16"></a><span class="bu">print</span>(<span class="st">"Noisy images"</span>)</span>
<span id="cb71-17"><a href="#cb71-17"></a>plot_multiple_images(X_dict[<span class="st">"X_noisy"</span>][:<span class="dv">8</span>].numpy())</span>
<span id="cb71-18"><a href="#cb71-18"></a>plt.show()</span>
<span id="cb71-19"><a href="#cb71-19"></a><span class="bu">print</span>(<span class="st">"Noise to predict"</span>)</span>
<span id="cb71-20"><a href="#cb71-20"></a>plot_multiple_images(Y_noise[:<span class="dv">8</span>].numpy())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Original images</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-49-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Time steps: [3405  312 3441 1991 2443 1657 3308 1151]
Noisy images</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-49-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Noise to predict</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-49-output-6.png" class="img-fluid"></p>
</div>
</div>
<p>Now we’re ready to build the diffusion model itself. <strong>It will need to process both images and times.</strong> We will encode the times using a sinusoidal encoding, as suggested in the DDPM paper, just like in the <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a> paper. Given a vector of <em>m</em> integers representing time indices (integers), the layer returns an <em>m</em> × <em>d</em> matrix, where <em>d</em> is the chosen embedding size.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1"></a><span class="co"># Implements a custom time encoding layer</span></span>
<span id="cb75-2"><a href="#cb75-2"></a></span>
<span id="cb75-3"><a href="#cb75-3"></a>embed_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb75-4"><a href="#cb75-4"></a></span>
<span id="cb75-5"><a href="#cb75-5"></a><span class="kw">class</span> TimeEncoding(tf.keras.layers.Layer):</span>
<span id="cb75-6"><a href="#cb75-6"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, T, embed_size, dtype<span class="op">=</span>tf.float32, <span class="op">**</span>kwargs):</span>
<span id="cb75-7"><a href="#cb75-7"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(dtype<span class="op">=</span>dtype, <span class="op">**</span>kwargs)</span>
<span id="cb75-8"><a href="#cb75-8"></a>        <span class="cf">assert</span> embed_size <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>, <span class="st">"embed_size must be even"</span></span>
<span id="cb75-9"><a href="#cb75-9"></a>        p, i <span class="op">=</span> np.meshgrid(np.arange(T <span class="op">+</span> <span class="dv">1</span>), <span class="dv">2</span> <span class="op">*</span> np.arange(embed_size <span class="op">//</span> <span class="dv">2</span>))</span>
<span id="cb75-10"><a href="#cb75-10"></a>        t_emb <span class="op">=</span> np.empty((T <span class="op">+</span> <span class="dv">1</span>, embed_size))</span>
<span id="cb75-11"><a href="#cb75-11"></a>        t_emb[:, ::<span class="dv">2</span>] <span class="op">=</span> np.sin(p <span class="op">/</span> <span class="dv">10_000</span> <span class="op">**</span> (i <span class="op">/</span> embed_size)).T</span>
<span id="cb75-12"><a href="#cb75-12"></a>        t_emb[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> np.cos(p <span class="op">/</span> <span class="dv">10_000</span> <span class="op">**</span> (i <span class="op">/</span> embed_size)).T</span>
<span id="cb75-13"><a href="#cb75-13"></a>        <span class="va">self</span>.time_encodings <span class="op">=</span> tf.constant(t_emb.astype(<span class="va">self</span>.dtype))</span>
<span id="cb75-14"><a href="#cb75-14"></a></span>
<span id="cb75-15"><a href="#cb75-15"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb75-16"><a href="#cb75-16"></a>        <span class="cf">return</span> tf.gather(<span class="va">self</span>.time_encodings, inputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s build the model. In the Improved DDPM paper, they use a UNet model. We’ll create a UNet-like model, that processes the image through <code>Conv2D</code> + <code>BatchNormalization</code> layers and skip connections, gradually downsampling the image (using <code>MaxPooling</code> layers with <code>strides=2</code>), then growing it back again (using <code>Upsampling2D</code> layers). Skip connections are also added across the downsampling part and the upsampling part. We also add the time encodings to the output of each block, after passing them through a <code>Dense</code> layer to resize them to the right dimension.</p>
<ul>
<li><strong>Note</strong>: an image’s time encoding is added to every pixel in the image, along the last axis (channels). So the number of units in the <code>Conv2D</code> layer must correspond to the embedding size, and we must reshape the <code>time_enc</code> tensor to add the width and height dimensions.</li>
<li>This UNet implementation was inspired by keras.io’s <a href="https://keras.io/examples/vision/oxford_pets_image_segmentation/">image segmentation example</a>, as well as from the <a href="https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/models/unet.py">official diffusion models implementation</a>. Compared to the first implementation, I added a few things, especially time encodings and skip connections across down/up parts. Compared to the second implementation, I removed a few things, especially the attention layers. It seemed like overkill for Fashion MNIST, but feel free to add them.</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1"></a>tf.keras.backend.clear_session()</span>
<span id="cb76-2"><a href="#cb76-2"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility on CPU</span></span>
<span id="cb76-3"><a href="#cb76-3"></a></span>
<span id="cb76-4"><a href="#cb76-4"></a><span class="kw">def</span> build_diffusion_model():</span>
<span id="cb76-5"><a href="#cb76-5"></a>    X_noisy <span class="op">=</span> tf.keras.layers.Input(shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>], name<span class="op">=</span><span class="st">"X_noisy"</span>)</span>
<span id="cb76-6"><a href="#cb76-6"></a>    time_input <span class="op">=</span> tf.keras.layers.Input(shape<span class="op">=</span>[], dtype<span class="op">=</span>tf.int32, name<span class="op">=</span><span class="st">"time"</span>)</span>
<span id="cb76-7"><a href="#cb76-7"></a>    time_enc <span class="op">=</span> TimeEncoding(T, embed_size)(time_input)</span>
<span id="cb76-8"><a href="#cb76-8"></a></span>
<span id="cb76-9"><a href="#cb76-9"></a>    dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb76-10"><a href="#cb76-10"></a>    Z <span class="op">=</span> tf.keras.layers.ZeroPadding2D((<span class="dv">3</span>, <span class="dv">3</span>))(X_noisy)</span>
<span id="cb76-11"><a href="#cb76-11"></a>    Z <span class="op">=</span> tf.keras.layers.Conv2D(dim, <span class="dv">3</span>)(Z)</span>
<span id="cb76-12"><a href="#cb76-12"></a>    Z <span class="op">=</span> tf.keras.layers.BatchNormalization()(Z)</span>
<span id="cb76-13"><a href="#cb76-13"></a>    Z <span class="op">=</span> tf.keras.layers.Activation(<span class="st">"relu"</span>)(Z)</span>
<span id="cb76-14"><a href="#cb76-14"></a></span>
<span id="cb76-15"><a href="#cb76-15"></a>    time <span class="op">=</span> tf.keras.layers.Dense(dim)(time_enc)  <span class="co"># adapt time encoding</span></span>
<span id="cb76-16"><a href="#cb76-16"></a>    Z <span class="op">=</span> time[:, tf.newaxis, tf.newaxis, :] <span class="op">+</span> Z  <span class="co"># add time data to every pixel</span></span>
<span id="cb76-17"><a href="#cb76-17"></a></span>
<span id="cb76-18"><a href="#cb76-18"></a>    skip <span class="op">=</span> Z</span>
<span id="cb76-19"><a href="#cb76-19"></a>    cross_skips <span class="op">=</span> []  <span class="co"># skip connections across the down &amp; up parts of the UNet</span></span>
<span id="cb76-20"><a href="#cb76-20"></a></span>
<span id="cb76-21"><a href="#cb76-21"></a>    <span class="cf">for</span> dim <span class="kw">in</span> (<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">128</span>):</span>
<span id="cb76-22"><a href="#cb76-22"></a>        Z <span class="op">=</span> tf.keras.layers.Activation(<span class="st">"relu"</span>)(Z)</span>
<span id="cb76-23"><a href="#cb76-23"></a>        Z <span class="op">=</span> tf.keras.layers.SeparableConv2D(dim, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>)(Z)</span>
<span id="cb76-24"><a href="#cb76-24"></a>        Z <span class="op">=</span> tf.keras.layers.BatchNormalization()(Z)</span>
<span id="cb76-25"><a href="#cb76-25"></a></span>
<span id="cb76-26"><a href="#cb76-26"></a>        Z <span class="op">=</span> tf.keras.layers.Activation(<span class="st">"relu"</span>)(Z)</span>
<span id="cb76-27"><a href="#cb76-27"></a>        Z <span class="op">=</span> tf.keras.layers.SeparableConv2D(dim, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>)(Z)</span>
<span id="cb76-28"><a href="#cb76-28"></a>        Z <span class="op">=</span> tf.keras.layers.BatchNormalization()(Z)</span>
<span id="cb76-29"><a href="#cb76-29"></a></span>
<span id="cb76-30"><a href="#cb76-30"></a>        cross_skips.append(Z)</span>
<span id="cb76-31"><a href="#cb76-31"></a>        Z <span class="op">=</span> tf.keras.layers.MaxPooling2D(<span class="dv">3</span>, strides<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="st">"same"</span>)(Z)</span>
<span id="cb76-32"><a href="#cb76-32"></a>        skip_link <span class="op">=</span> tf.keras.layers.Conv2D(dim, <span class="dv">1</span>, strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb76-33"><a href="#cb76-33"></a>                                           padding<span class="op">=</span><span class="st">"same"</span>)(skip)</span>
<span id="cb76-34"><a href="#cb76-34"></a>        Z <span class="op">=</span> tf.keras.layers.add([Z, skip_link])</span>
<span id="cb76-35"><a href="#cb76-35"></a></span>
<span id="cb76-36"><a href="#cb76-36"></a>        time <span class="op">=</span> tf.keras.layers.Dense(dim)(time_enc)</span>
<span id="cb76-37"><a href="#cb76-37"></a>        Z <span class="op">=</span> time[:, tf.newaxis, tf.newaxis, :] <span class="op">+</span> Z</span>
<span id="cb76-38"><a href="#cb76-38"></a>        skip <span class="op">=</span> Z</span>
<span id="cb76-39"><a href="#cb76-39"></a></span>
<span id="cb76-40"><a href="#cb76-40"></a>    <span class="cf">for</span> dim <span class="kw">in</span> (<span class="dv">64</span>, <span class="dv">32</span>, <span class="dv">16</span>):</span>
<span id="cb76-41"><a href="#cb76-41"></a>        Z <span class="op">=</span> tf.keras.layers.Activation(<span class="st">"relu"</span>)(Z)</span>
<span id="cb76-42"><a href="#cb76-42"></a>        Z <span class="op">=</span> tf.keras.layers.Conv2DTranspose(dim, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>)(Z)</span>
<span id="cb76-43"><a href="#cb76-43"></a>        Z <span class="op">=</span> tf.keras.layers.BatchNormalization()(Z)</span>
<span id="cb76-44"><a href="#cb76-44"></a></span>
<span id="cb76-45"><a href="#cb76-45"></a>        Z <span class="op">=</span> tf.keras.layers.Activation(<span class="st">"relu"</span>)(Z)</span>
<span id="cb76-46"><a href="#cb76-46"></a>        Z <span class="op">=</span> tf.keras.layers.Conv2DTranspose(dim, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>)(Z)</span>
<span id="cb76-47"><a href="#cb76-47"></a>        Z <span class="op">=</span> tf.keras.layers.BatchNormalization()(Z)</span>
<span id="cb76-48"><a href="#cb76-48"></a></span>
<span id="cb76-49"><a href="#cb76-49"></a>        Z <span class="op">=</span> tf.keras.layers.UpSampling2D(<span class="dv">2</span>)(Z)</span>
<span id="cb76-50"><a href="#cb76-50"></a></span>
<span id="cb76-51"><a href="#cb76-51"></a>        skip_link <span class="op">=</span> tf.keras.layers.UpSampling2D(<span class="dv">2</span>)(skip)</span>
<span id="cb76-52"><a href="#cb76-52"></a>        skip_link <span class="op">=</span> tf.keras.layers.Conv2D(dim, <span class="dv">1</span>, padding<span class="op">=</span><span class="st">"same"</span>)(skip_link)</span>
<span id="cb76-53"><a href="#cb76-53"></a>        Z <span class="op">=</span> tf.keras.layers.add([Z, skip_link])</span>
<span id="cb76-54"><a href="#cb76-54"></a></span>
<span id="cb76-55"><a href="#cb76-55"></a>        time <span class="op">=</span> tf.keras.layers.Dense(dim)(time_enc)</span>
<span id="cb76-56"><a href="#cb76-56"></a>        Z <span class="op">=</span> time[:, tf.newaxis, tf.newaxis, :] <span class="op">+</span> Z</span>
<span id="cb76-57"><a href="#cb76-57"></a>        Z <span class="op">=</span> tf.keras.layers.concatenate([Z, cross_skips.pop()], axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb76-58"><a href="#cb76-58"></a>        skip <span class="op">=</span> Z</span>
<span id="cb76-59"><a href="#cb76-59"></a></span>
<span id="cb76-60"><a href="#cb76-60"></a>    outputs <span class="op">=</span> tf.keras.layers.Conv2D(<span class="dv">1</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>)(Z)[:, <span class="dv">2</span>:<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>:<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb76-61"><a href="#cb76-61"></a>    <span class="cf">return</span> tf.keras.Model(inputs<span class="op">=</span>[X_noisy, time_input], outputs<span class="op">=</span>[outputs])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:999,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684657221231,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="4ac7c7d8-a0b6-4da5-c51c-7cfbb85a3a68">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1"></a>model <span class="op">=</span> build_diffusion_model()</span>
<span id="cb77-2"><a href="#cb77-2"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span>tf.keras.losses.Huber(), optimizer<span class="op">=</span><span class="st">"nadam"</span>)</span>
<span id="cb77-3"><a href="#cb77-3"></a>model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 X_noisy (InputLayer)           [(None, 28, 28, 1)]  0           []                               
                                                                                                  
 time (InputLayer)              [(None,)]            0           []                               
                                                                                                  
 zero_padding2d (ZeroPadding2D)  (None, 34, 34, 1)   0           ['X_noisy[0][0]']                
                                                                                                  
 time_encoding (TimeEncoding)   (None, 64)           0           ['time[0][0]']                   
                                                                                                  
 conv2d (Conv2D)                (None, 32, 32, 16)   160         ['zero_padding2d[0][0]']         
                                                                                                  
 dense (Dense)                  (None, 16)           1040        ['time_encoding[0][0]']          
                                                                                                  
 batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 
 alization)                                                                                       
                                                                                                  
 tf.__operators__.getitem (Slic  (None, 1, 1, 16)    0           ['dense[0][0]']                  
 ingOpLambda)                                                                                     
                                                                                                  
 activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    
                                                                                                  
 tf.__operators__.add (TFOpLamb  (None, 32, 32, 16)  0           ['tf.__operators__.getitem[0][0]'
 da)                                                             , 'activation[0][0]']            
                                                                                                  
 activation_1 (Activation)      (None, 32, 32, 16)   0           ['tf.__operators__.add[0][0]']   
                                                                                                  
 separable_conv2d (SeparableCon  (None, 32, 32, 32)  688         ['activation_1[0][0]']           
 v2D)                                                                                             
                                                                                                  
 batch_normalization_1 (BatchNo  (None, 32, 32, 32)  128         ['separable_conv2d[0][0]']       
 rmalization)                                                                                     
                                                                                                  
 activation_2 (Activation)      (None, 32, 32, 32)   0           ['batch_normalization_1[0][0]']  
                                                                                                  
 separable_conv2d_1 (SeparableC  (None, 32, 32, 32)  1344        ['activation_2[0][0]']           
 onv2D)                                                                                           
                                                                                                  
 batch_normalization_2 (BatchNo  (None, 32, 32, 32)  128         ['separable_conv2d_1[0][0]']     
 rmalization)                                                                                     
                                                                                                  
 dense_1 (Dense)                (None, 32)           2080        ['time_encoding[0][0]']          
                                                                                                  
 max_pooling2d (MaxPooling2D)   (None, 16, 16, 32)   0           ['batch_normalization_2[0][0]']  
                                                                                                  
 conv2d_1 (Conv2D)              (None, 16, 16, 32)   544         ['tf.__operators__.add[0][0]']   
                                                                                                  
 tf.__operators__.getitem_1 (Sl  (None, 1, 1, 32)    0           ['dense_1[0][0]']                
 icingOpLambda)                                                                                   
                                                                                                  
 add (Add)                      (None, 16, 16, 32)   0           ['max_pooling2d[0][0]',          
                                                                  'conv2d_1[0][0]']               
                                                                                                  
 tf.__operators__.add_1 (TFOpLa  (None, 16, 16, 32)  0           ['tf.__operators__.getitem_1[0][0
 mbda)                                                           ]',                              
                                                                  'add[0][0]']                    
                                                                                                  
 activation_3 (Activation)      (None, 16, 16, 32)   0           ['tf.__operators__.add_1[0][0]'] 
                                                                                                  
 separable_conv2d_2 (SeparableC  (None, 16, 16, 64)  2400        ['activation_3[0][0]']           
 onv2D)                                                                                           
                                                                                                  
 batch_normalization_3 (BatchNo  (None, 16, 16, 64)  256         ['separable_conv2d_2[0][0]']     
 rmalization)                                                                                     
                                                                                                  
 activation_4 (Activation)      (None, 16, 16, 64)   0           ['batch_normalization_3[0][0]']  
                                                                                                  
 separable_conv2d_3 (SeparableC  (None, 16, 16, 64)  4736        ['activation_4[0][0]']           
 onv2D)                                                                                           
                                                                                                  
 batch_normalization_4 (BatchNo  (None, 16, 16, 64)  256         ['separable_conv2d_3[0][0]']     
 rmalization)                                                                                     
                                                                                                  
 dense_2 (Dense)                (None, 64)           4160        ['time_encoding[0][0]']          
                                                                                                  
 max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 64)    0           ['batch_normalization_4[0][0]']  
                                                                                                  
 conv2d_2 (Conv2D)              (None, 8, 8, 64)     2112        ['tf.__operators__.add_1[0][0]'] 
                                                                                                  
 tf.__operators__.getitem_2 (Sl  (None, 1, 1, 64)    0           ['dense_2[0][0]']                
 icingOpLambda)                                                                                   
                                                                                                  
 add_1 (Add)                    (None, 8, 8, 64)     0           ['max_pooling2d_1[0][0]',        
                                                                  'conv2d_2[0][0]']               
                                                                                                  
 tf.__operators__.add_2 (TFOpLa  (None, 8, 8, 64)    0           ['tf.__operators__.getitem_2[0][0
 mbda)                                                           ]',                              
                                                                  'add_1[0][0]']                  
                                                                                                  
 activation_5 (Activation)      (None, 8, 8, 64)     0           ['tf.__operators__.add_2[0][0]'] 
                                                                                                  
 separable_conv2d_4 (SeparableC  (None, 8, 8, 128)   8896        ['activation_5[0][0]']           
 onv2D)                                                                                           
                                                                                                  
 batch_normalization_5 (BatchNo  (None, 8, 8, 128)   512         ['separable_conv2d_4[0][0]']     
 rmalization)                                                                                     
                                                                                                  
 activation_6 (Activation)      (None, 8, 8, 128)    0           ['batch_normalization_5[0][0]']  
                                                                                                  
 separable_conv2d_5 (SeparableC  (None, 8, 8, 128)   17664       ['activation_6[0][0]']           
 onv2D)                                                                                           
                                                                                                  
 batch_normalization_6 (BatchNo  (None, 8, 8, 128)   512         ['separable_conv2d_5[0][0]']     
 rmalization)                                                                                     
                                                                                                  
 dense_3 (Dense)                (None, 128)          8320        ['time_encoding[0][0]']          
                                                                                                  
 max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 128)   0           ['batch_normalization_6[0][0]']  
                                                                                                  
 conv2d_3 (Conv2D)              (None, 4, 4, 128)    8320        ['tf.__operators__.add_2[0][0]'] 
                                                                                                  
 tf.__operators__.getitem_3 (Sl  (None, 1, 1, 128)   0           ['dense_3[0][0]']                
 icingOpLambda)                                                                                   
                                                                                                  
 add_2 (Add)                    (None, 4, 4, 128)    0           ['max_pooling2d_2[0][0]',        
                                                                  'conv2d_3[0][0]']               
                                                                                                  
 tf.__operators__.add_3 (TFOpLa  (None, 4, 4, 128)   0           ['tf.__operators__.getitem_3[0][0
 mbda)                                                           ]',                              
                                                                  'add_2[0][0]']                  
                                                                                                  
 activation_7 (Activation)      (None, 4, 4, 128)    0           ['tf.__operators__.add_3[0][0]'] 
                                                                                                  
 conv2d_transpose (Conv2DTransp  (None, 4, 4, 64)    73792       ['activation_7[0][0]']           
 ose)                                                                                             
                                                                                                  
 batch_normalization_7 (BatchNo  (None, 4, 4, 64)    256         ['conv2d_transpose[0][0]']       
 rmalization)                                                                                     
                                                                                                  
 activation_8 (Activation)      (None, 4, 4, 64)     0           ['batch_normalization_7[0][0]']  
                                                                                                  
 conv2d_transpose_1 (Conv2DTran  (None, 4, 4, 64)    36928       ['activation_8[0][0]']           
 spose)                                                                                           
                                                                                                  
 batch_normalization_8 (BatchNo  (None, 4, 4, 64)    256         ['conv2d_transpose_1[0][0]']     
 rmalization)                                                                                     
                                                                                                  
 up_sampling2d_1 (UpSampling2D)  (None, 8, 8, 128)   0           ['tf.__operators__.add_3[0][0]'] 
                                                                                                  
 dense_4 (Dense)                (None, 64)           4160        ['time_encoding[0][0]']          
                                                                                                  
 up_sampling2d (UpSampling2D)   (None, 8, 8, 64)     0           ['batch_normalization_8[0][0]']  
                                                                                                  
 conv2d_4 (Conv2D)              (None, 8, 8, 64)     8256        ['up_sampling2d_1[0][0]']        
                                                                                                  
 tf.__operators__.getitem_4 (Sl  (None, 1, 1, 64)    0           ['dense_4[0][0]']                
 icingOpLambda)                                                                                   
                                                                                                  
 add_3 (Add)                    (None, 8, 8, 64)     0           ['up_sampling2d[0][0]',          
                                                                  'conv2d_4[0][0]']               
                                                                                                  
 tf.__operators__.add_4 (TFOpLa  (None, 8, 8, 64)    0           ['tf.__operators__.getitem_4[0][0
 mbda)                                                           ]',                              
                                                                  'add_3[0][0]']                  
                                                                                                  
 concatenate (Concatenate)      (None, 8, 8, 192)    0           ['tf.__operators__.add_4[0][0]', 
                                                                  'batch_normalization_6[0][0]']  
                                                                                                  
 activation_9 (Activation)      (None, 8, 8, 192)    0           ['concatenate[0][0]']            
                                                                                                  
 conv2d_transpose_2 (Conv2DTran  (None, 8, 8, 32)    55328       ['activation_9[0][0]']           
 spose)                                                                                           
                                                                                                  
 batch_normalization_9 (BatchNo  (None, 8, 8, 32)    128         ['conv2d_transpose_2[0][0]']     
 rmalization)                                                                                     
                                                                                                  
 activation_10 (Activation)     (None, 8, 8, 32)     0           ['batch_normalization_9[0][0]']  
                                                                                                  
 conv2d_transpose_3 (Conv2DTran  (None, 8, 8, 32)    9248        ['activation_10[0][0]']          
 spose)                                                                                           
                                                                                                  
 batch_normalization_10 (BatchN  (None, 8, 8, 32)    128         ['conv2d_transpose_3[0][0]']     
 ormalization)                                                                                    
                                                                                                  
 up_sampling2d_3 (UpSampling2D)  (None, 16, 16, 192)  0          ['concatenate[0][0]']            
                                                                                                  
 dense_5 (Dense)                (None, 32)           2080        ['time_encoding[0][0]']          
                                                                                                  
 up_sampling2d_2 (UpSampling2D)  (None, 16, 16, 32)  0           ['batch_normalization_10[0][0]'] 
                                                                                                  
 conv2d_5 (Conv2D)              (None, 16, 16, 32)   6176        ['up_sampling2d_3[0][0]']        
                                                                                                  
 tf.__operators__.getitem_5 (Sl  (None, 1, 1, 32)    0           ['dense_5[0][0]']                
 icingOpLambda)                                                                                   
                                                                                                  
 add_4 (Add)                    (None, 16, 16, 32)   0           ['up_sampling2d_2[0][0]',        
                                                                  'conv2d_5[0][0]']               
                                                                                                  
 tf.__operators__.add_5 (TFOpLa  (None, 16, 16, 32)  0           ['tf.__operators__.getitem_5[0][0
 mbda)                                                           ]',                              
                                                                  'add_4[0][0]']                  
                                                                                                  
 concatenate_1 (Concatenate)    (None, 16, 16, 96)   0           ['tf.__operators__.add_5[0][0]', 
                                                                  'batch_normalization_4[0][0]']  
                                                                                                  
 activation_11 (Activation)     (None, 16, 16, 96)   0           ['concatenate_1[0][0]']          
                                                                                                  
 conv2d_transpose_4 (Conv2DTran  (None, 16, 16, 16)  13840       ['activation_11[0][0]']          
 spose)                                                                                           
                                                                                                  
 batch_normalization_11 (BatchN  (None, 16, 16, 16)  64          ['conv2d_transpose_4[0][0]']     
 ormalization)                                                                                    
                                                                                                  
 activation_12 (Activation)     (None, 16, 16, 16)   0           ['batch_normalization_11[0][0]'] 
                                                                                                  
 conv2d_transpose_5 (Conv2DTran  (None, 16, 16, 16)  2320        ['activation_12[0][0]']          
 spose)                                                                                           
                                                                                                  
 batch_normalization_12 (BatchN  (None, 16, 16, 16)  64          ['conv2d_transpose_5[0][0]']     
 ormalization)                                                                                    
                                                                                                  
 up_sampling2d_5 (UpSampling2D)  (None, 32, 32, 96)  0           ['concatenate_1[0][0]']          
                                                                                                  
 dense_6 (Dense)                (None, 16)           1040        ['time_encoding[0][0]']          
                                                                                                  
 up_sampling2d_4 (UpSampling2D)  (None, 32, 32, 16)  0           ['batch_normalization_12[0][0]'] 
                                                                                                  
 conv2d_6 (Conv2D)              (None, 32, 32, 16)   1552        ['up_sampling2d_5[0][0]']        
                                                                                                  
 tf.__operators__.getitem_6 (Sl  (None, 1, 1, 16)    0           ['dense_6[0][0]']                
 icingOpLambda)                                                                                   
                                                                                                  
 add_5 (Add)                    (None, 32, 32, 16)   0           ['up_sampling2d_4[0][0]',        
                                                                  'conv2d_6[0][0]']               
                                                                                                  
 tf.__operators__.add_6 (TFOpLa  (None, 32, 32, 16)  0           ['tf.__operators__.getitem_6[0][0
 mbda)                                                           ]',                              
                                                                  'add_5[0][0]']                  
                                                                                                  
 concatenate_2 (Concatenate)    (None, 32, 32, 48)   0           ['tf.__operators__.add_6[0][0]', 
                                                                  'batch_normalization_2[0][0]']  
                                                                                                  
 conv2d_7 (Conv2D)              (None, 32, 32, 1)    433         ['concatenate_2[0][0]']          
                                                                                                  
 tf.__operators__.getitem_7 (Sl  (None, 28, 28, 1)   0           ['conv2d_7[0][0]']               
 icingOpLambda)                                                                                   
                                                                                                  
==================================================================================================
Total params: 280,369
Trainable params: 278,993
Non-trainable params: 1,376
__________________________________________________________________________________________________</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:4707937,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684661965627,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="172fe94a-3e34-46f9-85d9-f84ae40f0f9d">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1"></a><span class="co"># adds a ModelCheckpoint callback</span></span>
<span id="cb79-2"><a href="#cb79-2"></a>checkpoint_cb <span class="op">=</span> tf.keras.callbacks.ModelCheckpoint(<span class="st">"my_diffusion_model"</span>, save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb79-3"><a href="#cb79-3"></a></span>
<span id="cb79-4"><a href="#cb79-4"></a>history <span class="op">=</span> model.fit(train_set, validation_data<span class="op">=</span>valid_set, epochs<span class="op">=</span><span class="dv">100</span>, callbacks<span class="op">=</span>[checkpoint_cb])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/100
1719/1719 [==============================] - ETA: 0s - loss: 0.1045</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 90s 36ms/step - loss: 0.1045 - val_loss: 0.0704
Epoch 2/100
1719/1719 [==============================] - ETA: 0s - loss: 0.0614</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 58s 34ms/step - loss: 0.0614 - val_loss: 0.0570
Epoch 3/100
1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0525</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 57s 33ms/step - loss: 0.0525 - val_loss: 0.0513
Epoch 4/100
1719/1719 [==============================] - ETA: 0s - loss: 0.0487</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 53s 31ms/step - loss: 0.0487 - val_loss: 0.0482
Epoch 5/100
1719/1719 [==============================] - 44s 26ms/step - loss: 0.0469 - val_loss: 0.0485
Epoch 6/100
1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0455</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 54s 31ms/step - loss: 0.0454 - val_loss: 0.0454
Epoch 7/100
1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0445</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 54s 31ms/step - loss: 0.0446 - val_loss: 0.0448
Epoch 8/100
1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0436</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 52s 30ms/step - loss: 0.0436 - val_loss: 0.0431
Epoch 9/100
1719/1719 [==============================] - 44s 26ms/step - loss: 0.0430 - val_loss: 0.0431
Epoch 10/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0424 - val_loss: 0.0433
Epoch 11/100
1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0423</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 54s 32ms/step - loss: 0.0423 - val_loss: 0.0419
Epoch 12/100
1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0420</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 54s 32ms/step - loss: 0.0420 - val_loss: 0.0413
Epoch 13/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0416 - val_loss: 0.0418
Epoch 14/100
1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0414</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 53s 31ms/step - loss: 0.0414 - val_loss: 0.0412
Epoch 15/100
1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0411</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 55s 32ms/step - loss: 0.0411 - val_loss: 0.0405
Epoch 16/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0410 - val_loss: 0.0423
Epoch 17/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0409 - val_loss: 0.0418
Epoch 18/100
1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0407</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 51s 30ms/step - loss: 0.0407 - val_loss: 0.0402
Epoch 19/100
1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0406</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 53s 31ms/step - loss: 0.0406 - val_loss: 0.0399
Epoch 20/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0405 - val_loss: 0.0403
Epoch 21/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0403 - val_loss: 0.0405
Epoch 22/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0402 - val_loss: 0.0404
Epoch 23/100
1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0400</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 52s 30ms/step - loss: 0.0400 - val_loss: 0.0393
Epoch 24/100
1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0401</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 54s 31ms/step - loss: 0.0401 - val_loss: 0.0392
Epoch 25/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0399 - val_loss: 0.0396
Epoch 26/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0399 - val_loss: 0.0404
Epoch 27/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0399 - val_loss: 0.0395
Epoch 28/100
1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0397</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 52s 30ms/step - loss: 0.0397 - val_loss: 0.0392
Epoch 29/100
1719/1719 [==============================] - 42s 25ms/step - loss: 0.0397 - val_loss: 0.0392
Epoch 30/100
1719/1719 [==============================] - ETA: 0s - loss: 0.0396</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 53s 31ms/step - loss: 0.0396 - val_loss: 0.0391
Epoch 31/100
1719/1719 [==============================] - ETA: 0s - loss: 0.0396</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 55s 32ms/step - loss: 0.0396 - val_loss: 0.0388
Epoch 32/100
1719/1719 [==============================] - 44s 25ms/step - loss: 0.0394 - val_loss: 0.0391
Epoch 33/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0395 - val_loss: 0.0391
Epoch 34/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0394 - val_loss: 0.0404
Epoch 35/100
1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0393</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 53s 31ms/step - loss: 0.0393 - val_loss: 0.0387
Epoch 36/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0393 - val_loss: 0.0388
Epoch 37/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0393 - val_loss: 0.0395
Epoch 38/100
1719/1719 [==============================] - 45s 26ms/step - loss: 0.0392 - val_loss: 0.0387
Epoch 39/100
1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0391</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 54s 32ms/step - loss: 0.0391 - val_loss: 0.0384
Epoch 40/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0391 - val_loss: 0.0388
Epoch 41/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0393 - val_loss: 0.0386
Epoch 42/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0391 - val_loss: 0.0386
Epoch 43/100
1719/1719 [==============================] - ETA: 0s - loss: 0.0391</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 51s 30ms/step - loss: 0.0391 - val_loss: 0.0383
Epoch 44/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0391 - val_loss: 0.0387
Epoch 45/100
1719/1719 [==============================] - 42s 25ms/step - loss: 0.0391 - val_loss: 0.0392
Epoch 46/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0390 - val_loss: 0.0387
Epoch 47/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0389 - val_loss: 0.0400
Epoch 48/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0389 - val_loss: 0.0391
Epoch 49/100
1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0389</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 53s 31ms/step - loss: 0.0389 - val_loss: 0.0383
Epoch 50/100
1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0389</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 51s 30ms/step - loss: 0.0389 - val_loss: 0.0383
Epoch 51/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0388 - val_loss: 0.0411
Epoch 52/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0387 - val_loss: 0.0384
Epoch 53/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0387 - val_loss: 0.0387
Epoch 54/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0387 - val_loss: 0.0384
Epoch 55/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0386 - val_loss: 0.0383
Epoch 56/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0387 - val_loss: 0.0384
Epoch 57/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0387 - val_loss: 0.0393
Epoch 58/100
1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0386</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 52s 30ms/step - loss: 0.0386 - val_loss: 0.0380
Epoch 59/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0387 - val_loss: 0.0391
Epoch 60/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0385 - val_loss: 0.0389
Epoch 61/100
1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0386</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 54s 31ms/step - loss: 0.0387 - val_loss: 0.0380
Epoch 62/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0386 - val_loss: 0.0386
Epoch 63/100
1719/1719 [==============================] - 44s 25ms/step - loss: 0.0386 - val_loss: 0.0380
Epoch 64/100
1719/1719 [==============================] - ETA: 0s - loss: 0.0384</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 52s 30ms/step - loss: 0.0384 - val_loss: 0.0377
Epoch 65/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0385 - val_loss: 0.0386
Epoch 66/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0386 - val_loss: 0.0388
Epoch 67/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0384 - val_loss: 0.0392
Epoch 68/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0383 - val_loss: 0.0380
Epoch 69/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0384 - val_loss: 0.0389
Epoch 70/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0385 - val_loss: 0.0395
Epoch 71/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0384 - val_loss: 0.0386
Epoch 72/100
1719/1719 [==============================] - 44s 25ms/step - loss: 0.0383 - val_loss: 0.0380
Epoch 73/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0384 - val_loss: 0.0384
Epoch 74/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0383 - val_loss: 0.0384
Epoch 75/100
1719/1719 [==============================] - 42s 25ms/step - loss: 0.0383 - val_loss: 0.0399
Epoch 76/100
1719/1719 [==============================] - 44s 26ms/step - loss: 0.0383 - val_loss: 0.0378
Epoch 77/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0382 - val_loss: 0.0384
Epoch 78/100
1719/1719 [==============================] - 45s 26ms/step - loss: 0.0383 - val_loss: 0.0378
Epoch 79/100
1719/1719 [==============================] - ETA: 0s - loss: 0.0384</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 55s 32ms/step - loss: 0.0384 - val_loss: 0.0377
Epoch 80/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0384 - val_loss: 0.0387
Epoch 81/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0382 - val_loss: 0.0388
Epoch 82/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0381 - val_loss: 0.0378
Epoch 83/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0383 - val_loss: 0.0380
Epoch 84/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0381 - val_loss: 0.0384
Epoch 85/100
1719/1719 [==============================] - 42s 25ms/step - loss: 0.0381 - val_loss: 0.0401
Epoch 86/100
1719/1719 [==============================] - 43s 25ms/step - loss: 0.0382 - val_loss: 0.0386
Epoch 87/100
1719/1719 [==============================] - ETA: 0s - loss: 0.0381</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 51s 30ms/step - loss: 0.0381 - val_loss: 0.0375
Epoch 88/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0381 - val_loss: 0.0382
Epoch 89/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0380 - val_loss: 0.0384
Epoch 90/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0381 - val_loss: 0.0376
Epoch 91/100
1719/1719 [==============================] - 42s 24ms/step - loss: 0.0380 - val_loss: 0.0385
Epoch 92/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0381 - val_loss: 0.0379
Epoch 93/100
1719/1719 [==============================] - 42s 25ms/step - loss: 0.0379 - val_loss: 0.0386
Epoch 94/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0381 - val_loss: 0.0376
Epoch 95/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0380 - val_loss: 0.0378
Epoch 96/100
1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0381</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>1719/1719 [==============================] - 52s 30ms/step - loss: 0.0381 - val_loss: 0.0373
Epoch 97/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0380 - val_loss: 0.0390
Epoch 98/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0380 - val_loss: 0.0382
Epoch 99/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0381 - val_loss: 0.0375
Epoch 100/100
1719/1719 [==============================] - 41s 24ms/step - loss: 0.0380 - val_loss: 0.0377</code></pre>
</div>
</div>
<p>Now that the model is trained, we can use it to generate new images. For this, we just generate Gaussian noise, and pretend this is the result of the diffusion process, and we’re at time <span class="math inline">\(T\)</span>. Then we use the model to predict the image at time <span class="math inline">\(T - 1\)</span>, then we call it again to get <span class="math inline">\(T - 2\)</span>, and so on, removing a bit of noise at each step. At the end, we get an image that looks like it’s from the Fashion MNIST dataset. The equation for this reverse process is at the top of page 4 in the DDPM paper (step 4 in algorithm 2).</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:252087,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684662217704,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="2a8876d7-86c0-48ec-ee07-bf740a5b9f7e">
<div class="sourceCode cell-code" id="cb139"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1"></a><span class="kw">def</span> generate(model, batch_size<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb139-2"><a href="#cb139-2"></a>    X <span class="op">=</span> tf.random.normal([batch_size, <span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>])</span>
<span id="cb139-3"><a href="#cb139-3"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T <span class="op">-</span> <span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb139-4"><a href="#cb139-4"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\r</span><span class="ss">t = </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">" "</span>)  <span class="co"># show progress</span></span>
<span id="cb139-5"><a href="#cb139-5"></a>        noise <span class="op">=</span> (tf.random.normal <span class="cf">if</span> t <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> tf.zeros)(tf.shape(X))</span>
<span id="cb139-6"><a href="#cb139-6"></a>        X_noise <span class="op">=</span> model({<span class="st">"X_noisy"</span>: X, <span class="st">"time"</span>: tf.constant([t] <span class="op">*</span> batch_size)})</span>
<span id="cb139-7"><a href="#cb139-7"></a>        X <span class="op">=</span> (</span>
<span id="cb139-8"><a href="#cb139-8"></a>            <span class="dv">1</span> <span class="op">/</span> alpha[t] <span class="op">**</span> <span class="fl">0.5</span></span>
<span id="cb139-9"><a href="#cb139-9"></a>            <span class="op">*</span> (X <span class="op">-</span> beta[t] <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> alpha_cumprod[t]) <span class="op">**</span> <span class="fl">0.5</span> <span class="op">*</span> X_noise)</span>
<span id="cb139-10"><a href="#cb139-10"></a>            <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> alpha[t]) <span class="op">**</span> <span class="fl">0.5</span> <span class="op">*</span> noise</span>
<span id="cb139-11"><a href="#cb139-11"></a>        )</span>
<span id="cb139-12"><a href="#cb139-12"></a>    <span class="cf">return</span> X</span>
<span id="cb139-13"><a href="#cb139-13"></a></span>
<span id="cb139-14"><a href="#cb139-14"></a>X_gen <span class="op">=</span> generate(model)  <span class="co"># generated images</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>t = 1 </code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:3306,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681547540451,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="a5f47d54-98a3-457d-b367-9a6630d000b0">
<div class="sourceCode cell-code" id="cb141"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1"></a>plot_multiple_images(X_gen.numpy(), <span class="dv">8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-55-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>There is no shortcut in the reverse process. <strong>This may take a minute or two. That’s the main drawback of diffusion models: generating images is slow since the model needs to be called many times.</strong> It’s possible to make this faster by using a smaller <code>T</code> value, or by using the same model prediction for several steps at a time, but the resulting images may not look as nice. That said, despite this speed limitation, diffusion models do produce high-quality and diverse images</p>
</section>
<section id="stable-diffusion-with-diffuser" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="stable-diffusion-with-diffuser"><span class="header-section-number">12.6</span> Stable Diffusion with <code>diffuser</code></h2>
<section id="delve-into-stable-difussion" class="level3" data-number="12.6.1">
<h3 data-number="12.6.1" class="anchored" data-anchor-id="delve-into-stable-difussion"><span class="header-section-number">12.6.1</span> Delve into stable difussion</h3>
<p>Stable Diffusion is based on a particular type of diffusion model called <strong>Latent Diffusion</strong>, proposed in <a href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a>. General diffusion models are machine learning systems that are trained to <em>denoise</em> random gaussian noise step by step, to get to a sample of interest, such as an <em>image</em>. For a more detailed overview of how they work, check <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb">this colab</a>.</p>
<p>Diffusion models have shown to achieve state-of-the-art results for generating image data. But one downside of diffusion models is that the reverse denoising process is slow. In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images. Therefore, it is challenging to train these models and also use them for inference.</p>
<p>Latent diffusion can reduce the memory and compute complexity by applying the diffusion process over a lower dimensional <em>latent</em> space, instead of using the actual pixel space. This is the key difference between standard diffusion and latent diffusion models: <strong>in latent diffusion the model is trained to generate latent (compressed) representations of the images.</strong></p>
<p>There are three main components in latent diffusion.</p>
<ol type="1">
<li><p>An autoencoder (VAE): The encoder is used to convert the image into a low dimensional latent representation, which will serve as the input to the <em>U-Net</em> model. The decoder, conversely, transforms the latent representation back into an image. During latent diffusion <em>training</em>, the encoder is used to get the latent representations (<em>latents</em>) of the images for the forward diffusion process, which applies more and more noise at each step. During <em>inference</em>, the denoised latents generated by the reverse diffusion process are converted back into images using the VAE decoder.</p></li>
<li><p>A <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq">U-Net</a>: The U-Net has an encoder part and a decoder part both comprised of ResNet blocks.More specifically, the U-Net output predicts the noise residual which can be used to compute the predicted denoised image representation. To prevent the U-Net from losing important information while downsampling, short-cut connections are usually added between the downsampling ResNets of the encoder to the upsampling ResNets of the decoder. <strong>Additionally, the stable diffusion U-Net is able to condition its output on text-embeddings via cross-attention layers.</strong></p></li>
<li><p>A text-encoder, <em>e.g.</em> <a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel">CLIP’s Text Encoder</a>: The text-encoder is responsible for transforming the input prompt, <em>e.g.</em> “An astronout riding a horse” into an embedding space that can be understood by the U-Net. It is usually a simple <em>transformer-based</em> encoder that maps a sequence of input tokens to a sequence of latent text-embeddings. Stable Diffusion does <strong>not</strong> train the text-encoder during training and simply uses an CLIP’s already trained text encoder, <a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel">CLIPTextModel</a>.</p></li>
</ol>
<p align="center">
<img src="https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png" alt="sd-pipeline" width="500">
</p>
<div data-align="center">
source: https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png
</div>
<p>The stable diffusion model takes both a latent seed and a text prompt as an input. The latent seed is then used to generate random latent image representations of size <span class="math inline">\(64 \times 64\)</span> where as the text prompt is transformed to text embeddings of size <span class="math inline">\(77 \times 768\)</span> via CLIP’s text encoder.</p>
<p>Next the U-Net iteratively <em>denoises</em> the random latent image representations while being conditioned on the text embeddings. The output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm (<span class="math inline">\(\alpha_t\)</span> or <span class="math inline">\(\beta_t\)</span>). Many different scheduler algorithms can be used for this computation, each having its pros and cons. For more information, we recommend looking into <a href="https://arxiv.org/abs/2206.00364">Elucidating the Design Space of Diffusion-Based Generative Models</a></p>
<p>The <em>denoising</em> process is repeated <em>ca.</em> 50 times to step-by-step retrieve better latent image representations. Once complete, the latent image representation is decoded by the decoder part of the VAE.</p>
</section>
<section id="write-your-own-inference-pipeline-with-diffusers" class="level3" data-number="12.6.2">
<h3 data-number="12.6.2" class="anchored" data-anchor-id="write-your-own-inference-pipeline-with-diffusers"><span class="header-section-number">12.6.2</span> Write your own inference pipeline with <code>diffusers</code></h3>
<p>Let’s go through the <code>StableDiffusionPipeline</code> step by step to see how we could have written it ourselves. We will start by loading the individual models involved.</p>
<p>The <a href="https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/main">pre-trained model</a> includes all the components required to setup a complete diffusion pipeline. They are stored in the following folders: - <code>text_encoder</code>: Stable Diffusion uses CLIP, but other diffusion models may use other encoders such as <code>BERT</code>. - <code>tokenizer</code>. It must match the one used by the <code>text_encoder</code> model. - <code>scheduler</code>: The scheduling algorithm used to progressively add noise to the image during training. - <code>unet</code>: The model used to generate the latent representation of the input. - <code>vae</code>: Autoencoder module that we’ll use to decode latent representations into real images.</p>
<p>We can load the components by referring to the folder they were saved, using the <code>subfolder</code> argument to <code>from_pretrained()</code>.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:30120,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681618226801,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="6f0631b0-cb85-49de-d509-790c26c7948d">
<div class="sourceCode cell-code" id="cb142"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1"></a><span class="co"># 1. Load the autoencoder model which will be used to decode the latents into image space. </span></span>
<span id="cb142-2"><a href="#cb142-2"></a>vae <span class="op">=</span> AutoencoderKL.from_pretrained(<span class="st">"CompVis/stable-diffusion-v1-4"</span>, subfolder<span class="op">=</span><span class="st">"vae"</span>)</span>
<span id="cb142-3"><a href="#cb142-3"></a></span>
<span id="cb142-4"><a href="#cb142-4"></a><span class="co"># 2. Load the tokenizer and text encoder to tokenize and encode the text. </span></span>
<span id="cb142-5"><a href="#cb142-5"></a>tokenizer <span class="op">=</span> CLIPTokenizer.from_pretrained(<span class="st">"openai/clip-vit-large-patch14"</span>)</span>
<span id="cb142-6"><a href="#cb142-6"></a>text_encoder <span class="op">=</span> CLIPTextModel.from_pretrained(<span class="st">"openai/clip-vit-large-patch14"</span>)</span>
<span id="cb142-7"><a href="#cb142-7"></a></span>
<span id="cb142-8"><a href="#cb142-8"></a><span class="co"># 3. The UNet model for generating the latents.</span></span>
<span id="cb142-9"><a href="#cb142-9"></a>unet <span class="op">=</span> UNet2DConditionModel.from_pretrained(<span class="st">"CompVis/stable-diffusion-v1-4"</span>, subfolder<span class="op">=</span><span class="st">"unet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'logit_scale', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'text_projection.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias']
- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).</code></pre>
</div>
</div>
<section id="vae-configuration" class="level4" data-number="12.6.2.1">
<h4 data-number="12.6.2.1" class="anchored" data-anchor-id="vae-configuration"><span class="header-section-number">12.6.2.1</span> VAE configuration</h4>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:5,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681618226802,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="8d368f3d-99a2-4a6b-fc4e-a88f9a596ef3">
<div class="sourceCode cell-code" id="cb144"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1"></a>vae.config</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="84">
<pre><code>FrozenDict([('in_channels', 3),
            ('out_channels', 3),
            ('down_block_types',
             ['DownEncoderBlock2D',
              'DownEncoderBlock2D',
              'DownEncoderBlock2D',
              'DownEncoderBlock2D']),
            ('up_block_types',
             ['UpDecoderBlock2D',
              'UpDecoderBlock2D',
              'UpDecoderBlock2D',
              'UpDecoderBlock2D']),
            ('block_out_channels', [128, 256, 512, 512]),
            ('layers_per_block', 2),
            ('act_fn', 'silu'),
            ('latent_channels', 4),
            ('norm_num_groups', 32),
            ('sample_size', 512),
            ('scaling_factor', 0.18215),
            ('_class_name', 'AutoencoderKL'),
            ('_diffusers_version', '0.2.2'),
            ('_name_or_path', 'CompVis/stable-diffusion-v1-4')])</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:802,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681618227601,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="34689c59-c632-4401-c11b-f96b524958ce">
<div class="sourceCode cell-code" id="cb146"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1"></a>summary(vae, (<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">512</span>, <span class="dv">512</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.9/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  action_fn=lambda data: sys.getsizeof(data.storage()),
/usr/local/lib/python3.9/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return super().__sizeof__() + self.nbytes()</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="85">
<pre><code>====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
AutoencoderKL                                      [1, 3, 512, 512]          --
├─Encoder: 1-1                                     [1, 8, 64, 64]            --
│    └─Conv2d: 2-1                                 [1, 128, 512, 512]        3,584
│    └─ModuleList: 2-2                             --                        --
│    │    └─DownEncoderBlock2D: 3-1                [1, 128, 256, 256]        738,944
│    │    └─DownEncoderBlock2D: 3-2                [1, 256, 128, 128]        2,690,304
│    │    └─DownEncoderBlock2D: 3-3                [1, 512, 64, 64]          10,754,560
│    │    └─DownEncoderBlock2D: 3-4                [1, 512, 64, 64]          9,443,328
│    └─UNetMidBlock2D: 2-3                         [1, 512, 64, 64]          --
│    │    └─ModuleList: 3-7                        --                        (recursive)
│    │    └─ModuleList: 3-6                        --                        1,051,648
│    │    └─ModuleList: 3-7                        --                        (recursive)
│    └─GroupNorm: 2-4                              [1, 512, 64, 64]          1,024
│    └─SiLU: 2-5                                   [1, 512, 64, 64]          --
│    └─Conv2d: 2-6                                 [1, 8, 64, 64]            36,872
├─Conv2d: 1-2                                      [1, 8, 64, 64]            72
├─Conv2d: 1-3                                      [1, 4, 64, 64]            20
├─Decoder: 1-4                                     [1, 3, 512, 512]          --
│    └─Conv2d: 2-7                                 [1, 512, 64, 64]          18,944
│    └─UNetMidBlock2D: 2-8                         [1, 512, 64, 64]          --
│    │    └─ModuleList: 3-10                       --                        (recursive)
│    │    └─ModuleList: 3-9                        --                        1,051,648
│    │    └─ModuleList: 3-10                       --                        (recursive)
│    └─ModuleList: 2-9                             --                        --
│    │    └─UpDecoderBlock2D: 3-11                 [1, 512, 128, 128]        16,524,800
│    │    └─UpDecoderBlock2D: 3-12                 [1, 512, 256, 256]        16,524,800
│    │    └─UpDecoderBlock2D: 3-13                 [1, 256, 512, 512]        4,855,296
│    │    └─UpDecoderBlock2D: 3-14                 [1, 128, 512, 512]        1,067,648
│    └─GroupNorm: 2-10                             [1, 128, 512, 512]        256
│    └─SiLU: 2-11                                  [1, 128, 512, 512]        --
│    └─Conv2d: 2-12                                [1, 3, 512, 512]          3,459
====================================================================================================
Total params: 83,653,863
Trainable params: 83,653,863
Non-trainable params: 0
Total mult-adds (T): 1.77
====================================================================================================
Input size (MB): 3.15
Forward/backward pass size (MB): 12640.19
Params size (MB): 334.62
Estimated Total Size (MB): 12977.95
====================================================================================================</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1660,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681618562678,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="887665b0-2eff-414e-f8e3-90fcca78d578">
<div class="sourceCode cell-code" id="cb149"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1"></a>download_from_pokemondb(<span class="st">'https://img.pokemondb.net/sprites/go/normal/dragonite.png'</span>,<span class="st">'dragonite.png'</span>)</span>
<span id="cb149-2"><a href="#cb149-2"></a>im <span class="op">=</span> resize(rgba2rgb(imread(<span class="st">'dragonite.png'</span>)), (<span class="dv">512</span>, <span class="dv">512</span>)).astype(np.float32)</span>
<span id="cb149-3"><a href="#cb149-3"></a>plt.imshow(im)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="95">
<pre><code>&lt;matplotlib.image.AxesImage at 0x7fbcc5b47370&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-59-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1937,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681618572877,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="26163d28-9bc3-400a-ede0-4e9e74686351">
<div class="sourceCode cell-code" id="cb151"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1"></a><span class="co"># Encode to the latent space</span></span>
<span id="cb151-2"><a href="#cb151-2"></a>encoded <span class="op">=</span> pil_to_latent(im)</span>
<span id="cb151-3"><a href="#cb151-3"></a><span class="bu">print</span>(encoded.shape)</span>
<span id="cb151-4"><a href="#cb151-4"></a><span class="co"># Let's visualize the four channels of this latent representation:</span></span>
<span id="cb151-5"><a href="#cb151-5"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">4</span>))</span>
<span id="cb151-6"><a href="#cb151-6"></a><span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb151-7"><a href="#cb151-7"></a>    axs[c].imshow(encoded[<span class="dv">0</span>][c].cpu(), cmap<span class="op">=</span><span class="st">'Greys'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([1, 4, 64, 64])</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-60-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2085,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681618658943,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="80e47b58-736d-4c57-b2a8-f8f372457416">
<div class="sourceCode cell-code" id="cb153"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1"></a><span class="co"># Decode this latent representation back into an image</span></span>
<span id="cb153-2"><a href="#cb153-2"></a>decoded <span class="op">=</span> latents_to_pil(encoded)[<span class="dv">0</span>]</span>
<span id="cb153-3"><a href="#cb153-3"></a>plt.imshow(decoded)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="98">
<pre><code>&lt;matplotlib.image.AxesImage at 0x7fbccc515220&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-61-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>We start with a <code>3x512x512</code> image and it get compressed to a latent vector <code>4x64x64</code>. Each <code>3x8x8</code> pixel volume in the input image gets compressed down to just 4 numbers(<code>4x1x1</code>). This greatly speed up the task!</p>
</section>
<section id="unet-configuration" class="level4" data-number="12.6.2.2">
<h4 data-number="12.6.2.2" class="anchored" data-anchor-id="unet-configuration"><span class="header-section-number">12.6.2.2</span> UNet Configuration</h4>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:14,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681615550999,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="e940f40d-8de6-4067-c4be-cf7643759080">
<div class="sourceCode cell-code" id="cb155"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1"></a>unet.config</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>FrozenDict([('sample_size', 64),
            ('in_channels', 4),
            ('out_channels', 4),
            ('center_input_sample', False),
            ('flip_sin_to_cos', True),
            ('freq_shift', 0),
            ('down_block_types',
             ['CrossAttnDownBlock2D',
              'CrossAttnDownBlock2D',
              'CrossAttnDownBlock2D',
              'DownBlock2D']),
            ('mid_block_type', 'UNetMidBlock2DCrossAttn'),
            ('up_block_types',
             ['UpBlock2D',
              'CrossAttnUpBlock2D',
              'CrossAttnUpBlock2D',
              'CrossAttnUpBlock2D']),
            ('only_cross_attention', False),
            ('block_out_channels', [320, 640, 1280, 1280]),
            ('layers_per_block', 2),
            ('downsample_padding', 1),
            ('mid_block_scale_factor', 1),
            ('act_fn', 'silu'),
            ('norm_num_groups', 32),
            ('norm_eps', 1e-05),
            ('cross_attention_dim', 768),
            ('encoder_hid_dim', None),
            ('attention_head_dim', 8),
            ('dual_cross_attention', False),
            ('use_linear_projection', False),
            ('class_embed_type', None),
            ('num_class_embeds', None),
            ('upcast_attention', False),
            ('resnet_time_scale_shift', 'default'),
            ('resnet_skip_time_act', False),
            ('resnet_out_scale_factor', 1.0),
            ('time_embedding_type', 'positional'),
            ('time_embedding_act_fn', None),
            ('timestep_post_act', None),
            ('time_cond_proj_dim', None),
            ('conv_in_kernel', 3),
            ('conv_out_kernel', 3),
            ('projection_class_embeddings_input_dim', None),
            ('class_embeddings_concat', False),
            ('mid_block_only_cross_attention', None),
            ('cross_attention_norm', None),
            ('_class_name', 'UNet2DConditionModel'),
            ('_diffusers_version', '0.2.2'),
            ('_name_or_path', 'CompVis/stable-diffusion-v1-4')])</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2499,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681615553488,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="ec49cd6b-0adb-4481-ff2f-645c2f3065c5">
<div class="sourceCode cell-code" id="cb157"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1"></a>summary(unet, [(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">64</span>, <span class="dv">64</span>), (<span class="dv">1</span>,), (<span class="dv">1</span>, <span class="dv">77</span>, <span class="dv">768</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>===================================================================================================================
Layer (type:depth-idx)                                            Output Shape              Param #
===================================================================================================================
UNet2DConditionModel                                              [1, 4, 64, 64]            --
├─Timesteps: 1-1                                                  [1, 320]                  --
├─TimestepEmbedding: 1-2                                          [1, 1280]                 --
│    └─Linear: 2-1                                                [1, 1280]                 410,880
│    └─SiLU: 2-2                                                  [1, 1280]                 --
│    └─Linear: 2-3                                                [1, 1280]                 1,639,680
├─Conv2d: 1-3                                                     [1, 320, 64, 64]          11,840
├─ModuleList: 1-4                                                 --                        --
│    └─CrossAttnDownBlock2D: 2-4                                  [1, 320, 32, 32]          --
│    │    └─ModuleList: 3-3                                       --                        (recursive)
│    │    └─ModuleList: 3-4                                       --                        (recursive)
│    │    └─ModuleList: 3-3                                       --                        (recursive)
│    │    └─ModuleList: 3-4                                       --                        (recursive)
│    │    └─ModuleList: 3-5                                       --                        921,920
│    └─CrossAttnDownBlock2D: 2-5                                  [1, 640, 16, 16]          --
│    │    └─ModuleList: 3-8                                       --                        (recursive)
│    │    └─ModuleList: 3-9                                       --                        (recursive)
│    │    └─ModuleList: 3-8                                       --                        (recursive)
│    │    └─ModuleList: 3-9                                       --                        (recursive)
│    │    └─ModuleList: 3-10                                      --                        3,687,040
│    └─CrossAttnDownBlock2D: 2-6                                  [1, 1280, 8, 8]           --
│    │    └─ModuleList: 3-13                                      --                        (recursive)
│    │    └─ModuleList: 3-14                                      --                        (recursive)
│    │    └─ModuleList: 3-13                                      --                        (recursive)
│    │    └─ModuleList: 3-14                                      --                        (recursive)
│    │    └─ModuleList: 3-15                                      --                        14,746,880
│    └─DownBlock2D: 2-7                                           [1, 1280, 8, 8]           --
│    │    └─ModuleList: 3-16                                      --                        62,277,120
├─UNetMidBlock2DCrossAttn: 1-5                                    [1, 1280, 8, 8]           --
│    └─ModuleList: 2-10                                           --                        (recursive)
│    │    └─ResnetBlock2D: 3-17                                   [1, 1280, 8, 8]           31,138,560
│    └─ModuleList: 2-9                                            --                        --
│    │    └─Transformer2DModel: 3-18                              [1, 1280, 8, 8]           34,760,960
│    └─ModuleList: 2-10                                           --                        (recursive)
│    │    └─ResnetBlock2D: 3-19                                   [1, 1280, 8, 8]           31,138,560
├─ModuleList: 1-6                                                 --                        --
│    └─UpBlock2D: 2-11                                            [1, 1280, 16, 16]         --
│    │    └─ModuleList: 3-20                                      --                        147,494,400
│    │    └─ModuleList: 3-21                                      --                        14,746,880
│    └─CrossAttnUpBlock2D: 2-12                                   [1, 1280, 32, 32]         --
│    │    └─ModuleList: 3-26                                      --                        (recursive)
│    │    └─ModuleList: 3-27                                      --                        (recursive)
│    │    └─ModuleList: 3-26                                      --                        (recursive)
│    │    └─ModuleList: 3-27                                      --                        (recursive)
│    │    └─ModuleList: 3-26                                      --                        (recursive)
│    │    └─ModuleList: 3-27                                      --                        (recursive)
│    │    └─ModuleList: 3-28                                      --                        14,746,880
│    └─CrossAttnUpBlock2D: 2-13                                   [1, 640, 64, 64]          --
│    │    └─ModuleList: 3-33                                      --                        (recursive)
│    │    └─ModuleList: 3-34                                      --                        (recursive)
│    │    └─ModuleList: 3-33                                      --                        (recursive)
│    │    └─ModuleList: 3-34                                      --                        (recursive)
│    │    └─ModuleList: 3-33                                      --                        (recursive)
│    │    └─ModuleList: 3-34                                      --                        (recursive)
│    │    └─ModuleList: 3-35                                      --                        3,687,040
│    └─CrossAttnUpBlock2D: 2-14                                   [1, 320, 64, 64]          --
│    │    └─ModuleList: 3-40                                      --                        (recursive)
│    │    └─ModuleList: 3-41                                      --                        (recursive)
│    │    └─ModuleList: 3-40                                      --                        (recursive)
│    │    └─ModuleList: 3-41                                      --                        (recursive)
│    │    └─ModuleList: 3-40                                      --                        (recursive)
│    │    └─ModuleList: 3-41                                      --                        (recursive)
├─GroupNorm: 1-7                                                  [1, 320, 64, 64]          640
├─SiLU: 1-8                                                       [1, 320, 64, 64]          --
├─Conv2d: 1-9                                                     [1, 4, 64, 64]            11,524
===================================================================================================================
Total params: 859,520,964
Trainable params: 859,520,964
Non-trainable params: 0
Total mult-adds (G): 222.30
===================================================================================================================
Input size (MB): 0.30
Forward/backward pass size (MB): 2530.96
Params size (MB): 3438.08
Estimated Total Size (MB): 5969.35
===================================================================================================================</code></pre>
</div>
</div>
</section>
<section id="schedulers" class="level4" data-number="12.6.2.3">
<h4 data-number="12.6.2.3" class="anchored" data-anchor-id="schedulers"><span class="header-section-number">12.6.2.3</span> <a href="https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers">Schedulers</a></h4>
<p>Now instead of loading the pre-defined scheduler, we’ll use the K-LMS scheduler instead.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb159"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb159-1"><a href="#cb159-1"></a>scheduler <span class="op">=</span> LMSDiscreteScheduler.from_pretrained(<span class="st">"CompVis/stable-diffusion-v1-4"</span>, subfolder<span class="op">=</span><span class="st">"scheduler"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681618881871,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="3411383c-518e-428d-80ef-7c9678d21a77">
<div class="sourceCode cell-code" id="cb160"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb160-1"><a href="#cb160-1"></a>scheduler.config</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="100">
<pre><code>FrozenDict([('num_train_timesteps', 1000),
            ('beta_start', 0.00085),
            ('beta_end', 0.012),
            ('beta_schedule', 'scaled_linear'),
            ('trained_betas', None),
            ('prediction_type', 'epsilon'),
            ('_class_name', 'PNDMScheduler'),
            ('_diffusers_version', '0.7.0.dev0'),
            ('set_alpha_to_one', False),
            ('skip_prk_steps', True),
            ('steps_offset', 1),
            ('clip_sample', False)])</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1119,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681618923754,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="7db04c21-28c4-41c4-e8db-e1992211359f">
<div class="sourceCode cell-code" id="cb162"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1"></a><span class="co"># Plotting this noise schedule:</span></span>
<span id="cb162-2"><a href="#cb162-2"></a>plt.plot(scheduler.sigmas)</span>
<span id="cb162-3"><a href="#cb162-3"></a>plt.title(<span class="st">'Noise Schedule'</span>)</span>
<span id="cb162-4"><a href="#cb162-4"></a>plt.xlabel(<span class="st">'Sampling step'</span>)</span>
<span id="cb162-5"><a href="#cb162-5"></a>plt.ylabel(<span class="st">'sigma'</span>)</span>
<span id="cb162-6"><a href="#cb162-6"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-66-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This ‘sigma’ is the amount of noise added to the latent representation.</p>
</section>
</section>
<section id="actual-inference" class="level3" data-number="12.6.3">
<h3 data-number="12.6.3" class="anchored" data-anchor-id="actual-inference"><span class="header-section-number">12.6.3</span> Actual inference</h3>
<p>Next we move the models to the GPU.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb163"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb163-1"><a href="#cb163-1"></a>torch_device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb163-2"><a href="#cb163-2"></a></span>
<span id="cb163-3"><a href="#cb163-3"></a>vae <span class="op">=</span> vae.to(torch_device)</span>
<span id="cb163-4"><a href="#cb163-4"></a>text_encoder <span class="op">=</span> text_encoder.to(torch_device)</span>
<span id="cb163-5"><a href="#cb163-5"></a>unet <span class="op">=</span> unet.to(torch_device) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We now define the parameters we’ll use to generate images.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb164"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb164-1"><a href="#cb164-1"></a>prompt <span class="op">=</span> [<span class="st">"photograph of an astronaut riding a horse"</span>]</span>
<span id="cb164-2"><a href="#cb164-2"></a></span>
<span id="cb164-3"><a href="#cb164-3"></a>height <span class="op">=</span> <span class="dv">512</span>                        <span class="co"># default height of Stable Diffusion</span></span>
<span id="cb164-4"><a href="#cb164-4"></a>width <span class="op">=</span> <span class="dv">512</span>                         <span class="co"># default width of Stable Diffusion</span></span>
<span id="cb164-5"><a href="#cb164-5"></a>num_inference_steps <span class="op">=</span> <span class="dv">100</span>            <span class="co"># Number of denoising steps</span></span>
<span id="cb164-6"><a href="#cb164-6"></a>guidance_scale <span class="op">=</span> <span class="fl">7.5</span>                <span class="co"># Scale for classifier-free guidance</span></span>
<span id="cb164-7"><a href="#cb164-7"></a>generator <span class="op">=</span> torch.manual_seed(<span class="dv">32</span>)   <span class="co"># Seed generator to create the inital latent noise</span></span>
<span id="cb164-8"><a href="#cb164-8"></a></span>
<span id="cb164-9"><a href="#cb164-9"></a>batch_size <span class="op">=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>guidance_scale</code> is a way to increase the adherence to the text as well as overall sample quality. In simple terms it force generation to better match with the prompt. Numbers like <code>7</code> or <code>8.5</code> give good results, if you use a very large number the images might look good, but will be less diverse.</p>
<section id="text-embedding" class="level4" data-number="12.6.3.1">
<h4 data-number="12.6.3.1" class="anchored" data-anchor-id="text-embedding"><span class="header-section-number">12.6.3.1</span> Text embedding</h4>
<p>First, we get the <code>text_embeddings</code> for the prompt. These embeddings will be used to condition the UNet model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb165"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb165-1"><a href="#cb165-1"></a>text_input <span class="op">=</span> tokenizer(prompt, padding<span class="op">=</span><span class="st">"max_length"</span>, max_length<span class="op">=</span>tokenizer.model_max_length, truncation<span class="op">=</span><span class="va">True</span>, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb165-2"><a href="#cb165-2"></a></span>
<span id="cb165-3"><a href="#cb165-3"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb165-4"><a href="#cb165-4"></a>  text_embeddings <span class="op">=</span> text_encoder(text_input.input_ids.to(torch_device))[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll also get the unconditional text embeddings, which are just the embeddings for the padding token (empty text). They need to have the same shape as the conditional <code>text_embeddings</code> (<code>batch_size</code> and <code>seq_length</code>)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb166"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb166-1"><a href="#cb166-1"></a>max_length <span class="op">=</span> text_input.input_ids.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb166-2"><a href="#cb166-2"></a>uncond_input <span class="op">=</span> tokenizer([<span class="st">""</span>] <span class="op">*</span> batch_size, padding<span class="op">=</span><span class="st">"max_length"</span>, max_length<span class="op">=</span>max_length, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb166-3"><a href="#cb166-3"></a></span>
<span id="cb166-4"><a href="#cb166-4"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb166-5"><a href="#cb166-5"></a>  uncond_embeddings <span class="op">=</span> text_encoder(uncond_input.input_ids.to(torch_device))[<span class="dv">0</span>]   </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb167"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb167-1"><a href="#cb167-1"></a>text_embeddings <span class="op">=</span> torch.cat([uncond_embeddings, text_embeddings]) <span class="co"># Concatenate together</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>See <a href="https://betterprogramming.pub/diffusion-models-ddpms-ddims-and-classifier-free-guidance-e07b297b2869">classifier-free guidence</a> for more detail.</p>
</section>
<section id="diffussion-models" class="level4" data-number="12.6.3.2">
<h4 data-number="12.6.3.2" class="anchored" data-anchor-id="diffussion-models"><span class="header-section-number">12.6.3.2</span> Diffussion models</h4>
<p>Generate the intial random noise.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb168"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1"></a>latents <span class="op">=</span> torch.randn(</span>
<span id="cb168-2"><a href="#cb168-2"></a>  (batch_size, unet.in_channels, height <span class="op">//</span> <span class="dv">8</span>, width <span class="op">//</span> <span class="dv">8</span>),</span>
<span id="cb168-3"><a href="#cb168-3"></a>  generator<span class="op">=</span>generator,</span>
<span id="cb168-4"><a href="#cb168-4"></a>)</span>
<span id="cb168-5"><a href="#cb168-5"></a>latents <span class="op">=</span> latents.to(torch_device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we initialize the scheduler with our chosen <code>num_inference_steps</code>. This will compute the <code>sigmas</code> (similar to <span class="math inline">\(\alpha_t\)</span>) and exact time step values to be used during the denoising process.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681616364183,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="1583c684-ba0d-458e-874b-dc8f518b5817">
<div class="sourceCode cell-code" id="cb169"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb169-1"><a href="#cb169-1"></a>scheduler.set_timesteps(num_inference_steps)</span>
<span id="cb169-2"><a href="#cb169-2"></a></span>
<span id="cb169-3"><a href="#cb169-3"></a><span class="co"># The scheduler need to scale the latent</span></span>
<span id="cb169-4"><a href="#cb169-4"></a>latents <span class="op">=</span> latents <span class="op">*</span> scheduler.init_noise_sigma</span>
<span id="cb169-5"><a href="#cb169-5"></a>latents.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>torch.Size([1, 4, 64, 64])</code></pre>
</div>
</div>
<p>We are ready to write the denoising loop:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:58538,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681616424401,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="86a5d782-ca0e-42c3-d1c5-d58df0387a91">
<div class="sourceCode cell-code" id="cb171"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb171-1"><a href="#cb171-1"></a><span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(tqdm(scheduler.timesteps)):</span>
<span id="cb171-2"><a href="#cb171-2"></a>  <span class="co"># expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.</span></span>
<span id="cb171-3"><a href="#cb171-3"></a>  latent_model_input <span class="op">=</span> torch.cat([latents] <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb171-4"><a href="#cb171-4"></a></span>
<span id="cb171-5"><a href="#cb171-5"></a>  latent_model_input <span class="op">=</span> scheduler.scale_model_input(latent_model_input, t)</span>
<span id="cb171-6"><a href="#cb171-6"></a></span>
<span id="cb171-7"><a href="#cb171-7"></a>  <span class="co"># predict the noise residual</span></span>
<span id="cb171-8"><a href="#cb171-8"></a>  <span class="cf">with</span> torch.no_grad():</span>
<span id="cb171-9"><a href="#cb171-9"></a>    noise_pred <span class="op">=</span> unet(latent_model_input, t, encoder_hidden_states<span class="op">=</span>text_embeddings).sample</span>
<span id="cb171-10"><a href="#cb171-10"></a></span>
<span id="cb171-11"><a href="#cb171-11"></a>  <span class="co"># perform guidance</span></span>
<span id="cb171-12"><a href="#cb171-12"></a>  noise_pred_uncond, noise_pred_text <span class="op">=</span> noise_pred.chunk(<span class="dv">2</span>) <span class="co"># We have two samples here</span></span>
<span id="cb171-13"><a href="#cb171-13"></a>  noise_pred <span class="op">=</span> noise_pred_uncond <span class="op">+</span> guidance_scale <span class="op">*</span> (noise_pred_text <span class="op">-</span> noise_pred_uncond)</span>
<span id="cb171-14"><a href="#cb171-14"></a></span>
<span id="cb171-15"><a href="#cb171-15"></a>  <span class="co"># compute the previous noisy sample x_t -&gt; x_t-1</span></span>
<span id="cb171-16"><a href="#cb171-16"></a>  latents <span class="op">=</span> scheduler.step(noise_pred, t, latents).prev_sample</span>
<span id="cb171-17"><a href="#cb171-17"></a></span>
<span id="cb171-18"><a href="#cb171-18"></a>  <span class="co"># 3. optionally look at image</span></span>
<span id="cb171-19"><a href="#cb171-19"></a>  <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb171-20"><a href="#cb171-20"></a>      latents_viz <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> <span class="fl">0.18215</span> <span class="op">*</span> latents</span>
<span id="cb171-21"><a href="#cb171-21"></a>      <span class="cf">with</span> torch.no_grad():</span>
<span id="cb171-22"><a href="#cb171-22"></a>        image <span class="op">=</span> vae.decode(latents_viz).sample</span>
<span id="cb171-23"><a href="#cb171-23"></a>      image <span class="op">=</span> (image <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="fl">0.5</span>).clamp(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb171-24"><a href="#cb171-24"></a>      image <span class="op">=</span> image.detach().cpu().permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>).numpy()</span>
<span id="cb171-25"><a href="#cb171-25"></a>      images <span class="op">=</span> (image <span class="op">*</span> <span class="dv">255</span>).<span class="bu">round</span>().astype(<span class="st">"uint8"</span>)</span>
<span id="cb171-26"><a href="#cb171-26"></a>      plt.imshow(images.squeeze())</span>
<span id="cb171-27"><a href="#cb171-27"></a>      plt.axis(<span class="st">"off"</span>)</span>
<span id="cb171-28"><a href="#cb171-28"></a>      plt.show()</span>
<span id="cb171-29"><a href="#cb171-29"></a>      <span class="kw">del</span> image, latents_viz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Output hidden; open in https://colab.research.google.com to view.</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb173"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb173-1"><a href="#cb173-1"></a>noise_pred_text <span class="op">=</span> noise_pred.chunk(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:89,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681616424403,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="a00567db-4d2b-4a0a-e677-52d73be4f114">
<div class="sourceCode cell-code" id="cb174"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb174-1"><a href="#cb174-1"></a>noise_pred_text[<span class="dv">0</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>torch.Size([1, 4, 64, 64])</code></pre>
</div>
</div>
<p>All schedulers provide one or multiple <code>step()</code> methods that can be used to compute the slightly less noisy image. The <code>step()</code> method may vary from one scheduler to another, but normally expects at least the model output, the <code>timestep</code> and the current <code>noisy_sample</code>.</p>
</section>
<section id="vae" class="level4" data-number="12.6.3.3">
<h4 data-number="12.6.3.3" class="anchored" data-anchor-id="vae"><span class="header-section-number">12.6.3.3</span> VAE</h4>
<p>We now use the <code>vae</code> to decode the generated <code>latents</code> back into the image.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb176"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb176-1"><a href="#cb176-1"></a><span class="co"># scale and decode the image latents with vae</span></span>
<span id="cb176-2"><a href="#cb176-2"></a>latents <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> <span class="fl">0.18215</span> <span class="op">*</span> latents</span>
<span id="cb176-3"><a href="#cb176-3"></a></span>
<span id="cb176-4"><a href="#cb176-4"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb176-5"><a href="#cb176-5"></a>  image <span class="op">=</span> vae.decode(latents).sample</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb177"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb177-1"><a href="#cb177-1"></a>image <span class="op">=</span> (image <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="fl">0.5</span>).clamp(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb177-2"><a href="#cb177-2"></a>image <span class="op">=</span> image.detach().cpu().permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>).numpy()</span>
<span id="cb177-3"><a href="#cb177-3"></a>images <span class="op">=</span> (image <span class="op">*</span> <span class="dv">255</span>).<span class="bu">round</span>().astype(<span class="st">"uint8"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:6,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681616425693,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="6481e0a0-9bd1-4c3d-e599-f59bdb24f6ea">
<div class="sourceCode cell-code" id="cb178"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb178-1"><a href="#cb178-1"></a>plt.imshow(images.squeeze())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre><code>&lt;matplotlib.image.AxesImage at 0x7fbd19ed9190&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-79-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>See <a href="https://github.com/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb">here</a> and <a href="https://github.com/fastai/diffusion-nbs/blob/master/stable_diffusion.ipynb">here</a> for more in depth discussion. If you are interested in training, see <a href="https://github.com/huggingface/diffusers/tree/main/examples">here</a>.</p>
</section>
</section>
<section id="stable-diffusion-with-pipeline-of-difusser" class="level3" data-number="12.6.4">
<h3 data-number="12.6.4" class="anchored" data-anchor-id="stable-diffusion-with-pipeline-of-difusser"><span class="header-section-number">12.6.4</span> Stable diffusion with Pipeline of <code>difusser</code></h3>
<p>There are many built-in pipelines <a href="https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines">available</a>.</p>
<p><code>StableDiffusionPipeline</code> is an end-to-end inference pipeline that you can use to generate images from text with just a few lines of code. First, we load the pre-trained weights of all components of the model. We use Stable Diffusion version 1.4 (<a href="https://huggingface.co/CompVis/stable-diffusion-v1-4">CompVis/stable-diffusion-v1-4</a>), but there are other variants that you may want to try:</p>
<ul>
<li><p><a href="https://huggingface.co/runwayml/stable-diffusion-v1-5">runwayml/stable-diffusion-v1-5</a></p></li>
<li><p><a href="https://huggingface.co/stabilityai/stable-diffusion-2-1-base">stabilityai/stable-diffusion-2-1-base</a></p></li>
<li><p><a href="https://huggingface.co/stabilityai/stable-diffusion-2-1">stabilityai/stable-diffusion-2-1</a>. This version can produce images with a resolution of <span class="math inline">\(768 \times 768\)</span>, while the others work at <span class="math inline">\(512 \times 512\)</span>.</p></li>
</ul>
<p>In addition to the model id <a href="https://huggingface.co/CompVis/stable-diffusion-v1-4">CompVis/stable-diffusion-v1-4</a>, we’re also passing a specific <code>revision</code> and <code>torch_dtype</code> to the <code>from_pretrained()</code> method.</p>
<p>We’re loading the weights from the half-precision branch <a href="https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/fp16"><code>fp16</code></a> and also tell <code>diffusers</code> to expect the weights in float16 precision by passing <code>torch_dtype=torch.float16</code>. If you want to ensure the highest possible precision, please make sure to remove <code>torch_dtype=torch.float16</code> at the cost of a higher memory usage.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:30440,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681612944424,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="02b9d247-ad70-4678-8648-2e70691d1f8b">
<div class="sourceCode cell-code" id="cb180"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb180-1"><a href="#cb180-1"></a>pipe <span class="op">=</span> StableDiffusionPipeline.from_pretrained(<span class="st">"CompVis/stable-diffusion-v1-4"</span>, torch_dtype<span class="op">=</span>torch.float16)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.9/dist-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.
  warnings.warn(
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:362,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681612958089,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="ba5e8902-45a1-481d-da4f-d0b007c82683">
<div class="sourceCode cell-code" id="cb182"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb182-1"><a href="#cb182-1"></a>pipe <span class="op">=</span> pipe.to(<span class="st">"cuda"</span>) <span class="co"># let's move the pipeline to GPU to have faster inference.</span></span>
<span id="cb182-2"><a href="#cb182-2"></a>pipe</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>StableDiffusionPipeline {
  "_class_name": "StableDiffusionPipeline",
  "_diffusers_version": "0.15.0",
  "feature_extractor": [
    "transformers",
    "CLIPFeatureExtractor"
  ],
  "requires_safety_checker": true,
  "safety_checker": [
    "stable_diffusion",
    "StableDiffusionSafetyChecker"
  ],
  "scheduler": [
    "diffusers",
    "PNDMScheduler"
  ],
  "text_encoder": [
    "transformers",
    "CLIPTextModel"
  ],
  "tokenizer": [
    "transformers",
    "CLIPTokenizer"
  ],
  "unet": [
    "diffusers",
    "UNet2DConditionModel"
  ],
  "vae": [
    "diffusers",
    "AutoencoderKL"
  ]
}</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:24199,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681609800453,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="81bad211-cc8b-47c8-a609-fbd458fcb0fc">
<div class="sourceCode cell-code" id="cb184"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb184-1"><a href="#cb184-1"></a>prompt <span class="op">=</span> <span class="st">"photograph of an astronaut riding a horse"</span></span>
<span id="cb184-2"><a href="#cb184-2"></a>image <span class="op">=</span> pipe(prompt).images[<span class="dv">0</span>]  <span class="co"># image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)</span></span>
<span id="cb184-3"><a href="#cb184-3"></a></span>
<span id="cb184-4"><a href="#cb184-4"></a><span class="co"># If you're in a google colab you can directly display it with </span></span>
<span id="cb184-5"><a href="#cb184-5"></a>image</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"53a3df0150ad41eaa97551d2cd7433e6","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<p><img src="12_Representation_learning_files/figure-html/cell-82-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>You can change the number of inference steps using the <code>num_inference_steps</code> argument. In general, results are better the more steps you use. If you want faster results you can use a smaller number. If you want deterministic output you can pass a random seed to the pipeline. Every time you use the same seed you’ll have the same image result.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:7159,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681610005060,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="37494d82-fe53-421f-bc1d-0ee6a77ab543">
<div class="sourceCode cell-code" id="cb185"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb185-1"><a href="#cb185-1"></a>generator <span class="op">=</span> torch.Generator(<span class="st">"cuda"</span>).manual_seed(<span class="dv">1024</span>)</span>
<span id="cb185-2"><a href="#cb185-2"></a>image <span class="op">=</span> pipe(prompt, num_inference_steps<span class="op">=</span><span class="dv">35</span>, generator<span class="op">=</span>generator).images[<span class="dv">0</span>]</span>
<span id="cb185-3"><a href="#cb185-3"></a></span>
<span id="cb185-4"><a href="#cb185-4"></a>image</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"85347eac2c644e9aa539eb1a48be7b86","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="8">
<p><img src="12_Representation_learning_files/figure-html/cell-83-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>To generate multiple images for the same prompt, we simply use a list with the same prompt repeated several times. We’ll send the list to the pipeline instead of the string we used before.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:21758,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681610117795,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="17b8ca4e-3388-45dd-9653-3ceac9f31fb0">
<div class="sourceCode cell-code" id="cb186"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb186-1"><a href="#cb186-1"></a>num_images <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb186-2"><a href="#cb186-2"></a>prompt <span class="op">=</span> [<span class="st">"photograph of an astronaut riding a horse"</span>] <span class="op">*</span> num_images</span>
<span id="cb186-3"><a href="#cb186-3"></a></span>
<span id="cb186-4"><a href="#cb186-4"></a>images <span class="op">=</span> pipe(prompt).images</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5f380f2b03764f518f44ad491b008a33","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:6666,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681610186582,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="e3244c58-c0a4-4210-d4f3-4de147a3aff9">
<div class="sourceCode cell-code" id="cb187"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb187-1"><a href="#cb187-1"></a>plot_images(images)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-85-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>See <a href="https://github.com/huggingface/diffusers/tree/main/examples">here</a> and <a href="https://github.com/huggingface/notebooks/tree/main/diffusers">here</a> for more examples or training.</p>
</section>
</section>
<section id="high-performance-image-generation-using-stable-diffusion-in-kerascv" class="level2" data-number="12.7">
<h2 data-number="12.7" class="anchored" data-anchor-id="high-performance-image-generation-using-stable-diffusion-in-kerascv"><span class="header-section-number">12.7</span> High-performance image generation using Stable Diffusion in <code>KerasCV</code></h2>
<p>In this guide, we will show how to generate novel images based on a text prompt using the <code>KerasCV</code> implementation of text-to-image model, Stable Diffusion.</p>
<p>Stable Diffusion is a powerful, open-source text-to-image generation model. While there exist multiple open-source implementations that allow you to easily create images from textual prompts, <code>KerasCV</code>’s offers a few distinct advantages. These include XLA compilation and mixed precision support, which together achieve state-of-the-art generation speed.</p>
<p>In this guide, we will explore <code>KerasCV</code>’s Stable Diffusion implementation, show how to use these powerful performance boosts, and explore the performance benefits that they offer.</p>
<p>First, we construct a model:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:261,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681547751143,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="718dc07f-8c3e-47e0-f7c2-6e8889c4a979">
<div class="sourceCode cell-code" id="cb188"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb188-1"><a href="#cb188-1"></a>model <span class="op">=</span> keras_cv.models.StableDiffusion(img_width<span class="op">=</span><span class="dv">512</span>, img_height<span class="op">=</span><span class="dv">512</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE</code></pre>
</div>
</div>
<p>Next, we give it a prompt:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:327945,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681548114078,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="1a749c52-0f45-4458-a325-35ba8cdd6a98">
<div class="sourceCode cell-code" id="cb190"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb190-1"><a href="#cb190-1"></a>images <span class="op">=</span> model.text_to_image(<span class="st">"photograph of an astronaut riding a horse"</span>, batch_size<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb190-2"><a href="#cb190-2"></a></span>
<span id="cb190-3"><a href="#cb190-3"></a>plot_images(images)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading data from https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true
1356917/1356917 [==============================] - 0s 0us/step
Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_encoder.h5
492466864/492466864 [==============================] - 5s 0us/step
Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_diffusion_model.h5
3439090152/3439090152 [==============================] - 53s 0us/step
50/50 [==============================] - 275s 3s/step
Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_decoder.h5
198180272/198180272 [==============================] - 2s 0us/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-87-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>But that’s not all this model can do. Let’s try a more complex prompt:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:160301,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681548274377,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="5a1d2a30-cdfd-4292-dc08-3252ffb7b0dc">
<div class="sourceCode cell-code" id="cb192"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb192-1"><a href="#cb192-1"></a>images <span class="op">=</span> model.text_to_image(</span>
<span id="cb192-2"><a href="#cb192-2"></a>    <span class="st">"cute magical flying dog, fantasy art, "</span></span>
<span id="cb192-3"><a href="#cb192-3"></a>    <span class="st">"golden color, high quality, highly detailed, elegant, sharp focus, "</span></span>
<span id="cb192-4"><a href="#cb192-4"></a>    <span class="st">"concept art, character concepts, digital painting, mystery, adventure"</span>,</span>
<span id="cb192-5"><a href="#cb192-5"></a>    batch_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb192-6"><a href="#cb192-6"></a>)</span>
<span id="cb192-7"><a href="#cb192-7"></a>plot_images(images)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>50/50 [==============================] - 157s 3s/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-88-output-2.png" class="img-fluid"></p>
</div>
</div>
<section id="perks-of-kerascv" class="level3" data-number="12.7.1">
<h3 data-number="12.7.1" class="anchored" data-anchor-id="perks-of-kerascv"><span class="header-section-number">12.7.1</span> Perks of KerasCV</h3>
<p>With several implementations of Stable Diffusion publicly available why should you use <code>keras_cv.models.StableDiffusion</code>?</p>
<p>Aside from the easy-to-use API, KerasCV’s Stable Diffusion model comes with some powerful advantages, including:</p>
<ul>
<li>Graph mode execution</li>
<li>XLA compilation through <code>jit_compile=True</code></li>
<li>Support for mixed precision computation</li>
</ul>
<p>When these are combined, the <code>KerasCV</code> Stable Diffusion model runs orders of magnitude faster than naive implementations. This section shows how to enable all of these features, and the resulting performance gain yielded from using them.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:768573,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1684716659212,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="e5954b5c-9541-4a3a-e468-13ad27c10746">
<div class="sourceCode cell-code" id="cb194"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb194-1"><a href="#cb194-1"></a>tf.keras.mixed_precision.set_global_policy(<span class="st">"mixed_float16"</span>)</span>
<span id="cb194-2"><a href="#cb194-2"></a>model <span class="op">=</span> keras_cv.models.StableDiffusion(jit_compile<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb194-3"><a href="#cb194-3"></a></span>
<span id="cb194-4"><a href="#cb194-4"></a><span class="co"># Let's make sure to warm up the model</span></span>
<span id="cb194-5"><a href="#cb194-5"></a>images <span class="op">=</span> model.text_to_image(</span>
<span id="cb194-6"><a href="#cb194-6"></a>    <span class="st">"Teddy bears conducting machine learning research"</span>,</span>
<span id="cb194-7"><a href="#cb194-7"></a>    batch_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb194-8"><a href="#cb194-8"></a>)</span>
<span id="cb194-9"><a href="#cb194-9"></a>plot_images(images)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE
50/50 [==============================] - 543s 640ms/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-89-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:40103,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1681549167152,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="ac8c53d0-da8f-4eb9-d545-829b02b07be6">
<div class="sourceCode cell-code" id="cb196"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb196-1"><a href="#cb196-1"></a>images <span class="op">=</span> model.text_to_image(</span>
<span id="cb196-2"><a href="#cb196-2"></a>    <span class="st">"Sunset on the beach and mountain"</span>,</span>
<span id="cb196-3"><a href="#cb196-3"></a>    batch_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb196-4"><a href="#cb196-4"></a>)</span>
<span id="cb196-5"><a href="#cb196-5"></a>plot_images(images)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>50/50 [==============================] - 37s 750ms/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_Representation_learning_files/figure-html/cell-90-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="references" class="level1" data-number="13">
<h1 data-number="13"><span class="header-section-number">13</span> References</h1>
<ol type="1">
<li><p><a href="https://github.com/ageron/handson-ml3/">https://github.com/ageron/handson-ml3/</a></p></li>
<li><p><a href="https://github.com/fchollet/deep-learning-with-python-notebooks_">https://github.com/fchollet/deep-learning-with-python-notebooks_</a></p></li>
<li><p><a href="https://github.com/probml/pml-book/tree/main">https://github.com/probml/pml-book/tree/main</a></p></li>
<li><p><a href="https://github.com/huggingface/notebooks/tree/main/diffusers">https://github.com/huggingface/notebooks/tree/main/diffusers</a></p></li>
<li><p><a href="https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/">https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/</a></p></li>
</ol>


</section>

</main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"035e7280c58f4ae69c3ee13cb147aef1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10a10eba147045d9931a739d181b71cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bb4343c14f24a7b9c221f45dd719835","placeholder":"​","style":"IPY_MODEL_8fd971f2925b454092af66f152cf709b","value":" 35/35 [00:05&lt;00:00,  6.80it/s]"}},"1909c93d940243a8ab77ca6041deb09e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_847e86898ce54203b5e69cd0b7770f86","placeholder":"​","style":"IPY_MODEL_035e7280c58f4ae69c3ee13cb147aef1","value":" 100/100 [00:57&lt;00:00,  1.20it/s]"}},"267c374d515b475eb8773d02e4414c77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"32c1861993ea4b209ad56baa4c55566c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32eefaf9ced941878bb0d60bf9982200":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e859501ff92e4ebba4d450f282775355","placeholder":"​","style":"IPY_MODEL_dc25ec76ad8d44c0bd3cc2ca224b5b29","value":"100%"}},"40430c86c6234a648b6fcd8676a594fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a860ef2c6794081bcb3a3935ae7554c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ebd0ffba6f244c798a67cc9a1dcd56e","placeholder":"​","style":"IPY_MODEL_8cea5c0111594fd181d3cd69a4b7697f","value":"100%"}},"53a3df0150ad41eaa97551d2cd7433e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a860ef2c6794081bcb3a3935ae7554c","IPY_MODEL_9a4dcb7b01e444a3a2d8be5ba504966b","IPY_MODEL_60863cc19e5b4fc6a3f50b9829b90b52"],"layout":"IPY_MODEL_e0d98a6008f0439ea69192b87308fe56"}},"5a3f35c53b75465abe4aa0a0e66f6f36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_976f8e2173d0408b97956423cc70ebbc","max":35,"min":0,"orientation":"horizontal","style":"IPY_MODEL_267c374d515b475eb8773d02e4414c77","value":35}},"5f380f2b03764f518f44ad491b008a33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7741f7eb93dd4b1c89ae7c68c2128e7b","IPY_MODEL_c9a30a8efbbb40ca846b2f0cb1ecf3fc","IPY_MODEL_7c9a0323d4d64593a233909cb428244a"],"layout":"IPY_MODEL_7e95faf814254692ba0fd5a6b85deb86"}},"60863cc19e5b4fc6a3f50b9829b90b52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40430c86c6234a648b6fcd8676a594fd","placeholder":"​","style":"IPY_MODEL_a78a79fc322a45c0a06d2abd31842848","value":" 50/50 [00:10&lt;00:00,  7.25it/s]"}},"69483a03d90a473aa4ba5a50246c4aef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7741f7eb93dd4b1c89ae7c68c2128e7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69483a03d90a473aa4ba5a50246c4aef","placeholder":"​","style":"IPY_MODEL_c330ae8a9e8e4c1180c846940a27d3ae","value":"100%"}},"7b9469561a3b49a78348c76ddcacc8a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32eefaf9ced941878bb0d60bf9982200","IPY_MODEL_c769a293243d4f3a88176cdaea411953","IPY_MODEL_1909c93d940243a8ab77ca6041deb09e"],"layout":"IPY_MODEL_ac9ee67483ed427791a16a6d7c52a8b1"}},"7bb4343c14f24a7b9c221f45dd719835":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c9a0323d4d64593a233909cb428244a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfd3708df0f845b0a8b39332cc4260f4","placeholder":"​","style":"IPY_MODEL_a9e20f31adc64c58ac188d89b29d344e","value":" 50/50 [00:20&lt;00:00,  2.48it/s]"}},"7d381948c24b42ea9613218a09d391f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e95faf814254692ba0fd5a6b85deb86":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"847e86898ce54203b5e69cd0b7770f86":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85347eac2c644e9aa539eb1a48be7b86":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4fe970b52b4489eaf6688a576b2c37f","IPY_MODEL_5a3f35c53b75465abe4aa0a0e66f6f36","IPY_MODEL_10a10eba147045d9931a739d181b71cc"],"layout":"IPY_MODEL_b7ece4ffb7a34e90847544bc056ea41c"}},"8cea5c0111594fd181d3cd69a4b7697f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8fd971f2925b454092af66f152cf709b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"976f8e2173d0408b97956423cc70ebbc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a4dcb7b01e444a3a2d8be5ba504966b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d381948c24b42ea9613218a09d391f0","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fd8952b9b2494c5fa1ca7aafc63b8b24","value":50}},"9bc78ee728754889af6cc2076158ff18":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ebd0ffba6f244c798a67cc9a1dcd56e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a78a79fc322a45c0a06d2abd31842848":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9e20f31adc64c58ac188d89b29d344e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac9ee67483ed427791a16a6d7c52a8b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7ece4ffb7a34e90847544bc056ea41c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c06ebb1adc484affa3a0928b14dc818d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1fb50b2ff6d4ae8b4eea1ce276968bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c330ae8a9e8e4c1180c846940a27d3ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c769a293243d4f3a88176cdaea411953":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c06ebb1adc484affa3a0928b14dc818d","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd8933ac0db14462b2adf0ab801105c3","value":100}},"c9a30a8efbbb40ca846b2f0cb1ecf3fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_32c1861993ea4b209ad56baa4c55566c","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0dd2e44735a49a593955aadc3047dff","value":50}},"cfd3708df0f845b0a8b39332cc4260f4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0dd2e44735a49a593955aadc3047dff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc25ec76ad8d44c0bd3cc2ca224b5b29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd8933ac0db14462b2adf0ab801105c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e0d98a6008f0439ea69192b87308fe56":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e859501ff92e4ebba4d450f282775355":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4fe970b52b4489eaf6688a576b2c37f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1fb50b2ff6d4ae8b4eea1ce276968bc","placeholder":"​","style":"IPY_MODEL_9bc78ee728754889af6cc2076158ff18","value":"100%"}},"fd8952b9b2494c5fa1ca7aafc63b8b24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./11_Transfer_learning.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Transfer learning and self-supervised learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./13_Hyperparameter.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>