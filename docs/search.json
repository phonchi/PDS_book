[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical and Innovative Analytics in Data Science",
    "section": "",
    "text": "Preface\nThis is the companion book for the course Practical and Innovative Analytics in Data Science open in the Department of Applied Mathematics, National Sun Yat-sen University. This course focuses on the practical aspect of data science in the real world. In the course, students will learn to engage in a real-world project requiring them to apply skills from the entire data science pipeline: preparing, organizing, and transforming data, constructing a model, and evaluating results. Moreover, high-level descriptions of how to apply deep learning for computer vision and natural language problems will also be covered.\nThe book is based on several well-known books and resources, including:\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition\n\nIf you would like to review basics about statistical learning, you may refer to\n\nStatistical Learning and Data Mining\n\nIf you would like to review basics about Python, you may refer to\n\nComputer Programming"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#setup",
    "href": "01_end_to_end_machine_learning_project.html#setup",
    "title": "1  End-to-end Machine Learning project",
    "section": "1.1 Setup",
    "text": "1.1 Setup\nFirst, let’s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20.\n\n!pip install scikit-learn -U -qq\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/9.8 MB ? eta -:--:--     ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/9.8 MB 64.2 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 5.1/9.8 MB 75.9 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 8.4/9.8 MB 80.8 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 9.8/9.8 MB 81.0 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 9.8/9.8 MB 81.0 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.8/9.8 MB 50.8 MB/s eta 0:00:00\n\n\nRemember to restart the notebook to update the modules\n\n# Python ≥3.7 is required\nimport sys\nassert sys.version_info &gt;= (3, 7)\n\n# Scikit-Learn ≥1.2 is recommend\nfrom packaging import version\nimport sklearn\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\n# Common imports\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os\nimport tarfile\nimport urllib.request\n\n\n# To plot pretty figures # This tells Jupyter to set up Matplotlib so it uses Jupyter’s own backend.\n# https://github.com/pycaret/pycaret/issues/319\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'    \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\nimport seaborn as sns\nsns.set_context(\"notebook\")\n\n# Where to save the figures\nIMAGES_PATH = Path() / \"images\" / \"end_to_end_project\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#get-the-data",
    "href": "01_end_to_end_machine_learning_project.html#get-the-data",
    "title": "1  End-to-end Machine Learning project",
    "section": "1.2 Get the Data",
    "text": "1.2 Get the Data\n\n1.2.1 Download the Data\nIt is preferable to create a small function to do that. It is useful in particular\n\nIf data changes regularly, as it allows you to write a small script that you can run whenever you need to fetch the latest data (or you can set up a scheduled job to do that automatically at regular intervals).\nAutomating the process of fetching the data is also useful if you need to install the dataset on multiple machines.\n\nHere is the function to fetch and load the data:\n\ndef load_housing_data():\n    \"\"\"When load_housing_data() is called, it looks for the datasets/housing.tgz file. If it\n    does not find it, it creates the datasets directory inside the current directory\n    downloads the housing.tgz file from the ageron/data GitHub repository, \n    and extracts its content into the datasets directory\n    \"\"\"\n    tarball_path = Path(\"datasets/housing.tgz\")\n    if not tarball_path.is_file():\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n        urllib.request.urlretrieve(url, tarball_path)\n        with tarfile.open(tarball_path) as housing_tarball:\n            housing_tarball.extractall(path=\"datasets\")\n    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n\n\n\n1.2.2 Take a Quick Look at the Data Structure\nNow let’s load the data using Pandas. Once again you should write a small function to load the data:\n\nhousing = load_housing_data()\n\nhttps://www.kaggle.com/camnugent/california-housing-prices\n\n#Let’s take a look at the top five rows.\nhousing.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nEach row represents one district. There are 10 attributes\n\n# The info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values\nhousing.info(), housing.shape \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\n(None, (20640, 10))\n\n\nNotice that the total_bedrooms attribute has only 20,433 non-null values, meaning that 207 districts are missing this feature. We will need to take care of this later.\nAll attributes are numerical, except for ocean_proximity. Its type is object, so it could hold any kind of Python object. But since you loaded this data from a CSV file, you know that it must be a text attribute. When you looked at the top five rows, you probably noticed that the values in the ocean_proximity column were repetitive, which means that it is probably a categorical attribute. You can find out what categories exist and how many districts belong to each category by using the value_counts() method.\n\nhousing[\"ocean_proximity\"].value_counts()\n\n&lt;1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: ocean_proximity, dtype: int64\n\n\nNote that the null values are ignored below (so, for example, count of total_bedrooms is 20,433, not 20,640).\n\nhousing.describe() # The describe() method shows a summary of the numerical attributes\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAnother quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis).\nYou can either plot this one attribute at a time, or you can call the hist() method on the whole dataset (dataframe), and it will plot a histogram for each numerical attribute:\n\nhousing.hist(bins=50, figsize=(12,8))\nplt.show()\n\n\n\n\nNotice a few things in these histograms: 1. First, the median income attribute does not look like it is expressed in US dollars (USD). After checking with the team that collected the data, you are told that the data has been scaled and capped at 15 (actually 15.0001) for higher median incomes, and at 0.5 (actually 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about $30,000). Working with preprocessed attributes is common in Machine Learning, and it is not necessarily a problem, but you should try to understand how the data was computed.\n\nThe housing median age and the median house value were also capped. The latter may be a serious problem since it is your target attribute (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond $500,000, then you have mainly two options:\n\n\nCollect proper labels for the districts whose labels were capped.\nRemove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond $500,000).\n\n\nThese attributes have very different scales. We will discuss this later in this chapter when we explore feature scaling.\nFinally, many histograms are heavy tail and skewed right: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes later on to have more bell-shaped distributions.\n\n\n\n1.2.3 Create a Test Set\nTo avoid the data snooping bias. Creating a test set! This is theoretically quite simple: just pick some instances randomly, typically 20% of the dataset (or less if your dataset is very large), and set them aside\n\n# to make this notebook's output identical at every run\n# https://en.wikipedia.org/wiki/42_(number)\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\n\nlen(train_set)\n\n16512\n\n\n\nlen(test_set)\n\n4128\n\n\n\ntest_set.head(10)\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n20046\n-122.38\n40.67\n10.0\n2281.0\n444.0\n1274.0\n438.0\n2.2120\n65600.0\nINLAND\n\n\n3024\n-118.37\n33.83\n35.0\n1207.0\n207.0\n601.0\n213.0\n4.7308\n353400.0\n&lt;1H OCEAN\n\n\n15663\n-117.24\n32.72\n39.0\n3089.0\n431.0\n1175.0\n432.0\n7.5925\n466700.0\nNEAR OCEAN\n\n\n20484\n-118.44\n34.05\n18.0\n4780.0\n1192.0\n1886.0\n1036.0\n4.4674\n500001.0\n&lt;1H OCEAN\n\n\n9814\n-118.44\n34.18\n33.0\n2127.0\n414.0\n1056.0\n391.0\n4.3750\n286100.0\n&lt;1H OCEAN\n\n\n13311\n-121.76\n36.92\n36.0\n2096.0\n409.0\n1454.0\n394.0\n3.2216\n238300.0\n&lt;1H OCEAN\n\n\n7113\n-120.45\n34.91\n16.0\n712.0\n147.0\n355.0\n162.0\n2.5600\n150000.0\n&lt;1H OCEAN\n\n\n7668\n-117.86\n33.79\n31.0\n3523.0\n922.0\n2660.0\n949.0\n3.1792\n146400.0\n&lt;1H OCEAN\n\n\n18246\n-117.43\n33.91\n15.0\n14281.0\n2511.0\n7540.0\n2245.0\n4.3222\n138000.0\nINLAND\n\n\n5723\n-118.18\n34.13\n44.0\n2734.0\n415.0\n1057.0\n424.0\n7.9213\n477800.0\n&lt;1H OCEAN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n1.2.3.1 Optional\nSo far we have considered purely random sampling methods. This is generally fine if your dataset is large enough (especially relative to the number of attributes), but if it is not, you run the risk of introducing a significant sampling bias. When a survey company decides to call 1,000 people to ask them a few questions, they don’t just pick 1,000 people randomly in a phone book. They try to ensure that these 1,000 people are representative of the whole population.\nFor example, the US population is composed of 51.3% female and 48.7% male, so a well-conducted survey in the US would try to maintain this ratio in the sample: 513 female and 487 male. This is called stratified sampling: the population is divided into homogeneous subgroups called strata, and the right number of instances is sampled from each stratum to guarantee that the test set is representative of the overall population. If they used purely random sampling, the survey results may be significantly biased.\nSuppose you chatted with experts who told you that the median income is a very important attribute to predict median housing prices. You may want to ensure that the test set is representative of the various categories of incomes in the whole dataset.\nSince the median income is a continuous numerical attribute, you first need to create an income category attribute. Let’s look at the median income histogram more closely.\n\nhousing[\"median_income\"].hist()\nplt.show()\n\n\n\n\nMost median income values are clustered around 1.5 to 6 (i.e., $15,000 – $60,000), but some median incomes go far beyond 6. It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of the stratum’s importance may be biased. This means that you should not have too many strata, and each stratum should be large enough. The following code uses the pd.cut() function to create an income category attribute with 5 categories (labeled from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from 1.5 to 3, and so on:\n\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\n\n\nhousing[\"income_cat\"].value_counts()\n\n3    7236\n2    6581\n4    3639\n5    2362\n1     822\nName: income_cat, dtype: int64\n\n\n\nhousing[\"income_cat\"].hist()\nplt.show()\n\n\n\n\nNow you are ready to do stratified sampling based on the income category. For this you can use Scikit-Learn’s StratifiedShuffleSplit class:\nhttps://scikit-learn.org/stable/modules/cross_validation.html#stratified-shuffle-split\n\nstrat_train_set, strat_test_set = train_test_split(\n    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)\n\nLet’s see if this worked as expected. You can start by looking at the income category proportions in the test set:\n\nstrat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n\n3    0.350533\n2    0.318798\n4    0.176357\n5    0.114341\n1    0.039971\nName: income_cat, dtype: float64\n\n\n\nhousing[\"income_cat\"].value_counts() / len(housing)\n\n3    0.350581\n2    0.318847\n4    0.176308\n5    0.114438\n1    0.039826\nName: income_cat, dtype: float64\n\n\nWith similar code you can measure the income category proportions in the full dataset. Below we compare the income category proportions in the overall dataset, in the test set generated with stratified sampling, and in a test set generated using purely random sampling. As you can see, the test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is quite skewed.\n\ndef income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() / len(data)\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall %\": income_cat_proportions(housing),\n    \"Stratified %\": income_cat_proportions(strat_test_set),\n    \"Random %\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props.index.name = \"Income Category\"\ncompare_props[\"Strat. Error %\"] = (compare_props[\"Stratified %\"]/compare_props[\"Overall %\"] - 1)\ncompare_props[\"Rand. Error %\"] = (compare_props[\"Random %\"]/compare_props[\"Overall %\"] - 1)\n(compare_props * 100).round(2)\n\n\n  \n    \n      \n\n\n\n\n\n\nOverall %\nStratified %\nRandom %\nStrat. Error %\nRand. Error %\n\n\nIncome Category\n\n\n\n\n\n\n\n\n\n1\n3.98\n4.00\n4.24\n0.36\n6.45\n\n\n2\n31.88\n31.88\n30.74\n-0.02\n-3.59\n\n\n3\n35.06\n35.05\n34.52\n-0.01\n-1.53\n\n\n4\n17.63\n17.64\n18.41\n0.03\n4.42\n\n\n5\n11.44\n11.43\n12.09\n-0.08\n5.63\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou won’t use the income_cat column again, so you might as well drop it, reverting the data back to its original state:\n\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#discover-and-visualize-the-data-to-gain-insights",
    "href": "01_end_to_end_machine_learning_project.html#discover-and-visualize-the-data-to-gain-insights",
    "title": "1  End-to-end Machine Learning project",
    "section": "1.3 Discover and Visualize the Data to Gain Insights",
    "text": "1.3 Discover and Visualize the Data to Gain Insights\nLet’s create a copy so you can play with it without harming the training set\n\nhousing = strat_train_set.copy()\n#housing = train_set.copy()\n\n\n1.3.1 Visualizing Geographical Data\nSince there is geographical information (latitude and longitude), it is a good idea to create a scatterplot of all districts to visualize the data\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True)\nplt.show()\n\n\n\n\nThis looks like California all right, but other than that it is hard to see any particular pattern. Setting the alpha option to 0.2 makes it much easier to visualize the places where there is a high density of data points\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2)\nsave_fig(\"better_visualization_plot\")\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nNow that’s much better: you can clearly see the high-density areas, namely the Bay Area and around Los Angeles and San Diego, plus a long line of fairly high-density areas in the Central Valley.\nNow let’s look at the housing prices (Figure 2-13). The radius of each circle represents the district’s population (option s), and the color represents the price (option c). We will use a predefined color map (option cmap) called jet, which ranges from blue (low values) to red (high prices):\nThe argument sharex=False fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https://github.com/pandas-dev/pandas/issues/10611 ).\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n             s= housing[\"population\"] / 100, label=\"population\",\n             c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n             legend=True, sharex=False, figsize=(10, 7))\nplt.show()\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nThis image tells you that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density, as you probably knew already.\nIt will probably be useful to use a clustering algorithm to detect the main clusters, and add new features that measure the proximity to the cluster centers. The ocean proximity attribute may be useful as well, although in Northern California the housing prices in coastal districts are not too high, so it is not a simple rule.\n\n# Download the California image\nfilename = \"california.png\"\nif not (IMAGES_PATH / filename).is_file():\n    homl3_root = \"https://github.com/ageron/handson-ml3/raw/main/\"\n    url = homl3_root + \"images/end_to_end_project/\" + filename\n    print(\"Downloading\", filename)\n    urllib.request.urlretrieve(url, IMAGES_PATH / filename)\n\nDownloading california.png\n\n\n\nhousing_renamed = housing.rename(columns={\n    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n    \"population\": \"Population\",\n    \"median_house_value\": \"Median house value (ᴜsᴅ)\"})\nhousing_renamed.plot(\n             kind=\"scatter\", x=\"Longitude\", y=\"Latitude\",\n             s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n             c=\"Median house value (ᴜsᴅ)\", cmap=\"jet\", colorbar=True,\n             legend=True, sharex=False, figsize=(10, 7))\n\ncalifornia_img = plt.imread(IMAGES_PATH / filename)\naxis = -124.55, -113.95, 32.45, 42.05\nplt.axis(axis)\nplt.imshow(california_img, extent=axis)\nplt.show()\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\n1.3.2 Looking for Correlations\nSince the dataset is not too large, you can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes\n\ncorr_matrix = housing.corr()\n\nNow let’s look at how much each attribute correlates with the median house value:\n\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.688380\ntotal_rooms           0.137455\nhousing_median_age    0.102175\nhouseholds            0.071426\ntotal_bedrooms        0.054635\npopulation           -0.020153\nlongitude            -0.050859\nlatitude             -0.139584\nName: median_house_value, dtype: float64\n\n\nThe correlation coefficient ranges from –1 to 1. When it is close to 1, it means that there is a strong positive correlation; For example, the median house value tends to go up when the median income goes up.\nWhen the coefficient is close to –1, it means that there is a strong negative correlation; you can see a small negative correlation between the latitude and the median house value (i.e., prices have a slight tendency to go down when you go north).\nFinally, coefficients close to zero mean that there is no linear correlation. It may completely miss out on nonlinear relationships (e.g., “if x is close to zero then y generally goes up”)\nAnother way to check for correlation between attributes is to use Pandas’ scatter_matrix function, which plots every numerical attribute against every other numerical attribute. Since there are now 11 numerical attributes, you would get 121 plots (including index), which would not fit on a page, so let’s just focus on a few promising attributes that seem most correlated with the median housing value\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\nplt.figure(figsize=[12,8])\nsns.pairplot(housing[attributes])\nplt.show()\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nThe most promising attribute to predict the median house value is the median income, so let’s zoom in on their correlation scatterplot\n\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1, grid=True)\nplt.axis([0, 16, 0, 550000])\nplt.show()\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nThis plot reveals a few things.\n\nFirst, the correlation is indeed very strong; you can clearly see the upward trend and the points are not too dispersed.\nSecond, the price cap that we noticed earlier is clearly visible as a horizontal line at $500,000. But this plot reveals other less obvious straight lines: a horizontal line around $450,000, another around $350,000, perhaps one around $280,000, and a few more below that. You may want to try removing the corresponding districts to prevent your algorithms from learning to reproduce these data quirks.\n\n\n\n1.3.3 Experimenting with Attribute Combinations\nHopefully the previous sections gave you an idea of a few ways you can explore the data and gain insights.\n\nWe identified a few data quirks that you may want to clean up before feeding the data to a machine learning algorithm\nWe found interesting correlations between attributes, in particular with the target attribute\nWe also noticed that some attributes have a skewed-right distribution, so you may want to transform them (e.g., by computing their logarithm or square root).\n\nOf course, your mileage will vary considerably with each project, but the general ideas are similar.\nOne last thing you may want to do before actually preparing the data for Machine Learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you really want is the number of rooms per household.\nSimilarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at.\nLet’s create these new attributes:\n\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\nhousing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"] = housing[\"population\"]/housing[\"households\"]\n\nAnd now let’s look at the correlation matrix again:\n\ncorr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value          1.000000\nmedian_income               0.688380\nrooms_per_household         0.143663\ntotal_rooms                 0.137455\nhousing_median_age          0.102175\nhouseholds                  0.071426\ntotal_bedrooms              0.054635\npopulation                 -0.020153\npopulation_per_household   -0.038224\nlongitude                  -0.050859\nlatitude                   -0.139584\nbedrooms_ratio             -0.256397\nName: median_house_value, dtype: float64\n\n\nHey, not bad! The new bedrooms_ratio attribute is much more correlated with the median house value than the total number of rooms or bedrooms. Apparently houses with a lower bedroom/room ratio tend to be more expensive. The number of rooms per household is also more informative than the total number of rooms in a district—obviously the larger the houses, the more expensive they are.\nThis round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first reasonably good prototype. But this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#prepare-the-data-for-machine-learning-algorithms",
    "href": "01_end_to_end_machine_learning_project.html#prepare-the-data-for-machine-learning-algorithms",
    "title": "1  End-to-end Machine Learning project",
    "section": "1.4 Prepare the Data for Machine Learning Algorithms",
    "text": "1.4 Prepare the Data for Machine Learning Algorithms\nLet’s separate the predictors and the labels since we don’t necessarily want to apply the same transformations to the predictors and the target values (note that drop() creates a copy of the data and does not affect strat_train_set):\n\nhousing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set X\nhousing_labels = strat_train_set[\"median_house_value\"].copy() # y\n\n\n1.4.1 Data Cleaning\nMost Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. You noticed earlier that the total_bedrooms attribute has some missing values, so let’s fix this. You have three options:\n\nGet rid of the corresponding districts.\nGet rid of the whole attribute.\nSet the values to some value (zero, the mean, the median, etc.).\n\n\nsample_incomplete_rows = housing[housing.isnull().any(axis=1)]\nsample_incomplete_rows\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n14452\n-120.67\n40.50\n15.0\n5343.0\nNaN\n2503.0\n902.0\n3.5962\nINLAND\n\n\n18217\n-117.96\n34.03\n35.0\n2093.0\nNaN\n1755.0\n403.0\n3.4115\n&lt;1H OCEAN\n\n\n11889\n-118.05\n34.04\n33.0\n1348.0\nNaN\n1098.0\n257.0\n4.2917\n&lt;1H OCEAN\n\n\n20325\n-118.88\n34.17\n15.0\n4260.0\nNaN\n1701.0\n669.0\n5.1033\n&lt;1H OCEAN\n\n\n14360\n-117.87\n33.62\n8.0\n1266.0\nNaN\n375.0\n183.0\n9.8020\n&lt;1H OCEAN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2348\n-122.70\n38.35\n14.0\n2313.0\nNaN\n954.0\n397.0\n3.7813\n&lt;1H OCEAN\n\n\n366\n-122.50\n37.75\n44.0\n1819.0\nNaN\n1137.0\n354.0\n3.4919\nNEAR OCEAN\n\n\n18241\n-121.44\n38.54\n39.0\n2855.0\nNaN\n1217.0\n562.0\n3.2404\nINLAND\n\n\n18493\n-116.21\n33.75\n22.0\n894.0\nNaN\n830.0\n202.0\n3.0673\nINLAND\n\n\n16519\n-117.86\n34.01\n16.0\n4632.0\nNaN\n3038.0\n727.0\n5.1762\n&lt;1H OCEAN\n\n\n\n\n\n168 rows × 9 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nsample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # option 1\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nsample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # option 2\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n14452\n-120.67\n40.50\n15.0\n5343.0\n2503.0\n902.0\n3.5962\nINLAND\n\n\n18217\n-117.96\n34.03\n35.0\n2093.0\n1755.0\n403.0\n3.4115\n&lt;1H OCEAN\n\n\n11889\n-118.05\n34.04\n33.0\n1348.0\n1098.0\n257.0\n4.2917\n&lt;1H OCEAN\n\n\n20325\n-118.88\n34.17\n15.0\n4260.0\n1701.0\n669.0\n5.1033\n&lt;1H OCEAN\n\n\n14360\n-117.87\n33.62\n8.0\n1266.0\n375.0\n183.0\n9.8020\n&lt;1H OCEAN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2348\n-122.70\n38.35\n14.0\n2313.0\n954.0\n397.0\n3.7813\n&lt;1H OCEAN\n\n\n366\n-122.50\n37.75\n44.0\n1819.0\n1137.0\n354.0\n3.4919\nNEAR OCEAN\n\n\n18241\n-121.44\n38.54\n39.0\n2855.0\n1217.0\n562.0\n3.2404\nINLAND\n\n\n18493\n-116.21\n33.75\n22.0\n894.0\n830.0\n202.0\n3.0673\nINLAND\n\n\n16519\n-117.86\n34.01\n16.0\n4632.0\n3038.0\n727.0\n5.1762\n&lt;1H OCEAN\n\n\n\n\n\n168 rows × 8 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nmedian = housing[\"total_bedrooms\"].median()\nsample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3\nsample_incomplete_rows\n\n/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py:6392: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  return self._update_inplace(result)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n14452\n-120.67\n40.50\n15.0\n5343.0\n434.0\n2503.0\n902.0\n3.5962\nINLAND\n\n\n18217\n-117.96\n34.03\n35.0\n2093.0\n434.0\n1755.0\n403.0\n3.4115\n&lt;1H OCEAN\n\n\n11889\n-118.05\n34.04\n33.0\n1348.0\n434.0\n1098.0\n257.0\n4.2917\n&lt;1H OCEAN\n\n\n20325\n-118.88\n34.17\n15.0\n4260.0\n434.0\n1701.0\n669.0\n5.1033\n&lt;1H OCEAN\n\n\n14360\n-117.87\n33.62\n8.0\n1266.0\n434.0\n375.0\n183.0\n9.8020\n&lt;1H OCEAN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2348\n-122.70\n38.35\n14.0\n2313.0\n434.0\n954.0\n397.0\n3.7813\n&lt;1H OCEAN\n\n\n366\n-122.50\n37.75\n44.0\n1819.0\n434.0\n1137.0\n354.0\n3.4919\nNEAR OCEAN\n\n\n18241\n-121.44\n38.54\n39.0\n2855.0\n434.0\n1217.0\n562.0\n3.2404\nINLAND\n\n\n18493\n-116.21\n33.75\n22.0\n894.0\n434.0\n830.0\n202.0\n3.0673\nINLAND\n\n\n16519\n-117.86\n34.01\n16.0\n4632.0\n434.0\n3038.0\n727.0\n5.1762\n&lt;1H OCEAN\n\n\n\n\n\n168 rows × 9 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nIf you choose option 3, you should compute the median value on the training set, and use it to fill the missing values in the training set, but also don’t forget to save the median value that you have computed. You will need it later to replace missing values in the test set when you want to evaluate your system, and also once the system goes live to replace missing values in new data.\nScikit-Learn provides a handy class to take care of missing values: SimpleImputer.\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")\n\nSince the median can only be computed on numerical attributes, you then need to create a copy of the data with only the numerical attributes (this will exclude the text attribute ocean_proximity)\n\nhousing_num = housing.select_dtypes(include=[np.number])\n\n\nimputer.fit(housing_num)\n\nSimpleImputer(strategy='median')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputerSimpleImputer(strategy='median')\n\n\n\nAll objects in SKlearn share a consistent and simple interface: 1. Estimators: Any object that can estimate some parameters based on a dataset is called an estimator (e.g., a SimpleImputer is an estimator). The estimation itself is performed by the fit() method, and it takes a dataset as a parameter, or two for supervised learning algorithms—the second dataset contains the labels. Any other parameter needed to guide the estimation process is considered a hyperparameter (such as a SimpleImputer’s strategy), and it must be set as an instance variable (generally via a constructor parameter). 2. Transformers: Some estimators (such as a SimpleImputer) can also transform a dataset; these are called transformers. Once again, the API is simple: the transformation is performed by the transform() method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters, as is the case for a SimpleImputer. All transformers also have a convenience method called fit_transform(), which is equivalent to calling fit() and then transform() (but sometimes fit_transform() is optimized and runs much faster). 3. Predictors: Finally, some estimators, given a dataset, are capable of making predictions; they are called predictors. For example, the LinearRegression model was a predictor. A predictor has a predict() method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a score() method that measures the quality of the predictions, given a test set (and the corresponding labels, in the case of supervised learning algorithms).\n\n\nAs with all estimators, it is important to fit the scalers to the training data only: never use fit() or fit_transform() for anything else than the training set. Once you have a trained scaler, you can then use it to transform() any other set, including the validation set, the test set, and new data.\n\nThe imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable. Only the total_bedrooms attribute had missing values, but we cannot be sure that there won’t be any missing values in new data after the system goes live, so it is safer to apply the imputer to all the numerical attributes:\n\nimputer.statistics_\n\narray([-118.51  ,   34.26  ,   29.    , 2125.    ,  434.    , 1167.    ,\n        408.    ,    3.5385])\n\n\n\nimputer.feature_names_in_\n\narray(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n       'total_bedrooms', 'population', 'households', 'median_income'],\n      dtype=object)\n\n\nNow you can use this “trained” imputer to transform the training set by replacing missing values with the learned medians:\n\nX = imputer.transform(housing_num)\n\nThe result is a plain NumPy array containing the transformed features. If you want to put it back into a Pandas DataFrame, it’s simple:\n\n#housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)\nfrom sklearn import config_context\nwith config_context(transform_output=\"pandas\"):\n    housing_tr = imputer.transform(housing_num)\n\nSee https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py\n\nhousing_tr.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\n\n\n\n\n13096\n-122.42\n37.80\n52.0\n3321.0\n1115.0\n1576.0\n1034.0\n2.0987\n\n\n14973\n-118.38\n34.14\n40.0\n1965.0\n354.0\n666.0\n357.0\n6.0876\n\n\n3785\n-121.98\n38.36\n33.0\n1083.0\n217.0\n562.0\n203.0\n2.4330\n\n\n14689\n-117.11\n33.75\n17.0\n4174.0\n851.0\n1845.0\n780.0\n2.2618\n\n\n20507\n-118.15\n33.77\n36.0\n4366.0\n1211.0\n1912.0\n1172.0\n3.5292\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n1.4.2 Dealing with outlier (Optional)\n\nfrom sklearn.ensemble import IsolationForest\n\nisolation_forest = IsolationForest(random_state=42)\noutlier_pred = isolation_forest.fit_predict(X)\n\nIf you wanted to drop outliers, you would run the following code:\n\noutlier_pred\n\n\n#housing = housing.iloc[outlier_pred == 1]\n#housing_labels = housing_labels.iloc[outlier_pred == 1]\n\n\n\n1.4.3 Handling Text and Categorical Attributes\nSo far we have only dealt with numerical attributes, but your data may also contain text attributes. In this dataset, there is just one: the ocean_proximity attribute. Let’s look at its value for the first few instances:\n\nhousing_cat = housing[[\"ocean_proximity\"]] # Note the double square bracket\nhousing_cat.head(10)\n\n\n  \n    \n      \n\n\n\n\n\n\nocean_proximity\n\n\n\n\n13096\nNEAR BAY\n\n\n14973\n&lt;1H OCEAN\n\n\n3785\nINLAND\n\n\n14689\nINLAND\n\n\n20507\nNEAR OCEAN\n\n\n1286\nINLAND\n\n\n18078\n&lt;1H OCEAN\n\n\n4396\nNEAR BAY\n\n\n18031\n&lt;1H OCEAN\n\n\n6753\n&lt;1H OCEAN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nMost Machine Learning algorithms prefer to work with numbers anyway, so let’s convert these categories from text to numbers.\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:10]\n\narray([[3.],\n       [0.],\n       [1.],\n       [1.],\n       [4.],\n       [1.],\n       [0.],\n       [3.],\n       [0.],\n       [0.]])\n\n\nYou can get the list of categories using the categories_ instance variable. It is a list containing a 1D array of categories for each categorical attribute (in this case, a list containing a single array since there is just one categorical attribute):\n\nordinal_encoder.categories_\n\n[array(['&lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n       dtype=object)]\n\n\nOne issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad”, “average”, “good”, “excellent”), but it is obviously not the case for the ocean_proximity column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1).\nTo fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is “&lt;1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called dummy attributes. Scikit-Learn provides a OneHotEncoder class to convert categorical values into one-hot vectors\n\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot\n\n&lt;16512x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 16512 stored elements in Compressed Sparse Row format&gt;\n\n\nBy default, the OneHotEncoder class returns a sparse array, but we can convert it to a dense array if needed by calling the toarray() method:\n\nhousing_cat_1hot.toarray()\n\narray([[0., 0., 0., 1., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       ...,\n       [0., 0., 0., 0., 1.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1.]])\n\n\n\ncat_encoder.categories_\n\n[array(['&lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n       dtype=object)]\n\n\nThe advantage of OneHotEncoder over get_dummies() is that\n\nIt remembers which categories it was trained on. This is very important because once your model is in production, it should be fed exactly the same features as during training: no more, no less.\nOneHotEncoder is smarter: it will detect the unknown category and raise an exception. If you prefer, you can set the handle_unknown hyperparameter to \"ignore\", in which case it will just represent the unknown category with zeros\n\n\ndf_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"&lt;2H OCEAN\", \"ISLAND\"]})\npd.get_dummies(df_test_unknown)\n\n\n  \n    \n      \n\n\n\n\n\n\nocean_proximity_&lt;2H OCEAN\nocean_proximity_ISLAND\n\n\n\n\n0\n1\n0\n\n\n1\n0\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ncat_encoder.handle_unknown = \"ignore\"\ncat_encoder.transform(df_test_unknown).toarray()\n\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0.]])\n\n\nIf a categorical attribute has a large number of possible categories (e.g., country code, profession, species, etc.), then one-hot encoding will result in a large number of input features. This may slow down training and degrade performance.\nIf this happens, you may want to replace the categorical input with useful numerical features related to the categories: for example, you could replace the ocean_proximity feature with the distance to the ocean (similarly, a country code could be replaced with the country’s population and GDP per capita).\nAlternatively, you could replace each category with a learnable low dimensional vector called an embedding. Each category’s representation would be learned during training: this is an example of representation learning.\nWhen you fit any Scikit-Learn estimator using a DataFrame, the estimator stores the column names in the feature_names_in_ attribute. Scikit-Learn then ensures that any DataFrame fed to this estimator after that (e.g., to transform() or predict()) has the same column names. Transformers also provide a get_feature_names_out() method that you can use to build a DataFrame around the transformer’s output:\n\ncat_encoder.feature_names_in_\n\narray(['ocean_proximity'], dtype=object)\n\n\n\ncat_encoder.get_feature_names_out()\n\narray(['ocean_proximity_&lt;1H OCEAN', 'ocean_proximity_INLAND',\n       'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',\n       'ocean_proximity_NEAR OCEAN'], dtype=object)\n\n\n\ndf_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"&lt;2H OCEAN\", \"ISLAND\"]})\ndf_output = pd.DataFrame(cat_encoder.transform(df_test_unknown).toarray(),\n                         columns=cat_encoder.get_feature_names_out(),\n                         index=df_test_unknown.index)\ndf_output\n\n\n  \n    \n      \n\n\n\n\n\n\nocean_proximity_&lt;1H OCEAN\nocean_proximity_INLAND\nocean_proximity_ISLAND\nocean_proximity_NEAR BAY\nocean_proximity_NEAR OCEAN\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n1.4.4 Feature Scaling and Transformation\nOne of the most important transformations you need to apply to your data is feature scaling. With few exceptions, machine learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Without any scaling, most models will be biased toward ignoring the median income and focusing more on the number of rooms.\n\n1.4.4.1 Min-max scaling\nMin-max scaling (many people call this normalization) is the simplest: for each attribute, the values are shifted and rescaled so that they end up ranging from 0 to 1. This is performed by subtracting the min value and dividing by the difference between the min and the max. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if, for some reason, you don’t want 0–1\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmin_max_scaler = MinMaxScaler(feature_range=(-1, 1))\nhousing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)\n\n\n\n1.4.4.2 Standardization\nStandardization is different: first it subtracts the mean value (so standardized values have a zero mean), then it divides the result by the standard deviation (so standardized values have a standard deviation equal to 1). Unlike min-max scaling, standardization does not restrict values to a specific range. However, standardization is much less affected by outliers.\n\nfrom sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\nhousing_num_std_scaled = std_scaler.fit_transform(housing_num)\n\n\n\n1.4.4.3 Other scaling (Optional)\nWhen a feature’s distribution has a heavy tail (i.e., when values far from the mean are not exponentially rare), both min-max scaling and standardization will squash most values into a small range. Machine learning models generally don’t like this at all. So before you scale the feature, you should first transform it to shrink the heavy tail, and if possible to make the distribution roughly symmetrical. For example, a common way to do this for positive features with a heavy tail to the right is to replace the feature with its square root (or raise the feature to a power between 0 and 1). If the feature has a really long and heavy tail, such as a power law distribution, then replacing the feature with its logarithm may help.\nFor example, the population feature roughly follows a power law\n\nfig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\nhousing[\"population\"].hist(ax=axs[0], bins=50)\nhousing[\"population\"].apply(np.log).hist(ax=axs[1], bins=50)\naxs[0].set_xlabel(\"Population\")\naxs[1].set_xlabel(\"Log of population\")\naxs[0].set_ylabel(\"Number of districts\")\nplt.show()\n\n\n\n\nAnother approach to handle heavy-tailed features consists in bucketizing the feature. This means chopping its distribution into roughly equal-sized buckets, and replacing each feature value with the index of the bucket it belongs to. For example, you could replace each value with its percentile. Bucketizing with equal-sized buckets results in a feature with an almost uniform distribution, so there’s no need for further scaling, or you can just divide by the number of buckets to force the values to the 0–1 range.\n\npercentiles = [np.percentile(housing[\"median_income\"], p) for p in range(1, 100)]\nflattened_median_income = pd.cut(housing[\"median_income\"], bins=[-np.inf] + percentiles + [np.inf], labels=range(1, 100 + 1))\nflattened_median_income.hist(bins=50)\nplt.xlabel(\"Median income percentile\")\nplt.ylabel(\"Number of districts\")\nplt.show()\n# Note: incomes below the 1st percentile are labeled 1, and incomes above the\n# 99th percentile are labeled 100. This is why the distribution below ranges\n# from 1 to 100 (not 0 to 100).\n\n\n\n\n\nWhen a feature has a multimodal distribution (i.e., with two or more clear peaks, called modes), such as the housing_median_age feature, it can also be helpful to bucketize it, but this time treating the bucket IDs as categories, rather than as numerical values. This means that the bucket indices must be encoded, for example using a OneHotEncoder (so you usually don’t want to use too many buckets). This approach will allow the regression model to more easily learn different rules for different ranges of this feature value.\n\nFor example, perhaps houses built around 35 years ago have a peculiar style that fell out of fashion, and therefore they’re cheaper than their age alone would suggest.\nAnother approach to transforming multimodal distributions is to add a feature for each of the modes (at least the main ones), representing the similarity between the housing median age and that particular mode. The similarity measure is typically computed using a radial basis function (RBF). Using Scikit-Learn’s rbf_kernel() function, you can create a new Gaussian RBF feature measuring the similarity between the housing median age and 35:\n\nfrom sklearn.metrics.pairwise import rbf_kernel\n\nages = np.linspace(housing[\"housing_median_age\"].min(),\n                   housing[\"housing_median_age\"].max(),\n                   500).reshape(-1, 1)\ngamma1 = 0.1\ngamma2 = 0.03\nrbf1 = rbf_kernel(ages, [[35]], gamma=gamma1)\nrbf2 = rbf_kernel(ages, [[35]], gamma=gamma2)\n\nfig, ax1 = plt.subplots()\n\nax1.set_xlabel(\"Housing median age\")\nax1.set_ylabel(\"Number of districts\")\nax1.hist(housing[\"housing_median_age\"], bins=50)\n\nax2 = ax1.twinx()  # create a twin axis that shares the same x-axis\ncolor = \"blue\"\nax2.plot(ages, rbf1, color=color, label=\"gamma = 0.10\")\nax2.plot(ages, rbf2, color=color, label=\"gamma = 0.03\", linestyle=\"--\")\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylabel(\"Age similarity\", color=color)\n\nplt.legend(loc=\"upper left\")\nplt.show()\n\n\n\n\nAs the chart shows, the new age similarity feature peaks at 35, right around the spike in the housing median age distribution: if this particular age group is well correlated with lower prices, there’s a good chance that this new feature will help.\n\n\n\n1.4.5 Custom Transformers\nAlthough Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific attributes.\n\n1.4.5.1 Function with no training parameter (Optional)\nFor transformations that don’t require any training, you can just write a function that takes a NumPy array as input and outputs the transformed. Let’s create a log-transformer and apply it to the population feature:\n\nfrom sklearn.preprocessing import FunctionTransformer\nlog_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\nlog_pop = log_transformer.transform(housing[[\"population\"]])\n\n\n\n1.4.5.2 Transform with training parameter\nFunctionTransformer is very handy, but what if you would like your transformer to be trainable, learning some parameters in the fit() method and using them later in the transform() method? You can get fit_transform() for free by simply adding TransformerMixin as a base class: the default implementation will just call fit() and then transform(). If you add BaseEstimator as a base class (and avoid using *args and **kwargs in your constructor), you will also get two extra methods: get_params() and set_params(). These will be useful for automatic hyperparameter tuning.\nFor example, the following code demonstrates custom transformer that uses a KMeans clusterer in the fit() method to identify the main clusters in the training data, and then uses rbf_kernel() in the transform() method to measure how similar each sample is to each cluster center:\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.cluster import KMeans\n\nclass ClusterSimilarity(BaseEstimator, TransformerMixin):\n    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n        self.n_clusters = n_clusters\n        self.gamma = gamma\n        self.random_state = random_state\n\n    def fit(self, X, y=None, sample_weight=None):\n        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n        self.kmeans_.fit(X, sample_weight=sample_weight)\n        return self  # always return self!\n\n    def transform(self, X):\n        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n    \n    def get_feature_names_out(self, names=None):\n        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n\nK-means is a clustering algorithm that locates clusters in the data. How many it searches for is controlled by the n_clusters hyperparameter. After training, the cluster centers are available via the cluster_centers_ attribute. The fit() method of KMeans supports an optional argument sample_weight, which lets the user specify the relative weights of the samples. k-means is a stochastic algorithm, meaning that it relies on randomness to locate the clusters, so if you want reproducible results, you must set the random_state parameter. As you can see, despite the complexity of the task, the code is fairly straightforward. Now let’s use this custom transformer:\n\ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\nsimilarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]], sample_weight=housing_labels)\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nThis code creates a ClusterSimilarity transformer, setting the number of clusters to 10. Then it calls fit_transform() with the latitude and longitude of every district in the training set, weighting each district by its median house value. The transformer uses k-means to locate the clusters, then measures the Gaussian RBF similarity between each district and all 10 cluster centers. The result is a matrix with one row per district, and one column per cluster.\n\nSee https://www.kaggle.com/code/ryanholbrook/clustering-with-k-means for the rationale of using clustering results as features.\n\n\nhousing_renamed = housing.rename(columns={\n    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n    \"population\": \"Population\",\n    \"median_house_value\": \"Median house value (ᴜsᴅ)\"})\nhousing_renamed[\"Max cluster similarity\"] = similarities.max(axis=1)\n\nhousing_renamed.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", grid=True,\n                     s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n                     c=\"Max cluster similarity\",\n                     cmap=\"jet\", colorbar=True,\n                     legend=True, sharex=False, figsize=(10, 7))\nplt.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],\n         cluster_simil.kmeans_.cluster_centers_[:, 0],\n         linestyle=\"\", color=\"black\", marker=\"X\", markersize=20,\n         label=\"Cluster centers\")\nplt.legend(loc=\"upper right\")\nplt.show()\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nThe figure shows the 10 cluster centers found by k-means. The districts are colored according to their geographic similarity to their closest cluster center. As you can see, most clusters are located in highly populated and expensive areas.\n\n\n\n1.4.6 Transformation Pipelines\nAs you can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with such sequences of transformations. Here is a small pipeline for numerical attributes, which will first impute then scale the input features:\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\nnum_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"standardize\", StandardScaler()),\n])\nnum_pipeline\n\nPipeline(steps=[('impute', SimpleImputer(strategy='median')),\n                ('standardize', StandardScaler())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('impute', SimpleImputer(strategy='median')),\n                ('standardize', StandardScaler())])SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()\n\n\nThe Pipeline constructor takes a list of name/estimator pairs (2-tuples) defining a sequence of steps. The names can be anything you like, as long as they are unique and don’t contain double underscores (__). They will be useful later, when we discuss hyperparameter tuning. The estimators must all be transformers (i.e., they must have a fit_transform() method), except for the last one, which can be anything: a transformer, a predictor, or any other type of estimator.\nWhen you call the pipeline’s fit() method, it calls fit_transform() sequentially on all the transformers, passing the output of each call as the parameter to the next call until it reaches the final estimator, for which it just calls the fit() method. The pipeline exposes the same methods as the final estimator. In this example the last estimator is a StandardScaler, which is a transformer, so the pipeline also acts like a transformer.\nIf you call the pipeline’s transform() method, it will sequentially apply all the transformations to the data. If the last estimator were a predictor instead of a transformer, then the pipeline would have a predict() method rather than a transform() method. Calling it would sequentially apply all the transformations to the data and pass the result to the predictor’s predict() method.\nLet’s call the pipeline’s fit_transform() method and look at the output’s first two rows, rounded to two decimal places:\n\nhousing_num_prepared = num_pipeline.fit_transform(housing_num)\nhousing_num_prepared[:2].round(2)\n\narray([[-1.42,  1.01,  1.86,  0.31,  1.37,  0.14,  1.39, -0.94],\n       [ 0.6 , -0.7 ,  0.91, -0.31, -0.44, -0.69, -0.37,  1.17]])\n\n\nSo far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer capable of handling all columns, applying the appropriate transformations to each column. For this, you can use a ColumnTransformer. For example, the following ColumnTransformer will apply num_pipeline (the one we just defined) to the numerical attributes and cat_pipeline to the categorical attribute:\n\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\ncat_attribs = [\"ocean_proximity\"]\n\ncat_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n    ])\n\npreprocessing = ColumnTransformer([\n    (\"num\", num_pipeline, num_attribs),\n    (\"cat\", cat_pipeline, cat_attribs),\n])\n\nWe construct a ColumnTransformer. Its constructor requires a list of triplets (3-tuples), each containing a name (which must be unique and not contain double underscores), a transformer, and a list of names (or indices) of columns that the transformer should be applied to.\n\nInstead of using a transformer, you can specify the string “drop” if you want the columns to be dropped, or you can specify “passthrough” if you want the columns to be left untouched. By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but you can set the remainder hyperparameter to any transformer (or to “passthrough”) if you want these columns to be handled differently.\n\nSince listing all the column names is not very convenient, Scikit-Learn provides a make_column_selector() function that returns a selector function you can use to automatically select all the features of a given type, such as numerical or categorical.\nYou can pass this selector function to the ColumnTransformer instead of column names or indices. Moreover, if you don’t care about naming the transformers, you can use make_column_transformer(), which chooses the names for you.\n\nfrom sklearn.compose import make_column_selector, make_column_transformer\n\npreprocessing = make_column_transformer(\n    (num_pipeline, make_column_selector(dtype_include=np.number)),\n    (cat_pipeline, make_column_selector(dtype_include=object)),\n)\n\nNow we’re ready to apply this ColumnTransformer to the housing data:\n\n# https://github.com/scikit-learn/scikit-learn/issues/25224\n# with config_context(transform_output=\"pandas\"):\n#  housing_prepared = preprocessing.fit_transform(housing)\n\nhousing_prepared = preprocessing.fit_transform(housing)\n\nOnce again this returns a NumPy array, but you can get the column names using preprocessing.get_feature_names_out() and wrap the data in a nice DataFrame as we did before.\n\nhousing_prepared_fr = pd.DataFrame(\n    housing_prepared,\n    columns=preprocessing.get_feature_names_out(),\n    index=housing.index)\nhousing_prepared_fr.head(2)\n\n\n  \n    \n      \n\n\n\n\n\n\npipeline-1__longitude\npipeline-1__latitude\npipeline-1__housing_median_age\npipeline-1__total_rooms\npipeline-1__total_bedrooms\npipeline-1__population\npipeline-1__households\npipeline-1__median_income\npipeline-2__ocean_proximity_&lt;1H OCEAN\npipeline-2__ocean_proximity_INLAND\npipeline-2__ocean_proximity_ISLAND\npipeline-2__ocean_proximity_NEAR BAY\npipeline-2__ocean_proximity_NEAR OCEAN\n\n\n\n\n13096\n-1.423037\n1.013606\n1.861119\n0.311912\n1.368167\n0.137460\n1.394812\n-0.936491\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n14973\n0.596394\n-0.702103\n0.907630\n-0.308620\n-0.435925\n-0.693771\n-0.373485\n1.171942\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe code that builds the pipeline to do all of this should look familiar to you by now:\n\nMissing values in numerical features will be imputed by replacing them with the median, as most ML algorithms don’t expect missing values. In categorical features, missing values will be replaced by the most frequent category.\nThe categorical feature will be one-hot encoded, as most ML algorithms only accept numerical inputs.\nA few ratio features will be computed and added: bedrooms_ratio,rooms_per_house, and people_per_house. Hopefully these will better correlate with the median house value, and thereby help the ML models.\nA few cluster similarity features will also be added. These will likely be more useful to the model than latitude and longitude.\nFeatures with a long tail will be replaced by their logarithm, as most models prefer features with roughly uniform or Gaussian distributions.\nAll numerical features will be standardized, as most ML algorithms prefer when all features have roughly the same scale.\n\n\nIf you don’t want to name the transformers, you can use the make_pipeline() function instead Pipeline\n\n\ndef column_ratio(X):\n    return X[:, [0]] / X[:, [1]]\n\ndef ratio_name(function_transformer, feature_names_in):\n    return [\"ratio\"]  # feature names out\n\ndef ratio_pipeline():\n    return make_pipeline(\n        SimpleImputer(strategy=\"median\"),\n        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n        StandardScaler())\n\nlog_pipeline = make_pipeline(\n    SimpleImputer(strategy=\"median\"),\n    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n    StandardScaler())\n\ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\ndefault_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n\npreprocessing = ColumnTransformer([\n        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n                               \"households\", \"median_income\"]),\n        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n    ],\n    remainder=default_num_pipeline)  # one column remaining: housing_median_age\n\nIf you run this ColumnTransformer, it performs all the transformations and outputs a NumPy array with 24 features:\n\nhousing_prepared = preprocessing.fit_transform(housing)\nhousing_prepared.shape\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n(16512, 24)"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#select-and-train-a-model",
    "href": "01_end_to_end_machine_learning_project.html#select-and-train-a-model",
    "title": "1  End-to-end Machine Learning project",
    "section": "1.5 Select and Train a Model",
    "text": "1.5 Select and Train a Model\nAt last! You framed the problem, you got the data and explored it, you sampled a training set and a test set, and you wrote a preprocessing pipeline to automatically clean up and prepare your data for machine learning algorithms. You are now ready to select and train a machine learning model.\n\n1.5.1 Training and Evaluating on the Training Set\nLet’s first train a Linear Regression model\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = make_pipeline(preprocessing, LinearRegression())\nlin_reg.fit(housing, housing_labels)\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2...\n                                                   'households',\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                ('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2...\n                                                   'households',\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                ('linearregression', LinearRegression())])columntransformer: ColumnTransformerColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                             SimpleImputer(strategy='median')),\n                                            ('standardscaler',\n                                             StandardScaler())]),\n                  transformers=[('bedrooms',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                                                                      func=&lt;function column_ratio...\n                                 ['total_bedrooms', 'total_rooms', 'population',\n                                  'households', 'median_income']),\n                                ('geo', ClusterSimilarity(random_state=42),\n                                 ['latitude', 'longitude']),\n                                ('cat',\n                                 Pipeline(steps=[('impute',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])bedrooms['total_bedrooms', 'total_rooms']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()rooms_per_house['total_rooms', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()people_per_house['population', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out='one-to-one', func=&lt;ufunc 'log'&gt;)StandardScalerStandardScaler()geo['latitude', 'longitude']ClusterSimilarityClusterSimilarity(random_state=42)cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(handle_unknown='ignore')remainder['housing_median_age']SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()LinearRegressionLinearRegression()\n\n\nYou now have a working linear regression model. You try it out on the training set, looking at the first five predictions and comparing them to the labels:\n\nhousing_predictions = lin_reg.predict(housing)\nhousing_predictions[:5].round(-2)  # -2 = rounded to the nearest hundred\n\narray([243700., 372400., 128800.,  94400., 328300.])\n\n\n\nhousing_labels.iloc[:5].values\n\narray([458300., 483800., 101700.,  96100., 361800.])\n\n\nRemember that you chose to use the RMSE as your performance measure, so you want to measure this regression model’s RMSE on the whole training set using Scikit-Learn’s mean_squared_error() function, with the squared argument set to False:\n\nfrom sklearn.metrics import mean_squared_error\n\nlin_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)\nlin_rmse\n\n68687.89176590038\n\n\nThis is better than nothing, but clearly not a great score: the median_housing_values of most districts range between $120,000 and $265,000, so a typical prediction error of $68,628 is really not very satisfying. This is an example of a model underfitting the training data. When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough.\nThe main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. This model is not regularized, which rules out the last option. You could try to add more features, but first you want to try a more complex model to see how it does. We decide to try a DecisionTreeRegressor, as this is a fairly powerful model capable of finding complex nonlinear relationships in the data.\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\ntree_reg.fit(housing, housing_labels)\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2...\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                ('decisiontreeregressor',\n                 DecisionTreeRegressor(random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2...\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                ('decisiontreeregressor',\n                 DecisionTreeRegressor(random_state=42))])columntransformer: ColumnTransformerColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                             SimpleImputer(strategy='median')),\n                                            ('standardscaler',\n                                             StandardScaler())]),\n                  transformers=[('bedrooms',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                                                                      func=&lt;function column_ratio...\n                                 ['total_bedrooms', 'total_rooms', 'population',\n                                  'households', 'median_income']),\n                                ('geo', ClusterSimilarity(random_state=42),\n                                 ['latitude', 'longitude']),\n                                ('cat',\n                                 Pipeline(steps=[('impute',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])bedrooms['total_bedrooms', 'total_rooms']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()rooms_per_house['total_rooms', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()people_per_house['population', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out='one-to-one', func=&lt;ufunc 'log'&gt;)StandardScalerStandardScaler()geo['latitude', 'longitude']ClusterSimilarityClusterSimilarity(random_state=42)cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(handle_unknown='ignore')remainder['housing_median_age']SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()DecisionTreeRegressorDecisionTreeRegressor(random_state=42)\n\n\n\nhousing_predictions = tree_reg.predict(housing)\ntree_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)\ntree_rmse\n\n0.0\n\n\nIt is much more likely that the model has badly overfit the data. How can you be sure? As we saw earlier, you don’t want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training, and part for model validation.\n\n\n1.5.2 Better Evaluation Using Cross-Validation\nOne way to evaluate the Decision Tree model would be to use the train_test_split function to split the training set into a smaller training set and a validation set, then train your models against the smaller training set and evaluate them against the validation set.\nA great alternative is to use Scikit-Learn’s K-fold cross-validation feature. The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores:\n\nfrom sklearn.model_selection import cross_val_score\n\n# Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better),\ntree_rmses = -cross_val_score(tree_reg, housing, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10)\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nLet’s look at the results:\n\npd.Series(tree_rmses).describe()\n\ncount       10.000000\nmean     66868.027288\nstd       2060.966425\nmin      63649.536493\n25%      65338.078316\n50%      66801.953094\n75%      68229.934454\nmax      70094.778246\ndtype: float64\n\n\nNow the decision tree doesn’t look as good as it did earlier. In fact, it seems to perform almost as poorly as the linear regression model! Notice that cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The decision tree has an RMSE of about 66,868, with a standard deviation of about 2,061. You would not have this information if you just used one validation set! We know there’s an overfitting problem because the training error is low (actually zero) while the validation error is high.\nLet’s compute the same scores for the Linear Regression model just to be sure\n\nlin_rmses = -cross_val_score(lin_reg, housing, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10)\npd.Series(lin_rmses).describe()\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\ncount       10.000000\nmean     69858.018195\nstd       4182.205077\nmin      65397.780144\n25%      68070.536263\n50%      68619.737842\n75%      69810.076342\nmax      80959.348171\ndtype: float64\n\n\nLet’s try one last model now: the RandomForestRegressor.Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions.\nBuilding a model on top of many other models is called Ensemble Learning, and it is often a great way to push ML algorithms even further.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = make_pipeline(preprocessing, RandomForestRegressor(random_state=42))\nforest_rmses = -cross_val_score(forest_reg, housing, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10)\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n\npd.Series(forest_rmses).describe()\n\ncount       10.000000\nmean     47019.561281\nstd       1033.957120\nmin      45458.112527\n25%      46464.031184\n50%      46967.596354\n75%      47325.694987\nmax      49243.765795\ndtype: float64\n\n\nWow, this is much better: random forests really look very promising for this task! However, if you train a RandomForest and measure the RMSE on the training set, you will find roughly 17,474: that’s much lower, meaning that there’s still quite a lot of overfitting going on.\n\nforest_reg.fit(housing, housing_labels)\nhousing_predictions = forest_reg.predict(housing)\nforest_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)\nforest_rmse\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n17474.619286483998\n\n\nPossible solutions are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data. Before you dive much deeper into random forests, however, you should try out many other models from various categories of machine learning algorithms (e.g., several support vector machines with different kernels, and possibly a neural network), without spending too much time tweaking the hyperparameters. The goal is to shortlist a few (two to five) promising models."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#fine-tune-your-model",
    "href": "01_end_to_end_machine_learning_project.html#fine-tune-your-model",
    "title": "1  End-to-end Machine Learning project",
    "section": "1.6 Fine-Tune Your Model",
    "text": "1.6 Fine-Tune Your Model\nLet’s assume that you now have a shortlist of promising models. You now need to fine-tune them. Let’s look at a few ways you can do that.\n\n1.6.1 Grid Search\nOne way to do that would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.\nInstead you should get Scikit-Learn’s GridSearchCV to search for you. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation. For example, the following code searches for the best combination of hyperparameter values for the RandomForestRegressor:\n\nfrom sklearn.model_selection import GridSearchCV\n\nfull_pipeline = Pipeline([\n    (\"preprocessing\", preprocessing),\n    (\"random_forest\", RandomForestRegressor(random_state=42)),\n])\nparam_grid = [\n    {'preprocessing__geo__n_clusters': [5, 8, 10],\n     'random_forest__max_features': [4, 6, 8]},\n    {'preprocessing__geo__n_clusters': [10, 15],\n     'random_forest__max_features': [6, 8, 10]},\n]\ngrid_search = GridSearchCV(full_pipeline, param_grid, cv=3, scoring='neg_root_mean_squared_error')\ngrid_search.fit(housing, housing_labels)\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('preprocessing',\n                                        ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                                                     SimpleImputer(strategy='median')),\n                                                                                    ('standardscaler',\n                                                                                     StandardScaler())]),\n                                                          transformers=[('bedrooms',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('functiontransformer',\n                                                                                          FunctionTransformer(feature_names_out=&lt;f...\n                                                                         &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                                       ('random_forest',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid=[{'preprocessing__geo__n_clusters': [5, 8, 10],\n                          'random_forest__max_features': [4, 6, 8]},\n                         {'preprocessing__geo__n_clusters': [10, 15],\n                          'random_forest__max_features': [6, 8, 10]}],\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('preprocessing',\n                                        ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                                                     SimpleImputer(strategy='median')),\n                                                                                    ('standardscaler',\n                                                                                     StandardScaler())]),\n                                                          transformers=[('bedrooms',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('functiontransformer',\n                                                                                          FunctionTransformer(feature_names_out=&lt;f...\n                                                                         &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                                       ('random_forest',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid=[{'preprocessing__geo__n_clusters': [5, 8, 10],\n                          'random_forest__max_features': [4, 6, 8]},\n                         {'preprocessing__geo__n_clusters': [10, 15],\n                          'random_forest__max_features': [6, 8, 10]}],\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('preprocessing',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14b...\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                ('random_forest', RandomForestRegressor(random_state=42))])preprocessing: ColumnTransformerColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                             SimpleImputer(strategy='median')),\n                                            ('standardscaler',\n                                             StandardScaler())]),\n                  transformers=[('bedrooms',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                                                                      func=&lt;function column_ratio...\n                                 ['total_bedrooms', 'total_rooms', 'population',\n                                  'households', 'median_income']),\n                                ('geo', ClusterSimilarity(random_state=42),\n                                 ['latitude', 'longitude']),\n                                ('cat',\n                                 Pipeline(steps=[('impute',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])bedrooms['total_bedrooms', 'total_rooms']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()rooms_per_house['total_rooms', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()people_per_house['population', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out='one-to-one', func=&lt;ufunc 'log'&gt;)StandardScalerStandardScaler()geo['latitude', 'longitude']ClusterSimilarityClusterSimilarity(random_state=42)cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(handle_unknown='ignore')remainder['housing_median_age']SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\nNotice that you can refer to any hyperparameter of any estimator in a pipeline, even if this estimator is nested deep inside several pipelines and column transformers. For example, when Scikit-Learn sees \"preprocessing__geo__n_clusters\", it splits this string at the double underscores, then it looks for an estimator named \"preprocessing\" in the pipeline and finds the preprocessing ColumnTransformer. Next, it looks for a transformer named \"geo\" inside this ColumnTransformer and finds the ClusterSimilarity transformer we used on the latitude and longitude attributes. Then it finds this transformer’s n_clusters hyperparameter. Similarly, random_forest__max_features refers to the max_features hyperparameter of the estimator named \"random_forest\", which is of course the RandomForest model\nThere are two dictionaries in this param_grid, so GridSearchCV will first evaluate all 3 × 3 = 9 combinations of n_clusters and max_features hyperparameter values specified in the first dict, then it will try all 2 × 3 = 6 combinations of hyperparameter values in the second dict. So in total the grid search will explore 9 + 6 = 15 combinations of hyperparameter values, and it will train the pipeline 3 times per combination, since we are using 3-fold cross validation. This means there will be a grand total of 15 × 3 = 45 rounds of training!\n\ngrid_search.best_params_\n\n# Since 15 is the maximum values that were evaluated, you\n# should probably try searching again with higher values, since the\n# score may continue to improve.\n\n{'preprocessing__geo__n_clusters': 15, 'random_forest__max_features': 6}\n\n\n\ngrid_search.best_estimator_\n\nPipeline(steps=[('preprocessing',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14b...\n                                                  ClusterSimilarity(n_clusters=15,\n                                                                    random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b14c8a1f0&gt;)])),\n                ('random_forest',\n                 RandomForestRegressor(max_features=6, random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessing',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14b...\n                                                  ClusterSimilarity(n_clusters=15,\n                                                                    random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b14c8a1f0&gt;)])),\n                ('random_forest',\n                 RandomForestRegressor(max_features=6, random_state=42))])preprocessing: ColumnTransformerColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                             SimpleImputer(strategy='median')),\n                                            ('standardscaler',\n                                             StandardScaler())]),\n                  transformers=[('bedrooms',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                                                                      func=&lt;function column_ratio...\n                                 ['total_bedrooms', 'total_rooms', 'population',\n                                  'households', 'median_income']),\n                                ('geo',\n                                 ClusterSimilarity(n_clusters=15,\n                                                   random_state=42),\n                                 ['latitude', 'longitude']),\n                                ('cat',\n                                 Pipeline(steps=[('impute',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b14c8a1f0&gt;)])bedrooms['total_bedrooms', 'total_rooms']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()rooms_per_house['total_rooms', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()people_per_house['population', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out='one-to-one', func=&lt;ufunc 'log'&gt;)StandardScalerStandardScaler()geo['latitude', 'longitude']ClusterSimilarityClusterSimilarity(n_clusters=15, random_state=42)cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b14c8a1f0&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(handle_unknown='ignore')remainder['housing_median_age']SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(max_features=6, random_state=42)\n\n\nYou can access the best estimator using grid_search.best_estimator_. If GridSearchCV is initialized with refit=True (which is the default), then once it finds the best estimator using cross-validation, it retrains it on the whole training set. This is usually a good idea, since feeding it more data will likely improve its performance.\nThe evaluation scores are available using grid_search.cv_results_. This is a dictionary, but if you wrap it in a DataFrame you get a nice list of all the test scores for each combination of hyperparameters and for each cross-validation split, as well as the mean test score across all splits:\nLet’s look at the score of each hyperparameter combination tested during the grid search:\n\ncv_res = pd.DataFrame(grid_search.cv_results_)\ncv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n\ncv_res = cv_res[[\"param_preprocessing__geo__n_clusters\",\n                 \"param_random_forest__max_features\", \"split0_test_score\",\n                 \"split1_test_score\", \"split2_test_score\", \"mean_test_score\"]]\nscore_cols = [\"split0\", \"split1\", \"split2\", \"mean_test_rmse\"]\ncv_res.columns = [\"n_clusters\", \"max_features\"] + score_cols\ncv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)\n\ncv_res.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nn_clusters\nmax_features\nsplit0\nsplit1\nsplit2\nmean_test_rmse\n\n\n\n\n12\n15\n6\n43460\n43919\n44748\n44042\n\n\n13\n15\n8\n44132\n44075\n45010\n44406\n\n\n14\n15\n10\n44374\n44286\n45316\n44659\n\n\n7\n10\n6\n44683\n44655\n45657\n44999\n\n\n9\n10\n6\n44683\n44655\n45657\n44999\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe mean test RMSE score for the best model is 44,042, which is better than the score you got earlier using the default hyperparameter values (which was 47,019). Congratulations, you have successfully fine-tuned your best model!\n\n\n1.6.2 Randomized Search\nThe grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use RandomizedSearchCV instead. This class can be used in much the same way as the GridSearchCV class, but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration.\nFor each hyperparameter, you must provide either a list of possible values, or a probability distribution:\nTry 30 (n_iter × cv) random combinations of hyperparameters:\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),\n          'random_forest__max_features': randint(low=2, high=20)}\n\nrnd_search = RandomizedSearchCV(\n    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,\n    scoring='neg_root_mean_squared_error', random_state=42)\n\nrnd_search.fit(housing, housing_labels)\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[('preprocessing',\n                                              ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                                                           SimpleImputer(strategy='median')),\n                                                                                          ('standardscaler',\n                                                                                           StandardScaler())]),\n                                                                transformers=[('bedrooms',\n                                                                               Pipeline(steps=[('simpleimputer',\n                                                                                                SimpleImputer(strategy='median')),\n                                                                                               ('functiontransformer',\n                                                                                                FunctionTransformer(feature_names_...\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                                             ('random_forest',\n                                              RandomForestRegressor(random_state=42))]),\n                   param_distributions={'preprocessing__geo__n_clusters': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f2b14de5280&gt;,\n                                        'random_forest__max_features': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f2b146d3160&gt;},\n                   random_state=42, scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[('preprocessing',\n                                              ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                                                           SimpleImputer(strategy='median')),\n                                                                                          ('standardscaler',\n                                                                                           StandardScaler())]),\n                                                                transformers=[('bedrooms',\n                                                                               Pipeline(steps=[('simpleimputer',\n                                                                                                SimpleImputer(strategy='median')),\n                                                                                               ('functiontransformer',\n                                                                                                FunctionTransformer(feature_names_...\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                                             ('random_forest',\n                                              RandomForestRegressor(random_state=42))]),\n                   param_distributions={'preprocessing__geo__n_clusters': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f2b14de5280&gt;,\n                                        'random_forest__max_features': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f2b146d3160&gt;},\n                   random_state=42, scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('preprocessing',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14b...\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                ('random_forest', RandomForestRegressor(random_state=42))])preprocessing: ColumnTransformerColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                             SimpleImputer(strategy='median')),\n                                            ('standardscaler',\n                                             StandardScaler())]),\n                  transformers=[('bedrooms',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                                                                      func=&lt;function column_ratio...\n                                 ['total_bedrooms', 'total_rooms', 'population',\n                                  'households', 'median_income']),\n                                ('geo', ClusterSimilarity(random_state=42),\n                                 ['latitude', 'longitude']),\n                                ('cat',\n                                 Pipeline(steps=[('impute',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])bedrooms['total_bedrooms', 'total_rooms']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()rooms_per_house['total_rooms', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()people_per_house['population', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out='one-to-one', func=&lt;ufunc 'log'&gt;)StandardScalerStandardScaler()geo['latitude', 'longitude']ClusterSimilarityClusterSimilarity(random_state=42)cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(handle_unknown='ignore')remainder['housing_median_age']SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\ncv_res = pd.DataFrame(rnd_search.cv_results_)\ncv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\ncv_res = cv_res[[\"param_preprocessing__geo__n_clusters\",\n                 \"param_random_forest__max_features\", \"split0_test_score\",\n                 \"split1_test_score\", \"split2_test_score\", \"mean_test_score\"]]\ncv_res.columns = [\"n_clusters\", \"max_features\"] + score_cols\ncv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)\ncv_res.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nn_clusters\nmax_features\nsplit0\nsplit1\nsplit2\nmean_test_rmse\n\n\n\n\n1\n45\n9\n41287\n42071\n42627\n41995\n\n\n8\n32\n7\n41690\n42513\n43224\n42475\n\n\n0\n41\n16\n42223\n42959\n43321\n42834\n\n\n5\n42\n4\n41818\n43094\n43817\n42910\n\n\n2\n23\n8\n42264\n42996\n43830\n43030\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAnother way to fine-tune your system is to try to combine the models that perform best. This is especially true if the individual models make very different types of errors.\n\n\n1.6.3 Analyze the Best Models and Their Errors\nYou will often gain good insights on the problem by inspecting the best models. For example, the RandomForestRegressor can indicate the relative importance of each attribute for making accurate predictions\n\nfinal_model = rnd_search.best_estimator_  # includes preprocessing\nfeature_importances = final_model[\"random_forest\"].feature_importances_\nfeature_importances.round(2)\n\narray([0.07, 0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.19, 0.04, 0.01, 0.  ,\n       0.01, 0.01, 0.01, 0.01, 0.01, 0.  , 0.01, 0.01, 0.01, 0.  , 0.01,\n       0.01, 0.01, 0.01, 0.01, 0.  , 0.  , 0.02, 0.01, 0.01, 0.01, 0.02,\n       0.01, 0.  , 0.02, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01,\n       0.01, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01, 0.  , 0.07,\n       0.  , 0.  , 0.  , 0.01])\n\n\n\nsorted(zip(feature_importances,\n           final_model[\"preprocessing\"].get_feature_names_out()),\n           reverse=True)\n\n[(0.18694559869103852, 'log__median_income'),\n (0.0748194905715524, 'cat__ocean_proximity_INLAND'),\n (0.06926417748515576, 'bedrooms__ratio'),\n (0.05446998753775219, 'rooms_per_house__ratio'),\n (0.05262301809680712, 'people_per_house__ratio'),\n (0.03819415873915732, 'geo__Cluster 0 similarity'),\n (0.02879263999929514, 'geo__Cluster 28 similarity'),\n (0.023530192521380392, 'geo__Cluster 24 similarity'),\n (0.020544786346378206, 'geo__Cluster 27 similarity'),\n (0.019873052631077512, 'geo__Cluster 43 similarity'),\n (0.018597511022930273, 'geo__Cluster 34 similarity'),\n (0.017409085415656868, 'geo__Cluster 37 similarity'),\n (0.015546519677632162, 'geo__Cluster 20 similarity'),\n (0.014230331127504292, 'geo__Cluster 17 similarity'),\n (0.0141032216204026, 'geo__Cluster 39 similarity'),\n (0.014065768027447325, 'geo__Cluster 9 similarity'),\n (0.01354220782825315, 'geo__Cluster 4 similarity'),\n (0.01348963625822907, 'geo__Cluster 3 similarity'),\n (0.01338319626383868, 'geo__Cluster 38 similarity'),\n (0.012240533790212824, 'geo__Cluster 31 similarity'),\n (0.012089046542256785, 'geo__Cluster 7 similarity'),\n (0.01152326329703204, 'geo__Cluster 23 similarity'),\n (0.011397459905603558, 'geo__Cluster 40 similarity'),\n (0.011282340924816439, 'geo__Cluster 36 similarity'),\n (0.01104139770781063, 'remainder__housing_median_age'),\n (0.010671123191312802, 'geo__Cluster 44 similarity'),\n (0.010296376177202627, 'geo__Cluster 5 similarity'),\n (0.010184798445004483, 'geo__Cluster 42 similarity'),\n (0.010121853542225083, 'geo__Cluster 11 similarity'),\n (0.009795219101117579, 'geo__Cluster 35 similarity'),\n (0.00952581084310724, 'geo__Cluster 10 similarity'),\n (0.009433209165984823, 'geo__Cluster 13 similarity'),\n (0.00915075361116215, 'geo__Cluster 1 similarity'),\n (0.009021485619463173, 'geo__Cluster 30 similarity'),\n (0.00894936224917583, 'geo__Cluster 41 similarity'),\n (0.008901832702357514, 'geo__Cluster 25 similarity'),\n (0.008897504713401587, 'geo__Cluster 29 similarity'),\n (0.0086846298524955, 'geo__Cluster 21 similarity'),\n (0.008061104590483955, 'geo__Cluster 15 similarity'),\n (0.00786048176566994, 'geo__Cluster 16 similarity'),\n (0.007793633130749198, 'geo__Cluster 22 similarity'),\n (0.007501766442066527, 'log__total_rooms'),\n (0.0072024111938241275, 'geo__Cluster 32 similarity'),\n (0.006947156598995616, 'log__population'),\n (0.006800076770899128, 'log__households'),\n (0.006736105364684462, 'log__total_bedrooms'),\n (0.006315268213499131, 'geo__Cluster 33 similarity'),\n (0.005796398579893261, 'geo__Cluster 14 similarity'),\n (0.005234954623294958, 'geo__Cluster 6 similarity'),\n (0.0045514083468621595, 'geo__Cluster 12 similarity'),\n (0.004546042080216035, 'geo__Cluster 18 similarity'),\n (0.004314514641115755, 'geo__Cluster 2 similarity'),\n (0.003953528110719969, 'geo__Cluster 19 similarity'),\n (0.003297404747742136, 'geo__Cluster 26 similarity'),\n (0.00289453474290887, 'cat__ocean_proximity_&lt;1H OCEAN'),\n (0.0016978863168109126, 'cat__ocean_proximity_NEAR OCEAN'),\n (0.0016391131530559377, 'geo__Cluster 8 similarity'),\n (0.00015061247730531558, 'cat__ocean_proximity_NEAR BAY'),\n (7.301686597099842e-05, 'cat__ocean_proximity_ISLAND')]\n\n\n\nextra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\n#cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n#cat_one_hot_attribs = list(cat_encoder.categories_[0])\n#attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n\nrel_imp = pd.Series(feature_importances, index=final_model[\"preprocessing\"].get_feature_names_out()).sort_values(inplace=False)\nprint(rel_imp[-15:])\nrel_imp[-15:].T.plot(kind='barh', color='r')\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None\n\ngeo__Cluster 39 similarity     0.014103\ngeo__Cluster 17 similarity     0.014230\ngeo__Cluster 20 similarity     0.015547\ngeo__Cluster 37 similarity     0.017409\ngeo__Cluster 34 similarity     0.018598\ngeo__Cluster 43 similarity     0.019873\ngeo__Cluster 27 similarity     0.020545\ngeo__Cluster 24 similarity     0.023530\ngeo__Cluster 28 similarity     0.028793\ngeo__Cluster 0 similarity      0.038194\npeople_per_house__ratio        0.052623\nrooms_per_house__ratio         0.054470\nbedrooms__ratio                0.069264\ncat__ocean_proximity_INLAND    0.074819\nlog__median_income             0.186946\ndtype: float64\n\n\n\n\n\nWith this information, you may want to try dropping some of the less useful features (e.g., apparently only one ocean_proximity category is really useful, so you could try dropping the others)\nYou should also look at the specific errors that your system makes, then try to understand why it makes them and what could fix the problem (adding extra features or, on the contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\n\nNow is also a good time to ensure that your model not only works well on average, but also on all categories of districts, whether they’re rural or urban, rich or poor, northern or southern, minority or not, etc. Creating subsets of your validation set for each category takes a bit of work, but it’s important: if your model performs poorly on a whole category of districts, then it should probably not be deployed until the issue is solved, or at least it should not be used to make predictions for that category, as it may do more harm than good!\n\n\n\n1.6.4 Evaluate Your System on the Test Set\nAfter tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set. Just get the predictors and the labels from your test set, and run your final_model to transform the data and make predictions, then evaluate these predictions:\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nfinal_predictions = final_model.predict(X_test)\n\nfinal_rmse = mean_squared_error(y_test, final_predictions, squared=False)\nprint(final_rmse)\n\n41424.40026462184\n\n\nIn some cases, such a point estimate of the generalization error will not be quite enough to convince you to launch: what if it is just 0.1% better than the model currently in production? You might want to have an idea of how precise this estimate is.\nFor this, you can compute a 95% confidence interval for the generalization error using scipy.stats.t.interval():\n\nfrom scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))\n\narray([39275.40861216, 43467.27680583])\n\n\nWe could compute the interval manually like this:\n\nm = len(squared_errors)\nmean = squared_errors.mean()\ntscore = stats.t.ppf((1 + confidence) / 2, df=m - 1)\ntmargin = tscore * squared_errors.std(ddof=1) / np.sqrt(m)\nnp.sqrt(mean - tmargin), np.sqrt(mean + tmargin)\n\n(39275.40861216077, 43467.2768058342)\n\n\nAlternatively, we could use a z-scores rather than t-scores:\n\nzscore = stats.norm.ppf((1 + confidence) / 2)\nzmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m)\nnp.sqrt(mean - zmargin), np.sqrt(mean + zmargin)\n\n(39276.05610140007, 43466.691749969636)\n\n\nThe performance will usually be slightly worse than what you measured using cross-validation if you did a lot of hyperparameter tuning (because your system ends up fine-tuned to perform well on the validation data, and will likely not perform as well on unknown datasets). It is not the case in this example, but when this happens you must resist the temptation to tweak the hyperparameters to make the numbers look good on the test set; the improvements would be unlikely to generalize to new data.\nNow comes the project prelaunch phase: you need to present your solution highlighting what you have learned, what worked and what did not, what assumptions were made, and what your system’s limitations are), document everything, and create nice presentations with clear visualizations and easy-to-remember statements (e.g., “the median income is the number one predictor of housing prices”).\nIn this California housing example, the final performance of the system is not much better than the experts’, but it may still be a good idea to launch it, especially if this frees up some time for the experts so they can work on more interesting and productive tasks."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#launch-monitor-and-maintain-your-system",
    "href": "01_end_to_end_machine_learning_project.html#launch-monitor-and-maintain-your-system",
    "title": "1  End-to-end Machine Learning project",
    "section": "1.7 Launch, Monitor, and Maintain Your System",
    "text": "1.7 Launch, Monitor, and Maintain Your System\nPerfect, you got approval to launch! You now need to get your solution ready for production (e.g., polish the code, write documentation and tests, and so on). Then you can deploy your model to your production environment.\nThe most basic way to do this is just to save the best model you trained, transfer the file to your production environment, and load it. To save the model, you can use the joblib library like this:\n\nimport joblib\n\njoblib.dump(final_model, \"my_california_housing_model.pkl\")\n\n['my_california_housing_model.pkl']\n\n\nOnce your model is transferred to production, you can load it and use it. For this you must first import any custom classes and functions the model relies on (which means transferring the code to production), then load the model using joblib and use it to make predictions:\n\nimport joblib\n\nfinal_model_reloaded = joblib.load(\"my_california_housing_model.pkl\")\n\nnew_data = housing.iloc[:5]  # pretend these are new districts\npredictions = final_model_reloaded.predict(new_data)\n\nFor example, perhaps the model will be used within a website: the user will type in some data about a new district and click the Estimate Price button. This will send a query containing the data to the web server, which will forward it to your web application, and finally your code will simply call the model’s predict() method (you want to load the model upon server startup, rather than every time the model is used).\nAlternatively, you can wrap the model within a dedicated web service that your web application can query through a REST API. This makes it easier to upgrade your model to new versions without interrupting the main application. It also simplifies scaling, since you can start as many web services as needed and load-balance the requests coming from your web application across these web services. Moreover, it allows your web application to use any language, not just Python.\nAnother popular strategy is to deploy your model to the cloud, for example on Google’s Vertex AI (formerly known as Google Cloud AI Platform and Google Cloud ML Engine): just save your model using joblib and upload it to Google Cloud Storage (GCS), then head over to Vertex AI and create a new model version, pointing it to the GCS file. That’s it! This gives you a simple web service that takes care of load balancing and scaling for you.\nIt take JSON requests containing the input data (e.g., of a district) and returns JSON responses containing the predictions. You can then use this web service in your website (or whatever production environment you are using). As we will see in model serving lesson that we will use it on AI Platform is not much different from deploying Scikit-Learn models.\nBut deployment is not the end of the story. You also need to write monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops. This could be a steep drop, likely due to a broken component in your infrastructure, but be aware that it could also be a gentle decay that could easily go unnoticed for a long time. This is quite common because models tend to “rot” over time: indeed, the world changes, so if the model was trained with last year’s data, it may not be adapted to today’s data.\n\n1.7.1 Deployment\nSo you need to monitor your model’s live performance. But how do you that? Well, it depends. In some cases, the model’s performance can be inferred from downstream metrics. For example, if your model is part of a recommender system and it suggests products that the users may be interested in, then it’s easy to monitor the number of recommended products sold each day. If this number drops (compared to non-recommended products), then the prime suspect is the model. This may be because the data pipeline is broken, or perhaps the model needs to be retrained on fresh data.\nHowever, it’s not always possible to determine the model’s performance without any human analysis. For example, suppose you trained an image classification model to detect several product defects on a production line. How can you get an alert if the model’s performance drops, before thousands of defective products get shipped to your clients? One solution is to send to human raters a sample of all the pictures that the model classified (especially pictures that the model wasn’t so sure about). Depending on the task, the raters may need to be experts, or they could be nonspecialists, such as workers on a crowdsourcing platform (e.g., Amazon Mechanical Turk). In some applications they could even be the users themselves, responding for example via surveys or repurposed captchas. Either way, you need to put in place a monitoring system (with or without human raters to evaluate the live model), as well as all the relevant processes to define what to do in case of failures and how to prepare for them.\nUnfortunately, this can be a lot of work. In fact, it is often much more work than building and training a model. If the data keeps evolving, you will need to update your datasets and retrain your model regularly. You should probably automate the whole process as much as possible. Here are a few things you can automate:\n\nCollect fresh data regularly and label it (e.g., using human raters).\nWrite a script to train the model and fine-tune the hyperparameters automatically. This script could run automatically, for example every day or every week, depending on your needs.\nWrite another script that will evaluate both the new model and the previous model on the updated test set, and deploy the model to production if the performance has not decreased (if it did, make sure you investigate why). The script should probably test the performance of your model on various subsets of the test set, such as poor or rich districts, rural or urban districts, etc.\n\nYou should also make sure you evaluate the model’s input data quality. Sometimes performance will degrade slightly because of a poor-quality signal (e.g., a malfunctioning sensor sending random values, or another team’s output becoming stale), but it may take a while before your system’s performance degrades enough to trigger an alert. If you monitor your model’s inputs, you may catch this earlier. For example, you could trigger an alert if more and more inputs are missing a feature, or if its mean or standard deviation drifts too far from the training set, or a categorical feature starts containing new categories.\nFinally, make sure you keep backups of every model you create and have the process and tools in place to roll back to a previous model quickly, in case the new model starts failing badly for some reason. Having backups also makes it possible to easily compare new models with previous ones. Similarly, you should keep backups of every version of your datasets so that you can roll back to a previous dataset if the new one ever gets corrupted (e.g., if the fresh data that gets added to it turns out to be full of outliers). Having backups of your datasets also allows you to evaluate any model against any previous dataset.\nAs you can see, Machine Learning involves quite a lot of infrastructure, so don’t be surprised if your first ML project takes a lot of effort and time to build and deploy to production. Fortunately, once all the infrastructure is in place, going from idea to production will be much faster."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#low-code-ml-using-pycaret",
    "href": "01_end_to_end_machine_learning_project.html#low-code-ml-using-pycaret",
    "title": "1  End-to-end Machine Learning project",
    "section": "1.8 Low-code ML using PyCaret",
    "text": "1.8 Low-code ML using PyCaret\nPyCaret is a high-level, low-code Python library that makes it easy to compare, train, evaluate, tune, and deploy machine learning models with only a few lines of code.\nAt its core, PyCaret is basically just a large wrapper over many data science libraries such as Scikit-learn, Yellowbrick, SHAP, Optuna, and Spacy. Yes, you could use these libraries for the same tasks, but if you don’t want to write a lot of code, PyCaret could save you a lot of time.\nhttps://pycaret.readthedocs.io/en/latest/api/regression.html\n\n!pip install --pre pycaret[full] -qq\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 480.1/480.1 KB 25.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.2/9.2 MB 93.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.9/79.9 MB 11.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213.0/213.0 KB 28.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.2/280.2 KB 24.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.0/16.0 MB 73.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 81.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.8/106.8 KB 12.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.8/45.8 KB 6.5 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n  WARNING: Requested plotly-resampler&gt;=0.7.2.2 from https://files.pythonhosted.org/packages/d7/5e/71a9e34a36c1855d0c4e30a88405d58c4bbbe7ece802b188628a643f2cda/plotly_resampler-0.8.4rc1.tar.gz#sha256=154ebffa9778813fe457ac9112e71e085ac283644a1e26b869bd5c6b7bf064a2 (from pycaret[full]), but installing version 0.8.4rc1\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.2/81.2 KB 11.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 KB 9.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 KB 6.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 50.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.7/147.7 KB 20.6 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 84.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 99.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 93.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 324.4/324.4 KB 35.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.3/100.3 KB 14.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.2/362.2 KB 29.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.0/92.0 KB 10.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 61.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.6/76.6 MB 11.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.9/56.9 KB 7.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 70.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.0/83.0 KB 10.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.1/317.1 KB 28.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.2/88.2 KB 9.9 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 75.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 277.8/277.8 KB 29.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 575.9/575.9 KB 54.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.1/41.1 KB 6.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.5/177.5 KB 23.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.2/56.2 KB 7.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.2/14.2 MB 84.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.5/101.5 KB 17.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 365.3/365.3 KB 38.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.2/92.2 KB 12.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.4/57.4 MB 14.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.6/193.6 MB 7.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.7/64.7 KB 9.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.0/17.0 MB 68.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.7/132.7 KB 13.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 464.9/464.9 KB 37.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/12.9 MB 71.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.9/240.9 KB 19.2 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 66.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.6/16.6 MB 49.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 58.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/10.4 MB 105.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.6/79.6 KB 10.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.8/227.8 KB 1.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.3/54.3 KB 4.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 KB 8.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 508.4/508.4 KB 45.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/3.5 MB 94.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 KB 26.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.5/135.5 KB 20.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 793.8/793.8 KB 54.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.4/129.4 KB 17.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 KB 5.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 92.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 102.5 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 662.4/662.4 KB 53.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.7/57.7 KB 9.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.0/471.0 KB 50.7 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.0/47.0 KB 5.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 219.7/219.7 KB 25.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.1/83.1 KB 11.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.8/65.8 KB 10.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 KB 15.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 KB 19.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.7/232.7 KB 28.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 187.8/187.8 KB 22.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.1/154.1 KB 20.3 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.6/71.6 KB 8.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.5/227.5 KB 21.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 51.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.5/71.5 KB 9.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.0/107.0 KB 11.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.7/140.7 KB 15.9 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 59.1 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 KB 11.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.5/200.5 KB 21.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.9/8.9 MB 94.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 29.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 300.0/300.0 KB 19.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 965.4/965.4 KB 43.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.3/82.3 KB 11.5 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.5/147.5 KB 16.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.0/184.0 KB 23.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 KB 11.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 KB 6.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210.6/210.6 KB 22.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 27.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.1/51.1 KB 5.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 344.5/344.5 KB 34.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 68.8/68.8 KB 6.7 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.5/45.5 KB 4.8 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.6/44.6 KB 5.0 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.0/96.0 KB 8.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 185.1/185.1 KB 12.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.7/8.7 MB 8.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 KB 7.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.6/12.6 MB 10.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.5/85.5 MB 7.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.0/4.0 MB 22.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 24.0 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 KB 8.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.0/132.0 KB 16.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 983.2/983.2 KB 43.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 KB 11.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.9/55.9 KB 7.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 KB 17.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 934.8/934.8 KB 59.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 144.2/144.2 KB 20.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 KB 9.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 275.7/275.7 KB 32.1 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 72.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 78.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 758.0/758.0 KB 58.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 KB 7.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/18.5 MB 65.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 384.9/384.9 KB 39.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 KB 10.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.3/135.3 KB 15.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 468.5/468.5 KB 37.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.4/64.4 KB 8.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 136.5/136.5 KB 15.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.6/69.6 KB 8.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.0/121.0 KB 16.6 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.7/102.7 KB 10.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.4/9.4 MB 63.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 679.5/679.5 KB 43.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 87.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 296.5/296.5 KB 35.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 261.4/261.4 KB 27.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.4/133.4 KB 13.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.3/57.3 KB 8.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 357.2/357.2 KB 42.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.6/83.6 KB 11.6 MB/s eta 0:00:00\n  Building wheel for pyod (setup.py) ... done\n  Building wheel for umap-learn (setup.py) ... done\n  Building wheel for databricks-cli (setup.py) ... done\n  Building wheel for fugue-sql-antlr (setup.py) ... done\n  Building wheel for pynndescent (setup.py) ... done\n  Building wheel for PyNomaly (setup.py) ... done\n  Building wheel for dash-auth (setup.py) ... done\n  Building wheel for emoji (setup.py) ... done\n  Building wheel for ffmpy (setup.py) ... done\n  Building wheel for python-multipart (setup.py) ... done\n  Building wheel for htmlmin (setup.py) ... done\n  Building wheel for lime (setup.py) ... done\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 7.34.0 which is incompatible.\n\n\nRemember to restart the notebook to update the modules\n\n1.8.1 Get the data\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nhousing = load_housing_data() #Let’s take a look at the top five rows. Each row represents one district. There are 10 attributes\nhousing.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNow that we have the data, we can initialize a PyCaret experiment, which will preprocess the data and enable logging for all of the models that we will train on this dataset.\n\n\n1.8.2 Explore the data\nydata-profiling primary goal is to provide a one-line Exploratory Data Analysis (EDA) experience in a consistent and fast solution. Like pandas df.describe() function, that is so handy, ydata-profiling delivers an extended analysis of a DataFrame while allowing the data analysis to be exported in different formats such as html and json.\nhttps://github.com/ydataai/ydata-profiling\n\nfrom ydata_profiling import ProfileReport\n\n\nprofile = ProfileReport(housing, title=\"Profiling Report\")\n\n\nprofile.to_notebook_iframe()\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nThe setup() function initializes the environment in pycaret and creates the transformation pipeline to prepare the data for modeling and deployment. setup() must be called before executing any other function in pycaret. It takes two mandatory parameters: a pandas dataframe and the name of the target column. All other parameters are optional and are used to customize the pre-processing pipeline.\nWhen setup() is executed, PyCaret’s inference algorithm will automatically infer the data types for all features based on certain properties. The data type should be inferred correctly but this is not always the case. To account for this, PyCaret displays a table containing the features and their inferred data types after setup() is executed.\n\nEnsuring that the data types are correct is of fundamental importance in PyCaret as it automatically performs a few pre-processing tasks which are imperative to any machine learning experiment. These tasks are performed differently for each data type which means it is very important for them to be correctly configured.\n\nhttps://pycaret.gitbook.io/docs/get-started/functions/initialize\n\nfrom pycaret.regression import *\nexp1 = setup(data = housing, target = 'median_house_value', session_id=123)\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n123\n\n\n1\nTarget\nmedian_house_value\n\n\n2\nTarget type\nRegression\n\n\n3\nOriginal data shape\n(20640, 10)\n\n\n4\nTransformed data shape\n(20640, 14)\n\n\n5\nTransformed train set shape\n(14447, 14)\n\n\n6\nTransformed test set shape\n(6193, 14)\n\n\n7\nNumeric features\n8\n\n\n8\nCategorical features\n1\n\n\n9\nRows with missing values\n1.0%\n\n\n10\nPreprocess\nTrue\n\n\n11\nImputation type\nsimple\n\n\n12\nNumeric imputation\nmean\n\n\n13\nCategorical imputation\nmode\n\n\n14\nMaximum one-hot encoding\n25\n\n\n15\nEncoding method\nNone\n\n\n16\nFold Generator\nKFold\n\n\n17\nFold Number\n10\n\n\n18\nCPU Jobs\n-1\n\n\n19\nUse GPU\nFalse\n\n\n20\nLog Experiment\nFalse\n\n\n21\nExperiment Name\nreg-default-name\n\n\n22\nUSI\n1e11\n\n\n\n\n\nThe eda() function generates automated Exploratory Data Analysis (EDA) using the AutoViz library.\nhttps://pycaret.gitbook.io/docs/get-started/functions/analyze#eda\n\neda(display_format = 'bokeh')\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\n1.8.3 Prepare the data\nIn order to demonstrate the predict_model() function on unseen data, a sample of 10% records has been withheld from the original dataset to be used for predictions. This should not be confused with a train/test split as this particular split is performed to simulate a real life scenario. Another way to think about this is that these records are not available at the time when the machine learning experiment was performed.\n\nfrom sklearn.model_selection import train_test_split\n\nhousing, data_unseen = train_test_split(housing, test_size=0.1, random_state=42)\n\nhttps://pycaret.gitbook.io/docs/get-started/preprocessing\n\n\"\"\"\nclass columnDropperTransformer(TransformerMixin):\n    def __init__(self,columns):\n        self.columns=columns\n\n    def transform(self,X,y=None):\n        return X.drop(self.columns,axis=1)\n\n    def fit(self, X, y=None):\n        return self \n\"\"\"\n\n'\\nclass columnDropperTransformer(TransformerMixin):\\n    def __init__(self,columns):\\n        self.columns=columns\\n\\n    def transform(self,X,y=None):\\n        return X.drop(self.columns,axis=1)\\n\\n    def fit(self, X, y=None):\\n        return self \\n'\n\n\nNote that starting from PyCaret 3.0, it supports OOP API.\n\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\n\n# log_experiment = 'wandb',\nreg_experiment = setup(housing, \n            target = 'median_house_value', \n            train_size = 0.8,\n            data_split_stratify = ['income_cat'],\n            numeric_imputation = 'median',\n            categorical_imputation = 'mode',\n            normalize = True,\n            remove_outliers = True,\n            #custom_pipeline = columnDropperTransformer(['income_cat_1.0','income_cat_2.0','income_cat_3.0','income_cat_4.0','income_cat_5.0']),\n            session_id = 42, \n            #log_experiment=True,\n            profile = True,\n            experiment_name='ca_housing')\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n42\n\n\n1\nTarget\nmedian_house_value\n\n\n2\nTarget type\nRegression\n\n\n3\nOriginal data shape\n(18576, 11)\n\n\n4\nTransformed data shape\n(17951, 19)\n\n\n5\nTransformed train set shape\n(14236, 19)\n\n\n6\nTransformed test set shape\n(3716, 19)\n\n\n7\nNumeric features\n8\n\n\n8\nCategorical features\n2\n\n\n9\nRows with missing values\n1.0%\n\n\n10\nPreprocess\nTrue\n\n\n11\nImputation type\nsimple\n\n\n12\nNumeric imputation\nmedian\n\n\n13\nCategorical imputation\nmode\n\n\n14\nMaximum one-hot encoding\n25\n\n\n15\nEncoding method\nNone\n\n\n16\nRemove outliers\nTrue\n\n\n17\nOutliers threshold\n0.050000\n\n\n18\nNormalize\nTrue\n\n\n19\nNormalize method\nzscore\n\n\n20\nFold Generator\nKFold\n\n\n21\nFold Number\n10\n\n\n22\nCPU Jobs\n-1\n\n\n23\nUse GPU\nFalse\n\n\n24\nLog Experiment\nFalse\n\n\n25\nExperiment Name\nca_housing\n\n\n26\nUSI\n6018\n\n\n\n\n\nLoading profile... Please Wait!\n\n\n\nX_train_transformed = get_config(\"X_train_transformed\")\n\n\nX_train_transformed\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity_NEAR BAY\nocean_proximity_NEAR OCEAN\nocean_proximity_&lt;1H OCEAN\nocean_proximity_INLAND\nocean_proximity_ISLAND\nincome_cat_2.0\nincome_cat_1.0\nincome_cat_3.0\nincome_cat_4.0\nincome_cat_5.0\n\n\n\n\n0\n-1.364134\n1.046776\n1.859278\n0.197765\n0.481069\n-0.119739\n0.622318\n-1.069305\n2.901312\n-0.374453\n-0.909639\n-0.682598\n-0.014579\n1.421609\n-0.176182\n-0.749410\n-0.459495\n-0.342352\n\n\n1\n0.702926\n-0.861667\n0.235242\n-0.144908\n0.641720\n2.210955\n0.728542\n-0.811247\n-0.344672\n2.670564\n-0.909639\n-0.682598\n-0.014579\n1.421609\n-0.176182\n-0.749410\n-0.459495\n-0.342352\n\n\n3\n-0.889063\n0.379292\n-0.576776\n0.403949\n0.519485\n1.617214\n0.614730\n-0.619632\n-0.344672\n-0.374453\n1.099337\n-0.682598\n-0.014579\n1.421609\n-0.176182\n-0.749410\n-0.459495\n-0.342352\n\n\n4\n0.778736\n-0.894571\n-0.576776\n4.699704\n4.252885\n4.036448\n4.317402\n0.354490\n-0.344672\n-0.374453\n1.099337\n-0.682598\n-0.014579\n-0.703428\n-0.176182\n1.334383\n-0.459495\n-0.342352\n\n\n5\n0.025697\n-0.574930\n-0.414373\n0.763321\n0.376296\n0.597698\n0.550237\n0.781639\n-0.344672\n2.670564\n-0.909639\n-0.682598\n-0.014579\n-0.703428\n-0.176182\n-0.749410\n2.176301\n-0.342352\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n14854\n0.192478\n-0.664242\n0.072838\n-0.925360\n-0.898438\n-0.733011\n-0.861027\n0.026329\n-0.344672\n2.670564\n-0.909639\n-0.682598\n-0.014579\n-0.703428\n-0.176182\n1.334383\n-0.459495\n-0.342352\n\n\n14855\n1.344774\n-1.308224\n-0.901583\n0.901261\n0.868728\n0.729207\n0.975134\n-0.624362\n-0.344672\n-0.374453\n1.099337\n-0.682598\n-0.014579\n1.421609\n-0.176182\n-0.749410\n-0.459495\n-0.342352\n\n\n14856\n1.238642\n-1.275320\n-1.145189\n-0.385215\n-0.884468\n-0.584575\n-0.815502\n2.130376\n-0.344672\n-0.374453\n1.099337\n-0.682598\n-0.014579\n-0.703428\n-0.176182\n-0.749410\n-0.459495\n2.920971\n\n\n14858\n0.611955\n-0.730051\n1.534471\n-1.074917\n-0.814620\n-1.132744\n-0.849646\n-0.115229\n-0.344672\n-0.374453\n1.099337\n-0.682598\n-0.014579\n-0.703428\n-0.176182\n1.334383\n-0.459495\n-0.342352\n\n\n14859\n0.718087\n-0.734750\n0.803655\n0.014086\n-0.154552\n-0.075468\n-0.128839\n-0.016915\n-0.344672\n-0.374453\n1.099337\n-0.682598\n-0.014579\n-0.703428\n-0.176182\n1.334383\n-0.459495\n-0.342352\n\n\n\n\n\n14117 rows × 18 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nhttps://www.kaggle.com/code/uyeanil/titanic-custom-transformer-pipeline-pycaret as AutoML\n\n\"\"\"\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.impute import SimpleImputer\n\nclass ClusterSimilarity(BaseEstimator, TransformerMixin):\n    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n        self.n_clusters = n_clusters\n        self.gamma = gamma\n        self.random_state = random_state\n\n    def fit(self, X, y=None, sample_weight=None):\n        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n        self.kmeans_.fit(X, sample_weight=sample_weight)\n        return self  # always return self!\n\n    def transform(self, X):\n        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n    \n    def get_feature_names_out(self, names=None):\n        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n        \ndef column_ratio(X):\n    return X[:, [0]] / X[:, [1]]\n\ndef ratio_name(function_transformer, feature_names_in):\n    return [\"ratio\"]  # feature names out\n\ndef ratio_pipeline():\n    return make_pipeline(\n        SimpleImputer(strategy=\"median\"),\n        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n        StandardScaler())\n\nlog_pipeline = make_pipeline(\n    SimpleImputer(strategy=\"median\"),\n    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n    StandardScaler())\n\ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\ncat_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n    ])\ndefault_num_pipeline = make_pipeline(StandardScaler())\n\npreprocessing = ColumnTransformer([\n        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n                               \"households\", \"median_income\"]),\n        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n    ],\n    remainder=default_num_pipeline)  # one column remaining: housing_median_age\n\"\"\"\n\n\n\n1.8.4 Select and train a model\nComparing all models to evaluate performance is the recommended starting point for modeling once the setup is completed (unless you exactly know what kind of model you need, which is often not the case). This function trains all models in the model library and scores them using k-fold cross validation for metric evaluation. The output prints a score grid that shows average MAE, MSE, RMSE,R2, RMSLE and MAPE accross the folds (10 by default) along with training time.\nhttps://pycaret.gitbook.io/docs/get-started/functions/train\n\nbest_model = compare_models(sort = 'RMSE', fold=5, verbose=True)\n\n\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\nTT (Sec)\n\n\n\n\ncatboost\nCatBoost Regressor\n30861.9104\n2181034968.8429\n46691.5909\n0.8358\n0.2308\n0.1717\n6.1980\n\n\nlightgbm\nLight Gradient Boosting Machine\n32355.3704\n2354387054.3994\n48510.4899\n0.8227\n0.2380\n0.1806\n0.4200\n\n\nrf\nRandom Forest Regressor\n33150.0424\n2572097820.1463\n50704.3716\n0.8063\n0.2444\n0.1852\n6.1680\n\n\net\nExtra Trees Regressor\n37005.2973\n3043882815.8823\n55163.1011\n0.7707\n0.2641\n0.2075\n4.0100\n\n\ngbr\nGradient Boosting Regressor\n38408.0096\n3067882575.8698\n55381.6992\n0.7690\n0.2719\n0.2180\n2.3660\n\n\nknn\nK Neighbors Regressor\n43466.1945\n4131793971.2000\n64271.9602\n0.6888\n0.3034\n0.2376\n0.2460\n\n\nbr\nBayesian Ridge\n49242.1016\n4720977912.2509\n68696.1739\n0.6445\n0.3801\n0.2878\n0.2680\n\n\nridge\nRidge Regression\n49252.5068\n4722997394.5170\n68710.6807\n0.6443\n0.3779\n0.2880\n0.1380\n\n\nlasso\nLasso Regression\n49254.5804\n4723361290.9467\n68713.2920\n0.6443\n0.3780\n0.2880\n0.1860\n\n\nllar\nLasso Least Angle Regression\n49254.4740\n4723363267.9404\n68713.3056\n0.6443\n0.3780\n0.2880\n0.2560\n\n\nlar\nLeast Angle Regression\n49255.2461\n4723522771.5529\n68714.4540\n0.6443\n0.3780\n0.2880\n0.2480\n\n\nlr\nLinear Regression\n49255.2461\n4723522771.5529\n68714.4540\n0.6443\n0.3780\n0.2880\n1.3200\n\n\nhuber\nHuber Regressor\n47867.1926\n4780685846.9827\n69135.6429\n0.6400\n0.3670\n0.2630\n0.3700\n\n\npar\nPassive Aggressive Regressor\n47814.5390\n4846281518.3859\n69609.0491\n0.6351\n0.3529\n0.2576\n0.9300\n\n\ndt\nDecision Tree Regressor\n44960.7247\n5036168100.5834\n70933.1545\n0.6209\n0.3270\n0.2450\n0.2160\n\n\nen\nElastic Net\n53387.4699\n5226586829.8307\n72292.3985\n0.6065\n0.3573\n0.3180\n0.1400\n\n\nomp\nOrthogonal Matching Pursuit\n62099.6132\n6940291836.2596\n83299.1586\n0.4773\n0.4256\n0.3821\n0.2980\n\n\nada\nAdaBoost Regressor\n78936.3612\n8428966889.7267\n91785.6668\n0.3653\n0.4911\n0.5492\n1.2680\n\n\ndummy\nDummy Regressor\n90681.3531\n13288744960.0000\n115272.3750\n-0.0002\n0.5881\n0.6153\n0.2420\n\n\n\n\n\n\n\n\n\n\n\nThe score grid printed above highlights the highest performing metric for comparison purposes only. By passing sort parameter, compare_models(sort = 'RMSE') will sort the grid by RMSE (lower to higher since lower is better). We also change the fold parameter from the default value of 10 to a different value by using compare_models(fold = 5) which will compare all models on 5 fold cross validation. Reducing the number of folds will improve the training time. By default, compare_models return the best performing model based on default sort order but can be used to return a list of top N models by using n_select parameter. Notice that you can also use exclude parameter to block certain models.\nThere are 25 regressors available in the model library of PyCaret. To see list of all regressors either check the docstring or use models() function to see the library.\n\nmodels()\n\n\n  \n    \n      \n\n\n\n\n\n\nName\nReference\nTurbo\n\n\nID\n\n\n\n\n\n\n\nlr\nLinear Regression\nsklearn.linear_model._base.LinearRegression\nTrue\n\n\nlasso\nLasso Regression\nsklearn.linear_model._coordinate_descent.Lasso\nTrue\n\n\nridge\nRidge Regression\nsklearn.linear_model._ridge.Ridge\nTrue\n\n\nen\nElastic Net\nsklearn.linear_model._coordinate_descent.ElasticNet\nTrue\n\n\nlar\nLeast Angle Regression\nsklearn.linear_model._least_angle.Lars\nTrue\n\n\nllar\nLasso Least Angle Regression\nsklearn.linear_model._least_angle.LassoLars\nTrue\n\n\nomp\nOrthogonal Matching Pursuit\nsklearn.linear_model._omp.OrthogonalMatchingPursuit\nTrue\n\n\nbr\nBayesian Ridge\nsklearn.linear_model._bayes.BayesianRidge\nTrue\n\n\nard\nAutomatic Relevance Determination\nsklearn.linear_model._bayes.ARDRegression\nFalse\n\n\npar\nPassive Aggressive Regressor\nsklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor\nTrue\n\n\nransac\nRandom Sample Consensus\nsklearn.linear_model._ransac.RANSACRegressor\nFalse\n\n\ntr\nTheilSen Regressor\nsklearn.linear_model._theil_sen.TheilSenRegressor\nFalse\n\n\nhuber\nHuber Regressor\nsklearn.linear_model._huber.HuberRegressor\nTrue\n\n\nkr\nKernel Ridge\nsklearn.kernel_ridge.KernelRidge\nFalse\n\n\nsvm\nSupport Vector Regression\nsklearn.svm._classes.SVR\nFalse\n\n\nknn\nK Neighbors Regressor\nsklearn.neighbors._regression.KNeighborsRegressor\nTrue\n\n\ndt\nDecision Tree Regressor\nsklearn.tree._classes.DecisionTreeRegressor\nTrue\n\n\nrf\nRandom Forest Regressor\nsklearn.ensemble._forest.RandomForestRegressor\nTrue\n\n\net\nExtra Trees Regressor\nsklearn.ensemble._forest.ExtraTreesRegressor\nTrue\n\n\nada\nAdaBoost Regressor\nsklearn.ensemble._weight_boosting.AdaBoostRegressor\nTrue\n\n\ngbr\nGradient Boosting Regressor\nsklearn.ensemble._gb.GradientBoostingRegressor\nTrue\n\n\nmlp\nMLP Regressor\nsklearn.neural_network._multilayer_perceptron.MLPRegressor\nFalse\n\n\nxgboost\nExtreme Gradient Boosting\nxgboost.sklearn.XGBRegressor\nTrue\n\n\nlightgbm\nLight Gradient Boosting Machine\nlightgbm.sklearn.LGBMRegressor\nTrue\n\n\ncatboost\nCatBoost Regressor\ncatboost.core.CatBoostRegressor\nTrue\n\n\ndummy\nDummy Regressor\nsklearn.dummy.DummyRegressor\nTrue\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\ncreate_model() is the most granular function in PyCaret and is often the foundation behind most of the PyCaret functionalities. As the name suggests this function trains and evaluates a model using cross validation that can be set with fold parameter. The output prints a score grid that shows MAE, MSE, RMSE, R2, RMSLE and MAPE by fold.\n\nlightgbm = create_model('lightgbm')\n\n\n\n\n\n\n\n\n\n \nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\nFold\n \n \n \n \n \n \n\n\n\n\n0\n32167.4099\n2367963276.0495\n48661.7229\n0.8085\n0.2347\n0.1779\n\n\n1\n30984.6322\n2126158343.9367\n46110.2846\n0.8432\n0.2215\n0.1683\n\n\n2\n32185.5007\n2194345568.5475\n46843.8424\n0.8403\n0.2336\n0.1781\n\n\n3\n32027.3585\n2474181783.1165\n49741.1478\n0.8163\n0.2314\n0.1733\n\n\n4\n32129.8307\n2404652199.2175\n49037.2532\n0.8279\n0.2486\n0.1844\n\n\n5\n31591.5676\n2244223522.5267\n47373.2364\n0.8248\n0.2421\n0.1823\n\n\n6\n32913.7391\n2408284988.0534\n49074.2803\n0.8144\n0.2315\n0.1744\n\n\n7\n33253.8539\n2476742658.1617\n49766.8831\n0.8121\n0.2603\n0.2009\n\n\n8\n30294.3307\n2075253002.2693\n45554.9449\n0.8428\n0.2271\n0.1742\n\n\n9\n32522.3678\n2296816948.4132\n47925.1181\n0.8304\n0.2328\n0.1789\n\n\nMean\n32007.0591\n2306862229.0292\n48008.8714\n0.8261\n0.2364\n0.1793\n\n\nStd\n828.6481\n135432354.9804\n1417.9208\n0.0124\n0.0107\n0.0084\n\n\n\n\n\n\n\n\n\n\n\n\nlightgbm.get_params()\n\n{'boosting_type': 'gbdt',\n 'class_weight': None,\n 'colsample_bytree': 1.0,\n 'importance_type': 'split',\n 'learning_rate': 0.1,\n 'max_depth': -1,\n 'min_child_samples': 20,\n 'min_child_weight': 0.001,\n 'min_split_gain': 0.0,\n 'n_estimators': 100,\n 'n_jobs': -1,\n 'num_leaves': 31,\n 'objective': None,\n 'random_state': 42,\n 'reg_alpha': 0.0,\n 'reg_lambda': 0.0,\n 'silent': 'warn',\n 'subsample': 1.0,\n 'subsample_for_bin': 200000,\n 'subsample_freq': 0}\n\n\n\n\n1.8.5 Fine-Tune your model\nhttps://pycaret.gitbook.io/docs/get-started/functions/optimize\nhttps://pycaret.gitbook.io/docs/get-started/functions/others#automl\nhttps://pycaret.gitbook.io/docs/get-started/functions/optimize#ensemble_model\nWhen a model is created using the create_model function it uses the default hyperparameters to train the model. In order to tune hyperparameters, the tune_model function is used. This function automatically tunes the hyperparameters of a model using Random Grid Search on a pre-defined search space. The output prints a score grid that shows MAE, MSE, RMSE, R2, RMSLE and MAPE by fold. To use the custom search grid, you can pass custom_grid parameter in the tune_model function.\n\n#lgbm_params = {'num_leaves': np.arange(10,200,10),\n#                        'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n#                        'learning_rate': np.arange(0.1,1,0.1)\n#                        }\ntuned_lightgbm = tune_model(lightgbm, n_iter = 20, optimize = 'RMSE')\n\n\n\n\n\n\n\n\n\n \nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\nFold\n \n \n \n \n \n \n\n\n\n\n0\n32351.2276\n2387101345.3071\n48857.9712\n0.8069\n0.2367\n0.1792\n\n\n1\n31455.2358\n2118511518.6187\n46027.2910\n0.8438\n0.2208\n0.1698\n\n\n2\n32328.5698\n2238357372.3281\n47311.2817\n0.8371\n0.2347\n0.1778\n\n\n3\n31700.3079\n2440822708.2834\n49404.6831\n0.8188\n0.2295\n0.1688\n\n\n4\n32539.5859\n2446707135.0858\n49464.2005\n0.8249\n0.2557\n0.1874\n\n\n5\n31239.9483\n2198566379.3510\n46888.8727\n0.8284\n0.2394\n0.1791\n\n\n6\n32818.6665\n2397818205.7179\n48967.5219\n0.8152\n0.2312\n0.1741\n\n\n7\n33076.8939\n2438819409.6154\n49384.4045\n0.8149\n0.2605\n0.1986\n\n\n8\n30685.3025\n2109069565.3140\n45924.6074\n0.8402\n0.2293\n0.1752\n\n\n9\n32208.8041\n2309496367.8215\n48057.2197\n0.8295\n0.2362\n0.1793\n\n\nMean\n32040.4542\n2308527000.7443\n48028.8054\n0.8260\n0.2374\n0.1789\n\n\nStd\n712.2017\n126737551.3240\n1326.9723\n0.0115\n0.0115\n0.0083\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 20 candidates, totalling 200 fits\n\n\n\n\n\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\n\n1.8.6 Analyze the model\nBefore model finalization, the plot_model() function can be used to analyze the performance across different aspects such as Residuals Plot, Prediction Error, Feature Importance etc. This function takes a trained model object and returns a plot based on the test / hold-out set.\nThe evaluate_model() displays a user interface for analyzing the performance of a trained model. It calls the plot_model() function internally.\n\nevaluate_model(tuned_lightgbm)\n\n\n\n\nhttps://pycaret.gitbook.io/docs/get-started/functions/analyze\n\n#plot_model(tuned_lightgbm)\n#plot_model(tuned_lightgbm, plot = 'error')\n#plot_model(tuned_lightgbm, plot = 'feature')\n\nThe interpret_model() analyzes the predictions generated from a trained model. Most plots in this function are implemented based on the SHAP (Shapley Additive exPlanations)\n\ninterpret_model(tuned_lightgbm)\n\n\n\n\n\n\n1.8.7 Predict on test set\nBefore finalizing the model, it is advisable to perform one final check by predicting the test/hold-out set and reviewing the evaluation metrics.\n\npredict_model(tuned_lightgbm)\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\nLight Gradient Boosting Machine\n32502.7734\n2287056237.8276\n47823.1768\n0.8326\n0.2390\n0.1826\n\n\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\nincome_cat\nmedian_house_value\nprediction_label\n\n\n\n\n14860\n-117.650002\n34.020000\n9.0\n2107.0\n411.0\n1138.0\n389.0\n4.4042\nINLAND\n3\n159100.0\n166584.740965\n\n\n14861\n-117.680000\n34.150002\n4.0\n4082.0\n578.0\n1996.0\n580.0\n6.7813\nINLAND\n5\n286300.0\n263679.509928\n\n\n14862\n-118.290001\n34.080002\n23.0\n1864.0\n937.0\n2795.0\n858.0\n1.8495\n&lt;1H OCEAN\n2\n212500.0\n212056.088491\n\n\n14863\n-117.970001\n33.990002\n23.0\n3335.0\n570.0\n1560.0\n555.0\n5.7268\n&lt;1H OCEAN\n4\n300300.0\n271747.627121\n\n\n14864\n-117.320000\n34.099998\n42.0\n801.0\n176.0\n711.0\n183.0\n1.8681\nINLAND\n2\n59700.0\n65521.582721\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n18571\n-121.000000\n37.259998\n45.0\n1750.0\n371.0\n847.0\n354.0\n1.7062\nINLAND\n2\n77400.0\n89862.446840\n\n\n18572\n-122.169998\n37.720001\n43.0\n3783.0\n814.0\n2139.0\n789.0\n4.0202\nNEAR BAY\n3\n166300.0\n196701.827793\n\n\n18573\n-117.120003\n32.580002\n34.0\n2003.0\n466.0\n1226.0\n443.0\n3.0613\nNEAR OCEAN\n3\n136700.0\n135883.426084\n\n\n18574\n-122.040001\n38.250000\n52.0\n582.0\n131.0\n241.0\n106.0\n2.4000\nINLAND\n2\n125000.0\n130034.478754\n\n\n18575\n-120.849998\n37.490002\n42.0\n264.0\n72.0\n310.0\n70.0\n1.4063\nINLAND\n1\n61500.0\n95609.262658\n\n\n\n\n\n3716 rows × 12 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n1.8.8 Finalize model\nModel finalization is the last step in the experiment. A normal machine learning workflow in PyCaret starts with setup(), followed by comparing all models using compare_models() and shortlisting a few candidate models (based on the metric of interest) to perform several modeling techniques such as hyperparameter tuning, ensembling, stacking etc. This workflow will eventually lead you to the best model for use in making predictions on new and unseen data. The finalize_model() function fits the model onto the complete dataset including the test/hold-out sample (20% in this case). The purpose of this function is to train the model on the complete dataset before it is deployed in production.\n\nfinal_lightgbm = finalize_model(tuned_lightgbm)\n\n\npredict_model(final_lightgbm)\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\nLight Gradient Boosting Machine\n29413.3892\n1872817647.7416\n43276.0632\n0.8629\n0.2199\n0.1665\n\n\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\nincome_cat\nmedian_house_value\nprediction_label\n\n\n\n\n14860\n-117.650002\n34.020000\n9.0\n2107.0\n411.0\n1138.0\n389.0\n4.4042\nINLAND\n3\n159100.0\n162164.238793\n\n\n14861\n-117.680000\n34.150002\n4.0\n4082.0\n578.0\n1996.0\n580.0\n6.7813\nINLAND\n5\n286300.0\n273877.534930\n\n\n14862\n-118.290001\n34.080002\n23.0\n1864.0\n937.0\n2795.0\n858.0\n1.8495\n&lt;1H OCEAN\n2\n212500.0\n209148.262865\n\n\n14863\n-117.970001\n33.990002\n23.0\n3335.0\n570.0\n1560.0\n555.0\n5.7268\n&lt;1H OCEAN\n4\n300300.0\n277503.747233\n\n\n14864\n-117.320000\n34.099998\n42.0\n801.0\n176.0\n711.0\n183.0\n1.8681\nINLAND\n2\n59700.0\n63081.895276\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n18571\n-121.000000\n37.259998\n45.0\n1750.0\n371.0\n847.0\n354.0\n1.7062\nINLAND\n2\n77400.0\n93080.033327\n\n\n18572\n-122.169998\n37.720001\n43.0\n3783.0\n814.0\n2139.0\n789.0\n4.0202\nNEAR BAY\n3\n166300.0\n193502.069024\n\n\n18573\n-117.120003\n32.580002\n34.0\n2003.0\n466.0\n1226.0\n443.0\n3.0613\nNEAR OCEAN\n3\n136700.0\n130554.077380\n\n\n18574\n-122.040001\n38.250000\n52.0\n582.0\n131.0\n241.0\n106.0\n2.4000\nINLAND\n2\n125000.0\n111131.581617\n\n\n18575\n-120.849998\n37.490002\n42.0\n264.0\n72.0\n310.0\n70.0\n1.4063\nINLAND\n1\n61500.0\n90052.157496\n\n\n\n\n\n3716 rows × 12 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe predict_model() function is also used to predict on the unseen dataset. data_unseen is the variable created at the beginning of the tutorial and contains 10% of the original dataset which was never exposed to PyCaret\n\ndata_unseen[\"income_cat\"] = pd.cut(data_unseen[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\nunseen_predictions = predict_model(final_lightgbm, data=data_unseen)\nunseen_predictions.head()\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\nLight Gradient Boosting Machine\n32474.6118\n2364513076.8764\n48626.2591\n0.8164\n0.2404\n0.1824\n\n\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\nincome_cat\nmedian_house_value\nprediction_label\n\n\n\n\n0\n-122.379997\n40.669998\n10.0\n2281.0\n444.0\n1274.0\n438.0\n2.2120\nINLAND\n2\n65600.0\n68770.661309\n\n\n1\n-118.370003\n33.830002\n35.0\n1207.0\n207.0\n601.0\n213.0\n4.7308\n&lt;1H OCEAN\n4\n353400.0\n316812.741266\n\n\n2\n-117.239998\n32.720001\n39.0\n3089.0\n431.0\n1175.0\n432.0\n7.5925\nNEAR OCEAN\n5\n466700.0\n442420.543218\n\n\n3\n-118.440002\n34.049999\n18.0\n4780.0\n1192.0\n1886.0\n1036.0\n4.4674\n&lt;1H OCEAN\n3\n500001.0\n431798.750579\n\n\n4\n-118.440002\n34.180000\n33.0\n2127.0\n414.0\n1056.0\n391.0\n4.3750\n&lt;1H OCEAN\n3\n286100.0\n262450.257792\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nfrom pycaret.utils.generic import check_metric\ncheck_metric(unseen_predictions.median_house_value, unseen_predictions.prediction_label, 'RMSE')\n\n48626.2591\n\n\n\n\n1.8.9 Save and load the model\nWe have now finished the experiment by finalizing the tuned_lightgbm model which is now stored in final_lightgbm variable. We have also used the model stored in final_lightgbm to predict data_unseen. This brings us to the end of our experiment, but one question is still to be asked: What happens when you have more new data to predict? Do you have to go through the entire experiment again? The answer is no, PyCaret’s inbuilt function save_model() allows you to save the model along with entire transformation pipeline for later use.\n\nsave_model(final_lightgbm,'Final LightGBM Model 02Feb2022')\n\nTransformation Pipeline and Model Successfully Saved\n\n\n(Pipeline(memory=FastMemory(location=/tmp/joblib),\n          steps=[('numerical_imputer',\n                  TransformerWrapper(include=['longitude', 'latitude',\n                                              'housing_median_age',\n                                              'total_rooms', 'total_bedrooms',\n                                              'population', 'households',\n                                              'median_income'],\n                                     transformer=SimpleImputer(strategy='median'))),\n                 ('categorical_imputer',\n                  TransformerWrapper(include=['ocean_proximity', 'income_cat...\n                  TransformerWrapper(include=['ocean_proximity', 'income_cat'],\n                                     transformer=OneHotEncoder(cols=['ocean_proximity',\n                                                                     'income_cat'],\n                                                               handle_missing='return_nan',\n                                                               use_cat_names=True))),\n                 ('remove_outliers',\n                  TransformerWrapper(transformer=RemoveOutliers())),\n                 ('normalize', TransformerWrapper(transformer=StandardScaler())),\n                 ('actual_estimator', LGBMRegressor(random_state=42))]),\n 'Final LightGBM Model 02Feb2022.pkl')\n\n\nTo load a saved model at a future date in the same or an alternative environment, we would use PyCaret’s load_model() function and then easily apply the saved model on new unseen data for prediction.\n\nsaved_final_lightgbm = load_model('Final LightGBM Model 02Feb2022')\n\nTransformation Pipeline and Model Successfully Loaded\n\n\n\nnew_prediction = predict_model(saved_final_lightgbm, data=data_unseen)\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\nLight Gradient Boosting Machine\n32474.6118\n2364513076.8764\n48626.2591\n0.8164\n0.2404\n0.1824\n\n\n\n\n\n\nfrom pycaret.utils.generic import check_metric\ncheck_metric(new_prediction.median_house_value, new_prediction.prediction_label, 'RMSE')\n\n48626.2591\n\n\n\n\n1.8.10 Deplpy\nhttps://pycaret.gitbook.io/docs/get-started/functions/deploy\nhttps://pycaret.gitbook.io/docs/get-started/functions/others#get_config\n\ncreate_app(final_lightgbm)\n\nColab notebook detected. To show errors in colab notebook, set debug=True in launch()\nNote: opening Chrome Inspector may crash demo inside Colab notebooks.\n\nTo create a public link, set `share=True` in `launch()`.\n\n\n\n\n\n\n\n\n\n# deploy a model\n# deploy_model(final_lightgbm, model_name = 'final_lightgbm', platform = 'gcp', authentication = {'project': 'gcp-project-name', 'bucket' : 'gcp-bucket-name'})"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#reference",
    "href": "01_end_to_end_machine_learning_project.html#reference",
    "title": "1  End-to-end Machine Learning project",
    "section": "1.9 Reference",
    "text": "1.9 Reference\n\nhttps://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb\nhttps://pycaret.org/\nhttps://pycaret.readthedocs.io/en/latest/"
  },
  {
    "objectID": "02_Dataset.html#setup",
    "href": "02_Dataset.html#setup",
    "title": "2  Framing the problem and constructing the dataset",
    "section": "2.1 Setup",
    "text": "2.1 Setup\n\n!pip install git+https://github.com/phonchi/pigeonXT.git \n!pip install cleanlab -qq\n!pip install modAL -qq\n!pip install snorkel -qq\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting git+https://github.com/phonchi/pigeonXT.git\n  Cloning https://github.com/phonchi/pigeonXT.git to /tmp/pip-req-build-3jpt3q7q\n  Running command git clone --filter=blob:none --quiet https://github.com/phonchi/pigeonXT.git /tmp/pip-req-build-3jpt3q7q\n  Resolved https://github.com/phonchi/pigeonXT.git to commit 6564faf5101f33724a63a36231f74a3c4fa2ff3c\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting numpy&lt;2.0,&gt;=1.23\n  Downloading numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 38.4 MB/s eta 0:00:00\nRequirement already satisfied: pandas&lt;2.0,&gt;=1.3 in /usr/local/lib/python3.8/dist-packages (from pigeonxt-jupyter==0.7.3) (1.3.5)\nRequirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas&lt;2.0,&gt;=1.3-&gt;pigeonxt-jupyter==0.7.3) (2022.7.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas&lt;2.0,&gt;=1.3-&gt;pigeonxt-jupyter==0.7.3) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&lt;2.0,&gt;=1.3-&gt;pigeonxt-jupyter==0.7.3) (1.15.0)\nBuilding wheels for collected packages: pigeonxt-jupyter\n  Building wheel for pigeonxt-jupyter (pyproject.toml) ... done\n  Created wheel for pigeonxt-jupyter: filename=pigeonxt_jupyter-0.7.3-py3-none-any.whl size=12846 sha256=7c5f5b55a53ee9fc099d85c70fc907a29f7561eaf5a99a13f3b4e79d2ee8270d\n  Stored in directory: /tmp/pip-ephem-wheel-cache-t836gymf/wheels/18/f0/1e/314aebea9e2497eb5ebe4d5eddf3243311bd319fc13a6aee71\nSuccessfully built pigeonxt-jupyter\nInstalling collected packages: numpy, pigeonxt-jupyter\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscipy 1.7.3 requires numpy&lt;1.23.0,&gt;=1.16.5, but you have numpy 1.24.2 which is incompatible.\nnumba 0.56.4 requires numpy&lt;1.24,&gt;=1.18, but you have numpy 1.24.2 which is incompatible.\nSuccessfully installed numpy-1.24.2 pigeonxt-jupyter-0.7.3\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 157.5/157.5 KB 6.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.9/16.9 MB 49.1 MB/s eta 0:00:00\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npigeonxt-jupyter 0.7.3 requires numpy&lt;2.0,&gt;=1.23, but you have numpy 1.22.4 which is incompatible.\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.3/103.3 KB 6.0 MB/s eta 0:00:00\n\n\n\n# Built-in function\nimport glob\nimport os\nimport subprocess\nimport re\nimport random\n\n# Scientific computing\nimport numpy as np\nimport pandas as pd\nfrom IPython import display\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# Modeling\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\n# Manual labeling\nimport pigeonXT as pixt\n\n# Concensus algorithm\nfrom cleanlab.multiannotator import get_label_quality_multiannotator, get_majority_vote_label\nfrom cleanlab.benchmarking.noise_generation import generate_noise_matrix_from_trace\nfrom cleanlab.benchmarking.noise_generation import generate_noisy_labels\n\n# Active learning\nfrom modAL.models import ActiveLearner\nfrom modAL.uncertainty import uncertainty_sampling\n\n# Weak supervision\nfrom snorkel.labeling import PandasLFApplier\nfrom snorkel.labeling import labeling_function\nfrom snorkel.labeling import LFAnalysis\nfrom snorkel.labeling import LabelingFunction\nfrom snorkel.labeling.model import MajorityLabelVoter\nfrom snorkel.labeling.model import LabelModel\nfrom snorkel.labeling import filter_unlabeled_dataframe\nfrom snorkel.utils import probs_to_preds\nfrom snorkel.preprocess.nlp import SpacyPreprocessor\nfrom snorkel.labeling.lf.nlp import nlp_labeling_function\n\n# Data augmentation\nfrom skimage import io\nimport albumentations as A\n\n/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n\n\n\ndef visualize(image):\n    plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    plt.imshow(image)"
  },
  {
    "objectID": "02_Dataset.html#scraping-data-using-beautifulsoup",
    "href": "02_Dataset.html#scraping-data-using-beautifulsoup",
    "title": "2  Framing the problem and constructing the dataset",
    "section": "2.2 Scraping data using BeautifulSoup",
    "text": "2.2 Scraping data using BeautifulSoup\nSometimes we have to build our dataset using crawler. See 1. https://github.com/marvelje/weather_forecasting (Constructing weather dataset)\n\nhttps://www.kaggle.com/code/pandula/a-simple-tutorial-on-beautifulsoup/notebook (Constructing IMDB dataset)\nhttps://github.com/sharmasapna/Web-scraping-for-images (Constructing image dataset)"
  },
  {
    "objectID": "02_Dataset.html#manual-labeling-with-pigeonxt",
    "href": "02_Dataset.html#manual-labeling-with-pigeonxt",
    "title": "2  Framing the problem and constructing the dataset",
    "section": "2.3 Manual Labeling with pigeonXT",
    "text": "2.3 Manual Labeling with pigeonXT\nIn many data science projects, the first step at which the data science team gets involved is in labeling the image data. Even if the labeling will be automated, the first few images in a proof of concept are almost always hand-labeled. The form and organization will differ based on the problem type (image classification or object detection) and whether an image can have multiple labels or only one. To hand-label images, a rater views the image, determines the label(s), and records the label(s). There are two typical approaches to doing this recording: using a folder structure and a metadata table.\nIn a folder organization, raters simply move images to different folders depending on what their label is. All flowers that are daisies are stored in a folder named daisy, for example. Raters can do this quickly because most operating systems provide previews of images and handy ways to select groups of images and move them into folders.\nThe problem with the folder approach is that it leads to duplication if an image can have multiple labels—for example, if an image contains both roses and daisies. The alternative, and recommended, approach is to record the label(s) in a metadata table (such as in a spreadsheet or a CSV file) that has at least two columns - one column is the filename of the image file, and the other is the list of labels that are valid for the image.\nA labeling tool should have a facility to display the image, and enable the rater to quickly select valid categories and save the rating to a database. We will use pigeonXT which is a wrapper of Jupyter widget next.\n\n2.3.1 Image classification\n\n%%bash\nmkdir flower_images\nfor filename in 100080576_f52e8ee070_n.jpg 10140303196_b88d3d6cec.jpg 10172379554_b296050f82_n.jpg; do\n  gsutil cp gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/$filename flower_images\ndone\n\nCopying gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/100080576_f52e8ee070_n.jpg...\n/ [0 files][    0.0 B/ 26.2 KiB]                                                / [1 files][ 26.2 KiB/ 26.2 KiB]                                                \nOperation completed over 1 objects/26.2 KiB.                                     \nCopying gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/10140303196_b88d3d6cec.jpg...\n/ [0 files][    0.0 B/114.5 KiB]                                                / [1 files][114.5 KiB/114.5 KiB]                                                \nOperation completed over 1 objects/114.5 KiB.                                    \nCopying gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/10172379554_b296050f82_n.jpg...\n/ [0 files][    0.0 B/ 35.6 KiB]                                                / [1 files][ 35.6 KiB/ 35.6 KiB]                                                \nOperation completed over 1 objects/35.6 KiB.                                     \n\n\n\nfilenames = glob.glob('flower_images/*.jpg')\nprint(filenames)\n\n['flower_images/10140303196_b88d3d6cec.jpg', 'flower_images/100080576_f52e8ee070_n.jpg', 'flower_images/10172379554_b296050f82_n.jpg']\n\n\n\nannotations = pixt.annotate(\n      filenames,\n      options=['daisy', 'tulip', 'rose'],\n      display_fn=lambda filename: display.display(display.Image(filename))\n)\n\n\n\n\n\n\n\n\n\n\nAnnotation done.\n\n\n\nannotations\n\n\n  \n    \n      \n\n\n\n\n\n\nexample\nchanged\nlabel\n\n\n\n\n0\nflower_images/10140303196_b88d3d6cec.jpg\nFalse\n\n\n\n1\nflower_images/100080576_f52e8ee070_n.jpg\nTrue\ndaisy\n\n\n2\nflower_images/10172379554_b296050f82_n.jpg\nTrue\ndaisy\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n2.3.2 Binary or multi-class text classification\n\nannotations = pixt.annotate(\n        ['I love this movie', 'I was really disappointed by the book'],\n        options=['positive', 'negative', 'inbetween']\n    )\n\n\n\n\n\n\n\n\n\n\nAnnotation done.\n\n\n\nannotations\n\n\n  \n    \n      \n\n\n\n\n\n\nexample\nchanged\nlabel\n\n\n\n\n0\nI love this movie\nTrue\npositive\n\n\n1\nI was really disappointed by the book\nTrue\nnegative\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe output can be save to a CSV or JSON file.\n\nannotations.to_csv(\"text_class_dataset.csv\")\n\n\n\n2.3.3 Multi-label text classification\n\ndf = pd.DataFrame([\n        {'example': 'Star wars'},    \n        {'example': 'The Positively True Adventures of the Alleged Texas Cheerleader-Murdering Mom'},\n        {'example': 'Eternal Sunshine of the Spotless Mind'},\n        {'example': 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb'},    \n        {'example': 'Killer klowns from outer space'},    \n    ])\n\nlabels = ['Adventure', 'Romance', 'Fantasy', 'Science fiction', 'Horror', 'Thriller']\n    \nannotations = pixt.annotate(\n        df, \n        options=labels, \n        task_type='multilabel-classification',\n        buttons_in_a_row=3,\n        reset_buttons_after_click=True,\n        include_next=True,\n        include_back=True,\n    )  \n\n\n\n\n\n\n\n\n\n\n\nannotations\n\n\n  \n    \n      \n\n\n\n\n\n\nexample\nchanged\nAdventure\nRomance\nFantasy\nScience fiction\nHorror\nThriller\n\n\n\n\n0\nStar wars\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1\nThe Positively True Adventures of the Alleged ...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nEternal Sunshine of the Spotless Mind\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nDr. Strangelove or: How I Learned to Stop Worr...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nKiller klowns from outer space\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nannotations.to_csv(\"text_multilabel_dataset.csv\")\n\nAnything that can be displayed on Jupyter (text, images, audio, graphs, etc.)can be displayed by pigeonXT by providing the appropriate display_fn argument.\nAdditionally, custom hooks can be attached to each row update (example_process_fn), or when the annotating task is complete(final_process_fn). See https://github.com/dennisbakhuis/pigeonXT for more details."
  },
  {
    "objectID": "02_Dataset.html#improve-consensus-labels-for-multiannotator-data-with-cleanlab",
    "href": "02_Dataset.html#improve-consensus-labels-for-multiannotator-data-with-cleanlab",
    "title": "2  Framing the problem and constructing the dataset",
    "section": "2.4 Improve Consensus Labels for Multiannotator Data with Cleanlab",
    "text": "2.4 Improve Consensus Labels for Multiannotator Data with Cleanlab\nThis example contains classification data that has been labeled by multiple annotators (where each example has been labeled by at least one annotator, but not every annotator has labeled every example)\nFor this part, we will generate a toy dataset that has 50 annotators and 300 examples. There are three possible classes, 0, 1 and 2.\nEach annotator annotates approximately 10% of the examples. We also synthetically made the last 5 annotators in our toy dataset have much noisier labels than the rest of the annotators. To generate our multiannotator data, we define a make_data() method\n\nSEED = 111 # set to None for non-reproducible randomness\nnp.random.seed(seed=SEED)\n\ndef make_data(\n    means=[[3, 2], [7, 7], [0, 8]],\n    covs=[[[5, -1.5], [-1.5, 1]], [[1, 0.5], [0.5, 4]], [[5, 1], [1, 5]]],\n    sizes=[150, 75, 75],\n    num_annotators=50,\n):\n\n    m = len(means)  # number of classes\n    n = sum(sizes)\n    local_data = []\n    labels = []\n\n    for idx in range(m):\n        local_data.append(\n            np.random.multivariate_normal(mean=means[idx], cov=covs[idx], size=sizes[idx])\n        )\n        labels.append(np.array([idx for i in range(sizes[idx])]))\n    X_train = np.vstack(local_data)\n    true_labels_train = np.hstack(labels)\n\n    # Compute p(true_label=k)\n    py = np.bincount(true_labels_train) / float(len(true_labels_train))\n\n    noise_matrix_better = generate_noise_matrix_from_trace(\n        m,\n        trace=0.8 * m,\n        py=py,\n        valid_noise_matrix=True,\n        seed=SEED,\n    )\n\n    noise_matrix_worse = generate_noise_matrix_from_trace(\n        m,\n        trace=0.35 * m,\n        py=py,\n        valid_noise_matrix=True,\n        seed=SEED,\n    )\n\n    # Generate our noisy labels using the noise_matrix for specified number of annotators.\n    s = pd.DataFrame(\n        np.vstack(\n            [\n                generate_noisy_labels(true_labels_train, noise_matrix_better)\n                if i &lt; num_annotators - 5\n                else generate_noisy_labels(true_labels_train, noise_matrix_worse)\n                for i in range(num_annotators)\n            ]\n        ).transpose()\n    )\n\n    # Each annotator only labels approximately 10% of the dataset\n    # (unlabeled points represented with NaN)\n    s = s.apply(lambda x: x.mask(np.random.random(n) &lt; 0.9)).astype(\"Int64\")\n    s.dropna(axis=1, how=\"all\", inplace=True)\n    s.columns = [\"A\" + str(i).zfill(4) for i in range(1, num_annotators+1)]\n\n    row_NA_check = pd.notna(s).any(axis=1)\n\n    return {\n        \"X_train\": X_train[row_NA_check],\n        \"true_labels_train\": true_labels_train[row_NA_check],\n        \"multiannotator_labels\": s[row_NA_check].reset_index(drop=True),\n    }\n\nLet’s view the first few rows of the data. Here are the labels selected by each annotator for the first few examples:\n\ndata_dict = make_data()\n\nX = data_dict[\"X_train\"]\nmultiannotator_labels = data_dict[\"multiannotator_labels\"]\ntrue_labels = data_dict[\"true_labels_train\"] # used for comparing the accuracy of consensus labels\n\n\nmultiannotator_labels.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nA0001\nA0002\nA0003\nA0004\nA0005\nA0006\nA0007\nA0008\nA0009\nA0010\n...\nA0041\nA0042\nA0043\nA0044\nA0045\nA0046\nA0047\nA0048\nA0049\nA0050\n\n\n\n\n0\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n...\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n1\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n0\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n...\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n2\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n...\n&lt;NA&gt;\n0\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n2\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n3\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n2\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n...\n0\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n4\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n...\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n2\n&lt;NA&gt;\n&lt;NA&gt;\n0\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n\n\n\n5 rows × 50 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nmultiannotator_labels contains the class labels that each annotator chose for each example, with examples that a particular annotator did not label represented using np.nan. X contains the features for each example.\n\n2.4.1 Get majority vote labels and compute out-of-sample predicted probabilites\nBefore training a machine learning model, we must first obtain the consensus labels from the annotators that labeled the data. The simplest way to obtain an initial set of consensus labels is to select it using majority vote.\n\nmajority_vote_label = get_majority_vote_label(multiannotator_labels)\n\nNext, we will train a model on the consensus labels obtained using majority vote to compute out-of-sample predicted probabilities. Here, we use a simple logistic regression model.\n\nmodel = LogisticRegression()\n\nnum_crossval_folds = 5\npred_probs = cross_val_predict(\n    estimator=model, X=X, y=majority_vote_label, cv=num_crossval_folds, method=\"predict_proba\"\n)\n\n\n\n2.4.2 Use cleanlab to get better consensus labels and other statistics\nUsing the annotators’ labels and the out-of-sample predicted probabilites from the model, cleanlab can help us obtain improved consensus labels for our data.\n\nresults = get_label_quality_multiannotator(multiannotator_labels, pred_probs, verbose=True)\n\nAnnotator(s) ['A0002' 'A0027' 'A0028' 'A0029' 'A0035'] did not annotate any examples that overlap with other annotators,                 \nusing the average annotator agreeement among other annotators as this annotator's agreement.\nAnnotator(s) ['A0002' 'A0027' 'A0028' 'A0029' 'A0035'] did not annotate any examples that overlap with other annotators,                 \nusing the average annotator agreeement among other annotators as this annotator's agreement.\n\n\nHere, we use the multiannotator.get_label_quality_multiannotator() function which returns a dictionary containing three items:\n\nlabel_quality which gives us the improved consensus labels using information from each of the annotators and the model. The DataFrame also contains information about the number of annotations, annotator agreement and consensus quality score for each example.\n\n\nresults[\"label_quality\"]\n\n\n  \n    \n      \n\n\n\n\n\n\nconsensus_label\nconsensus_quality_score\nannotator_agreement\nnum_annotations\n\n\n\n\n0\n0\n0.778153\n0.500000\n2\n\n\n1\n0\n0.840381\n1.000000\n3\n\n\n2\n0\n0.811415\n0.600000\n5\n\n\n3\n0\n0.764981\n0.600000\n5\n\n\n4\n0\n0.848161\n0.800000\n5\n\n\n...\n...\n...\n...\n...\n\n\n291\n2\n0.799866\n0.833333\n6\n\n\n292\n2\n0.774352\n0.666667\n6\n\n\n293\n2\n0.860814\n1.000000\n8\n\n\n294\n2\n0.946146\n0.500000\n4\n\n\n295\n2\n0.835593\n0.714286\n7\n\n\n\n\n\n296 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndetailed_label_quality which returns the label quality score for each label given by every annotator\n\n\nresults[\"detailed_label_quality\"]\n\n\n  \n    \n      \n\n\n\n\n\n\nquality_annotator_A0001\nquality_annotator_A0002\nquality_annotator_A0003\nquality_annotator_A0004\nquality_annotator_A0005\nquality_annotator_A0006\nquality_annotator_A0007\nquality_annotator_A0008\nquality_annotator_A0009\nquality_annotator_A0010\n...\nquality_annotator_A0041\nquality_annotator_A0042\nquality_annotator_A0043\nquality_annotator_A0044\nquality_annotator_A0045\nquality_annotator_A0046\nquality_annotator_A0047\nquality_annotator_A0048\nquality_annotator_A0049\nquality_annotator_A0050\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.840381\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\n0.811415\nNaN\nNaN\nNaN\nNaN\nNaN\n0.051990\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.172521\nNaN\nNaN\nNaN\n...\n0.764981\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n0.101573\nNaN\nNaN\n0.848161\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n291\n0.799866\nNaN\nNaN\nNaN\nNaN\nNaN\n0.799866\n0.131487\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n292\nNaN\nNaN\nNaN\n0.774352\nNaN\n0.774352\nNaN\nNaN\nNaN\n0.167851\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n293\nNaN\nNaN\n0.860814\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n0.860814\nNaN\nNaN\nNaN\nNaN\nNaN\n0.860814\nNaN\n\n\n294\nNaN\nNaN\n0.946146\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n0.026924\nNaN\nNaN\n0.026924\nNaN\n\n\n295\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n0.835593\nNaN\nNaN\nNaN\nNaN\n0.080003\nNaN\n0.080003\nNaN\nNaN\n\n\n\n\n\n296 rows × 50 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nannotator_stats which gives us the annotator quality score for each annotator, alongisde other information such as the number of examples each annotator labeled, their agreement with the consensus labels and the class they perform the worst at.\n\n\nresults[\"annotator_stats\"]\n\n\n  \n    \n      \n\n\n\n\n\n\nannotator_quality\nagreement_with_consensus\nworst_class\nnum_examples_labeled\n\n\n\n\nA0050\n0.262121\n0.250000\n2\n24\n\n\nA0047\n0.296369\n0.294118\n2\n34\n\n\nA0049\n0.320576\n0.310345\n1\n29\n\n\nA0046\n0.357324\n0.346154\n1\n26\n\n\nA0048\n0.468168\n0.520000\n2\n25\n\n\nA0031\n0.525641\n0.580645\n2\n31\n\n\nA0034\n0.544269\n0.607143\n2\n28\n\n\nA0021\n0.585709\n0.656250\n1\n32\n\n\nA0019\n0.610918\n0.678571\n2\n28\n\n\nA0015\n0.620574\n0.678571\n2\n28\n\n\nA0011\n0.622605\n0.692308\n1\n26\n\n\nA0025\n0.624784\n0.702703\n2\n37\n\n\nA0017\n0.632781\n0.680000\n2\n25\n\n\nA0003\n0.650724\n0.722222\n2\n36\n\n\nA0038\n0.654490\n0.714286\n1\n28\n\n\nA0037\n0.655256\n0.729730\n2\n37\n\n\nA0044\n0.656257\n0.738095\n2\n42\n\n\nA0040\n0.660856\n0.740741\n2\n27\n\n\nA0009\n0.677454\n0.750000\n2\n28\n\n\nA0006\n0.680579\n0.740741\n2\n27\n\n\nA0041\n0.697971\n0.766667\n2\n30\n\n\nA0033\n0.699469\n0.800000\n2\n20\n\n\nA0012\n0.701831\n0.772727\n0\n22\n\n\nA0014\n0.703866\n0.785714\n2\n28\n\n\nA0024\n0.703875\n0.794118\n2\n34\n\n\nA0004\n0.706232\n0.809524\n1\n21\n\n\nA0029\n0.707322\n0.781250\n2\n32\n\n\nA0002\n0.707889\n0.793103\n2\n29\n\n\nA0028\n0.709388\n0.815789\n1\n38\n\n\nA0045\n0.713798\n0.814815\n2\n27\n\n\nA0007\n0.714058\n0.805556\n2\n36\n\n\nA0042\n0.719011\n0.821429\n2\n28\n\n\nA0018\n0.725979\n0.823529\n2\n34\n\n\nA0043\n0.726029\n0.823529\n2\n34\n\n\nA0010\n0.726405\n0.833333\n0\n24\n\n\nA0030\n0.730296\n0.857143\n2\n28\n\n\nA0020\n0.731679\n0.827586\n2\n29\n\n\nA0036\n0.734518\n0.821429\n2\n28\n\n\nA0013\n0.735184\n0.863636\n2\n22\n\n\nA0016\n0.743784\n0.838710\n2\n31\n\n\nA0001\n0.743842\n0.848485\n2\n33\n\n\nA0026\n0.745471\n0.869565\n1\n23\n\n\nA0039\n0.746901\n0.857143\n2\n28\n\n\nA0027\n0.759070\n0.891892\n1\n37\n\n\nA0022\n0.767128\n0.862069\n2\n29\n\n\nA0035\n0.773640\n0.870968\n2\n31\n\n\nA0023\n0.781378\n0.909091\n1\n22\n\n\nA0005\n0.791291\n0.916667\n0\n24\n\n\nA0008\n0.792155\n0.916667\n1\n36\n\n\nA0032\n0.823238\n0.950000\n0\n20\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe can get the improved consensus labels from the label_quality DataFrame shown above. We can compare the accuracy of the consensus labels obtained using majority vote, and the improved consensus labels obtained using cleanlab.\n\nimproved_consensus_label = results[\"label_quality\"][\"consensus_label\"].values\n\nmajority_vote_accuracy = np.mean(true_labels == majority_vote_label)\ncleanlab_label_accuracy = np.mean(true_labels == improved_consensus_label)\n\nprint(f\"Accuracy of majority vote labels = {majority_vote_accuracy}\")\nprint(f\"Accuracy of cleanlab consensus labels = {cleanlab_label_accuracy}\")\n\nAccuracy of majority vote labels = 0.9054054054054054\nAccuracy of cleanlab consensus labels = 0.9763513513513513\n\n\nWe can see that the accuracy of the consensus labels improved as a result of using cleanlab!\nAfter obtaining the improved consensus labels, we can now retrain a better version of our machine learning model using these newly obtained labels. You can also repeatedly iterate this process of getting better consensus labels using the model’s out-of-sample predicted probabilites and then retraining the model with the improved labels to get even better predicted probabilities!"
  },
  {
    "objectID": "02_Dataset.html#active-learning-with-modal",
    "href": "02_Dataset.html#active-learning-with-modal",
    "title": "2  Framing the problem and constructing the dataset",
    "section": "2.5 Active Learning with modAL",
    "text": "2.5 Active Learning with modAL\nIn this example, the active learning workflow of modAL is demonstrated - with you in the loop! By running this notebook, you’ll be queried to label digits using the DIGITS dataset.\nHere we use the pool-based sampling. In this setting, we assume a small set of labeled data L and a large set of unlabeled data U such that |L|≪|U|.\nThe high level description about this strategy is as follows: Queries are selectively drawn from the pool, which is usually assumed to be closed (i.e., static or non-changing), although this is not strictly necessary. Typically, instances are queried in a greedy fashion, according to an informativeness measure used to evaluate all instances in the pool (or, perhaps if U is very large, some subsample thereof).\nNow we set up the initial training set for our classifier. If you would like to play around, you can try to modifiy the value n_initial below and see if it impacts the algorithm!\n\ndigits = load_digits()\n\nfig = plt.figure(figsize=(8, 8))  # figure size in inches\nfig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n\nfor i in range(64):\n    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n    # label the image with the target value\n    ax.text(0, 7, str(digits.target[i]))\n\n\n\n\n\nn_initial = 100\nX, y = load_digits(return_X_y=True)\nprint(X.shape)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\ninitial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)\n\nX_initial, y_initial = X_train[initial_idx], y_train[initial_idx]\nX_pool, y_pool = np.delete(X_train, initial_idx, axis=0), np.delete(y_train, initial_idx, axis=0)\nprint(X_pool.shape)\n\n(1797, 64)\n(1247, 64)\n\n\n\n2.5.1 Initializing the learner\nAlong with our pool-based sampling strategy, modAL’s modular design allows you to vary parameters surrounding the active learning process, including the core estimator and query strategy.\nNow we initialize the active learner. Feel free to change the underlying RandomForestClassifier or the uncertainty_sampling!\n\nlearner = ActiveLearner(\n    estimator=RandomForestClassifier(),\n    query_strategy=uncertainty_sampling,\n    X_training=X_initial, y_training=y_initial\n)\n\n## We also set how many queries we want to make. The more the better!\nn_queries = 20\n\n\n\n2.5.2 The active learning loop\n\naccuracy_scores = [learner.score(X_test, y_test)]\n\nfor i in range(n_queries):\n    display.clear_output(wait=True)\n    query_idx, query_inst = learner.query(X_pool)\n    with plt.style.context('seaborn-white'):\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.title('Digit to label')\n        plt.imshow(query_inst.reshape(8, 8))\n        plt.subplot(1, 2, 2)\n        plt.title('Accuracy of your model')\n        plt.plot(range(i+1), accuracy_scores)\n        plt.scatter(range(i+1), accuracy_scores)\n        plt.xlabel('number of queries')\n        plt.ylabel('accuracy')\n        display.display(plt.gcf())\n        plt.close('all')\n    # Query the rater\n    print(\"Which digit is this?\")\n    y_new = np.array([int(input())], dtype=int)\n    learner.teach(query_inst.reshape(1, -1), y_new)\n    X_pool, y_pool = np.delete(X_pool, query_idx, axis=0), np.delete(y_pool, query_idx, axis=0)\n    accuracy_scores.append(learner.score(X_test, y_test))\n\n\n\n\nWhich digit is this?\n3\n\n\nBy querying hard example to the rater, the performance increase from 83% to 86%!\n\n\n2.5.3 Iris example\nIn this example, we use scikit-learn’s k-nearest neighbors classifier as our estimator and default to modAL’s uncertainty sampling query strategy.\n\n# Set our RNG seed for reproducibility\nRANDOM_STATE_SEED = 123\nnp.random.seed(RANDOM_STATE_SEED)\n\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX_raw = iris['data']\ny_raw = iris['target']\nprint(X_raw.shape)\n\n(150, 4)\n\n\n\n# Define our PCA transformer and fit it onto our raw dataset.\npca = PCA(n_components=2, random_state=RANDOM_STATE_SEED)\ntransformed_iris = pca.fit_transform(X=X_raw)\n\n\n# Isolate the data we'll need for plotting.\nx_component, y_component = transformed_iris[:, 0], transformed_iris[:, 1]\n\n# Plot our dimensionality-reduced (via PCA) dataset.\nplt.figure(figsize=(8.5, 6), dpi=130)\nplt.scatter(x=x_component, y=y_component, c=y_raw, cmap='viridis', s=50, alpha=8/10)\nplt.title('Iris classes after PCA transformation')\nplt.show()\n\n\n\n\nNow we partition our iris dataset into a training set L and U. We first specify our training set L consisting of 3 random examples. The remaining examples go to our “unlabeled” pool U.\n\n# Isolate our examples for our labeled dataset.\nn_labeled_examples = X_raw.shape[0]\ntraining_indices = np.random.randint(low=0, high=n_labeled_examples + 1, size=3)\n\nX_train = X_raw[training_indices]\ny_train = y_raw[training_indices]\n\n# Isolate the non-training examples we'll be querying.\nX_pool = np.delete(X_raw, training_indices, axis=0)\ny_pool = np.delete(y_raw, training_indices, axis=0)\n\nFor the classification, we are going to use a simple k-nearest neighbors classifier. In this step, we are also going to initialize the ActiveLearner.\n\n# Specify our core estimator along with it's active learning model.\nknn = KNeighborsClassifier(n_neighbors=3)\nlearner = ActiveLearner(estimator=knn, X_training=X_train, y_training=y_train)\n\nLet’s see how our classifier performs on the initial training set!\n\n# Isolate the data we'll need for plotting.\nprint(y_train)\npredictions = learner.predict(X_raw)\nis_correct = (predictions == y_raw)\n\npredictions\n\n[2 2 1]\n\n\narray([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n\n# Record our learner's score on the raw data.\nunqueried_score = learner.score(X_raw, y_raw)\n\n# Plot our classification results.\nfig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\nax.scatter(x=x_component[is_correct],  y=y_component[is_correct],  c='g', marker='+', label='Correct',   alpha=8/10)\nax.scatter(x=x_component[~is_correct], y=y_component[~is_correct], c='r', marker='x', label='Incorrect', alpha=8/10)\nax.legend(loc='lower right')\nax.set_title(\"ActiveLearner class predictions (Accuracy: {score:.3f})\".format(score=unqueried_score))\nplt.show()\n\n\n\n\n\n2.5.3.1 Update our model by pool-based sampling our “unlabeled” dataset U\nAs we can see, our model is unable to properly learn the underlying data distribution. All of its predictions are for the third class label, and as such it is only as competitive as defaulting its predictions to a single class – if only we had more data!\nBelow, we tune our classifier by allowing it to query 20 instances it hasn’t seen before. Using uncertainty sampling, our classifier aims to reduce the amount of uncertainty in its predictions using a variety of measures — see https://modal-python.readthedocs.io/en/latest/index.html for more on specific classification uncertainty measures. With each requested query, we remove that record from our pool U and record our model’s accuracy on the raw dataset.\n\nN_QUERIES = 20\nperformance_history = [unqueried_score]\n\n# Allow our model to query our unlabeled dataset for the most\n# informative points according to our query strategy (uncertainty sampling).\nfor index in range(N_QUERIES):\n  query_index, query_instance = learner.query(X_pool)\n\n  # Teach our ActiveLearner model the record it has requested.\n  # Here we assume the label comes from the true label!!!\n  X, y = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )\n  learner.teach(X=X, y=y)\n\n  # Remove the queried instance from the unlabeled pool.\n  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)\n\n  # Calculate and report our model's accuracy.\n  model_accuracy = learner.score(X_raw, y_raw)\n  print('Accuracy after query {n}: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy))\n\n  # Save our model's performance for plotting.\n  performance_history.append(model_accuracy)\n\nAccuracy after query 1: 0.6667\nAccuracy after query 2: 0.6667\nAccuracy after query 3: 0.8800\nAccuracy after query 4: 0.8800\nAccuracy after query 5: 0.8733\nAccuracy after query 6: 0.8400\nAccuracy after query 7: 0.7400\nAccuracy after query 8: 0.7267\nAccuracy after query 9: 0.7267\nAccuracy after query 10: 0.7267\nAccuracy after query 11: 0.7267\nAccuracy after query 12: 0.7267\nAccuracy after query 13: 0.7267\nAccuracy after query 14: 0.7267\nAccuracy after query 15: 0.7200\nAccuracy after query 16: 0.8400\nAccuracy after query 17: 0.8800\nAccuracy after query 18: 0.8933\nAccuracy after query 19: 0.9267\nAccuracy after query 20: 0.9267\n\n\nHere, we first plot the query iteration index against model accuracy. To visualize the performance of our classifier, we also plot the correct and incorrect predictions on the full dataset.\n\n# Plot our performance over time.\nfig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n\nax.plot(performance_history)\nax.scatter(range(len(performance_history)), performance_history, s=13)\n\nax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=5, integer=True))\nax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10))\nax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n\nax.set_ylim(bottom=0, top=1)\nax.grid(True)\n\nax.set_title('Incremental classification accuracy')\nax.set_xlabel('Query iteration')\nax.set_ylabel('Classification Accuracy')\n\nplt.show()\n\n\n\n\n\n# Isolate the data we'll need for plotting.\npredictions = learner.predict(X_raw)\nis_correct = (predictions == y_raw)\n\n# Plot our updated classification results once we've trained our learner.\nfig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n\nax.scatter(x=x_component[is_correct],  y=y_component[is_correct],  c='g', marker='+', label='Correct',   alpha=8/10)\nax.scatter(x=x_component[~is_correct], y=y_component[~is_correct], c='r', marker='x', label='Incorrect', alpha=8/10)\n\nax.set_title('Classification accuracy after {n} queries: {final_acc:.3f}'.format(n=N_QUERIES, final_acc=performance_history[-1]))\nax.legend(loc='lower right')\n\nplt.show()"
  },
  {
    "objectID": "02_Dataset.html#weak-supervison-using-snorkel",
    "href": "02_Dataset.html#weak-supervison-using-snorkel",
    "title": "2  Framing the problem and constructing the dataset",
    "section": "2.6 Weak Supervison using Snorkel",
    "text": "2.6 Weak Supervison using Snorkel\nWe will walk through the process of using Snorkel to build a training set for classifying YouTube comments as spam or not spam. The goal of this tutorial is to illustrate the basic components and concepts of Snorkel in a simple way, but also to dive into the actual process of iteratively developing real applications in Snorkel.\nOur goal is to train a classifier over the comment data that can predict whether a comment is spam or not spam. We have access to a large amount of unlabeled data in the form of YouTube comments with some metadata. In order to train a classifier, we need to label our data, but doing so by hand for real world applications can often be prohibitively slow and expensive.\nIn these cases, we can turn to a weak supervision approach, using labeling functions (LFs) in Snorkel: noisy, programmatic rules and heuristics that assign labels to unlabeled training data.\nWe’ll dive into the Snorkel API and how we write labeling functions later in this tutorial, but as an example, we can write a LF that labels data points with “http” in the comment text as spam since many spam comments contain links.\nWe use a YouTube comments dataset that consists of YouTube comments from 5 videos. The task is to classify each comment as being\nHAM: comments relevant to the video (even very simple ones), or SPAM: irrelevant (often trying to advertise something) or inappropriate messages\n\n%%file download_data.sh\n\n#!/bin/bash\nset -euxo pipefail\n\nFILES=( \"Youtube01-Psy.csv\" \"Youtube02-KatyPerry.csv\" \"Youtube03-LMFAO.csv\" \"Youtube04-Eminem.csv\" \"Youtube05-Shakira.csv\" )\nDATA_URL=\"http://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip\"\nRELOAD=false\n\n# Check if at least any file is missing. If so, reload all data.\nfor filename in \"${FILES[@]}\"\ndo\n    if [ ! -e \"data/$filename\" ]; then\n        RELOAD=true\n    fi\ndone\n\nif [ \"$RELOAD\" = true ]; then\n    if [ -d \"data/\" ]; then rm -Rf \"data/\"; fi\n    mkdir -p data\n    curl $DATA_URL &gt; data.zip\n    mv data.zip data/\n    cd data\n    unzip data.zip\n    rm data.zip\n    rm -rf __MACOSX\n    cd ..\nfi\n\nWriting download_data.sh\n\n\n\ndef load_spam_dataset(load_train_labels: bool = False, split_dev_valid: bool = False):\n    if os.path.basename(os.getcwd()) == \"snorkel-tutorials\":\n        os.chdir(\"spam\")\n    try:\n        subprocess.run([\"bash\", \"download_data.sh\"], check=True, stderr=subprocess.PIPE)\n    except subprocess.CalledProcessError as e:\n        print(e.stderr.decode())\n        raise e\n    filenames = sorted(glob.glob(\"data/Youtube*.csv\"))\n\n    dfs = []\n    for i, filename in enumerate(filenames, start=1):\n        df = pd.read_csv(filename)\n        # Lowercase column names\n        df.columns = map(str.lower, df.columns)\n        # Remove comment_id field\n        df = df.drop(\"comment_id\", axis=1)\n        # Add field indicating source video\n        df[\"video\"] = [i] * len(df)\n        # Rename fields\n        df = df.rename(columns={\"class\": \"label\", \"content\": \"text\"})\n        # Shuffle order\n        df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n        dfs.append(df)\n\n    df_train = pd.concat(dfs[:4])\n    df_dev = df_train.sample(100, random_state=123)\n\n    if not load_train_labels:\n        df_train[\"label\"] = np.ones(len(df_train[\"label\"])) * -1\n    df_valid_test = dfs[4]\n    df_valid, df_test = train_test_split(\n        df_valid_test, test_size=250, random_state=123, stratify=df_valid_test.label\n    )\n\n    if split_dev_valid:\n        return df_train, df_dev, df_valid, df_test\n    else:\n        return df_train, df_test\n\nWe split our data into two sets:\n\nTraining Set: The largest split of the dataset, and the one without any ground truth (“gold”) labels. We will generate labels for these data points with weak supervision.\nTest Set: A small, standard held-out blind hand-labeled set for final evaluation of our classifier. This set should only be used for final evaluation, not error analysis.\n\n\nNote that in more advanced production settings, we will often further split up the available hand-labeled data into a development split, for getting ideas to write labeling functions, and a validation split for e.g. checking our performance without looking at test set scores, hyperparameter tuning, etc. These splits are omitted for simplicity here.\n\n\n2.6.1 Loading Data\nWe load the YouTube comments dataset and create Pandas DataFrame objects for the train and test sets. Snorkel provides native support for several DataFrame-like data structures, including Pandas, Dask, and PySpark.\nEach DataFrame consists of the following fields:\n\nauthor: Username of the comment author\ndata: Date and time the comment was posted\ntext: Raw text content of the comment\nlabel: Whether the comment is SPAM (1), HAM (0), or UNKNOWN/ABSTAIN (-1)\nvideo: Video the comment is associated with\n\nWe start by loading our data. The load_spam_dataset() method downloads the raw CSV files from the internet, divides them into splits, converts them into DataFrames, and shuffles them. As mentioned above, the dataset contains comments from 5 of the most popular YouTube videos during a period between 2014 and 2015. The first four videos’ comments are combined to form the train set. This set has no gold labels. The fifth video is part of the test set.\n\ndf_train, df_test = load_spam_dataset()\n\n# We pull out the label vectors for ease of use later\nY_test = df_test.label.values\n\n\ndf_train\n\n\n  \n    \n      \n\n\n\n\n\n\nauthor\ndate\ntext\nlabel\nvideo\n\n\n\n\n0\nAlessandro leite\n2014-11-05T22:21:36\npls http://www10.vakinha.com.br/VaquinhaE.aspx...\n-1.0\n1\n\n\n1\nSalim Tayara\n2014-11-02T14:33:30\nif your like drones, plz subscribe to Kamal Ta...\n-1.0\n1\n\n\n2\nPhuc Ly\n2014-01-20T15:27:47\ngo here to check the views :3﻿\n-1.0\n1\n\n\n3\nDropShotSk8r\n2014-01-19T04:27:18\nCame here to check the views, goodbye.﻿\n-1.0\n1\n\n\n4\ncss403\n2014-11-07T14:25:48\ni am 2,126,492,636 viewer :D﻿\n-1.0\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n443\nThemayerlife\nNaN\nCheck out my mummy chanel!\n-1.0\n4\n\n\n444\nFill Reseni\n2015-05-27T17:10:53.724000\nThe rap: cool Rihanna: STTUUPID﻿\n-1.0\n4\n\n\n445\nGreg Fils Aimé\nNaN\nI hope everyone is in good spirits I&#39;m a h...\n-1.0\n4\n\n\n446\nLil M\nNaN\nLil m !!!!! Check hi out!!!!! Does live the wa...\n-1.0\n4\n\n\n447\nAvidorFilms\nNaN\nPlease check out my youtube channel! Just uplo...\n-1.0\n4\n\n\n\n\n\n1586 rows × 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# For clarity, we define constants to represent the class labels for spam, ham, and abstaining.\nABSTAIN = -1\nHAM = 0\nSPAM = 1\n\n\n\n2.6.2 Writing Labeling Functions (LFs)\nLabeling functions (LFs) help users encode domain knowledge and other supervision sources programmatically.\nLFs are heuristics that take as input a data point and either assign a label to it (in this case, HAM or SPAM) or abstain (don’t assign any label). Labeling functions can be noisy: they don’t have perfect accuracy and don’t have to label every data point. Moreover, different labeling functions can overlap (label the same data point) and even conflict (assign different labels to the same data point). This is expected, and we demonstrate how we deal with this later.\nBecause their only requirement is that they map a data point a label (or abstain), they can wrap a wide variety of forms of supervision.\nTypical LF development cycles include multiple iterations of ideation, refining, evaluation, and debugging. A typical cycle consists of the following steps:\n\nLook at examples to generate ideas for LFs\nWrite an initial version of an LF\nSpot check its performance by looking at its output on data points in the training set (or development set if available)\nRefine and debug to improve coverage or accuracy as necessary\n\nOur goal for LF development is to create a high quality set of training labels for our unlabeled dataset, not to label everything or directly create a model for inference using the LFs. The training labels are used to train a separate discriminative model (in this case, one which just uses the comment text) in order to generalize to new, unseen data points. Using this model, we can make predictions for data points that our LFs don’t cover.\n\nPattern-matching LFs (regular expressions)\n\nLabeling functions in Snorkel are created with the @labeling_function decorator. The decorator can be applied to any Python function that returns a label for a single data point.\n\nSee https://realpython.com/primer-on-python-decorators/ for more details about decorators.\n\nLet’s start developing an LF to catch instances of commenters trying to get people to “check out” their channel, video, or website. We’ll start by just looking for the exact string “check out” in the text, and see how that compares to looking for just “check” in the text. For the two versions of our rule, we’ll write a Python function over a single data point that express it, then add the decorator.\nOne dominant pattern in the comments that look like spam (which we might know from prior domain experience, or from inspection of a few training data points) is the use of the phrase “check out” (e.g. “check out my channel”). Let’s start with that.\n\n@labeling_function()\ndef regex_check_out(x):\n    return SPAM if re.search(r\"check.*out\", x.text, flags=re.I) else ABSTAIN\n\n\nKeyword LFs\n\nFor text applications, some of the simplest LFs to write are often just keyword lookups. These will often follow the same execution pattern, so we can create a template and use the resources parameter to pass in LF-specific keywords. Similar to the labeling_function decorator, the LabelingFunction class wraps a Python function (the f parameter), and we can use the resources parameter to pass in keyword arguments (here, our keywords to lookup) to said function.\n\ndef keyword_lookup(x, keywords, label):\n    if any(word in x.text.lower() for word in keywords):\n        return label\n    return ABSTAIN\n\n\ndef make_keyword_lf(keywords, label=SPAM):\n    return LabelingFunction(\n        name=f\"keyword_{keywords[0]}\",\n        f=keyword_lookup,\n        resources=dict(keywords=keywords, label=label),\n    )\n\n\n\"\"\"Spam comments talk about 'my channel', 'my video', etc.\"\"\"\nkeyword_my = make_keyword_lf(keywords=[\"my\"])\n\n\"\"\"Spam comments ask users to subscribe to their channels.\"\"\"\nkeyword_subscribe = make_keyword_lf(keywords=[\"subscribe\"])\n\n\"\"\"Spam comments post links to other channels.\"\"\"\nkeyword_link = make_keyword_lf(keywords=[\"http\"])\n\n\"\"\"Spam comments suspicious.\"\"\"\nkeyword_guys = make_keyword_lf(keywords=[\"guys\"])\n\n\"\"\"Spam comments make requests rather than commenting.\"\"\"\nkeyword_please = make_keyword_lf(keywords=[\"please\", \"plz\"])\n\n\n\n\"\"\"Ham comments actually talk about the video's content.\"\"\"\nkeyword_song = make_keyword_lf(keywords=[\"song\"], label=HAM)\n\n\nHeuristic LFs There may other heuristics or “rules of thumb” that you come up with as you look at the data. So long as you can express it in a function, it’s a viable LF!\n\n\n@labeling_function()\ndef short_comment(x):\n    \"\"\"Ham comments are often short, such as 'cool video!'\"\"\"\n    return HAM if len(x.text.split()) &lt; 5 else ABSTAIN\n\n\nlfs = [\n    keyword_my,\n    keyword_subscribe,\n    keyword_link,\n    keyword_guys,\n    keyword_please,\n    keyword_song,\n    regex_check_out,\n    short_comment,\n]\n\nTo apply one or more LFs that we’ve written to a collection of data points, we use an LFApplier. Because our data points are represented with a Pandas DataFrame in this tutorial, we use the PandasLFApplier. Correspondingly, a single data point x that’s passed into our LFs will be a Pandas Series object.\nIt’s important to note that these LFs will work for any object with an attribute named text, not just Pandas objects. Snorkel has several other appliers for different data point collection types which you can browse in the https://snorkel.readthedocs.io/en/master/packages/labeling.html.\nThe output of the apply(...) method is a label matrix, a fundamental concept in Snorkel. It’s a NumPy array L with one column for each LF and one row for each data point, where L[i, j] is the label that the jth labeling function output for the ith data point. We’ll create a label matrix for the train set.\n\napplier = PandasLFApplier(lfs=lfs)\nL_train = applier.apply(df=df_train)\nL_test = applier.apply(df=df_test)\n\n100%|██████████| 1586/1586 [00:00&lt;00:00, 9646.05it/s]\n100%|██████████| 250/250 [00:00&lt;00:00, 8692.21it/s]\n\n\nWe can easily calculate the coverage of these LFs (i.e., the percentage of the dataset that they label) as follows:\n\ncoverage = (L_train != ABSTAIN).mean(axis=0)\ncoverage\n\narray([0.19861286, 0.12736444, 0.11916772, 0.05674653, 0.11223203,\n       0.14186633, 0.23392182, 0.22572509])\n\n\n\nL_train\n\narray([[-1, -1,  1, ..., -1, -1, -1],\n       [-1,  1, -1, ..., -1, -1, -1],\n       [-1, -1, -1, ..., -1, -1, -1],\n       ...,\n       [ 1,  1, -1, ..., -1,  1, -1],\n       [-1,  1, -1, ..., -1,  1, -1],\n       [ 1, -1, -1, ..., -1,  1, -1]])\n\n\nLots of statistics about labeling functions - like coverage - are useful when building any Snorkel application. So Snorkel provides tooling for common LF analyses using the LFAnalysis utility.\nCheckout the statistics here https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.LFAnalysis.html\n\nLFAnalysis(L=L_train, lfs=lfs).lf_summary()\n\n\n  \n    \n      \n\n\n\n\n\n\nj\nPolarity\nCoverage\nOverlaps\nConflicts\n\n\n\n\nkeyword_my\n0\n[1]\n0.198613\n0.172131\n0.044136\n\n\nkeyword_subscribe\n1\n[1]\n0.127364\n0.092055\n0.026482\n\n\nkeyword_http\n2\n[1]\n0.119168\n0.083859\n0.051702\n\n\nkeyword_guys\n3\n[1]\n0.056747\n0.055485\n0.005044\n\n\nkeyword_please\n4\n[1]\n0.112232\n0.104666\n0.023960\n\n\nkeyword_song\n5\n[0]\n0.141866\n0.071879\n0.045397\n\n\nregex_check_out\n6\n[1]\n0.233922\n0.116015\n0.022068\n\n\nshort_comment\n7\n[0]\n0.225725\n0.100883\n0.074401\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nOur goal is now to convert the labels from our LFs into a single noise-aware probabilistic (or confidence-weighted) label per data point. A simple baseline for doing this is to take the majority vote on a per-data point basis: if more LFs voted SPAM than HAM, label it SPAM (and vice versa). We can test this with the MajorityLabelVoter baseline model.\n\nmajority_model = MajorityLabelVoter()\npreds_train = majority_model.predict(L=L_train)\n\nThe LFs may have varying properties and should not be treated identically. In addition to having varied accuracies and coverages, LFs may be correlated, resulting in certain signals being overrepresented in a majority-vote-based model.\nTo handle these issues appropriately, we will instead use a more sophisticated Snorkel LabelModel to combine the outputs of the LFs. This model will ultimately produce a single set of noise-aware training labels, which are probabilistic or confidence-weighted labels. We will then use these labels to train a classifier for our task.\nNote that no gold labels are used during the training process. The only information we need is the label matrix, which contains the output of the LFs on our training set. The LabelModel is able to learn weights for the labeling functions using only the label matrix as input. We also specify the cardinality, or number of classes.\n\nlabel_model = LabelModel(cardinality=2, verbose=True)\nlabel_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123)\n\n100%|██████████| 500/500 [00:00&lt;00:00, 1051.93epoch/s]\n\n\n\nmajority_acc = majority_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n    \"accuracy\"\n]\nprint(f\"{'Majority Vote Accuracy:':&lt;25} {majority_acc * 100:.1f}%\")\n\nlabel_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n    \"accuracy\"\n]\nprint(f\"{'Label Model Accuracy:':&lt;25} {label_model_acc * 100:.1f}%\")\n\nMajority Vote Accuracy:   86.4%\nLabel Model Accuracy:     84.8%\n\n\nThe majority vote model or more sophisticated LabelModel could in principle be used directly as a classifier if the outputs of our labeling functions were made available at test time. However, these models (i.e. these re-weighted combinations of our labeling function’s votes) will abstain on the data points that our labeling functions don’t cover.\nWe will instead use the outputs of the LabelModel as training labels to train a discriminative classifier which can generalize beyond the labeling function outputs to see if we can improve performance further. This classifier will also only need the text of the comment to make predictions, making it much more suitable for inference over unseen comments.\nLet’s briefly confirm that the labels the LabelModel produces are indeed probabilistic in nature. The following histogram shows the confidences we have that each data point has the label SPAM. The points we are least certain about will have labels close to 0.5.\n\ndef plot_probabilities_histogram(Y):\n    plt.hist(Y, bins=10)\n    plt.xlabel(\"Probability of SPAM\")\n    plt.ylabel(\"Number of data points\")\n    plt.show()\n\n\nprobs_train = label_model.predict_proba(L=L_train)\nplot_probabilities_histogram(probs_train[:, SPAM])\n\n\n\n\nAs we saw earlier, some of the data points in our train set received no labels from any of our LFs. These data points convey no supervision signal and tend to hurt performance, so we filter them out before training using a built-in utility.\n\ndf_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n    X=df_train, y=probs_train, L=L_train\n)\n\n\n\n2.6.3 Training a Classifier\nIn this final section of the tutorial, we’ll use the probabilistic training labels we generated in the last section to train a classifier for our task. The output of the Snorkel LabelModel is just a set of labels which can be used with most popular libraries for performing supervised learning. Note that typically, Snorkel is used (and really shines!) with much more complex, training data-hungry models, but we will use Logistic Regression here for simplicity of exposition.\nFor simplicity and speed, we use a simple “bag of n-grams” feature representation: each data point is represented by a one-hot vector marking which words or 2-word combinations are present in the comment text.\n\nvectorizer = CountVectorizer(ngram_range=(1, 5))\nX_train = vectorizer.fit_transform(df_train_filtered.text.tolist())\nX_test = vectorizer.transform(df_test.text.tolist())\n\nIf we want to use a library or model that doesn’t accept probabilistic labels (such as Scikit-Learn), we can instead replace each label distribution with the label of the class that has the maximum probability. This can easily be done using the probs_to_preds helper method. We do note, however, that this transformation is lossy, as we no longer have values for our confidence in each label.\n\npreds_train_filtered = probs_to_preds(probs=probs_train_filtered)\n\nWe then use these labels to train a classifier as usual.\n\nsklearn_model = LogisticRegression(C=1e2, solver=\"liblinear\")\nsklearn_model.fit(X=X_train, y=preds_train_filtered)\n\nLogisticRegression(C=100.0, solver='liblinear')\n\n\n\nprint(f\"Test Accuracy: {sklearn_model.score(X=X_test, y=Y_test) * 100:.1f}%\")\n\nTest Accuracy: 91.2%\n\n\nWe observe an additional boost in accuracy over the LabelModel by multiple points! This is in part because the discriminative model generalizes beyond the labeling function’s labels and makes good predictions on all data points, not just the ones covered by labeling functions. By using the label model to transfer the domain knowledge encoded in our LFs to the discriminative model, we were able to generalize beyond the noisy labeling heuristics."
  },
  {
    "objectID": "02_Dataset.html#data-augmentation-using-albumentations",
    "href": "02_Dataset.html#data-augmentation-using-albumentations",
    "title": "2  Framing the problem and constructing the dataset",
    "section": "2.7 Data augmentation using Albumentations",
    "text": "2.7 Data augmentation using Albumentations\nScikit-image reads an image in RGB format which is consistent with Albumentations.\n\n!wget https://raw.githubusercontent.com/albumentations-team/albumentations_examples/master/notebooks/images/image_3.jpg\n\n--2023-02-20 03:54:21--  https://raw.githubusercontent.com/albumentations-team/albumentations_examples/master/notebooks/images/image_3.jpg\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 58095 (57K) [image/jpeg]\nSaving to: ‘image_3.jpg’\n\nimage_3.jpg         100%[===================&gt;]  56.73K  --.-KB/s    in 0.007s  \n\n2023-02-20 03:54:21 (7.89 MB/s) - ‘image_3.jpg’ saved [58095/58095]\n\n\n\n\nimage = io.imread('image_3.jpg')\nvisualize(image)\n\n\n\n\nWe fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn’t fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time\n\n# Define a single augmentation, pass the image to it and receive the augmented image\ntransform = A.HorizontalFlip(p=0.5)\nrandom.seed(7)\naugmented_image = transform(image=image)['image']\nvisualize(augmented_image)\n\n\n\n\n\ntransform = A.ShiftScaleRotate(p=0.5)\nrandom.seed(7) \naugmented_image = transform(image=image)['image']\nvisualize(augmented_image)\n\n\n\n\n\n#Define an augmentation pipeline using Compose, pass the image to it and receive the augmented imag\ntransform = A.Compose([\n    A.CLAHE(),\n    A.RandomRotate90(),\n    A.Transpose(),\n    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n    A.Blur(blur_limit=3),\n    A.OpticalDistortion(),\n    A.GridDistortion(),\n    A.HueSaturationValue(),\n])\nrandom.seed(42) \naugmented_image = transform(image=image)['image']\nvisualize(augmented_image)\n\n\n\n\n\nio.imsave('augmented_image.jpg', augmented_image)\n\n\nimage = io.imread('augmented_image.jpg')\nvisualize(image)\n\n\n\n\nFor more information, please refer to https://albumentations.ai/docs/#examples."
  },
  {
    "objectID": "02_Dataset.html#reference",
    "href": "02_Dataset.html#reference",
    "title": "2  Framing the problem and constructing the dataset",
    "section": "2.8 Reference",
    "text": "2.8 Reference\nThis notebook contains the sample from\n\nhttps://github.com/dennisbakhuis/pigeonXT/blob/master/pigeonXT_Examples.ipynb\nhttps://docs.cleanlab.ai/stable/tutorials/multiannotator.html\nhttps://modal-python.readthedocs.io/en/latest/content/examples/interactive_labeling.html_\nhttps://modal-python.readthedocs.io/en/latest/content/examples/pool-based_sampling.html\nhttps://www.snorkel.org/use-cases/01-spam-tutorial\nhttps://albumentations.ai/docs/#examples"
  },
  {
    "objectID": "03_Relational_Database_and_data_wrangling.html#queringing-data-with-bigquery",
    "href": "03_Relational_Database_and_data_wrangling.html#queringing-data-with-bigquery",
    "title": "3  Relational Database and data wrangling",
    "section": "3.1 Queringing data with BigQuery",
    "text": "3.1 Queringing data with BigQuery\n\n# For SQL\nfrom google.cloud import bigquery\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nStructured Query Language, or SQL, is the programming language used with databases, and it is an important skill for any data scientist. In this example, you’ll build your SQL skills using BigQuery, a web service work as database management system that lets you apply SQL to huge datasets.\n\n3.1.1 Preliminaries for google colab (optional)\nWe want to start exploring the Google BiqQuery public datasets. Let’s start by walking through the required setup steps, and then we can load and explore some data.\nIf you are using colab. Follow this quickstart guide, which will explain how to: 1. Create a Cloud Platform project if you don’t have one already. 2. Enable billing for the project (If you apply the free trial, you already satisfy this condition.) 3. Enable the BigQuery API 4. Enabling the Service account\nNow we need to authenticate to gain access to the BigQuery API. We will create a client, specifying the service account key file (replace ‘lunar-pact-378812-7a28b789bde2.json’ with your key file).\n\n# @title Setup\nfrom google.oauth2 import service_account\n\n# TODO(developer): Set key_path to the path to the service account key file.\n\nkey_path = \"lunar-pact-378812-7a28b789bde2.json\"\n\ncredentials = service_account.Credentials.from_service_account_file(\n    key_path\n)\n\n\n# @title Alternative Setup\n#from google.colab import auth\n#from google.cloud import bigquery\n#from google.colab import data_table\n\n#project = 'lunar-pact-378812' # Project ID inserted based on the query results selected to explore\n#location = 'US' # Location inserted based on the query results selected to explore\n#client = bigquery.Client(project=project, location=location)\n#data_table.enable_dataframe_formatter()\n#auth.authenticate_user()\n\nNow that we’re authenticated, we need to load the BigQuery package, and the google.colab.data_table package that can be used to display large pandas dataframes as an interactive data. Loading data_table is optional, but it will be useful for working with data in pandas.\n\n#%load_ext google.cloud.bigquery\n#%load_ext google.colab.data_table\n\n\nclient = bigquery.Client(credentials=credentials, project=credentials.project_id,)\n\n\n\n3.1.2 Create the reference\nYou can also work with Kaggle, which provide bigquery integration that you do not need to setup a google account. Each Kaggle user can scan 5TB every 30 days for free. Once you hit that limit, you’ll have to wait for it to reset. See https://www.kaggle.com/product-feedback/48573 for more details.\nThe first step in the workflow is to create a Client object. As you’ll soon see, this Client object will play a central role in retrieving information from BigQuery datasets.\n\n# Uncomment below and create a \"Client\" object if you are using Kaggle\n# client = bigquery.Client()\n\nWe’ll work with a dataset of posts on Hacker News, a website focusing on computer science and cybersecurity news. In BigQuery, each dataset is contained in a corresponding project. In this case, our hacker_news dataset is contained in the bigquery-public-data project.\nTo access the dataset, We begin by constructing a reference to the dataset with the dataset() method. Next, we use the get_dataset() method, along with the reference we just constructed, to fetch the dataset.\nSee the full list of public datasets or the kaggle bigquery dataset if you want to explore others.\n\n# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\nEvery dataset is just a collection of tables. You can think of a dataset as a spreadsheet file containing multiple tables, all composed of rows and columns.We use the list_tables() method to list the tables in the dataset.\n\n# List all the tables in the \"hacker_news\" dataset\ntables = list(client.list_tables(dataset))\n\n# Print names of all tables in the dataset (there are four!)\nfor table in tables:  \n    print(table.table_id)\n\ncomments\nfull\nfull_201510\nstories\n\n\nSimilar to how we fetched a dataset, we can fetch a table. In the code cell below, we fetch the full table in the hacker_news dataset\n\n# Construct a reference to the \"full\" table\ntable_ref = dataset_ref.table(\"full\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\nIn the next section, you’ll explore the contents of this table in more detail. For now, take the time to use the image below to consolidate what you’ve learned so far.\n\n\n\nfirst_commands\n\n\n\n\n3.1.3 Table schema\nThe structure of a table is called its schema. We need to understand a table’s schema to effectively pull out the data we want.\nIn this example, we’ll investigate the full table that we fetched above.\n\n# Print information on all the columns in the \"full\" table in the \"hacker_news\" dataset\ntable.schema\n\n[SchemaField('title', 'STRING', 'NULLABLE', None, 'Story title', (), None),\n SchemaField('url', 'STRING', 'NULLABLE', None, 'Story url', (), None),\n SchemaField('text', 'STRING', 'NULLABLE', None, 'Story or comment text', (), None),\n SchemaField('dead', 'BOOLEAN', 'NULLABLE', None, 'Is dead?', (), None),\n SchemaField('by', 'STRING', 'NULLABLE', None, \"The username of the item's author.\", (), None),\n SchemaField('score', 'INTEGER', 'NULLABLE', None, 'Story score', (), None),\n SchemaField('time', 'INTEGER', 'NULLABLE', None, 'Unix time', (), None),\n SchemaField('timestamp', 'TIMESTAMP', 'NULLABLE', None, 'Timestamp for the unix time', (), None),\n SchemaField('type', 'STRING', 'NULLABLE', None, 'Type of details (comment, comment_ranking, poll, story, job, pollopt)', (), None),\n SchemaField('id', 'INTEGER', 'NULLABLE', None, \"The item's unique id.\", (), None),\n SchemaField('parent', 'INTEGER', 'NULLABLE', None, 'Parent comment ID', (), None),\n SchemaField('descendants', 'INTEGER', 'NULLABLE', None, 'Number of story or poll descendants', (), None),\n SchemaField('ranking', 'INTEGER', 'NULLABLE', None, 'Comment ranking', (), None),\n SchemaField('deleted', 'BOOLEAN', 'NULLABLE', None, 'Is deleted?', (), None)]\n\n\nEach SchemaField tells us about a specific column (which we also refer to as a field). In order, the information is:\n\nThe name of the column\nThe field type (or datatype) in the column\nThe mode of the column ('NULLABLE' means that a column allows NULL values, and is the default)\nA description of the data in that column\n\nFor instance, the field has the SchemaField:\nSchemaField('by', 'string', 'NULLABLE', \"The username of the item's author.\",())\nThis tells us: - the field (or column) is called by, - the data in this field is strings, - NULL values are allowed, and - it contains the usernames corresponding to each item’s author.\nWe can use the list_rows() method to check just the first five lines of of the full table to make sure this is right. This returns a BigQuery RowIterator object that can quickly be converted to a pandas DataFrame with the to_dataframe() method.\n\n# Preview the first five lines of the \"full\" table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n  \n    \n      \n\n\n\n\n\n\ntitle\nurl\ntext\ndead\nby\nscore\ntime\ntimestamp\ntype\nid\nparent\ndescendants\nranking\ndeleted\n\n\n\n\n0\nNone\nNone\nI would rather just have wired earbuds, period...\n&lt;NA&gt;\nzeveb\n&lt;NA&gt;\n1591717736\n2020-06-09 15:48:56+00:00\ncomment\n23467666\n23456782\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n1\nNone\nNone\nDNS?\n&lt;NA&gt;\nnly\n&lt;NA&gt;\n1572810465\n2019-11-03 19:47:45+00:00\ncomment\n21436112\n21435130\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n2\nNone\nNone\nThese benchmarks seem pretty good. Filterable...\n&lt;NA&gt;\nmrkeen\n&lt;NA&gt;\n1591717727\n2020-06-09 15:48:47+00:00\ncomment\n23467665\n23467426\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n3\nNone\nNone\nOh really?&lt;p&gt;* Excel alone uses 86.1MB of priv...\n&lt;NA&gt;\noceanswave\n&lt;NA&gt;\n1462987532\n2016-05-11 17:25:32+00:00\ncomment\n11677248\n11676886\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n4\nNone\nNone\nThese systems are useless. Of the many flaws:...\n&lt;NA&gt;\nnyxxie\n&lt;NA&gt;\n1572810473\n2019-11-03 19:47:53+00:00\ncomment\n21436113\n21435025\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe list_rows() method will also let us look at just the information in a specific column. If we want to see the first five entries in the by column, for example, we can do that!\n\n# Preview the first five entries in the \"by\" column of the \"full\" table\nclient.list_rows(table, selected_fields=table.schema[4:5], max_results=5).to_dataframe()\n\n\n  \n    \n      \n\n\n\n\n\n\nby\n\n\n\n\n0\nzeveb\n\n\n1\nnly\n\n\n2\nmrkeen\n\n\n3\noceanswave\n\n\n4\nnyxxie\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n3.1.4 Select, From & Where\nNow that you know how to access and examine a dataset, you’re ready to write your first SQL query! As you’ll soon see, SQL queries will help you sort through a massive dataset, to retrieve only the information that you need. We’ll begin by using the keywords SELECT, FROM, and WHERE to get data from specific columns based on conditions you specify.\nWe’ll use an OpenAQ dataset about air quality. First, we’ll set up everything we need to run queries and take a quick peek at what tables are in our database.\n\n# Construct a reference to the \"openaq\" dataset\ndataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# List all the tables in the \"openaq\" dataset\ntables = list(client.list_tables(dataset))\n\n# Print names of all tables in the dataset (there's only one!)\nfor table in tables:  \n    print(table.table_id)\n\nglobal_air_quality\n\n\nThe dataset contains only one table, called global_air_quality. We’ll fetch the table and take a peek at the first few rows to see what sort of data it contains.\n\n# Construct a reference to the \"global_air_quality\" table\ntable_ref = dataset_ref.table(\"global_air_quality\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"global_air_quality\" table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n  \n    \n      \n\n\n\n\n\n\nlocation\ncity\ncountry\npollutant\nvalue\ntimestamp\nunit\nsource_name\nlatitude\nlongitude\naveraged_over_in_hours\nlocation_geom\n\n\n\n\n0\nBorówiec, ul. Drapałka\nBorówiec\nPL\nbc\n0.85217\n2022-04-28 07:00:00+00:00\nµg/m³\nGIOS\n1.0\n52.276794\n17.074114\nPOINT(52.276794 1)\n\n\n1\nKraków, ul. Bulwarowa\nKraków\nPL\nbc\n0.91284\n2022-04-27 23:00:00+00:00\nµg/m³\nGIOS\n1.0\n50.069308\n20.053492\nPOINT(50.069308 1)\n\n\n2\nPłock, ul. Reja\nPłock\nPL\nbc\n1.41000\n2022-03-30 04:00:00+00:00\nµg/m³\nGIOS\n1.0\n52.550938\n19.709791\nPOINT(52.550938 1)\n\n\n3\nElbląg, ul. Bażyńskiego\nElbląg\nPL\nbc\n0.33607\n2022-05-03 13:00:00+00:00\nµg/m³\nGIOS\n1.0\n54.167847\n19.410942\nPOINT(54.167847 1)\n\n\n4\nPiastów, ul. Pułaskiego\nPiastów\nPL\nbc\n0.51000\n2022-05-11 05:00:00+00:00\nµg/m³\nGIOS\n1.0\n52.191728\n20.837489\nPOINT(52.191728 1)\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nlet’s put together a query. Say we want to select all the values from the city column that are in rows where the country column is 'US' (for “United States”).\n\n# Query to select all the items from the \"city\" column where the \"country\" column is 'US'\n# SQL is almost completely case and indentation insensitive. The capitalization and\n# indentation style here is preferred style.\nquery = \"\"\"\n        SELECT city\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country = 'US'\n        \"\"\"\n\nNotice also that SQL statements requires single quotes for its strings inside python string (we use triple quotation mark here). We begin by setting up the query with the query() method.\n\n# Set up the query\nquery_job = client.query(query)\n\n# API request - run the query, and return a pandas DataFrame\nus_cities = query_job.to_dataframe()\n\nNow we’ve got a pandas DataFrame called us_cities, which we can use like any other DataFrame.\n\n# What five cities have the most measurements?\nus_cities.city.value_counts().head()\n\nPhoenix-Mesa-Scottsdale                     39414\nLos Angeles-Long Beach-Santa Ana            27479\nRiverside-San Bernardino-Ontario            26887\nNew York-Northern New Jersey-Long Island    25417\nSan Francisco-Oakland-Fremont               22710\nName: city, dtype: int64\n\n\nIf you want multiple columns, you can select them with a comma between the names:\n\nquery = \"\"\"\n        SELECT city, country\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country = 'US'\n        \"\"\"\n\nYou can select all columns with a * like this:\n\nquery = \"\"\"\n        SELECT *\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country = 'US'\n        \"\"\"\n\n\n\n3.1.5 Querying big dataset\nYou can estimate the size of any query before running it. Here is an example using the Hacker News dataset. To see how much data a query will scan, we create a QueryJobConfig object and set the dry_run parameter to True.\n\n# Query to get the score column from every row where the type column has value \"job\"\nquery = \"\"\"\n        SELECT score, title\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE type = \"job\" \n        \"\"\"\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(query, job_config=dry_run_config)\n\nprint(\"This query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed))\n\nThis query will process 553320240 bytes.\n\n\nYou can also specify a parameter when running the query to limit how much data you are willing to scan. Here’s an example with a low limit.\n\n# Only run the query if it's less than 1 MB\nONE_MB = 1000*1000\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_MB)\n\n# Set up the query (will only run if it's less than 1 MB)\nsafe_query_job = client.query(query, job_config=safe_config)\n\n# API request - try to run the query, and return a pandas DataFrame\nsafe_query_job.to_dataframe()\n\nInternalServerError: ignored\n\n\nIn this case, the query was cancelled, because the limit of 1 MB was exceeded. However, we can also increase the limit to run the query successfully!\n\n\n3.1.6 Group By, Having & Count\nNow that you can select raw data, you’re ready to learn how to group your data and count things within those groups.\nThe Hacker News dataset contains information on stories and comments from the Hacker News social networking site. We’ll work with the comments table and begin by printing the first few rows\n\n# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"comments\" table\ntable_ref = dataset_ref.table(\"comments\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"comments\" table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n  \n    \n      \n\n\n\n\n\n\nid\nby\nauthor\ntime\ntime_ts\ntext\nparent\ndeleted\ndead\nranking\n\n\n\n\n0\n9734136\nNone\nNone\n1434565400\n2015-06-17 18:23:20+00:00\nNone\n9733698\nTrue\n&lt;NA&gt;\n0\n\n\n1\n4921158\nNone\nNone\n1355496966\n2012-12-14 14:56:06+00:00\nNone\n4921100\nTrue\n&lt;NA&gt;\n0\n\n\n2\n7500568\nNone\nNone\n1396261158\n2014-03-31 10:19:18+00:00\nNone\n7499385\nTrue\n&lt;NA&gt;\n0\n\n\n3\n8909635\nNone\nNone\n1421627275\n2015-01-19 00:27:55+00:00\nNone\n8901135\nTrue\n&lt;NA&gt;\n0\n\n\n4\n9256463\nNone\nNone\n1427204705\n2015-03-24 13:45:05+00:00\nNone\n9256346\nTrue\n&lt;NA&gt;\n0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nLet’s use the table to see which comments generated the most replies. Since: - the parent column indicates the comment that was replied to, and - the id column has the unique ID used to identify each comment,\nwe can GROUP BY the parent column and COUNT() the id column in order to figure out the number of comments that were made as responses to a specific comment.\nFurthermore, since we’re only interested in popular comments, we’ll look at comments with more than ten replies. So, we’ll only return groups HAVING more than ten ID’s.\n\n# Query to select comments that received more than 10 replies\nquery_popular = \"\"\"\n                SELECT parent, COUNT(id)\n                FROM `bigquery-public-data.hacker_news.comments`\n                GROUP BY parent\n                HAVING COUNT(id) &gt; 10\n                \"\"\"\n\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_popular, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\npopular_comments = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\npopular_comments.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nparent\nf0_\n\n\n\n\n0\n2385424\n55\n\n\n1\n8441979\n57\n\n\n2\n7634152\n46\n\n\n3\n9062758\n49\n\n\n4\n5694173\n61\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npopular_comments\n\n\n  \n    \n      \n\n\n\n\n\n\nparent\nf0_\n\n\n\n\n0\n2385424\n55\n\n\n1\n8441979\n57\n\n\n2\n7634152\n46\n\n\n3\n9062758\n49\n\n\n4\n5694173\n61\n\n\n...\n...\n...\n\n\n77363\n1748827\n37\n\n\n77364\n3657756\n37\n\n\n77365\n2873865\n37\n\n\n77366\n9395540\n37\n\n\n77367\n1772903\n37\n\n\n\n\n\n77368 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nEach row in the popular_comments DataFrame corresponds to a comment that received more than ten replies.\nA couple hints to make your queries even better: - The column resulting from COUNT(id) was called f0__. That’s not a very descriptive name. You can change the name by adding AS NumPosts after you specify the aggregation. This is called aliasing. - If you are ever unsure what to put inside the COUNT() function, you can do COUNT(1) to count the rows in each group. Most people find it especially readable, because we know it’s not focusing on other columns. It also scans less data than if supplied column names (making it faster and using less of your data access quota).\nUsing these tricks, we can rewrite our query:\n\n# Improved version of earlier query, now with aliasing & improved readability\nquery_improved = \"\"\"\n                 SELECT parent, COUNT(1) AS NumPosts\n                 FROM `bigquery-public-data.hacker_news.comments`\n                 GROUP BY parent\n                 HAVING COUNT(1) &gt; 10\n                 \"\"\"\n\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_improved, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nimproved_df = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\nimproved_df.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nparent\nNumPosts\n\n\n\n\n0\n2970550\n63\n\n\n1\n8254532\n40\n\n\n2\n7687784\n44\n\n\n3\n4101992\n53\n\n\n4\n1632878\n39\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nimproved_df\n\n\n  \n    \n      \n\n\n\n\n\n\nparent\nNumPosts\n\n\n\n\n0\n2970550\n63\n\n\n1\n8254532\n40\n\n\n2\n7687784\n44\n\n\n3\n4101992\n53\n\n\n4\n1632878\n39\n\n\n...\n...\n...\n\n\n77363\n7844298\n37\n\n\n77364\n7864644\n37\n\n\n77365\n2866332\n37\n\n\n77366\n1885594\n37\n\n\n77367\n2175321\n37\n\n\n\n\n\n77368 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNow you have the data you want, and it has descriptive names.\n\n3.1.6.1 Note on using GROUP BY\nNote that because it tells SQL how to apply aggregate functions (like COUNT()), it doesn’t make sense to use GROUP BY without an aggregate function. Similarly, if you have any GROUP BY clause, then all variables must be passed to either a 1. GROUP BY command, or 2. an aggregation function.\nConsider the query below:\n\nquery_good = \"\"\"\n             SELECT parent, COUNT(id)\n             FROM `bigquery-public-data.hacker_news.comments`\n             GROUP BY parent\n             \"\"\"\n\nNote that there are two variables: parent and id. - parent was passed to a GROUP BY command (in GROUP BY parent), and - id was passed to an aggregate function (in COUNT(id)).\nAnd the query below won’t work, because the author column isn’t passed to an aggregate function or a GROUP BY clause:\n\nquery_bad = \"\"\"\n            SELECT author, parent, COUNT(id)\n            FROM `bigquery-public-data.hacker_news.comments`\n            GROUP BY parent\n            \"\"\"\n\n\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_bad, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nimproved_df = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\nimproved_df.head()\n\nBadRequest: ignored\n\n\n\n\n\n3.1.7 Order By\nFrequently, you’ll want to sort your results. Let’s use the US Traffic Fatality Records database, which contains information on traffic accidents in the US where at least one person died.\nWe’ll investigate the accident_2015 table. Here is a view of the first few rows.\n\n# Construct a reference to the \"nhtsa_traffic_fatalities\" dataset\ndataset_ref = client.dataset(\"nhtsa_traffic_fatalities\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"accident_2015\" table\ntable_ref = dataset_ref.table(\"accident_2015\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"accident_2015\" table\nclient.list_rows(table, max_results=5).to_dataframe()\n\nLet’s use the table to determine how the number of accidents varies with the day of the week. Since: - the consecutive_number column contains a unique ID for each accident, and - the timestamp_of_crash column contains the date of the accident in DATETIME format,\nwe can: - EXTRACT the day of the week (as day_of_week in the query below) from the timestamp_of_crash column, and - GROUP BY the day of the week, before we COUNT the consecutive_number column to determine the number of accidents for each day of the week.\nThen we sort the table with an ORDER BY clause, so the days with the most accidents are returned first.\n\n# Query to find out the number of accidents for each day of the week\nquery = \"\"\"\n        SELECT COUNT(consecutive_number) AS num_accidents, \n               EXTRACT(DAYOFWEEK FROM timestamp_of_crash) AS day_of_week\n        FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n        GROUP BY day_of_week\n        ORDER BY num_accidents DESC\n        \"\"\"\n\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 1 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**9)\nquery_job = client.query(query, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\naccidents_by_day = query_job.to_dataframe()\n\n# Print the DataFrame\naccidents_by_day\n\n\n  \n    \n      \n\n\n\n\n\n\nnum_accidents\nday_of_week\n\n\n\n\n0\n5659\n7\n\n\n1\n5298\n1\n\n\n2\n4916\n6\n\n\n3\n4460\n5\n\n\n4\n4182\n4\n\n\n5\n4038\n2\n\n\n6\n3985\n3\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNotice that the data is sorted by the num_accidents column, where the days with more traffic accidents appear first.\nTo map the numbers returned for the day_of_week column to the actual day, you might consult the BigQuery documentation on the DAYOFWEEK function. It says that it returns “an integer between 1 (Sunday) and 7 (Saturday), inclusively”. So, in 2015, most fatal motor accidents in the US occured on Sunday and Saturday, while the fewest happened on Tuesday.\n\n\n3.1.8 As and With\nOn its own, AS is a convenient way to clean up the data returned by your query. We’re going to use a common table expression (CTE) to find out how many Bitcoin transactions were made each day for the entire timespan of a bitcoin transaction dataset.\nWe’ll investigate the transactions table. Here is a view of the first few rows.\n\n# Construct a reference to the \"crypto_bitcoin\" dataset\ndataset_ref = client.dataset(\"crypto_bitcoin\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"transactions\" table\ntable_ref = dataset_ref.table(\"transactions\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"transactions\" table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n  \n    \n      \n\n\n\n\n\n\nhash\nsize\nvirtual_size\nversion\nlock_time\nblock_hash\nblock_number\nblock_timestamp\nblock_timestamp_month\ninput_count\noutput_count\ninput_value\noutput_value\nis_coinbase\nfee\ninputs\noutputs\n\n\n\n\n0\na16f3ce4dd5deb92d98ef5cf8afeaf0775ebca408f708b...\n275\n275\n1\n0\n00000000dc55860c8a29c58d45209318fa9e9dc2c1833a...\n181\n2009-01-12 06:02:13+00:00\n2009-01-01\n1\n2\n4000000000.000000000\n4000000000.000000000\nFalse\n0E-9\n[{'index': 0, 'spent_transaction_hash': 'f4184...\n[{'index': 0, 'script_asm': '04b5abd412d4341b4...\n\n\n1\n591e91f809d716912ca1d4a9295e70c3e78bab077683f7...\n275\n275\n1\n0\n0000000054487811fc4ff7a95be738aa5ad9320c394c48...\n182\n2009-01-12 06:12:16+00:00\n2009-01-01\n1\n2\n3000000000.000000000\n3000000000.000000000\nFalse\n0E-9\n[{'index': 0, 'spent_transaction_hash': 'a16f3...\n[{'index': 0, 'script_asm': '0401518fa1d1e1e3e...\n\n\n2\n12b5633bad1f9c167d523ad1aa1947b2732a865bf5414e...\n276\n276\n1\n0\n00000000f46e513f038baf6f2d9a95b2a28d8a6c985bcf...\n183\n2009-01-12 06:34:22+00:00\n2009-01-01\n1\n2\n2900000000.000000000\n2900000000.000000000\nFalse\n0E-9\n[{'index': 0, 'spent_transaction_hash': '591e9...\n[{'index': 0, 'script_asm': '04baa9d3665315562...\n\n\n3\n828ef3b079f9c23829c56fe86e85b4a69d9e06e5b54ea5...\n276\n276\n1\n0\n00000000fb5b44edc7a1aa105075564a179d65506e2bd2...\n248\n2009-01-12 20:04:20+00:00\n2009-01-01\n1\n2\n2800000000.000000000\n2800000000.000000000\nFalse\n0E-9\n[{'index': 0, 'spent_transaction_hash': '12b56...\n[{'index': 0, 'script_asm': '04bed827d37474bef...\n\n\n4\n35288d269cee1941eaebb2ea85e32b42cdb2b04284a56d...\n277\n277\n1\n0\n00000000689051c09ff2cd091cc4c22c10b965eb8db3ad...\n545\n2009-01-15 05:48:32+00:00\n2009-01-01\n1\n2\n2500000000.000000000\n2500000000.000000000\nFalse\n0E-9\n[{'index': 0, 'spent_transaction_hash': 'd71fd...\n[{'index': 0, 'script_asm': '044a656f065871a35...\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSince the block_timestamp column contains the date of each transaction in DATETIME format, we’ll convert these into DATE format using the DATE() command.\nWe do that using a CTE, and then the next part of the query counts the number of transactions for each date and sorts the table so that earlier dates appear first.\n\n# Query to select the number of transactions per date, sorted by date\nquery_with_CTE = \"\"\" \n                 WITH time AS \n                 (\n                     SELECT DATE(block_timestamp) AS trans_date\n                     FROM `bigquery-public-data.crypto_bitcoin.transactions`\n                 )\n                 SELECT COUNT(1) AS transactions,\n                        trans_date\n                 FROM time\n                 GROUP BY trans_date\n                 ORDER BY trans_date\n                 \"\"\"\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_with_CTE, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\ntransactions_by_date = query_job.to_dataframe()\n\n# Print the first five rows\ntransactions_by_date.head()\n\n\n  \n    \n      \n\n\n\n\n\n\ntransactions\ntrans_date\n\n\n\n\n0\n1\n2009-01-03\n\n\n1\n14\n2009-01-09\n\n\n2\n61\n2009-01-10\n\n\n3\n93\n2009-01-11\n\n\n4\n101\n2009-01-12\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSince they’re returned sorted, we can easily plot the raw results to show us the number of Bitcoin transactions per day over the whole timespan of this dataset.\n\ntransactions_by_date.set_index('trans_date').plot()\n\n&lt;AxesSubplot:xlabel='trans_date'&gt;\n\n\n\n\n\nAs you can see, common table expressions (CTEs) let you shift a lot of your data cleaning into SQL. That’s an especially good thing in the case of BigQuery, because it is vastly faster than doing the work in Pandas.\n\n\n3.1.9 Joining data\nWhen our data lives across different tables, how do we analyze it? By JOINing the tables together. A JOIN combines rows in the left table with corresponding rows in the right table, where the meaning of “corresponding” is based on how we specify the join.\nGitHub is the most popular place to collaborate on software projects. A GitHub repository (or repo) is a collection of files associated with a specific project. Most repos on GitHub are shared under a specific legal license, which determines the legal restrictions on how they are used. For our example, we’re going to look at how many different files have been released under each license.\nWe’ll work with two tables in the database. The first table is the licenses table, which provides the name of each GitHub repo (in the repo_name column) and its corresponding license. Here’s a view of the first five rows.\n\n# Construct a reference to the \"github_repos\" dataset\ndataset_ref = client.dataset(\"github_repos\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"licenses\" table\nlicenses_ref = dataset_ref.table(\"licenses\")\n\n# API request - fetch the table\nlicenses_table = client.get_table(licenses_ref)\n\n# Preview the first five lines of the \"licenses\" table\nclient.list_rows(licenses_table, max_results=5).to_dataframe()\n\n\n  \n    \n      \n\n\n\n\n\n\nrepo_name\nlicense\n\n\n\n\n0\nautarch/Dist-Zilla-Plugin-Test-TidyAll\nartistic-2.0\n\n\n1\nthundergnat/Prime-Factor\nartistic-2.0\n\n\n2\nkusha-b-k/Turabian_Engin_Fan\nartistic-2.0\n\n\n3\nonlinepremiumoutlet/onlinepremiumoutlet.github.io\nartistic-2.0\n\n\n4\nhuangyuanlove/LiaoBa_Service\nartistic-2.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe second table is the sample_files table, which provides, among other information, the GitHub repo that each file belongs to (in the repo_name column). The first several rows of this table are printed below.\n\n# Construct a reference to the \"sample_files\" table\nfiles_ref = dataset_ref.table(\"sample_files\")\n\n# API request - fetch the table\nfiles_table = client.get_table(files_ref)\n\n# Preview the first five lines of the \"sample_files\" table\nclient.list_rows(files_table, max_results=5).to_dataframe()\n\n\n  \n    \n      \n\n\n\n\n\n\nrepo_name\nref\npath\nmode\nid\nsymlink_target\n\n\n\n\n0\nEOL/eol\nrefs/heads/master\ngenerate/vendor/railties\n40960\n0338c33fb3fda57db9e812ac7de969317cad4959\n/usr/share/rails-ruby1.8/railties\n\n\n1\nnp/ling\nrefs/heads/master\ntests/success/merger_seq_inferred.t/merger_seq...\n40960\ndd4bb3d5ecabe5044d3fa5a36e0a9bf7ca878209\n../../../fixtures/all/merger_seq_inferred.ll\n\n\n2\nnp/ling\nrefs/heads/master\nfixtures/sequence/lettype.ll\n40960\n8fdf536def2633116d65b92b3b9257bcf06e3e45\n../all/lettype.ll\n\n\n3\nnp/ling\nrefs/heads/master\nfixtures/failure/wrong_order_seq3.ll\n40960\nc2509ae1196c4bb79d7e60a3d679488ca4a753e9\n../all/wrong_order_seq3.ll\n\n\n4\nnp/ling\nrefs/heads/master\nissues/sequence/keep.t\n40960\n5721de3488fb32745dfc11ec482e5dd0331fecaf\n../keep.t\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNext, we write a query that uses information in both tables to determine how many files are released in each license.\n\n# Query to determine the number of files per license, sorted by number of files\nquery = \"\"\"\n        SELECT L.license, COUNT(1) AS number_of_files\n        FROM `bigquery-public-data.github_repos.sample_files` AS sf\n        INNER JOIN `bigquery-public-data.github_repos.licenses` AS L \n            ON sf.repo_name = L.repo_name\n        GROUP BY L.license\n        ORDER BY number_of_files DESC\n        \"\"\"\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nfile_count_by_license = query_job.to_dataframe()\n\nIt’s a big query, and so we’ll investigate each piece separately.\n\nWe’ll begin with the JOIN (highlighted in blue above). This specifies the sources of data and how to join them. We use ON to specify that we combine the tables by matching the values in the repo_name columns in the tables.\nNext, we’ll talk about SELECT and GROUP BY (highlighted in yellow). The GROUP BY breaks the data into a different group for each license, before we COUNT the number of rows in the sample_files table that corresponds to each license. (Remember that you can count the number of rows with COUNT(1).)\nFinally, the ORDER BY (highlighted in purple) sorts the results so that licenses with more files appear first.\nIt was a big query, but it gave us a nice table summarizing how many files have been committed under each license:\n\n# Print the DataFrame\nfile_count_by_license\n\n\n  \n    \n      \n\n\n\n\n\n\nlicense\nnumber_of_files\n\n\n\n\n0\nmit\n20560894\n\n\n1\ngpl-2.0\n16608922\n\n\n2\napache-2.0\n7201141\n\n\n3\ngpl-3.0\n5107676\n\n\n4\nbsd-3-clause\n3465437\n\n\n5\nagpl-3.0\n1372100\n\n\n6\nlgpl-2.1\n799664\n\n\n7\nbsd-2-clause\n692357\n\n\n8\nlgpl-3.0\n582277\n\n\n9\nmpl-2.0\n457000\n\n\n10\ncc0-1.0\n449149\n\n\n11\nepl-1.0\n322255\n\n\n12\nunlicense\n208602\n\n\n13\nartistic-2.0\n147391\n\n\n14\nisc\n118332\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThere are a few more types of JOIN, along with how to use UNIONs to pull information from multiple tables. We’ll work with the Hacker News dataset. We begin by reviewing the first several rows of the comments table.\n\n# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"comments\" table\ntable_ref = dataset_ref.table(\"comments\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n  \n    \n      \n\n\n\n\n\n\nid\nby\nauthor\ntime\ntime_ts\ntext\nparent\ndeleted\ndead\nranking\n\n\n\n\n0\n9734136\nNone\nNone\n1434565400\n2015-06-17 18:23:20+00:00\nNone\n9733698\nTrue\n&lt;NA&gt;\n0\n\n\n1\n4921158\nNone\nNone\n1355496966\n2012-12-14 14:56:06+00:00\nNone\n4921100\nTrue\n&lt;NA&gt;\n0\n\n\n2\n7500568\nNone\nNone\n1396261158\n2014-03-31 10:19:18+00:00\nNone\n7499385\nTrue\n&lt;NA&gt;\n0\n\n\n3\n8909635\nNone\nNone\n1421627275\n2015-01-19 00:27:55+00:00\nNone\n8901135\nTrue\n&lt;NA&gt;\n0\n\n\n4\n9256463\nNone\nNone\n1427204705\n2015-03-24 13:45:05+00:00\nNone\n9256346\nTrue\n&lt;NA&gt;\n0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# Construct a reference to the \"stories\" table\ntable_ref = dataset_ref.table(\"stories\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n  \n    \n      \n\n\n\n\n\n\nid\nby\nscore\ntime\ntime_ts\ntitle\nurl\ntext\ndeleted\ndead\ndescendants\nauthor\n\n\n\n\n0\n6988445\ncflick\n0\n1388454902\n2013-12-31 01:55:02+00:00\nAppshare\nhttp://chadflick.ws/appshare.html\nDid facebook or angrybirds pay you? We will!\n&lt;NA&gt;\nTrue\n&lt;NA&gt;\ncflick\n\n\n1\n7047571\nRd2\n1\n1389562985\n2014-01-12 21:43:05+00:00\nJava in startups\n\nHello, hacker news!&lt;p&gt;Have any of you used jav...\n&lt;NA&gt;\nTrue\n&lt;NA&gt;\nRd2\n\n\n2\n9157712\nmo0\n1\n1425657937\n2015-03-06 16:05:37+00:00\nShow HN: Discover what songs were used in YouT...\nhttp://www.mooma.sh/\nThe user can paste a media url(currently only ...\n&lt;NA&gt;\nTrue\n&lt;NA&gt;\nmo0\n\n\n3\n8127403\nad11\n1\n1407052667\n2014-08-03 07:57:47+00:00\nMy poker project, what do you think?\n\nHi guys, what do you think about my poker proj...\n&lt;NA&gt;\nTrue\n&lt;NA&gt;\nad11\n\n\n4\n6933158\nemyy\n1\n1387432701\n2013-12-19 05:58:21+00:00\nChristmas Crafts Ideas - Easy and Simple Famil...\nhttp://www.winxdvd.com/resource/christmas-craf...\nThere are some free Christmas craft ideas to m...\n&lt;NA&gt;\nTrue\n&lt;NA&gt;\nemyy\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe query below pulls information from the stories and comments tables to create a table showing all stories posted on January 1, 2012, along with the corresponding number of comments. We use a LEFT JOIN so that the results include stories that didn’t receive any comments.\n\n# Query to select all stories posted on January 1, 2012, with number of comments\njoin_query = \"\"\"\n             WITH c AS\n             (\n             SELECT parent, COUNT(*) as num_comments\n             FROM `bigquery-public-data.hacker_news.comments` \n             GROUP BY parent\n             )\n             SELECT s.id as story_id, s.by, s.title, c.num_comments\n             FROM `bigquery-public-data.hacker_news.stories` AS s\n             LEFT JOIN c\n             ON s.id = c.parent\n             WHERE EXTRACT(DATE FROM s.time_ts) = '2012-01-01'\n             ORDER BY c.num_comments DESC\n             \"\"\"\n\n# Run the query, and return a pandas DataFrame\njoin_result = client.query(join_query).result().to_dataframe()\njoin_result.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nstory_id\nby\ntitle\nnum_comments\n\n\n\n\n0\n3412900\nwhoishiring\nAsk HN: Who is Hiring? (January 2012)\n154\n\n\n1\n3412901\nwhoishiring\nAsk HN: Freelancer? Seeking freelancer? (Janua...\n97\n\n\n2\n3412643\njemeshsu\nAvoid Apress\n30\n\n\n3\n3412891\nBrajeshwar\nThere's no shame in code that is simply \"good ...\n27\n\n\n4\n3414012\nramanujam\nImpress.js - a Prezi like implementation using...\n27\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSince the results are ordered by the num_comments column, stories without comments appear at the end of the DataFrame. (Remember that NaN stands for “not a number”.)\n\n# None of these stories received any comments\njoin_result.tail()\n\n\n  \n    \n      \n\n\n\n\n\n\nstory_id\nby\ntitle\nnum_comments\n\n\n\n\n439\n3413342\njonsteiman\nThe List You Can't Miss: Top 5 Blogs of 2011\n&lt;NA&gt;\n\n\n440\n3412327\narroyo\nCan your business card be followed?\n&lt;NA&gt;\n\n\n441\n3413203\nnorris_tony44\n10 Popular iPhone Games you Must Play\n&lt;NA&gt;\n\n\n442\n3412940\njulelara\nWashington Redskins vs Philadelphia Eagles liv...\n&lt;NA&gt;\n\n\n443\n3412632\nUsedCarFleetCom\nUsed Car fleet\n&lt;NA&gt;\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAs you’ve seen, JOINs horizontally combine results from different tables. If you instead would like to vertically concatenate columns, you can do so with a UNION.\nNext, we write a query to select all usernames corresponding to users who wrote stories or comments on January 1, 2014. We use UNION DISTINCT (instead of UNION ALL) to ensure that each user appears in the table at most once.\n\n# Query to select all users who posted stories or comments on January 1, 2014\nunion_query = \"\"\"\n              SELECT c.by\n              FROM `bigquery-public-data.hacker_news.comments` AS c\n              WHERE EXTRACT(DATE FROM c.time_ts) = '2014-01-01'\n              UNION DISTINCT\n              SELECT s.by\n              FROM `bigquery-public-data.hacker_news.stories` AS s\n              WHERE EXTRACT(DATE FROM s.time_ts) = '2014-01-01'\n              \"\"\"\n\n# Run the query, and return a pandas DataFrame\nunion_result = client.query(union_query).result().to_dataframe()\nunion_result.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nby\n\n\n\n\n0\nvidarh\n\n\n1\ntlarkworthy\n\n\n2\njbl\n\n\n3\ndmgrow\n\n\n4\nmaurorm\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nTo get the number of users who posted on January 1, 2014, we need only take the length of the DataFrame.\n\n# Number of users who posted stories or comments on January 1, 2014\nlen(union_result)\n\n2282\n\n\n\n\n3.1.10 Nested and Repeated data\nSo far, you’ve worked with many types of data, including numeric types (integers, floating point values), strings, and the DATETIME type. In this tutorial, you’ll learn how to query nested and repeated data. These are the most complex data types that you can find in BigQuery datasets!\nWe’ll work with the Google Analytics Sample dataset. It contains information tracking the behavior of visitors to the Google Merchandise store, an e-commerce website that sells Google branded items.\nWe begin by printing the first few rows of the ga_sessions_20170801 table. This table tracks visits to the website on August 1, 2017. The table has many nested fields from table preview:\n\n# Construct a reference to the \"google_analytics_sample\" dataset\ndataset_ref = client.dataset(\"google_analytics_sample\", project=\"bigquery-public-data\")\n\n# Construct a reference to the \"ga_sessions_20170801\" table\ntable_ref = dataset_ref.table(\"ga_sessions_20170801\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n  \n    \n      \n\n\n\n\n\n\nvisitorId\nvisitNumber\nvisitId\nvisitStartTime\ndate\ntotals\ntrafficSource\ndevice\ngeoNetwork\ncustomDimensions\nhits\nfullVisitorId\nuserId\nclientId\nchannelGrouping\nsocialEngagementType\n\n\n\n\n0\n&lt;NA&gt;\n1\n1501591568\n1501591568\n20170801\n{'visits': 1, 'hits': 1, 'pageviews': 1, 'time...\n{'referralPath': None, 'campaign': '(not set)'...\n{'browser': 'Chrome', 'browserVersion': 'not a...\n{'continent': 'Europe', 'subContinent': 'South...\n[]\n[{'hitNumber': 1, 'time': 0, 'hour': 5, 'minut...\n3418334011779872055\nNone\nNone\nOrganic Search\nNot Socially Engaged\n\n\n1\n&lt;NA&gt;\n2\n1501589647\n1501589647\n20170801\n{'visits': 1, 'hits': 1, 'pageviews': 1, 'time...\n{'referralPath': '/analytics/web/', 'campaign'...\n{'browser': 'Chrome', 'browserVersion': 'not a...\n{'continent': 'Asia', 'subContinent': 'Souther...\n[{'index': 4, 'value': 'APAC'}]\n[{'hitNumber': 1, 'time': 0, 'hour': 5, 'minut...\n2474397855041322408\nNone\nNone\nReferral\nNot Socially Engaged\n\n\n2\n&lt;NA&gt;\n1\n1501616621\n1501616621\n20170801\n{'visits': 1, 'hits': 1, 'pageviews': 1, 'time...\n{'referralPath': '/analytics/web/', 'campaign'...\n{'browser': 'Chrome', 'browserVersion': 'not a...\n{'continent': 'Europe', 'subContinent': 'North...\n[{'index': 4, 'value': 'EMEA'}]\n[{'hitNumber': 1, 'time': 0, 'hour': 12, 'minu...\n5870462820713110108\nNone\nNone\nReferral\nNot Socially Engaged\n\n\n3\n&lt;NA&gt;\n1\n1501601200\n1501601200\n20170801\n{'visits': 1, 'hits': 1, 'pageviews': 1, 'time...\n{'referralPath': '/analytics/web/', 'campaign'...\n{'browser': 'Firefox', 'browserVersion': 'not ...\n{'continent': 'Americas', 'subContinent': 'Nor...\n[{'index': 4, 'value': 'North America'}]\n[{'hitNumber': 1, 'time': 0, 'hour': 8, 'minut...\n9397809171349480379\nNone\nNone\nReferral\nNot Socially Engaged\n\n\n4\n&lt;NA&gt;\n1\n1501615525\n1501615525\n20170801\n{'visits': 1, 'hits': 1, 'pageviews': 1, 'time...\n{'referralPath': '/analytics/web/', 'campaign'...\n{'browser': 'Chrome', 'browserVersion': 'not a...\n{'continent': 'Americas', 'subContinent': 'Nor...\n[{'index': 4, 'value': 'North America'}]\n[{'hitNumber': 1, 'time': 0, 'hour': 12, 'minu...\n6089902943184578335\nNone\nNone\nReferral\nNot Socially Engaged\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNow we’ll work with the hits column as an example of data that is both nested and repeated. Since:\n\nhits is a STRUCT (contains nested data) and is repeated,\nhitNumber, page, and type are all nested inside the hits column, and\npagePath is nested inside the page field,\n\nwe can query these fields with the following syntax:\n\n# Query to determine most popular landing point on the website\nquery = \"\"\"\n        SELECT hits.page.pagePath as path,\n            COUNT(hits.page.pagePath) as counts\n        FROM `bigquery-public-data.google_analytics_sample.ga_sessions_20170801`, \n            UNNEST(hits) as hits\n        WHERE hits.type=\"PAGE\" and hits.hitNumber=1\n        GROUP BY path\n        ORDER BY counts DESC\n        \"\"\"\n\n# Run the query, and return a pandas DataFrame\nresult = client.query(query).result().to_dataframe()\nresult.head()\n\n\n  \n    \n      \n\n\n\n\n\n\npath\ncounts\n\n\n\n\n0\n/home\n1257\n\n\n1\n/google+redesign/shop+by+brand/youtube\n587\n\n\n2\n/google+redesign/apparel/mens/mens+t+shirts\n117\n\n\n3\n/signin.html\n78\n\n\n4\n/basket.html\n35\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n3.1.11 Analytic Function (Optional)\nYou can also define analytic functions, which also operate on a set of rows like aggregation function. However, unlike aggregate functions, analytic functions return a (potentially different) value for each row in the original table. Analytic functions allow us to perform complex calculations with relatively straightforward syntax. For instance, we can quickly calculate moving averages and running totals, among other quantities.\nWe’ll work with the San Francisco Open Data dataset.\n\n# Construct a reference to the \"san_francisco\" dataset\ndataset_ref = client.dataset(\"san_francisco\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"bikeshare_trips\" table\ntable_ref = dataset_ref.table(\"bikeshare_trips\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n  \n    \n      \n\n\n\n\n\n\ntrip_id\nduration_sec\nstart_date\nstart_station_name\nstart_station_id\nend_date\nend_station_name\nend_station_id\nbike_number\nzip_code\nsubscriber_type\n\n\n\n\n0\n1235850\n1540\n2016-06-11 08:19:00+00:00\nSan Jose Diridon Caltrain Station\n2\n2016-06-11 08:45:00+00:00\nSan Jose Diridon Caltrain Station\n2\n124\n15206\nCustomer\n\n\n1\n1219337\n6324\n2016-05-29 12:49:00+00:00\nSan Jose Diridon Caltrain Station\n2\n2016-05-29 14:34:00+00:00\nSan Jose Diridon Caltrain Station\n2\n174\n55416\nCustomer\n\n\n2\n793762\n115572\n2015-06-04 09:22:00+00:00\nSan Jose Diridon Caltrain Station\n2\n2015-06-05 17:28:00+00:00\nSan Jose Diridon Caltrain Station\n2\n190\n95391\nCustomer\n\n\n3\n453845\n54120\n2014-09-15 16:53:00+00:00\nSan Jose Diridon Caltrain Station\n2\n2014-09-16 07:55:00+00:00\nSan Jose Diridon Caltrain Station\n2\n127\n81\nCustomer\n\n\n4\n1245113\n5018\n2016-06-17 20:08:00+00:00\nSan Jose Diridon Caltrain Station\n2\n2016-06-17 21:32:00+00:00\nSan Jose Diridon Caltrain Station\n2\n153\n95070\nCustomer\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nEach row of the table corresponds to a different bike trip, and we can use an analytic function to calculate the cumulative number of trips for each date in 2015.\n\n# Query to count the (cumulative) number of trips per day\nnum_trips_query = \"\"\"\n                  WITH trips_by_day AS\n                  (\n                  SELECT DATE(start_date) AS trip_date,\n                      COUNT(*) as num_trips\n                  FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n                  WHERE EXTRACT(YEAR FROM start_date) = 2015\n                  GROUP BY trip_date\n                  )\n                  SELECT *,\n                      SUM(num_trips) \n                          OVER (\n                               ORDER BY trip_date\n                               ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n                               ) AS cumulative_trips\n                  FROM trips_by_day\n                  \"\"\"\n\n# Run the query, and return a pandas DataFrame\nnum_trips_result = client.query(num_trips_query).result().to_dataframe()\nnum_trips_result.head()\n\n\n  \n    \n      \n\n\n\n\n\n\ntrip_date\nnum_trips\ncumulative_trips\n\n\n\n\n0\n2015-01-13\n1368\n10709\n\n\n1\n2015-04-06\n1281\n91635\n\n\n2\n2015-04-25\n405\n111722\n\n\n3\n2015-06-18\n1352\n166602\n\n\n4\n2015-07-14\n1358\n192161\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe query uses a common table expression (CTE) to first calculate the daily number of trips. Then, we use SUM() as an aggregate function. - Since there is no PARTITION BY clause, the entire table is treated as a single partition. - The ORDER BY clause orders the rows by date, where earlier dates appear first. - By setting the window frame clause to ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW, we ensure that all rows up to and including the current date are used to calculate the (cumulative) sum. See https://cloud.google.com/bigquery/docs/reference/standard-sql/analytic-function-concepts#def_window_frame for more details.\nThe next query tracks the stations where each bike began (in start_station_id) and ended (in end_station_id) the day on October 25, 2015.\n\n# Query to track beginning and ending stations on October 25, 2015, for each bike\nstart_end_query = \"\"\"\n                  SELECT bike_number,\n                      TIME(start_date) AS trip_time,\n                      FIRST_VALUE(start_station_id)\n                          OVER (\n                               PARTITION BY bike_number\n                               ORDER BY start_date\n                               ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n                               ) AS first_station_id,\n                      LAST_VALUE(end_station_id)\n                          OVER (\n                               PARTITION BY bike_number\n                               ORDER BY start_date\n                               ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n                               ) AS last_station_id,\n                      start_station_id,\n                      end_station_id\n                  FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n                  WHERE DATE(start_date) = '2015-10-25' \n                  \"\"\"\n\n# Run the query, and return a pandas DataFrame\nstart_end_result = client.query(start_end_query).result().to_dataframe()\nstart_end_result.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nbike_number\ntrip_time\nfirst_station_id\nlast_station_id\nstart_station_id\nend_station_id\n\n\n\n\n0\n230\n22:41:00\n22\n22\n22\n22\n\n\n1\n601\n15:50:00\n68\n67\n68\n50\n\n\n2\n601\n23:27:00\n68\n67\n50\n67\n\n\n3\n604\n08:56:00\n70\n66\n70\n39\n\n\n4\n604\n12:34:00\n70\n66\n39\n67\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe query uses both FIRST_VALUE() and LAST_VALUE() as analytic functions. - The PARTITION BY clause breaks the data into partitions based on the bike_number column. Since this column holds unique identifiers for the bikes, this ensures the calculations are performed separately for each bike. - The ORDER BY clause puts the rows within each partition in chronological order. - Since the window frame clause is ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING, for each row, its entire partition is used to perform the calculation. (This ensures the calculated values for rows in the same partition are identical.)\nYou can check https://cloud.google.com/bigquery/docs/reference/standard-sql/introduction and https://googleapis.dev/python/bigquery/latest/index.html for more details."
  },
  {
    "objectID": "03_Relational_Database_and_data_wrangling.html#data-wrangling-with-pandas",
    "href": "03_Relational_Database_and_data_wrangling.html#data-wrangling-with-pandas",
    "title": "3  Relational Database and data wrangling",
    "section": "3.2 Data Wrangling with Pandas",
    "text": "3.2 Data Wrangling with Pandas\n\n3.2.1 Series objects\nThe Pandas library contains these useful data structures: * Series objects, that we will discuss now. A Series object is 1D array, similar to a column in a spreadsheet (with a column name and row labels). * DataFrame objects. This is a 2D table, similar to a spreadsheet (with column names and row labels).\n\n3.2.1.1 Creating a Series\nLet’s start by creating our first Series object!\n\ns = pd.Series([2,-1,3,5])\ns\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64\n\n\nArithmetic operations on Series are also possible, and they apply elementwise, just like for ndarrays in NumPy:\n\ns + [1000,2000,3000,4000]\n\n0    1002\n1    1999\n2    3003\n3    4005\ndtype: int64\n\n\n\ns + 1000 # Broadcasting\n\n0    1002\n1     999\n2    1003\n3    1005\ndtype: int64\n\n\n\n\n3.2.1.2 Index labels\nEach item in a Series object has a unique identifier called the index label. By default, it is simply the rank of the item in the Series (starting at 0) but you can also set the index labels manually:\n\ns2 = pd.Series([68, 83, 112, 68], index=[\"alice\", \"bob\", \"charles\", \"darwin\"])\ns2\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\nYou can then use the Series just like a dict:\n\ns2[\"bob\"]\n\n83\n\n\nYou can still access the items by integer location, like in a regular array:\n\ns2[1]\n\n83\n\n\nTo make it clear when you are accessing, it is recommended to always use the loc attribute when accessing by label, and the iloc attribute when accessing by integer location:\n\ns2.loc[\"bob\"]\n\n83\n\n\n\ns2.iloc[1]\n\n83\n\n\nSlicing a Series also slices the index labels:\n\ns2.iloc[1:3]\n\nbob         83\ncharles    112\ndtype: int64\n\n\n\n\n3.2.1.3 Initialize from dict\nYou can create a Series object from a dict. The keys will be used as index labels:\n\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\ns3 = pd.Series(weights)\ns3\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\nWhen an operation involves multiple Series objects, pandas automatically aligns items by matching index labels.\n\nprint(s2.keys())\nprint(s3.keys())\n\ns2 + s3\n\nIndex(['alice', 'bob', 'charles', 'darwin'], dtype='object')\nIndex(['alice', 'bob', 'colin', 'darwin'], dtype='object')\n\n\nalice      136.0\nbob        166.0\ncharles      NaN\ncolin        NaN\ndarwin     136.0\ndtype: float64\n\n\nThe resulting Series contains the union of index labels from s2 and s3. Since \"colin\" is missing from s2 and \"charles\" is missing from s3, these items have a NaN result value. (ie. Not-a-Number means missing).\nAutomatic alignment is very handy when working with data that may come from various sources with varying structure and missing items\n\n\n3.2.1.4 Initialize with a scalar\nYou can also initialize a Series object using a scalar and a list of index labels: all items will be set to the scalar.\n\nmeaning = pd.Series(42, [\"life\", \"universe\", \"everything\"])\nmeaning\n\nlife          42\nuniverse      42\neverything    42\ndtype: int64\n\n\nPandas makes it easy to plot Series data using matplotlib:\n\ntemperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5]\ns4 = pd.Series(temperatures, name=\"Temperature\")\ns4.plot()\nplt.show()\n\n\n\n\nYou can easily convert it to NumPy array by dicarding the index.\n\ns4.to_numpy()\n\narray([4.4, 5.1, 6.1, 6.2, 6.1, 6.1, 5.7, 5.2, 4.7, 4.1, 3.9, 3.5])\n\n\nThere are many options for plotting your data. It is not necessary to list them all here: if you need a particular type of plot (histograms, pie charts, etc.), just look for it in the excellent Visualization section of pandas’ documentation, and look at the example code.\n\n\n\n3.2.2 Handling time\nMany datasets have timestamps, and pandas is awesome at manipulating such data: * it can represent periods (such as 2016Q3) and frequencies (such as “monthly”) * it can convert periods to actual timestamps, and vice versa * it can resample data and aggregate values any way you like * it can handle timezones.\n\n3.2.2.1 Time range\nLet’s start by creating a time series using pd.date_range(). This returns a DatetimeIndex containing one datetime per hour for 12 hours starting on March 6th 2023 at 5:30pm.\n\ndates = pd.date_range('2023/03/06 5:30pm', periods=12, freq='H')\ndates\n\nDatetimeIndex(['2023-03-06 17:30:00', '2023-03-06 18:30:00',\n               '2023-03-06 19:30:00', '2023-03-06 20:30:00',\n               '2023-03-06 21:30:00', '2023-03-06 22:30:00',\n               '2023-03-06 23:30:00', '2023-03-07 00:30:00',\n               '2023-03-07 01:30:00', '2023-03-07 02:30:00',\n               '2023-03-07 03:30:00', '2023-03-07 04:30:00'],\n              dtype='datetime64[ns]', freq='H')\n\n\nThis DatetimeIndex may be used as an index in a Series:\n\ntemp_series = pd.Series(temperatures, dates)\ntemp_series\n\n2023-03-06 17:30:00    4.4\n2023-03-06 18:30:00    5.1\n2023-03-06 19:30:00    6.1\n2023-03-06 20:30:00    6.2\n2023-03-06 21:30:00    6.1\n2023-03-06 22:30:00    6.1\n2023-03-06 23:30:00    5.7\n2023-03-07 00:30:00    5.2\n2023-03-07 01:30:00    4.7\n2023-03-07 02:30:00    4.1\n2023-03-07 03:30:00    3.9\n2023-03-07 04:30:00    3.5\nFreq: H, dtype: float64\n\n\nLet’s plot this series:\n\ntemp_series.plot(kind=\"bar\")\n\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n3.2.3 Periods\nThe pd.period_range() function returns a PeriodIndex instead of a DatetimeIndex. For example, let’s get all quarters in 2022 and 2023:\n\nquarters = pd.period_range('2022Q1', periods=8, freq='Q')\nquarters\n\nPeriodIndex(['2022Q1', '2022Q2', '2022Q3', '2022Q4', '2023Q1', '2023Q2',\n             '2023Q3', '2023Q4'],\n            dtype='period[Q-DEC]')\n\n\nAdding a number N to a PeriodIndex shifts the periods by N times the PeriodIndex’s frequency:\n\nquarters + 3\n\nPeriodIndex(['2022Q4', '2023Q1', '2023Q2', '2023Q3', '2023Q4', '2024Q1',\n             '2024Q2', '2024Q3'],\n            dtype='period[Q-DEC]')\n\n\nPandas also provides many other time-related functions that we recommend you check out in the documentation\n\n\n3.2.4 DataFrame objects\nA DataFrame object represents a spreadsheet, with cell values, column names and row index labels. You can define expressions to compute columns based on other columns, create pivot-tables, group rows, draw graphs, etc. You can see DataFrame as dictionaries of Series.\n\n3.2.4.1 Creating a DataFrame\n\n#%unload_ext google.colab.data_table #cloab only\n\n\n#from google.colab import data_table\n#data_table.disable_dataframe_formatter()\n\nYou can create a DataFrame by passing a dictionary of Series objects:\n\npeople_dict = {\n    \"weight\": pd.Series([68, 83, 112], index=[\"alice\", \"bob\", \"charles\"]),\n    \"birthyear\": pd.Series([1984, 1985, 1992], index=[\"bob\", \"alice\", \"charles\"], name=\"year\"),\n    \"children\": pd.Series([0, 3], index=[\"charles\", \"bob\"]),\n    \"hobby\": pd.Series([\"Biking\", \"Dancing\"], index=[\"alice\", \"bob\"]),\n}\npeople = pd.DataFrame(people_dict)\npeople\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nbirthyear\nchildren\nhobby\n\n\n\n\nalice\n68\n1985\nNaN\nBiking\n\n\nbob\n83\n1984\n3.0\nDancing\n\n\ncharles\n112\n1992\n0.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nA few things to note: * the Series were automatically aligned based on their index, * missing values are represented as NaN, * Series names are ignored (the name \"year\" was dropped), * DataFrames are displayed nicely in Jupyter notebooks!\n\n\n3.2.4.2 Subsets - Accessing columns\nYou can access columns by using the column name or fancy indexing. They are returned as Series objects:\n\npeople[\"birthyear\"]\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\nYou can also get multiple columns at once:\n\npeople[[\"birthyear\", \"hobby\"]]\n\n\n  \n    \n      \n\n\n\n\n\n\nbirthyear\nhobby\n\n\n\n\nalice\n1985\nBiking\n\n\nbob\n1984\nDancing\n\n\ncharles\n1992\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAnother convenient way to create a DataFrame is to pass all the values to the constructor as an ndarray, or a list of lists, and specify the column names and row index labels separately:\n\nvalues = [\n            [1985, np.nan, \"Biking\",   68],\n            [1984, 3,      \"Dancing\",  83],\n            [1992, 0,      np.nan,    112]\n         ]\nd3 = pd.DataFrame(\n        values,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n  \n    \n      \n\n\n\n\n\n\nbirthyear\nchildren\nhobby\nweight\n\n\n\n\nalice\n1985\nNaN\nBiking\n68\n\n\nbob\n1984\n3.0\nDancing\n83\n\n\ncharles\n1992\n0.0\nNaN\n112\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n3.2.4.3 Multi-index (optional)\nYou can also create multi-index datafram as follows:\n\ndf = pd.DataFrame(\n    {\n        \"a\" : [4 ,5, 6],\n        \"b\" : [7, 8, 9],\n        \"c\" : [10, 11, 12]\n     },\n    index = pd.MultiIndex.from_tuples(\n        [('d',1),('d',2),('e',2)], names=['n','v']\n    )\n)\ndf\n\n\n  \n    \n      \n\n\n\n\n\n\n\na\nb\nc\n\n\nn\nv\n\n\n\n\n\n\n\nd\n1\n4\n7\n10\n\n\n2\n5\n8\n11\n\n\ne\n2\n6\n9\n12\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nIf all columns are tuples of the same size, then they are understood as a multi-index. The same goes for row index labels. For example:\n\nd5 = pd.DataFrame(\n  {\n    (\"public\", \"birthyear\"):\n        {(\"Paris\",\"alice\"):1985, (\"Paris\",\"bob\"): 1984, (\"London\",\"charles\"): 1992},\n    (\"public\", \"hobby\"):\n        {(\"Paris\",\"alice\"):\"Biking\", (\"Paris\",\"bob\"): \"Dancing\"},\n    (\"private\", \"weight\"):\n        {(\"Paris\",\"alice\"):68, (\"Paris\",\"bob\"): 83, (\"London\",\"charles\"): 112},\n    (\"private\", \"children\"):\n        {(\"Paris\", \"alice\"):np.nan, (\"Paris\",\"bob\"): 3, (\"London\",\"charles\"): 0}\n  }\n)\nd5\n\n\n  \n    \n      \n\n\n\n\n\n\n\npublic\nprivate\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nParis\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\nLondon\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou can now get a DataFrame containing all the “public” columns very simply:\n\nd5[\"public\"]\n\n\n  \n    \n      \n\n\n\n\n\n\n\nbirthyear\nhobby\n\n\n\n\nParis\nalice\n1985\nBiking\n\n\nbob\n1984\nDancing\n\n\nLondon\ncharles\n1992\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nIt is noted that most methods return modified copies in pandas.\n\n\n3.2.4.4 Subsets - Accessing rows\nLet’s go back to the people DataFrame:\n\npeople\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nbirthyear\nchildren\nhobby\n\n\n\n\nalice\n68\n1985\nNaN\nBiking\n\n\nbob\n83\n1984\n3.0\nDancing\n\n\ncharles\n112\n1992\n0.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe loc attribute lets you access rows instead of columns. The result is a Series object in which the DataFrame’s column names are mapped to row index labels:\n\npeople.loc[\"charles\"]\n\nweight        112\nbirthyear    1992\nchildren      0.0\nhobby         NaN\nName: charles, dtype: object\n\n\nYou can also access rows by integer location using the iloc attribute:\n\npeople.iloc[2]\n\nweight        112\nbirthyear    1992\nchildren      0.0\nhobby         NaN\nName: charles, dtype: object\n\n\nYou can also get a slice of rows, and this returns a DataFrame object:\n\npeople.iloc[1:3]\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nbirthyear\nchildren\nhobby\n\n\n\n\nbob\n83\n1984\n3.0\nDancing\n\n\ncharles\n112\n1992\n0.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nFinally, you can pass a boolean array to get the matching rows. This is most useful when combined with boolean expressions:\n\npeople[people[\"birthyear\"] &lt; 1990]\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nbirthyear\nchildren\nhobby\n\n\n\n\nalice\n68\n1985\nNaN\nBiking\n\n\nbob\n83\n1984\n3.0\nDancing\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou can also accessing columns by specifiying the second axis:\n\npeople.iloc[:,2]\n\nalice      NaN\nbob        3.0\ncharles    0.0\nName: children, dtype: float64\n\n\n\n\n3.2.4.5 Adding and removing columns\nYou can generally treat DataFrame objects like dictionaries of Series, so the following work fine:\n\npeople\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nbirthyear\nchildren\nhobby\n\n\n\n\nalice\n68\n1985\nNaN\nBiking\n\n\nbob\n83\n1984\n3.0\nDancing\n\n\ncharles\n112\n1992\n0.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npeople[\"age\"] = 2023 - people[\"birthyear\"]  # adds a new column \"age\"\npeople[\"over 30\"] = people[\"age\"] &gt; 30      # adds another column \"over 30\"\nbirthyears = people.pop(\"birthyear\")\npeople.drop(columns=['children'], inplace=True) # drop a column inplace\npeople\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nhobby\nage\nover 30\n\n\n\n\nalice\n68\nBiking\n38\nTrue\n\n\nbob\n83\nDancing\n39\nTrue\n\n\ncharles\n112\nNaN\n31\nTrue\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nbirthyears\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\nWhen you add a new column, it must have the same number of rows. Missing rows are filled with NaN, and extra rows are ignored:\n\npeople[\"pets\"] = pd.Series({\"bob\": 0, \"charles\": 5, \"eugene\":1})  # alice is missing, eugene is ignored\npeople\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nhobby\nage\nover 30\npets\n\n\n\n\nalice\n68\nBiking\n38\nTrue\nNaN\n\n\nbob\n83\nDancing\n39\nTrue\n0.0\n\n\ncharles\n112\nNaN\n31\nTrue\n5.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWhen adding a new column, it is added at the end (on the right) by default. You can also insert a column anywhere else using the insert() method:\n\npeople.insert(1, \"height\", [172, 181, 185])\npeople\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nheight\nhobby\nage\nover 30\npets\n\n\n\n\nalice\n68\n172\nBiking\n38\nTrue\nNaN\n\n\nbob\n83\n181\nDancing\n39\nTrue\n0.0\n\n\ncharles\n112\n185\nNaN\n31\nTrue\n5.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou can also create new columns by calling the assign() method. Note that this returns a new DataFrame object, the original is not modified\n\np2 = people.assign(\n    bmi = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n    has_pets = people[\"pets\"] &gt; 0\n)\np2\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nheight\nhobby\nage\nover 30\npets\nbmi\nhas_pets\n\n\n\n\nalice\n68\n172\nBiking\n38\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\n83\n181\nDancing\n39\nTrue\n0.0\n25.335002\nFalse\n\n\ncharles\n112\n185\nNaN\n31\nTrue\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou can also rename the column name:\n\np2.rename(columns={'bmi':'body_mass_index'})\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nheight\nhobby\nage\nover 30\npets\nbody_mass_index\nhas_pets\n\n\n\n\nalice\n68\n172\nBiking\n38\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\n83\n181\nDancing\n39\nTrue\n0.0\n25.335002\nFalse\n\n\ncharles\n112\n185\nNaN\n31\nTrue\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n3.2.4.6 Querying a DataFrame\nThe query() method lets you filter a DataFrame based on a query expression:\n\npeople.query(\"age &gt; 30 and pets == 0\")\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nheight\nhobby\nage\nover 30\npets\n\n\n\n\nbob\n83\n181\nDancing\n39\nTrue\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n3.2.4.7 Sorting a DataFrame\nYou can sort a DataFrame by calling its sort_index method. By default it sorts the rows by their index label, in ascending order, but let’s reverse the order:\n\npeople.sort_index(ascending=False)\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nheight\nhobby\nage\nover 30\npets\n\n\n\n\ncharles\n112\n185\nNaN\n31\nTrue\n5.0\n\n\nbob\n83\n181\nDancing\n39\nTrue\n0.0\n\n\nalice\n68\n172\nBiking\n38\nTrue\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNote that sort_index returned a sorted copy of the DataFrame. To modify people directly, we can set the inplace argument to True. Also, we can sort the columns instead of the rows by setting axis=1:\n\npeople.sort_index(axis=1, inplace=True)\npeople\n\n\n  \n    \n      \n\n\n\n\n\n\nage\nheight\nhobby\nover 30\npets\nweight\n\n\n\n\nalice\n38\n172\nBiking\nTrue\nNaN\n68\n\n\nbob\n39\n181\nDancing\nTrue\n0.0\n83\n\n\ncharles\n31\n185\nNaN\nTrue\n5.0\n112\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nTo sort the DataFrame by the values instead of the labels, we can use sort_values and specify the column to sort by:\n\npeople.sort_values(by=\"age\", inplace=True)\npeople\n\n\n  \n    \n      \n\n\n\n\n\n\nage\nheight\nhobby\nover 30\npets\nweight\n\n\n\n\ncharles\n31\n185\nNaN\nTrue\n5.0\n112\n\n\nalice\n38\n172\nBiking\nTrue\nNaN\n68\n\n\nbob\n39\n181\nDancing\nTrue\n0.0\n83\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n3.2.4.8 Plotting a DataFrame\nJust like for Series, pandas makes it easy to draw nice graphs based on a DataFrame.\nFor example, it is trivial to create a line plot from a DataFrame’s data by calling its plot method:\n\npeople = people.assign(\n    body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2\n)\npeople.plot(kind = \"line\", x = \"body_mass_index\", y = [\"height\", \"weight\"])\nplt.show()\n\n\n\n\nAgain, there are way too many options to list here: the best option is to scroll through the Visualization page in pandas’ documentation, find the plot you are interested in and look at the example code.\n\n\n3.2.4.9 Operations on DataFrames\nAlthough DataFrames do not try to mimick NumPy arrays, there are a few similarities. Let’s create a DataFrame to demonstrate this:\n\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"bob\",\"charles\",\"darwin\"])\ngrades\n\n\n  \n    \n      \n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n8\n8\n9\n\n\nbob\n10\n9\n9\n\n\ncharles\n4\n8\n2\n\n\ndarwin\n9\n10\n10\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou can apply NumPy mathematical functions on a DataFrame: the function is applied to all values:\n\nnp.sqrt(grades)\n\n\n  \n    \n      \n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n2.828427\n2.828427\n3.000000\n\n\nbob\n3.162278\n3.000000\n3.000000\n\n\ncharles\n2.000000\n2.828427\n1.414214\n\n\ndarwin\n3.000000\n3.162278\n3.162278\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ngrades + 1\n\n\n  \n    \n      \n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n9\n9\n10\n\n\nbob\n11\n10\n10\n\n\ncharles\n5\n9\n3\n\n\ndarwin\n10\n11\n11\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAggregation operations, such as computing the max(), the sum() or the mean() of a DataFrame, apply to each column, and you get back a Series object:\n\ngrades.mean()\n\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\n\n\nMost of these functions take an optional axis parameter which lets you specify along which axis of the DataFrame you want the operation executed. The default is axis=0, meaning that the operation is executed vertically (on each column). You can set axis=1 to execute the operation horizontally (on each row). For example, let’s find out which students had all grades greater than 5:\n\n(grades &gt; 5).all(axis = 1)\n\nalice       True\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nIf you add a Series object to a DataFrame (or execute any other binary operation), Pandas attempts to broadcast the operation to all rows in the DataFrame. This only works if the Series has the same size as the DataFrames rows. For example, let’s subtract the mean of the DataFrame (a Series object) from the DataFrame:\n\ngrades - grades.mean()  # equivalent to: grades - [7.75, 8.75, 7.50]\n\n\n  \n    \n      \n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n0.25\n-0.75\n1.5\n\n\nbob\n2.25\n0.25\n1.5\n\n\ncharles\n-3.75\n-0.75\n-5.5\n\n\ndarwin\n1.25\n1.25\n2.5\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nIf you want to subtract the global mean from every grade, here is one way to do it:\n\ngrades - grades.values.mean() # subtracts the global mean (8.00) from all grades\n\n\n  \n    \n      \n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n0.0\n0.0\n1.0\n\n\nbob\n2.0\n1.0\n1.0\n\n\ncharles\n-4.0\n0.0\n-6.0\n\n\ndarwin\n1.0\n2.0\n2.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe following shows the behavior of nan\n\nbonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]])\nbonus_points = pd.DataFrame(bonus_array, columns=[\"oct\", \"nov\", \"dec\"], index=[\"bob\",\"colin\", \"darwin\", \"charles\"])\nbonus_points\n\n\n  \n    \n      \n\n\n\n\n\n\noct\nnov\ndec\n\n\n\n\nbob\n0.0\nNaN\n2.0\n\n\ncolin\nNaN\n1.0\n0.0\n\n\ndarwin\n0.0\n1.0\n0.0\n\n\ncharles\n3.0\n3.0\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ngrades + bonus_points\n\n\n  \n    \n      \n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nalice\nNaN\nNaN\nNaN\nNaN\n\n\nbob\nNaN\nNaN\n9.0\nNaN\n\n\ncharles\nNaN\n5.0\n11.0\nNaN\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\n\n\ndarwin\nNaN\n11.0\n10.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n3.2.4.10 Handling missing data\nDealing with missing data is a frequent task when working with real life data. Pandas offers a few tools to handle missing data.\nLet’s try to fix the problem above. For example, we can decide that missing data should result in a zero, instead of NaN. We can replace all NaN values by a any value using the fillna() method:\n\n(grades + bonus_points).fillna(0)\n\n\n  \n    \n      \n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nalice\n0.0\n0.0\n0.0\n0.0\n\n\nbob\n0.0\n0.0\n9.0\n0.0\n\n\ncharles\n0.0\n5.0\n11.0\n0.0\n\n\ncolin\n0.0\n0.0\n0.0\n0.0\n\n\ndarwin\n0.0\n11.0\n10.0\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nfinal_grades = grades + bonus_points\nfinal_grades\n\n\n  \n    \n      \n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nalice\nNaN\nNaN\nNaN\nNaN\n\n\nbob\nNaN\nNaN\n9.0\nNaN\n\n\ncharles\nNaN\n5.0\n11.0\nNaN\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\n\n\ndarwin\nNaN\n11.0\n10.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe can call the dropna() method to get rid of rows that are full of NaNs:\n\nfinal_grades_clean = final_grades.dropna(how=\"all\")\nfinal_grades_clean\n\n\n  \n    \n      \n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nbob\nNaN\nNaN\n9.0\nNaN\n\n\ncharles\nNaN\n5.0\n11.0\nNaN\n\n\ndarwin\nNaN\n11.0\n10.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNow let’s remove columns that are full of NaNs by setting the axis argument to 1:\n\nfinal_grades_clean = final_grades_clean.dropna(axis=1, how=\"all\")\nfinal_grades_clean\n\n\n  \n    \n      \n\n\n\n\n\n\nnov\noct\n\n\n\n\nbob\nNaN\n9.0\n\n\ncharles\n5.0\n11.0\n\n\ndarwin\n11.0\n10.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n3.2.4.11 Aggregating with groupby\nSimilar to the SQL language, pandas allows grouping your data into groups to run calculations over each group.\nFirst, let’s add some extra data about each person so we can group them, and let’s go back to the final_grades DataFrame so we can see how NaN values are handled:\n\nfinal_grades[\"hobby\"] = [\"Biking\", \"Dancing\", np.nan, \"Dancing\", \"Biking\"]\nfinal_grades\n\n\n  \n    \n      \n\n\n\n\n\n\ndec\nnov\noct\nsep\nhobby\n\n\n\n\nalice\nNaN\nNaN\nNaN\nNaN\nBiking\n\n\nbob\nNaN\nNaN\n9.0\nNaN\nDancing\n\n\ncharles\nNaN\n5.0\n11.0\nNaN\nNaN\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\nDancing\n\n\ndarwin\nNaN\n11.0\n10.0\nNaN\nBiking\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNow let’s group data in this DataFrame by hobby:\n\ngrouped_grades = final_grades.groupby(\"hobby\")\n\nWe are ready to compute the average grade per hobby:\n\ngrouped_grades.mean()\n\n\n  \n    \n      \n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\nhobby\n\n\n\n\n\n\n\n\nBiking\nNaN\n11.0\n10.0\nNaN\n\n\nDancing\nNaN\nNaN\n9.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThat was easy! Note that the NaN values have simply been skipped when computing the means.\n\n\n3.2.4.12 Pivot tables (Optional)\nPandas supports spreadsheet-like pivot tables that allow quick data summarization.\n\n\n3.2.4.13 Overview functions\nWhen dealing with large DataFrames, it is useful to get a quick overview of its content. Pandas offers a few functions for this. First, let’s create a large DataFrame with a mix of numeric values, missing values and text values. Notice how Jupyter displays only the corners of the DataFrame:\n\nmuch_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26))\nlarge_df = pd.DataFrame(much_data, columns=list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\nlarge_df[large_df % 16 == 0] = np.nan\nlarge_df.insert(3,\"some_text\", \"Blabla\")\nlarge_df\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nsome_text\nD\nE\nF\nG\nH\nI\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\n0\nNaN\n11.0\n44.0\nBlabla\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n...\n11.0\nNaN\n11.0\n44.0\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n\n\n1\n11.0\n22.0\n55.0\nBlabla\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n...\n22.0\n11.0\n22.0\n55.0\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n\n\n2\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n3\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n4\n44.0\n55.0\n88.0\nBlabla\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n...\n55.0\n44.0\n55.0\n88.0\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\nNaN\nNaN\n33.0\nBlabla\n88.0\n165.0\n77.0\n11.0\n154.0\n132.0\n...\nNaN\nNaN\nNaN\n33.0\n88.0\n165.0\n77.0\n11.0\n154.0\n132.0\n\n\n9996\nNaN\n11.0\n44.0\nBlabla\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n...\n11.0\nNaN\n11.0\n44.0\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n\n\n9997\n11.0\n22.0\n55.0\nBlabla\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n...\n22.0\n11.0\n22.0\n55.0\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n\n\n9998\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n9999\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n\n\n\n10000 rows × 27 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe head() method returns the top 5 rows:\n\nlarge_df.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nsome_text\nD\nE\nF\nG\nH\nI\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\n0\nNaN\n11.0\n44.0\nBlabla\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n...\n11.0\nNaN\n11.0\n44.0\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n\n\n1\n11.0\n22.0\n55.0\nBlabla\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n...\n22.0\n11.0\n22.0\n55.0\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n\n\n2\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n3\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n4\n44.0\n55.0\n88.0\nBlabla\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n...\n55.0\n44.0\n55.0\n88.0\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n\n\n\n\n\n5 rows × 27 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nOf course there’s also a tail() function to view the bottom 5 rows. You can pass the number of rows you want:\n\nlarge_df.tail(n=2)\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nsome_text\nD\nE\nF\nG\nH\nI\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\n9998\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n9999\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n\n\n\n2 rows × 27 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe info() method prints out a summary of each columns contents:\n\nlarge_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 27 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   A          8823 non-null   float64\n 1   B          8824 non-null   float64\n 2   C          8824 non-null   float64\n 3   some_text  10000 non-null  object \n 4   D          8824 non-null   float64\n 5   E          8822 non-null   float64\n 6   F          8824 non-null   float64\n 7   G          8824 non-null   float64\n 8   H          8822 non-null   float64\n 9   I          8823 non-null   float64\n 10  J          8823 non-null   float64\n 11  K          8822 non-null   float64\n 12  L          8824 non-null   float64\n 13  M          8824 non-null   float64\n 14  N          8822 non-null   float64\n 15  O          8824 non-null   float64\n 16  P          8824 non-null   float64\n 17  Q          8824 non-null   float64\n 18  R          8823 non-null   float64\n 19  S          8824 non-null   float64\n 20  T          8824 non-null   float64\n 21  U          8824 non-null   float64\n 22  V          8822 non-null   float64\n 23  W          8824 non-null   float64\n 24  X          8824 non-null   float64\n 25  Y          8822 non-null   float64\n 26  Z          8823 non-null   float64\ndtypes: float64(26), object(1)\nmemory usage: 2.1+ MB\n\n\nFinally, the describe() method gives a nice overview of the main aggregated values over each column: * count: number of non-null (not NaN) values * mean: mean of non-null values * std: standard deviation of non-null values * min: minimum of non-null values * 25%, 50%, 75%: 25th, 50th and 75th percentile of non-null values * max: maximum of non-null values\n\nlarge_df.describe()\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\ncount\n8823.000000\n8824.000000\n8824.000000\n8824.000000\n8822.000000\n8824.000000\n8824.000000\n8822.000000\n8823.000000\n8823.000000\n...\n8824.000000\n8823.000000\n8824.000000\n8824.000000\n8824.000000\n8822.000000\n8824.000000\n8824.000000\n8822.000000\n8823.000000\n\n\nmean\n87.977559\n87.972575\n87.987534\n88.012466\n87.983791\n88.007480\n87.977561\n88.000000\n88.022441\n88.022441\n...\n87.972575\n87.977559\n87.972575\n87.987534\n88.012466\n87.983791\n88.007480\n87.977561\n88.000000\n88.022441\n\n\nstd\n47.535911\n47.535523\n47.521679\n47.521679\n47.535001\n47.519371\n47.529755\n47.536879\n47.535911\n47.535911\n...\n47.535523\n47.535911\n47.535523\n47.521679\n47.521679\n47.535001\n47.519371\n47.529755\n47.536879\n47.535911\n\n\nmin\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n...\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n\n\n25%\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n...\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n\n\n50%\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n...\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n\n\n75%\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n...\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n\n\nmax\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n...\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n\n\n\n\n\n8 rows × 26 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n3.2.4.14 Saving & loading\nPandas can save DataFrames to various backends, including file formats such as CSV, Excel, JSON, HTML and HDF5, or to a SQL database. Let’s create a DataFrame to demonstrate this:\n\nmy_df = pd.DataFrame(\n    [[\"Biking\", 68.5, 1985, np.nan], [\"Dancing\", 83.1, 1984, 3]], \n    columns=[\"hobby\",\"weight\",\"birthyear\",\"children\"],\n    index=[\"alice\", \"bob\"]\n)\nmy_df\n\n\n  \n    \n      \n\n\n\n\n\n\nhobby\nweight\nbirthyear\nchildren\n\n\n\n\nalice\nBiking\n68.5\n1985\nNaN\n\n\nbob\nDancing\n83.1\n1984\n3.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nLet’s save it to CSV, HTML and JSON:\n\nmy_df.to_csv(\"my_df.csv\")\nmy_df.to_html(\"my_df.html\")\nmy_df.to_json(\"my_df.json\")\n\nDone! Let’s take a peek at what was saved:\n\nfor filename in (\"my_df.csv\", \"my_df.html\", \"my_df.json\"):\n    print(\"#\", filename)\n    with open(filename, \"rt\") as f:\n        print(f.read())\n        print()\n\n# my_df.csv\n,hobby,weight,birthyear,children\nalice,Biking,68.5,1985,\nbob,Dancing,83.1,1984,3.0\n\n\n# my_df.html\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n  &lt;thead&gt;\n    &lt;tr style=\"text-align: right;\"&gt;\n      &lt;th&gt;&lt;/th&gt;\n      &lt;th&gt;hobby&lt;/th&gt;\n      &lt;th&gt;weight&lt;/th&gt;\n      &lt;th&gt;birthyear&lt;/th&gt;\n      &lt;th&gt;children&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr&gt;\n      &lt;th&gt;alice&lt;/th&gt;\n      &lt;td&gt;Biking&lt;/td&gt;\n      &lt;td&gt;68.5&lt;/td&gt;\n      &lt;td&gt;1985&lt;/td&gt;\n      &lt;td&gt;NaN&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;bob&lt;/th&gt;\n      &lt;td&gt;Dancing&lt;/td&gt;\n      &lt;td&gt;83.1&lt;/td&gt;\n      &lt;td&gt;1984&lt;/td&gt;\n      &lt;td&gt;3.0&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n\n# my_df.json\n{\"hobby\":{\"alice\":\"Biking\",\"bob\":\"Dancing\"},\"weight\":{\"alice\":68.5,\"bob\":83.1},\"birthyear\":{\"alice\":1985,\"bob\":1984},\"children\":{\"alice\":null,\"bob\":3.0}}\n\n\n\nNote that the index is saved as the first column (with no name) in a CSV file, as &lt;th&gt; tags in HTML and as keys in JSON.\nSaving to other formats works very similarly, but some formats require extra libraries to be installed. For example, saving to Excel requires the openpyxl library:\n\ntry:\n    my_df.to_excel(\"my_df.xlsx\", sheet_name='People')\nexcept ImportError as e:\n    print(e)\n\nNow let’s load our CSV file back into a DataFrame:\n\nmy_df_loaded = pd.read_csv(\"my_df.csv\", index_col=0)\nmy_df_loaded\n\n\n  \n    \n      \n\n\n\n\n\n\nhobby\nweight\nbirthyear\nchildren\n\n\n\n\nalice\nBiking\n68.5\n1985\nNaN\n\n\nbob\nDancing\n83.1\n1984\n3.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAs you might guess, there are similar read_json, read_html, read_excel functions as well. We can also read data straight from the Internet. For example, let’s load the top 1,000 U.S. cities from github:\n\nus_cities = None\ntry:\n    csv_url = \"https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv\"\n    us_cities = pd.read_csv(csv_url, index_col=0)\n    us_cities = us_cities.head()\nexcept IOError as e:\n    print(e)\nus_cities\n\n\n  \n    \n      \n\n\n\n\n\n\nState\nPopulation\nlat\nlon\n\n\nCity\n\n\n\n\n\n\n\n\nMarysville\nWashington\n63269\n48.051764\n-122.177082\n\n\nPerris\nCalifornia\n72326\n33.782519\n-117.228648\n\n\nCleveland\nOhio\n390113\n41.499320\n-81.694361\n\n\nWorcester\nMassachusetts\n182544\n42.262593\n-71.802293\n\n\nColumbia\nSouth Carolina\n133358\n34.000710\n-81.034814\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThere are more options available, in particular regarding datetime format. Check out the documentation for more details.\n\n\n3.2.4.15 Combining DataFrames\nOne powerful feature of Pandas is it’s ability to perform SQL-like joins on DataFrames. Various types of joins are supported: inner joins, left/right outer joins and full joins. To illustrate this, let’s start by creating a couple simple DataFrames:\n\ncity_loc = pd.DataFrame(\n    [\n        [\"CA\", \"San Francisco\", 37.781334, -122.416728],\n        [\"NY\", \"New York\", 40.705649, -74.008344],\n        [\"FL\", \"Miami\", 25.791100, -80.320733],\n        [\"OH\", \"Cleveland\", 41.473508, -81.739791],\n        [\"UT\", \"Salt Lake City\", 40.755851, -111.896657]\n    ], columns=[\"state\", \"city\", \"lat\", \"lng\"])\ncity_loc\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\ncity\nlat\nlng\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ncity_pop = pd.DataFrame(\n    [\n        [808976, \"San Francisco\", \"California\"],\n        [8363710, \"New York\", \"New-York\"],\n        [413201, \"Miami\", \"Florida\"],\n        [2242193, \"Houston\", \"Texas\"]\n    ], index=[3,4,5,6], columns=[\"population\", \"city\", \"state\"])\ncity_pop\n\n\n  \n    \n      \n\n\n\n\n\n\npopulation\ncity\nstate\n\n\n\n\n3\n808976\nSan Francisco\nCalifornia\n\n\n4\n8363710\nNew York\nNew-York\n\n\n5\n413201\nMiami\nFlorida\n\n\n6\n2242193\nHouston\nTexas\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNow let’s join these DataFrames using the merge() function:\n\npd.merge(left=city_loc, right=city_pop, on=\"city\")\n\n\n  \n    \n      \n\n\n\n\n\n\nstate_x\ncity\nlat\nlng\npopulation\nstate_y\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n808976\nCalifornia\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n8363710\nNew-York\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n413201\nFlorida\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNote that both DataFrames have a column named state, so in the result they got renamed to state_x and state_y.\nAlso, note that Cleveland, Salt Lake City and Houston were dropped because they don’t exist in both DataFrames. This is the equivalent of a SQL INNER JOIN. If you want a FULL OUTER JOIN, where no city gets dropped and NaN values are added, you must specify how=\"outer\":\n\nall_cities = pd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"outer\")\nall_cities\n\n\n  \n    \n      \n\n\n\n\n\n\nstate_x\ncity\nlat\nlng\npopulation\nstate_y\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n808976.0\nCalifornia\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n8363710.0\nNew-York\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n413201.0\nFlorida\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\nNaN\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\nNaN\nNaN\n\n\n5\nNaN\nHouston\nNaN\nNaN\n2242193.0\nTexas\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nOf course LEFT OUTER JOIN is also available by setting how=\"left\": only the cities present in the left DataFrame end up in the result. Similarly, with how=\"right\" only cities in the right DataFrame appear in the result. For example:\n\npd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"right\")\n\n\n  \n    \n      \n\n\n\n\n\n\nstate_x\ncity\nlat\nlng\npopulation\nstate_y\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n808976\nCalifornia\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n8363710\nNew-York\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n413201\nFlorida\n\n\n3\nNaN\nHouston\nNaN\nNaN\n2242193\nTexas\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n3.2.4.16 Concatenation\nRather than joining DataFrames, we may just want to concatenate them. That’s what concat() is for:\n\nresult_concat = pd.concat([city_loc, city_pop])\nresult_concat\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\nNaN\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\nNaN\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\nNaN\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\nNaN\n\n\n3\nCalifornia\nSan Francisco\nNaN\nNaN\n808976.0\n\n\n4\nNew-York\nNew York\nNaN\nNaN\n8363710.0\n\n\n5\nFlorida\nMiami\nNaN\nNaN\n413201.0\n\n\n6\nTexas\nHouston\nNaN\nNaN\n2242193.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNote that this operation aligned the data horizontally (by columns) but not vertically (by rows). In this example, we end up with multiple rows having the same index (eg. 3). Pandas handles this rather gracefully:\n\nresult_concat.loc[3]\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\n\n\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\n\n\n3\nCalifornia\nSan Francisco\nNaN\nNaN\n808976.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nOr you can tell Pandas to just ignore the index:\n\npd.concat([city_loc, city_pop], ignore_index=True)\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\nNaN\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\nNaN\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\nNaN\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\nNaN\n\n\n5\nCalifornia\nSan Francisco\nNaN\nNaN\n808976.0\n\n\n6\nNew-York\nNew York\nNaN\nNaN\n8363710.0\n\n\n7\nFlorida\nMiami\nNaN\nNaN\n413201.0\n\n\n8\nTexas\nHouston\nNaN\nNaN\n2242193.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNotice that when a column does not exist in a DataFrame, it acts as if it was filled with NaN values. If we set join=\"inner\", then only columns that exist in both DataFrames are returned:\n\npd.concat([city_loc, city_pop], join=\"inner\")\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\ncity\n\n\n\n\n0\nCA\nSan Francisco\n\n\n1\nNY\nNew York\n\n\n2\nFL\nMiami\n\n\n3\nOH\nCleveland\n\n\n4\nUT\nSalt Lake City\n\n\n3\nCalifornia\nSan Francisco\n\n\n4\nNew-York\nNew York\n\n\n5\nFlorida\nMiami\n\n\n6\nTexas\nHouston\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n3.2.4.17 Categories\nIt is quite frequent to have values that represent categories, for example 1 for female and 2 for male, or \"A\" for Good, \"B\" for Average, \"C\" for Bad. These categorical values can be hard to read and cumbersome to handle, but fortunately pandas makes it easy. To illustrate this, let’s take the city_pop DataFrame we created earlier, and add a column that represents a category:\n\ncity_eco = city_pop.copy()\ncity_eco[\"eco_code\"] = [17, 17, 34, 20]\ncity_eco\n\n\n  \n    \n      \n\n\n\n\n\n\npopulation\ncity\nstate\neco_code\n\n\n\n\n3\n808976\nSan Francisco\nCalifornia\n17\n\n\n4\n8363710\nNew York\nNew-York\n17\n\n\n5\n413201\nMiami\nFlorida\n34\n\n\n6\n2242193\nHouston\nTexas\n20\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nRight now the eco_code column is full of apparently meaningless codes. Let’s fix that. First, we will create a new categorical column based on the eco_codes:\n\ncity_eco[\"economy\"] = city_eco[\"eco_code\"].astype('category')\ncity_eco[\"economy\"].cat.categories\n\nInt64Index([17, 20, 34], dtype='int64')\n\n\nNow we can give each category a meaningful name:\n\ncity_eco[\"economy\"].cat.categories = [\"Finance\", \"Energy\", \"Tourism\"]\ncity_eco\n\n\n  \n    \n      \n\n\n\n\n\n\npopulation\ncity\nstate\neco_code\neconomy\n\n\n\n\n3\n808976\nSan Francisco\nCalifornia\n17\nFinance\n\n\n4\n8363710\nNew York\nNew-York\n17\nFinance\n\n\n5\n413201\nMiami\nFlorida\n34\nTourism\n\n\n6\n2242193\nHouston\nTexas\n20\nEnergy\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNote that categorical values are sorted according to their categorical order, not their alphabetical order:\n\ncity_eco.sort_values(by=\"economy\", ascending=False)\n\n\n  \n    \n      \n\n\n\n\n\n\npopulation\ncity\nstate\neco_code\neconomy\n\n\n\n\n5\n413201\nMiami\nFlorida\n34\nTourism\n\n\n6\n2242193\nHouston\nTexas\n20\nEnergy\n\n\n3\n808976\nSan Francisco\nCalifornia\n17\nFinance\n\n\n4\n8363710\nNew York\nNew-York\n17\nFinance"
  },
  {
    "objectID": "03_Relational_Database_and_data_wrangling.html#what-next",
    "href": "03_Relational_Database_and_data_wrangling.html#what-next",
    "title": "3  Relational Database and data wrangling",
    "section": "3.3 What next?",
    "text": "3.3 What next?\nAs you probably noticed by now, pandas is quite a large library with many features. Although we went through the most important features, there is still a lot to discover. Probably the best way to learn more is to get your hands dirty with some real-life data. It is also a good idea to go through pandas’ excellent documentation, in particular the Cookbook.\nYou can also work with BigQuery in Pandas. Check out https://pandas.pydata.org/docs/reference/api/pandas.read_gbq.html and https://pandas-gbq.readthedocs.io/en/latest/ for more details."
  },
  {
    "objectID": "03_Relational_Database_and_data_wrangling.html#references",
    "href": "03_Relational_Database_and_data_wrangling.html#references",
    "title": "3  Relational Database and data wrangling",
    "section": "3.4 References",
    "text": "3.4 References\n\nhttps://www.kaggle.com/learn/\nhttps://github.com/ageron/handson-ml3\nhttps://github.com/ageron/handson-ml3"
  },
  {
    "objectID": "04_Clean_feature_engineering.html",
    "href": "04_Clean_feature_engineering.html",
    "title": "4  Data cleaning and feature engineering",
    "section": "",
    "text": "5 References"
  },
  {
    "objectID": "04_Clean_feature_engineering.html#setup",
    "href": "04_Clean_feature_engineering.html#setup",
    "title": "4  Data cleaning and feature engineering",
    "section": "4.1 Setup",
    "text": "4.1 Setup\n\n!pip install fancyimpute -q\n!pip install thefuzz -q\n!pip install --upgrade xlrd -q\n!pip install category_encoders -q\n!pip install cleanlab -q\n\n\n# Scientific computing\nimport numpy as np\nfrom numpy.random import multivariate_normal\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# Modeling\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\n# Imputing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer # To use IterativeImputer, you need to explicitly import enable_iterative_imputer.\nfrom sklearn.impute import IterativeImputer\nfrom fancyimpute import SoftImpute\n\n# helpful character encoding module\nimport chardet\nfrom thefuzz import fuzz\nfrom thefuzz import process\n\n# Normalization\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import QuantileTransformer\n\n# Encoding\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom category_encoders import MEstimateEncoder\n\n# Fix labels\nimport cleanlab\nfrom cleanlab.classification import CleanLearning\nfrom cleanlab.benchmarking import noise_generation\n\n\n## Make synthetic dataset for cleanlab\n\nSEED = 0\n\ndef make_data(\n    means=[[3, 2], [7, 7], [0, 8], [0, 10]],\n    covs=[\n        [[5, -1.5], [-1.5, 1]],\n        [[1, 0.5], [0.5, 4]],\n        [[5, 1], [1, 5]],\n        [[3, 1], [1, 1]],\n    ],\n    sizes=[100, 50, 50, 50],\n    avg_trace=0.8,\n    seed=SEED,  # set to None for non-reproducible randomness\n):\n    np.random.seed(seed=SEED)\n\n    K = len(means)  # number of classes\n    data = []\n    labels = []\n    test_data = []\n    test_labels = []\n\n    for idx in range(K):\n        data.append(\n            np.random.multivariate_normal(\n                mean=means[idx], cov=covs[idx], size=sizes[idx]\n            )\n        )\n        test_data.append(\n            np.random.multivariate_normal(\n                mean=means[idx], cov=covs[idx], size=sizes[idx]\n            )\n        )\n        labels.append(np.array([idx for i in range(sizes[idx])]))\n        test_labels.append(np.array([idx for i in range(sizes[idx])]))\n    X_train = np.vstack(data)\n    y_train = np.hstack(labels)\n    X_test = np.vstack(test_data)\n    y_test = np.hstack(test_labels)\n\n    # Compute p(y=k) the prior distribution over true labels.\n    py_true = np.bincount(y_train) / float(len(y_train))\n\n    noise_matrix_true = noise_generation.generate_noise_matrix_from_trace(\n        K,\n        trace=avg_trace * K,\n        py=py_true,\n        valid_noise_matrix=True,\n        seed=SEED,\n    )\n\n    # Generate our noisy labels using the noise_marix.\n    s = noise_generation.generate_noisy_labels(y_train, noise_matrix_true)\n    s_test = noise_generation.generate_noisy_labels(y_test, noise_matrix_true)\n    ps = np.bincount(s) / float(len(s))  # Prior distribution over noisy labels\n\n    return {\n        \"data\": X_train,\n        \"true_labels\": y_train,  # You never get to see these perfect labels.\n        \"labels\": s,  # Instead, you have these labels, which have some errors.\n        \"test_data\": X_test,\n        \"test_labels\": y_test,  # Perfect labels used for \"true\" measure of model's performance during deployment.\n        \"noisy_test_labels\": s_test,  # With IID train/test split, you'd have these labels, which also have some errors.\n        \"ps\": ps,\n        \"py_true\": py_true,\n        \"noise_matrix_true\": noise_matrix_true,\n        \"class_names\": [\"purple\", \"blue\", \"seafoam green\", \"yellow\"],\n    }\n\n\n# Display dataset visually using matplotlib\ndef plot_data(data, circles, title, alpha=1.0):\n    plt.figure(figsize=(14, 5))\n    plt.scatter(data[:, 0], data[:, 1], c=labels, s=60)\n    for i in circles:\n        plt.plot(\n            data[i][0],\n            data[i][1],\n            \"o\",\n            markerfacecolor=\"none\",\n            markeredgecolor=\"red\",\n            markersize=14,\n            markeredgewidth=2.5,\n            alpha=alpha\n        )\n    _ = plt.title(title, fontsize=25)"
  },
  {
    "objectID": "04_Clean_feature_engineering.html#exploratory-data-analysis",
    "href": "04_Clean_feature_engineering.html#exploratory-data-analysis",
    "title": "4  Data cleaning and feature engineering",
    "section": "4.2 Exploratory Data Analysis",
    "text": "4.2 Exploratory Data Analysis\nYou can checkout some of useful EDA tools pandas-profiling, dataprep, lux or dtale"
  },
  {
    "objectID": "04_Clean_feature_engineering.html#handling-missing-value",
    "href": "04_Clean_feature_engineering.html#handling-missing-value",
    "title": "4  Data cleaning and feature engineering",
    "section": "4.3 Handling missing value",
    "text": "4.3 Handling missing value\nIn this section, you’ll learn why you’ve run into the data cleaning problems and, more importantly, how to fix them! Specifically, you’ll learn how to tackle some of the most common data cleaning problems so you can get to actually analyzing your data faster.\n\n4.3.1 Take a first look at the data\nFor demonstration, we’ll use a dataset of events that occured in American Football games. You’ll apply your new skills to a dataset of building permits issued in San Francisco. The dataset that we will use was made available by Kaggle. You can download the original dataset from https://www.kaggle.com/datasets/maxhorowitz/nflplaybyplay2009to2016.\nBut you can also use Kaggle API. First, you need to create a Kaggle API key and download it to your local machine. Just navigate to the Kaggle website in a web browser, log in, and go to the My Account page. In your account settings, you’ll find an API section. Clicking the Create New API Token button will generate a kaggle.json key file and will download it to your machine.\n\n\n\n\n# Upload the API’s key JSON file to your Colab\n# session by running the following code in a notebook cell:\nfrom google.colab import files\nfiles.upload()\n\n\n!mkdir ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n\n\nIn some dataset, when the first time you try to download the data, you may get a “403 Forbidden” error. That’s because you need to accept the terms associated with the dataset before you download it and click the “I Understand and Accept button”. You only need to do this once.\n\n\n!kaggle datasets download -d maxhorowitz/nflplaybyplay2009to2016\n\n\n!unzip -qq nflplaybyplay2009to2016\n\n\n# read in all our data\nnfl_data = pd.read_csv(\"NFL Play by Play 2009-2017 (v4).csv\")\n\n# set seed for reproducibility\nnp.random.seed(0) \n\nThe first thing to do when you get a new dataset is take a look at some of it. This lets you see that it all read in correctly and gives an idea of what’s going on with the data. In this case, let’s see if there are any missing values, which will be reprsented with NaN or None.\n\n# look at the first five rows of the nfl_data file. \n# I can see a handful of missing data already!\nnfl_data.head()\n\n\nnfl_data.shape\n\n(407688, 102)\n\n\n\n\n4.3.2 How many missing data points do we have?\nOk, now we know that we do have some missing values. Let’s see how many we have in each column.\n\n# get the number of missing data points per column\nmissing_values_count = nfl_data.isnull().sum()\n\n# look at the # of missing points in the first ten columns\nmissing_values_count[0:10]\n\nDate                0\nGameID              0\nDrive               0\nqtr                 0\ndown            61154\ntime              224\nTimeUnder           0\nTimeSecs          224\nPlayTimeDiff      444\nSideofField       528\ndtype: int64\n\n\n\n# how many total missing values do we have?\ntotal_cells = np.product(nfl_data.shape)\ntotal_missing = missing_values_count.sum()\n\n# percent of data that is missing\npercent_missing = (total_missing/total_cells) * 100\nprint(percent_missing)\n\n24.87214126835169\n\n\nAlmost a quarter of the cells in this dataset are empty! In the next step, we’re going to take a closer look at some of the columns with missing values and try to figure out what might be going on with them.\nLooking at the number of missing values in the nfl_data dataframe, we notice that the column TimesSecs has missing values in it. By looking at the documentation, we can see that this column has information on the number of seconds left in the game when the play was made. This means that these values are probably missing because they were not recorded, rather than because they don’t exist. So, it would make sense for us to try and guess what they should be rather than just leaving them as NA’s.\nOn the other hand, there are other fields, like PenalizedTeam that also have lot of missing fields. In this case, though, the field is missing because if there was no penalty then it doesn’t make sense to say which team was penalized. For this column, it would make more sense to either leave it empty or to add a third value like “neither” and use that to replace the NA’s.\nWe’ll cover some “quick and dirty” techniques that can help you with missing values but will probably also end up removing some useful information or adding some noise to your data.\n\n\n4.3.3 Drop missing values\nIf you’re sure you want to drop rows with missing values, pandas does have a handy function, dropna() to help you do this. Let’s try it out on our NFL dataset!\n\n# remove all the rows that contain a missing value results in empty dataset\n# This is because every row in our dataset had at least one missing value. \n# We might have better luck removing all the columns that have at least one missing value instead.\nnfl_data.dropna()\n\n\n  \n    \n      \n\n\n\n\n\n\nDate\nGameID\nDrive\nqtr\ndown\ntime\nTimeUnder\nTimeSecs\nPlayTimeDiff\nSideofField\n...\nyacEPA\nHome_WP_pre\nAway_WP_pre\nHome_WP_post\nAway_WP_post\nWin_Prob\nWPA\nairWPA\nyacWPA\nSeason\n\n\n\n\n\n\n\n0 rows × 102 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# remove all columns with at least one missing value\ncolumns_with_na_dropped = nfl_data.dropna(axis=1)\ncolumns_with_na_dropped.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nDate\nGameID\nDrive\nqtr\nTimeUnder\nydstogo\nydsnet\nPlayAttempted\nYards.Gained\nsp\n...\nTimeout_Indicator\nTimeout_Team\nposteam_timeouts_pre\nHomeTimeouts_Remaining_Pre\nAwayTimeouts_Remaining_Pre\nHomeTimeouts_Remaining_Post\nAwayTimeouts_Remaining_Post\nExPoint_Prob\nTwoPoint_Prob\nSeason\n\n\n\n\n0\n2009-09-10\n2009091000\n1\n1\n15\n0\n0\n1\n39\n0\n...\n0\nNone\n3\n3\n3\n3\n3\n0.0\n0.0\n2009\n\n\n1\n2009-09-10\n2009091000\n1\n1\n15\n10\n5\n1\n5\n0\n...\n0\nNone\n3\n3\n3\n3\n3\n0.0\n0.0\n2009\n\n\n2\n2009-09-10\n2009091000\n1\n1\n15\n5\n2\n1\n-3\n0\n...\n0\nNone\n3\n3\n3\n3\n3\n0.0\n0.0\n2009\n\n\n3\n2009-09-10\n2009091000\n1\n1\n14\n8\n2\n1\n0\n0\n...\n0\nNone\n3\n3\n3\n3\n3\n0.0\n0.0\n2009\n\n\n4\n2009-09-10\n2009091000\n1\n1\n14\n8\n2\n1\n0\n0\n...\n0\nNone\n3\n3\n3\n3\n3\n0.0\n0.0\n2009\n\n\n\n\n\n5 rows × 41 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# just how much data did we lose?\nprint(\"Columns in original dataset: %d \\n\" % nfl_data.shape[1])\nprint(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])\n\nColumns in original dataset: 102 \n\nColumns with na's dropped: 41\n\n\nNotice that this can drop a lot of data that you might want to keep, particularly in smaller datasets. What if you just want to drop rows or columns that contain several or even just all null values? You specify those setting in dropna with the how and thresh parameters.\nBy default, how='any'. You could alternatively specify how='all' so as to drop only rows or columns that contain all null values. The threshparameter gives you finer-grained control: you set the number of non-null values that a row or column needs to have in order to be kept.\n\ndf1 = pd.DataFrame([[ 1, np.nan, 7], \n                    [ 2,  5,  8], \n                    [ np.nan, 6, 9]])\ndf1[3] = np.nan\ndf1\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.0\nNaN\n7\nNaN\n\n\n1\n2.0\n5.0\n8\nNaN\n\n\n2\nNaN\n6.0\n9\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# the `thresh` parameter specifies the minimum number of non-null values required in a row or column for it to be retained.\ndf1.dropna(thresh=3)\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n1\n2.0\n5.0\n8\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nHere, the first and last row have been dropped, because they contain only two non-null values.\n\n\n4.3.4 Filling in missing values automatically\nDepending on your dataset, it can sometimes make more sense to fill null values with valid ones rather than drop them. Pandas provides fillna(), which returns a copy of the Series or DataFrame with the missing values replaced with one of your choosing.\n\n# You can fill all of the null entries with a single value, such as -9999:\ndf1.fillna(-9999)\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.0\n-9999.0\n7\n-9999.0\n\n\n1\n2.0\n5.0\n8\n-9999.0\n\n\n2\n-9999.0\n6.0\n9\n-9999.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe could also replace missing values with whatever value comes directly after/before it in the same column. (This makes a lot of sense for datasets where the observations have some sort of logical order to them.)\nYou can forward-fill null values, which is to use the last valid value to fill a null:\n\ndf1.fillna(method='ffill', axis=0)\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.0\nNaN\n7\nNaN\n\n\n1\n2.0\n5.0\n8\nNaN\n\n\n2\n2.0\n6.0\n9\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nBackward-fill to propagate the next valid value backward to fill a null:\n\ndf1.fillna(method='bfill', axis=0)\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.0\n5.0\n7\nNaN\n\n\n1\n2.0\n5.0\n8\nNaN\n\n\n2\nNaN\n6.0\n9\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNotice that when a previous/next value is not available for forward/backward-filling, the null value remains.\n\n\n4.3.5 Imputation of missing value\n\n4.3.5.1 Univariate feature imputation\nThe SimpleImputer class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.\n\ndf2 = pd.DataFrame([[1, 2], [np.nan, 3], [7, 6]])\ndf2\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1.0\n2\n\n\n1\nNaN\n3\n\n\n2\n7.0\n6\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean value of the columns that contain the missing values:\n\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp.fit_transform(df2)\n\narray([[1., 2.],\n       [4., 3.],\n       [7., 6.]])\n\n\nThe SimpleImputer class also supports categorical data represented as string values or pandas categoricals when using the most_frequent or constant strategy:\n\ndf2 = pd.DataFrame([[\"a\", \"x\"],\n                    [np.nan, \"y\"],\n                    [\"a\", np.nan],\n                    [\"b\", \"y\"]], \n                   dtype=\"category\")\ndf2\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n\n\n\n\n0\na\nx\n\n\n1\nNaN\ny\n\n\n2\na\nNaN\n\n\n3\nb\ny\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nimp = SimpleImputer(strategy=\"most_frequent\")\nprint(imp.fit_transform(df2))\n\n[['a' 'x']\n ['a' 'y']\n ['a' 'y']\n ['b' 'y']]\n\n\n\n\n4.3.5.2 Multivariate feature imputation\nA more sophisticated approach is to use the IterativeImputer class, which models each feature with missing values as a function of other features, and uses that estimate for imputation.\n\nimp = IterativeImputer(max_iter=10, random_state=0)\n# the model learns that the second feature is double of the first\nimp.fit_transform([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n\narray([[ 1.        ,  2.        ],\n       [ 3.        ,  6.        ],\n       [ 4.        ,  8.        ],\n       [ 1.50004509,  3.        ],\n       [ 7.        , 14.00004135]])\n\n\n\n # You can also use other regressor as well (default is BayesianRidge())\n est = ExtraTreesRegressor(n_estimators=10, random_state=0)\n imp = IterativeImputer(max_iter=10, random_state=0, estimator=est)\n imp.fit_transform([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n\narray([[1. , 2. ],\n       [3. , 6. ],\n       [4. , 8. ],\n       [1.6, 3. ],\n       [7. , 8. ]])\n\n\nThe KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors. Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor.\nThe following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean feature value of the two nearest neighbors of samples with missing values:\n\nX = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\nimputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\nimputer.fit_transform(X)\n\narray([[1. , 2. , 4. ],\n       [3. , 4. , 3. ],\n       [5.5, 6. , 5. ],\n       [8. , 8. , 7. ]])\n\n\nif you wishes to apply matrix completion to your data, you can use functions from fancyimpute\n\nSoftImpute().fit_transform(X)\n\n[SoftImpute] Max Singular Value of X_init = 16.044670\n[SoftImpute] Iter 1: observed MAE=0.129498 rank=3\n[SoftImpute] Iter 2: observed MAE=0.129346 rank=3\n[SoftImpute] Iter 3: observed MAE=0.129795 rank=3\n[SoftImpute] Iter 4: observed MAE=0.131896 rank=3\n[SoftImpute] Iter 5: observed MAE=0.134509 rank=3\n[SoftImpute] Iter 6: observed MAE=0.137663 rank=3\n[SoftImpute] Iter 7: observed MAE=0.141068 rank=3\n[SoftImpute] Iter 8: observed MAE=0.143794 rank=3\n[SoftImpute] Iter 9: observed MAE=0.145304 rank=3\n[SoftImpute] Iter 10: observed MAE=0.145850 rank=3\n[SoftImpute] Iter 11: observed MAE=0.145866 rank=3\n[SoftImpute] Iter 12: observed MAE=0.145914 rank=3\n[SoftImpute] Iter 13: observed MAE=0.146068 rank=3\n[SoftImpute] Iter 14: observed MAE=0.146126 rank=2\n[SoftImpute] Iter 15: observed MAE=0.146125 rank=2\n[SoftImpute] Iter 16: observed MAE=0.146132 rank=2\n[SoftImpute] Iter 17: observed MAE=0.146126 rank=2\n[SoftImpute] Iter 18: observed MAE=0.146092 rank=2\n[SoftImpute] Iter 19: observed MAE=0.146022 rank=2\n[SoftImpute] Iter 20: observed MAE=0.145907 rank=2\n[SoftImpute] Iter 21: observed MAE=0.145740 rank=2\n[SoftImpute] Iter 22: observed MAE=0.145510 rank=2\n[SoftImpute] Iter 23: observed MAE=0.145209 rank=2\n[SoftImpute] Iter 24: observed MAE=0.144824 rank=2\n[SoftImpute] Iter 25: observed MAE=0.144345 rank=2\n[SoftImpute] Iter 26: observed MAE=0.143761 rank=2\n[SoftImpute] Iter 27: observed MAE=0.143059 rank=2\n[SoftImpute] Iter 28: observed MAE=0.142233 rank=2\n[SoftImpute] Iter 29: observed MAE=0.141275 rank=2\n[SoftImpute] Iter 30: observed MAE=0.140185 rank=2\n[SoftImpute] Iter 31: observed MAE=0.138969 rank=2\n[SoftImpute] Iter 32: observed MAE=0.137638 rank=2\n[SoftImpute] Iter 33: observed MAE=0.136213 rank=2\n[SoftImpute] Iter 34: observed MAE=0.134720 rank=2\n[SoftImpute] Iter 35: observed MAE=0.133194 rank=2\n[SoftImpute] Iter 36: observed MAE=0.131669 rank=2\n[SoftImpute] Iter 37: observed MAE=0.130180 rank=2\n[SoftImpute] Iter 38: observed MAE=0.129421 rank=2\n[SoftImpute] Iter 39: observed MAE=0.128890 rank=2\n[SoftImpute] Iter 40: observed MAE=0.128397 rank=2\n[SoftImpute] Iter 41: observed MAE=0.127946 rank=2\n[SoftImpute] Iter 42: observed MAE=0.127542 rank=2\n[SoftImpute] Iter 43: observed MAE=0.127185 rank=2\n[SoftImpute] Iter 44: observed MAE=0.126874 rank=2\n[SoftImpute] Iter 45: observed MAE=0.126605 rank=2\n[SoftImpute] Iter 46: observed MAE=0.126375 rank=2\n[SoftImpute] Iter 47: observed MAE=0.126180 rank=2\n[SoftImpute] Iter 48: observed MAE=0.126016 rank=2\n[SoftImpute] Iter 49: observed MAE=0.125878 rank=2\n[SoftImpute] Iter 50: observed MAE=0.125763 rank=2\n[SoftImpute] Iter 51: observed MAE=0.125668 rank=2\n[SoftImpute] Stopped after iteration 51 for lambda=0.320893\n\n\narray([[1.        , 2.        , 1.29115131],\n       [3.        , 4.        , 3.        ],\n       [5.10495139, 6.        , 5.        ],\n       [8.        , 8.        , 7.        ]])\n\n\nFor more information, please refer to https://github.com/iskandr/fancyimpute or https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute."
  },
  {
    "objectID": "04_Clean_feature_engineering.html#other-data-cleaning-problem",
    "href": "04_Clean_feature_engineering.html#other-data-cleaning-problem",
    "title": "4  Data cleaning and feature engineering",
    "section": "4.4 Other data cleaning problem",
    "text": "4.4 Other data cleaning problem\n\n4.4.1 Duplicate data entry\nIn addition to missing data, you will often encounter duplicated data in real-world datasets. Fortunately, pandas provides an easy means of detecting and removing duplicate entries.\nYou can easily spot duplicate values using the duplicated() method in pandas, which returns a Boolean mask indicating whether an entry in a DataFrame is a duplicate of an earlier one. Let’s create another example DataFrame to see this in action.\n\ndf3 = pd.DataFrame({'letters': ['A','B'] * 2 + ['B'],\n                    'numbers': [1, 2, 1, 3, 3]})\ndf3\n\n\n  \n    \n      \n\n\n\n\n\n\nletters\nnumbers\n\n\n\n\n0\nA\n1\n\n\n1\nB\n2\n\n\n2\nA\n1\n\n\n3\nB\n3\n\n\n4\nB\n3\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf3.duplicated()\n\n0    False\n1    False\n2     True\n3    False\n4     True\ndtype: bool\n\n\ndrop_duplicates() will simply returns a copy of the data for which all of the duplicated values are False:\n\ndf3.drop_duplicates()\n\n\n  \n    \n      \n\n\n\n\n\n\nletters\nnumbers\n\n\n\n\n0\nA\n1\n\n\n1\nB\n2\n\n\n3\nB\n3\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nBoth duplicated() and drop_duplicates() default to consider all columns but you can specify that they examine only a subset of columns in your DataFrame:\n\ndf3.drop_duplicates(['letters'])\n\n\n  \n    \n      \n\n\n\n\n\n\nletters\nnumbers\n\n\n\n\n0\nA\n1\n\n\n1\nB\n2\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4.4.2 Inconsistent data entry\nWe will use the dataset that is modified from https://www.kaggle.com/datasets/zusmani/pakistanintellectualcapitalcs.\n\n!kaggle datasets download -d alexisbcook/pakistan-intellectual-capital\n!unzip -qq pakistan-intellectual-capital.zip\n\nDownloading pakistan-intellectual-capital.zip to /content\n  0% 0.00/47.8k [00:00&lt;?, ?B/s]\n100% 47.8k/47.8k [00:00&lt;00:00, 25.0MB/s]\n\n\n\n# read in all our data\nprofessors = pd.read_csv(\"pakistan_intellectual_capital.csv\")\n\n\nprofessors.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nUnnamed: 0\nS#\nTeacher Name\nUniversity Currently Teaching\nDepartment\nProvince University Located\nDesignation\nTerminal Degree\nGraduated from\nCountry\nYear\nArea of Specialization/Research Interests\nOther Information\n\n\n\n\n0\n2\n3\nDr. Abdul Basit\nUniversity of Balochistan\nComputer Science & IT\nBalochistan\nAssistant Professor\nPhD\nAsian Institute of Technology\nThailand\nNaN\nSoftware Engineering & DBMS\nNaN\n\n\n1\n4\n5\nDr. Waheed Noor\nUniversity of Balochistan\nComputer Science & IT\nBalochistan\nAssistant Professor\nPhD\nAsian Institute of Technology\nThailand\nNaN\nDBMS\nNaN\n\n\n2\n5\n6\nDr. Junaid Baber\nUniversity of Balochistan\nComputer Science & IT\nBalochistan\nAssistant Professor\nPhD\nAsian Institute of Technology\nThailand\nNaN\nInformation processing, Multimedia mining\nNaN\n\n\n3\n6\n7\nDr. Maheen Bakhtyar\nUniversity of Balochistan\nComputer Science & IT\nBalochistan\nAssistant Professor\nPhD\nAsian Institute of Technology\nThailand\nNaN\nNLP, Information Retrieval, Question Answering...\nNaN\n\n\n4\n24\n25\nSamina Azim\nSardar Bahadur Khan Women's University\nComputer Science\nBalochistan\nLecturer\nBS\nBalochistan University of Information Technolo...\nPakistan\n2005.0\nVLSI Electronics DLD Database\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSay we’re interested in cleaning up the Country column to make sure there’s no data entry inconsistencies in it. We could go through and check each row by hand, of course, and hand-correct inconsistencies when we find them. There’s a more efficient way to do this, though!\n\n# get all the unique values in the 'Country' column\ncountries = professors['Country'].unique()\n\n# sort them and then take a closer look\ncountries.sort()\ncountries\n\narray([' Germany', ' New Zealand', ' Sweden', ' USA', 'Australia',\n       'Austria', 'Canada', 'China', 'Finland', 'France', 'Greece',\n       'HongKong', 'Ireland', 'Italy', 'Japan', 'Macau', 'Malaysia',\n       'Mauritius', 'Netherland', 'New Zealand', 'Norway', 'Pakistan',\n       'Portugal', 'Russian Federation', 'Saudi Arabia', 'Scotland',\n       'Singapore', 'South Korea', 'SouthKorea', 'Spain', 'Sweden',\n       'Thailand', 'Turkey', 'UK', 'USA', 'USofA', 'Urbana', 'germany'],\n      dtype=object)\n\n\nJust looking at this, we can see some problems due to inconsistent data entry: ’ Germany’, and ‘germany’, for example, or ’ New Zealand’ (start wirh whitespace) and ‘New Zealand’.\nThe first thing we are going to do is make everything lower case (we can change it back at the end if we like) and remove any white spaces at the beginning and end of cells. Inconsistencies in capitalizations and trailing white spaces are very common in text data and you can fix a good 80% of your text data entry inconsistencies by doing this.\n\n# convert to lower case\nprofessors['Country'] = professors['Country'].str.lower()\n# remove trailing white spaces\nprofessors['Country'] = professors['Country'].str.strip()\n\nNote that .str() provide vectorized method for columns. See https://realpython.com/python-data-cleaning-numpy-pandas/#tidying-up-fields-in-the-data for more details.\n\n4.4.2.1 Use fuzzy matching to correct inconsistent data entry\nAlright, let’s take another look at the Country column and see if there’s any more data cleaning we need to do\n\n# get all the unique values in the 'Country' column\ncountries = professors['Country'].unique()\n\n# sort them and then take a closer look\ncountries.sort()\ncountries\n\narray(['australia', 'austria', 'canada', 'china', 'finland', 'france',\n       'germany', 'greece', 'hongkong', 'ireland', 'italy', 'japan',\n       'macau', 'malaysia', 'mauritius', 'netherland', 'new zealand',\n       'norway', 'pakistan', 'portugal', 'russian federation',\n       'saudi arabia', 'scotland', 'singapore', 'south korea',\n       'southkorea', 'spain', 'sweden', 'thailand', 'turkey', 'uk',\n       'urbana', 'usa', 'usofa'], dtype=object)\n\n\nIt does look like there is another inconsistency: ‘southkorea’ and ‘south korea’ should be the same. We’re going to use the thefuzz package to help identify which strings are closest to each other. This dataset is small enough that we could probably correct errors by hand, but that approach doesn’t scale well. (Would you want to correct a thousand errors by hand? What about ten thousand? Automating things as early as possible is generally a good idea!)\nthefuzz returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. Here, we’re going to get the ten strings from our list of cities that have the closest distance to “south korea”\n\n# get the top 10 closest matches to \"south korea\"\nmatches = process.extract(\"south korea\", countries, limit=10)\n\n# take a look at them\nmatches\n\n[('south korea', 100),\n ('southkorea', 95),\n ('ireland', 50),\n ('norway', 50),\n ('uk', 45),\n ('austria', 44),\n ('saudi arabia', 43),\n ('scotland', 42),\n ('australia', 40),\n ('france', 40)]\n\n\nWe can see that two of the items in the cities are very close to “south korea”: “south korea” and “southkorea”. Let’s replace all rows in our “Country” column that have a score &gt; 90 with “south korea”.\nTo do this, we are going to write a function.\n\n# function to replace rows in the provided column of the provided dataframe\n# that match the provided string above the provided ratio with the provided string\ndef replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n    # get a list of unique strings\n    strings = df[column].unique()\n    \n    # get the top 10 closest matches to our input string\n    matches = process.extract(string_to_match, strings, limit=10)\n\n    # only get matches with a ratio &gt; 90\n    close_matches = [matches[0] for matches in matches if matches[1] &gt;= min_ratio]\n\n    # get the rows of all the close matches in our dataframe\n    rows_with_matches = df[column].isin(close_matches)\n\n    # replace all rows with close matches with the input matches \n    df.loc[rows_with_matches, column] = string_to_match\n    \n    # let us know the function's done\n    print(\"All done!\")\n\nNow that we have a function, we can put it to the test!\n\n# use the function we just wrote to replace close matches to \"south korea\" with \"south korea\"\nreplace_matches_in_column(df=professors, column='Country', string_to_match=\"south korea\")\n\nAll done!\n\n\nAnd now let’s check the unique values in our “Country” column again and make sure we’ve tidied up “south korea” correctly.\n\n# get all the unique values in the 'Country' column\ncountries = professors['Country'].unique()\n\n# sort them alphabetically and then take a closer look\ncountries.sort()\ncountries\n\narray(['australia', 'austria', 'canada', 'china', 'finland', 'france',\n       'germany', 'greece', 'hongkong', 'ireland', 'italy', 'japan',\n       'macau', 'malaysia', 'mauritius', 'netherland', 'new zealand',\n       'norway', 'pakistan', 'portugal', 'russian federation',\n       'saudi arabia', 'scotland', 'singapore', 'south korea', 'spain',\n       'sweden', 'thailand', 'turkey', 'uk', 'urbana', 'usa', 'usofa'],\n      dtype=object)\n\n\nNow we only have “south korea” in our dataframe and we didn’t have to change anything by hand.\n\n\n\n4.4.3 Character encoding\nThere are two main data types you’ll encounter when working with text in Python 3. One is is the string, which is what text is by default.\n\n# start with a string\nbefore = \"This is the euro symbol: €\"\n\n# check to see what datatype it is\ntype(before)\n\nstr\n\n\nThe other data is the bytes data type, which is a sequence of integers. You can convert a string into bytes by specifying which encoding it’s in:\n\n# encode it to a different encoding, replacing characters that raise errors\nafter = before.encode(\"utf-8\", errors=\"replace\")\n\n# check the type\ntype(after)\n\nbytes\n\n\nIf you look at a bytes object, you’ll see that it has a b in front of it, and then maybe some text after. That’s because bytes are printed out as if they were characters encoded in UTF-8. Here you can see that our euro symbol has been replaced with some mojibake that looks like “” when it’s printed as if it were an UTF-8 string\n\n# take a look at what the bytes look like\nafter\n\nb'This is the euro symbol: \\xe2\\x82\\xac'\n\n\nWhen we convert our bytes back to a string with the correct encoding, we can see that our text is all there correctly, which is great! :)\n\n# convert it back to utf-8\nprint(after.decode(\"utf-8\"))\n\nThis is the euro symbol: €\n\n\nHowever, when we try to use a different encoding to map our bytes into a string, we get an error. This is because the encoding we’re trying to use doesn’t know what to do with the bytes we’re trying to pass it. You need to tell Python the encoding that the byte string is actually supposed to be in\n\n# try to decode our bytes with the ascii encoding\nprint(after.decode(\"ascii\"))\n\nUnicodeDecodeError: ignored\n\n\nThe best time to convert non UTF-8 input into UTF-8 is when you read in files, which we’ll talk about next.\n\n\n4.4.4 Reading in files with encoding problems\nMost files you’ll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won’t run into problems. However, sometimes you’ll get an error like this:\n\n!kaggle datasets download -d kemical/kickstarter-projects\n!unzip -qq kickstarter-projects.zip\n\nDownloading kickstarter-projects.zip to /content\n 68% 25.0M/36.8M [00:00&lt;00:00, 128MB/s] \n100% 36.8M/36.8M [00:00&lt;00:00, 142MB/s]\n\n\n\n# try to read in a file not in UTF-8\nkickstarter_2016 = pd.read_csv(\"ks-projects-201612.csv\")\n\nUnicodeDecodeError: ignored\n\n\nNotice that we get the same UnicodeDecodeError we got when we tried to decode UTF-8 bytes! This tells us that this file isn’t actually UTF-8. We don’t know what encoding it actually is though. One way to figure it out is to try and test a bunch of different character encodings and see if any of them work. A better way, though, is to use the chardet module to try and automatically guess what the right encoding is. It’s not 100% guaranteed to be right, but it’s usually faster than just trying to guess.\nWe are going to just look at the first ten thousand bytes of this file. This is usually enough for a good guess about what the encoding is and is much faster than trying to look at the whole file. (Especially with a large file this can be very slow.)\n\n# look at the first ten thousand bytes to guess the character encoding\nwith open(\"ks-projects-201612.csv\", 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(10000))\n\n# check what the character encoding might be\nprint(result)\n\n{'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n\n\nSo chardet is 73% confidence that the right encoding is “Windows-1252”. Let’s see if that’s correct:\n\n# read in the file with the encoding detected by chardet\nkickstarter_2016 = pd.read_csv(\"ks-projects-201612.csv\", encoding='Windows-1252')\n\n# look at the first few lines\nkickstarter_2016.head()\n\n/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (13,14,15) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\nname\ncategory\nmain_category\ncurrency\ndeadline\ngoal\nlaunched\npledged\nstate\nbackers\ncountry\nusd pledged\nUnnamed: 13\nUnnamed: 14\nUnnamed: 15\nUnnamed: 16\n\n\n\n\n0\n1000002330\nThe Songs of Adelaide & Abullah\nPoetry\nPublishing\nGBP\n2015-10-09 11:36:00\n1000\n2015-08-11 12:12:28\n0\nfailed\n0\nGB\n0\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1000004038\nWhere is Hank?\nNarrative Film\nFilm & Video\nUSD\n2013-02-26 00:20:50\n45000\n2013-01-12 00:20:50\n220\nfailed\n3\nUS\n220\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1000007540\nToshiCapital Rekordz Needs Help to Complete Album\nMusic\nMusic\nUSD\n2012-04-16 04:24:11\n5000\n2012-03-17 03:24:11\n1\nfailed\n1\nUS\n1\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1000011046\nCommunity Film Project: The Art of Neighborhoo...\nFilm & Video\nFilm & Video\nUSD\n2015-08-29 01:00:00\n19500\n2015-07-04 08:35:03\n1283\ncanceled\n14\nUS\n1283\nNaN\nNaN\nNaN\nNaN\n\n\n4\n1000014025\nMonarch Espresso Bar\nRestaurants\nFood\nUSD\n2016-04-01 13:38:27\n50000\n2016-02-26 13:38:27\n52375\nsuccessful\n224\nUS\n52375\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYep, looks like chardet was right! The file reads in with no problem (although we do get a warning about datatypes) and when we look at the first few rows it seems to be fine.\nWhat if the encoding chardet guesses isn’t right? Since chardet is basically just a fancy guesser, sometimes it will guess the wrong encoding. One thing you can try is looking at more or less of the file and seeing if you get a different result and then try that.\n\n\n4.4.5 Saving your files with UTF-8 encoding\nFinally, once you’ve gone through all the trouble of getting your file into UTF-8, you’ll probably want to keep it that way. The easiest way to do that is to save your files with UTF-8 encoding. The good news is, since UTF-8 is the standard encoding in Python, when you save a file it will be saved as UTF-8 by default:\n\n# save our file (will be saved as UTF-8 by default!)\nkickstarter_2016.to_csv(\"ks-projects-201612-utf8.csv\")\n\n\n# try to read in a file not in UTF-8\nkickstarter_2016 = pd.read_csv(\"ks-projects-201612-utf8.csv\")\nkickstarter_2016.head()\n\n/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (14,15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nUnnamed: 0\nID\nname\ncategory\nmain_category\ncurrency\ndeadline\ngoal\nlaunched\npledged\nstate\nbackers\ncountry\nusd pledged\nUnnamed: 13\nUnnamed: 14\nUnnamed: 15\nUnnamed: 16\n\n\n\n\n0\n0\n1000002330\nThe Songs of Adelaide & Abullah\nPoetry\nPublishing\nGBP\n2015-10-09 11:36:00\n1000\n2015-08-11 12:12:28\n0\nfailed\n0\nGB\n0\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1\n1000004038\nWhere is Hank?\nNarrative Film\nFilm & Video\nUSD\n2013-02-26 00:20:50\n45000\n2013-01-12 00:20:50\n220\nfailed\n3\nUS\n220\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2\n1000007540\nToshiCapital Rekordz Needs Help to Complete Album\nMusic\nMusic\nUSD\n2012-04-16 04:24:11\n5000\n2012-03-17 03:24:11\n1\nfailed\n1\nUS\n1\nNaN\nNaN\nNaN\nNaN\n\n\n3\n3\n1000011046\nCommunity Film Project: The Art of Neighborhoo...\nFilm & Video\nFilm & Video\nUSD\n2015-08-29 01:00:00\n19500\n2015-07-04 08:35:03\n1283\ncanceled\n14\nUS\n1283\nNaN\nNaN\nNaN\nNaN\n\n\n4\n4\n1000014025\nMonarch Espresso Bar\nRestaurants\nFood\nUSD\n2016-04-01 13:38:27\n50000\n2016-02-26 13:38:27\n52375\nsuccessful\n224\nUS\n52375\nNaN\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "04_Clean_feature_engineering.html#scaling-and-normalization",
    "href": "04_Clean_feature_engineering.html#scaling-and-normalization",
    "title": "4  Data cleaning and feature engineering",
    "section": "4.5 Scaling and normalization",
    "text": "4.5 Scaling and normalization\n\n4.5.1 Standardization\nBy scaling your variables, you can help compare different variables on equal footing. The preprocessing module provides the StandardScaler utility class, which is a quick and easy way to perform the following operation on an array-like dataset.\n\n# Uncpmment below if you are using Kaggle\n#!pip install gdown\n\n\n!gdown --fuzzy https://drive.google.com/file/d/1HEcKRMe_bpgmQH3vFlRZVDF_gAzYGETE/view?usp=sharing # use --fuzzy so that it directly accept link from google drive \n!gdown --fuzzy https://drive.google.com/file/d/1GQ1z0-aRPCzMQTcc1ckDgwkiF-GNP_t3/view?usp=sharing\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1HEcKRMe_bpgmQH3vFlRZVDF_gAzYGETE\nTo: /content/train_preprocessed.csv\n100% 1.14M/1.14M [00:00&lt;00:00, 101MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1GQ1z0-aRPCzMQTcc1ckDgwkiF-GNP_t3\nTo: /content/test_preprocessed.csv\n100% 1.12M/1.12M [00:00&lt;00:00, 112MB/s]\n\n\n\ntrain = pd.read_csv('train_preprocessed.csv')\ntrain_x = train.drop(['target'], axis=1)\ntrain_y = train['target']\ntest_x = pd.read_csv('test_preprocessed.csv')\ntrain_x_saved = train_x.copy()\ntest_x_saved = test_x.copy()\n\ndef load_data():\n    train_x, test_x = train_x_saved.copy(), test_x_saved.copy()\n    return train_x, test_x\n\ntrain\n\n\n  \n    \n      \n\n\n\n\n\n\nage\nsex\nheight\nweight\nproduct\namount\nmedical_info_a1\nmedical_info_a2\nmedical_info_a3\nmedical_info_b1\n...\nmedical_keyword_6\nmedical_keyword_7\nmedical_keyword_8\nmedical_keyword_9\nmedical_keyword_10\nyear\nmonth\nday\nyearmonth\ntarget\n\n\n\n\n0\n50\n1\n166.445608\n65.016732\n9\n7000000\n134\n202\n1\n11\n...\n1\n0\n1\n0\n0\n2015\n2\n3\n24182\n0\n\n\n1\n68\n0\n164.334615\n56.544217\n0\n7000000\n438\n263\n3\n14\n...\n0\n1\n1\n0\n0\n2015\n5\n9\n24185\n0\n\n\n2\n77\n1\n167.462917\n54.242267\n2\n6000000\n313\n325\n1\n18\n...\n1\n0\n1\n0\n0\n2016\n2\n13\n24194\n1\n\n\n3\n17\n1\n177.097725\n71.147762\n3\n8000000\n342\n213\n2\n11\n...\n0\n0\n1\n0\n0\n2015\n7\n6\n24187\n0\n\n\n4\n62\n0\n158.165788\n65.240697\n1\n9000000\n327\n102\n0\n14\n...\n0\n1\n1\n1\n0\n2016\n9\n17\n24201\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\n61\n1\n182.729800\n73.393777\n1\n2000000\n189\n232\n7\n17\n...\n0\n0\n1\n1\n0\n2015\n10\n21\n24190\n0\n\n\n9996\n33\n0\n167.701136\n75.006529\n8\n9000\n426\n202\n3\n19\n...\n0\n0\n1\n1\n0\n2015\n5\n28\n24185\n0\n\n\n9997\n44\n0\n145.609998\n47.739397\n8\n1000\n370\n274\n1\n11\n...\n0\n0\n1\n0\n1\n2016\n2\n29\n24194\n0\n\n\n9998\n34\n0\n165.796017\n57.567695\n6\n5000\n291\n105\n1\n13\n...\n1\n1\n1\n1\n0\n2016\n2\n27\n24194\n0\n\n\n9999\n31\n1\n180.301762\n71.425135\n4\n1000000\n288\n454\n4\n13\n...\n1\n0\n1\n0\n0\n2015\n7\n1\n24187\n0\n\n\n\n\n\n10000 rows × 29 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nnum_cols = ['age', 'height', 'weight', 'amount',\n            'medical_info_a1', 'medical_info_a2', 'medical_info_a3', 'medical_info_b1'] # some numerical columns\n\n\ntrain_x, test_x = load_data()\nscaler = StandardScaler()\nscaler.fit(train_x[num_cols])\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScalerStandardScaler()\n\n\nNotice that you should apply the same transfrom on both training and testing dataset.\n\ntrain_x[num_cols] = scaler.transform(train_x[num_cols])\ntest_x[num_cols] = scaler.transform(test_x[num_cols])\n\n\nscaler.mean_,  scaler.scale_\n\n(array([4.20265000e+01, 1.65892951e+02, 6.08570495e+01, 3.84084370e+06,\n        2.99101200e+02, 2.49454700e+02, 1.98780000e+00, 1.44192000e+01]),\n array([2.16749209e+01, 9.40817216e+00, 1.07177883e+01, 3.45926743e+06,\n        1.04491031e+02, 1.03710381e+02, 1.41733947e+00, 2.87131875e+00]))\n\n\nScaled data has zero mean and unit variance:\n\ntrain_x[num_cols].mean(axis=0)\n\nage                6.679102e-17\nheight            -2.836842e-15\nweight            -2.337686e-16\namount            -3.588241e-17\nmedical_info_a1   -5.684342e-17\nmedical_info_a2   -2.344791e-17\nmedical_info_a3    1.563194e-17\nmedical_info_b1   -3.410605e-17\ndtype: float64\n\n\n\ntrain_x[num_cols].std(axis=0)\n\nage                1.00005\nheight             1.00005\nweight             1.00005\namount             1.00005\nmedical_info_a1    1.00005\nmedical_info_a2    1.00005\nmedical_info_a3    1.00005\nmedical_info_b1    1.00005\ndtype: float64\n\n\nNote that it is possible to disable either centering or scaling by either passing with_mean=False or with_std=False to the constructor of StandardScaler.\n\n\n4.5.2 Scaling\nAn alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one.\n\nscaler = MinMaxScaler()\nscaler.fit(train_x[num_cols])\n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\ntrain_x[num_cols] = scaler.transform(train_x[num_cols])\ntest_x[num_cols] = scaler.transform(test_x[num_cols])\n\n\ntrain_x[num_cols].describe()\n\n\n  \n    \n      \n\n\n\n\n\n\nage\nheight\nweight\namount\nmedical_info_a1\nmedical_info_a2\nmedical_info_a3\nmedical_info_b1\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n0.500358\n0.524152\n0.380388\n0.384023\n0.470184\n0.487333\n0.220867\n0.491022\n\n\nstd\n0.292919\n0.143889\n0.125162\n0.345979\n0.136063\n0.130789\n0.157490\n0.319051\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.243243\n0.427145\n0.292879\n0.000800\n0.376302\n0.399748\n0.111111\n0.222222\n\n\n50%\n0.513514\n0.527079\n0.376458\n0.299930\n0.470703\n0.488020\n0.222222\n0.444444\n\n\n75%\n0.756757\n0.624006\n0.461614\n0.699970\n0.563802\n0.576293\n0.333333\n0.777778\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNote that you can scale to any range by specifying feature_range=(min, max).\n\n\n4.5.3 Logarithm transform and binning\n\nx = np.array([1.0, 10.0, 100.0, 1000.0, 10000.0])\n\n\nnp.log(x)\n\narray([0.        , 2.30258509, 4.60517019, 6.90775528, 9.21034037])\n\n\n\n# If your data contains zero value, try to plus one first\nnp.log1p(x)\n\narray([0.69314718, 2.39789527, 4.61512052, 6.90875478, 9.21044037])\n\n\nBinning allows you to transform numerical variable to categorical variable\n\nx = [1, 7, 5, 4, 6, 3]\n\n\nbin_edges = [-float('inf'), 3.0, 5.0, float('inf')]\nbinned = pd.cut(x, bin_edges, labels=False, retbins=True)\nprint(binned)\n\n(array([0, 2, 1, 1, 2, 0]), array([-inf,   3.,   5.,  inf]))\n\n\n\nbinned \n\n(array([0, 2, 1, 1, 2, 0]), array([-inf,   3.,   5.,  inf]))\n\n\n\n\n4.5.4 Power transfrom\n\ntrain_x, test_x = load_data()\n\nBox-cox transform only works for postive data\n\npos_cols = [c for c in num_cols if (train_x[c] &gt; 0.0).all() and (test_x[c] &gt; 0.0).all()] # List comprehension which is similar to set-builder notation\n\n\npt = PowerTransformer(method='box-cox')\npt.fit(train_x[pos_cols])\n\nPowerTransformer(method='box-cox')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PowerTransformerPowerTransformer(method='box-cox')\n\n\n\ntrain_x[pos_cols] = pt.transform(train_x[pos_cols])\ntest_x[pos_cols] = pt.transform(test_x[pos_cols])\n\n\nrng = np.random.RandomState(304)\noriginal_data = rng.lognormal(size=(1000,1))\npt = PowerTransformer(method='box-cox')\ntransformed_data = pt.fit_transform(original_data)\n\nfig, ax=plt.subplots(1, 2, figsize=(15, 3))\nsns.histplot(original_data, ax=ax[0], kde=True, legend=False)\nax[0].set_title(\"Original Data\")\nsns.histplot(transformed_data, ax=ax[1], kde=True, legend=False)\nax[1].set_title(\"Transformed data\")\nplt.show()\n\n\n\n\n\ntrain_x, test_x = load_data()\n\n\npt = PowerTransformer(method='yeo-johnson')\npt.fit(train_x[num_cols])\n\nPowerTransformer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PowerTransformerPowerTransformer()\n\n\n\ntrain_x[num_cols] = pt.transform(train_x[num_cols])\ntest_x[num_cols] = pt.transform(test_x[num_cols])\n\n\nrng = np.random.RandomState(304)\noriginal_data = rng.lognormal(size=(1000,1))\npt = PowerTransformer(method='yeo-johnson')\ntransformed_data = pt.fit_transform(original_data)\n\nfig, ax=plt.subplots(1, 2, figsize=(15, 3))\nsns.histplot(original_data, ax=ax[0], kde=True, legend=False)\nax[0].set_title(\"Original Data\")\nsns.histplot(transformed_data, ax=ax[1], kde=True, legend=False)\nax[1].set_title(\"Transformed data\")\nplt.show()\n\n\n\n\n\n\n4.5.5 Quantile transfrom\n\ntransformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\ntransformer.fit(train_x[num_cols])\n\nQuantileTransformer(n_quantiles=100, output_distribution='normal',\n                    random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.QuantileTransformerQuantileTransformer(n_quantiles=100, output_distribution='normal',\n                    random_state=0)\n\n\n\nrng = np.random.RandomState(304)\noriginal_data = rng.lognormal(size=(1000,1))\ntransformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\ntransformed_data = transformer.fit_transform(original_data)\n\nfig, ax=plt.subplots(1, 2, figsize=(15, 3))\nsns.histplot(original_data, ax=ax[0], kde=True, legend=False)\nax[0].set_title(\"Original Data\")\nsns.histplot(transformed_data, ax=ax[1], kde=True, legend=False)\nax[1].set_title(\"Transformed data\")\nplt.show()"
  },
  {
    "objectID": "04_Clean_feature_engineering.html#encoding",
    "href": "04_Clean_feature_engineering.html#encoding",
    "title": "4  Data cleaning and feature engineering",
    "section": "4.6 Encoding",
    "text": "4.6 Encoding\n\n4.6.1 One-hot encoding\nOne possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the OneHotEncoder, which transforms each categorical feature with n_categories possible values into n_categories binary features, with one of them 1, and all others 0.\n\nenc = OneHotEncoder(drop=None, handle_unknown='ignore')\n\n\nX = pd.DataFrame(np.array([['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox'], ['female', 'from Europe', 'uses Chrome']]), columns=['gender', 'locations', 'browsers'])\nX.head()\n\n\n  \n    \n      \n\n\n\n\n\n\ngender\nlocations\nbrowsers\n\n\n\n\n0\nmale\nfrom US\nuses Safari\n\n\n1\nfemale\nfrom Europe\nuses Firefox\n\n\n2\nfemale\nfrom Europe\nuses Chrome\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nonehot_encoded = enc.fit_transform(X).toarray()\npd.DataFrame(onehot_encoded, columns=enc.get_feature_names_out())\n\n\n  \n    \n      \n\n\n\n\n\n\ngender_female\ngender_male\nlocations_from Europe\nlocations_from US\nbrowsers_uses Chrome\nbrowsers_uses Firefox\nbrowsers_uses Safari\n\n\n\n\n0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n1\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n2\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWhen an unknown category is encountered during transform(), the resulting one-hot encoded columns for this feature will be all zeros:\n\nX_test = pd.DataFrame(np.array([['male', 'from US', 'uses Firefox'], ['female', 'from Europe', 'uses IE']]), columns=['gender', 'locations', 'browsers'])\nonehot_encoded = enc.transform(X_test).toarray()\npd.DataFrame(onehot_encoded, columns=enc.get_feature_names_out())\n\n\n  \n    \n      \n\n\n\n\n\n\ngender_female\ngender_male\nlocations_from Europe\nlocations_from US\nbrowsers_uses Chrome\nbrowsers_uses Firefox\nbrowsers_uses Safari\n\n\n\n\n0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n\n\n1\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4.6.2 Ordinal encoding\n\nenc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value= -1)\n\n\nX = pd.DataFrame(np.array([['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox'], ['female', 'from Europe', 'uses Chrome']]), columns=['gender', 'locations', 'browsers'])\nX.head()\n\n\n  \n    \n      \n\n\n\n\n\n\ngender\nlocations\nbrowsers\n\n\n\n\n0\nmale\nfrom US\nuses Safari\n\n\n1\nfemale\nfrom Europe\nuses Firefox\n\n\n2\nfemale\nfrom Europe\nuses Chrome\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nordinal_encoded = enc.fit_transform(X)\npd.DataFrame(ordinal_encoded, columns=enc.get_feature_names_out())\n\n\n  \n    \n      \n\n\n\n\n\n\ngender\nlocations\nbrowsers\n\n\n\n\n0\n1.0\n1.0\n2.0\n\n\n1\n0.0\n0.0\n1.0\n\n\n2\n0.0\n0.0\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nX_test = pd.DataFrame(np.array([['male', 'from US', 'uses Firefox'], ['female', 'from Europe', 'uses IE']]), columns=['gender', 'locations', 'browsers'])\nordinal_encoded = enc.transform(X_test)\npd.DataFrame(ordinal_encoded, columns=enc.get_feature_names_out())\n\n\n  \n    \n      \n\n\n\n\n\n\ngender\nlocations\nbrowsers\n\n\n\n\n0\n1.0\n1.0\n1.0\n\n\n1\n0.0\n0.0\n-1.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4.6.3 Target encoding\nThe MovieLens1M dataset contains one-million movie ratings by users of the MovieLens website, with features describing each user and movie. This hidden cell sets everything up:\n\n!kaggle datasets download -d odedgolden/movielens-1m-dataset\n\nDownloading movielens-1m-dataset.zip to /content\n  0% 0.00/5.83M [00:00&lt;?, ?B/s] 86% 5.00M/5.83M [00:00&lt;00:00, 46.2MB/s]\n100% 5.83M/5.83M [00:00&lt;00:00, 51.8MB/s]\n\n\n\n!unzip -qq movielens-1m-dataset.zip\n\n\nratings = pd.read_csv('ratings.dat',sep='::', header=None, names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"])\nusers = pd.read_csv('users.dat',sep='::', header=None, names=[\"UserID\", \"Gender\", \"Age\", \"Occupation\", \"Zip-code\"])\n\n/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators &gt; 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n  return func(*args, **kwargs)\n\n\n\ndf = pd.merge(left=ratings, right=users, how='inner', on='UserID')\n\n\ndf = df.astype(np.uint8, errors='ignore') # reduce memory footprint\nprint(\"Number of Unique Zipcodes: {}\".format(df[\"Zip-code\"].nunique()))\n\nNumber of Unique Zipcodes: 3439\n\n\n\ndf\n\n\n  \n    \n      \n\n\n\n\n\n\nUserID\nMovieID\nRating\nTimestamp\nGender\nAge\nOccupation\nZip-code\n\n\n\n\n0\n1\n169\n5\n88\nF\n1\n10\n48067\n\n\n1\n1\n149\n3\n157\nF\n1\n10\n48067\n\n\n2\n1\n146\n3\n16\nF\n1\n10\n48067\n\n\n3\n1\n80\n4\n115\nF\n1\n10\n48067\n\n\n4\n1\n51\n5\n99\nF\n1\n10\n48067\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1000204\n152\n67\n1\n253\nM\n25\n6\n11106\n\n\n1000205\n152\n70\n5\n119\nM\n25\n6\n11106\n\n\n1000206\n152\n50\n5\n234\nM\n25\n6\n11106\n\n\n1000207\n152\n72\n4\n128\nM\n25\n6\n11106\n\n\n1000208\n152\n73\n4\n49\nM\n25\n6\n11106\n\n\n\n\n\n1000209 rows × 8 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWith over 3000 categories, the Zipcode feature makes a good candidate for target encoding, and the size of this dataset (over one-million rows) means we can spare some data to create the encoding.\nWe’ll start by creating a 25% split to train the target encoder.\n\nX = df.copy()\ny = X.pop('Rating')\n\nX_encode = X.sample(frac=0.25)\ny_encode = y[X_encode.index]\nX_pretrain = X.drop(X_encode.index)\ny_train = y[X_pretrain.index]\n\nThe category_encoders package in scikit-learn-contrib implements an MEstimateEncoder, which we’ll use to encode our Zipcode feature.\n\n# Create the encoder instance. Choose m to control noise.\nencoder = MEstimateEncoder(cols=[\"Zip-code\"], m=5.0)\n\n# Fit the encoder on the encoding split.\nencoder.fit(X_encode, y_encode)\n\n# Encode the Zipcode column to create the final training data\nX_train = encoder.transform(X_pretrain)\n\nLet’s compare the encoded values to the target to see how informative our encoding might be.\n\nplt.figure(dpi=90)\nax = sns.distplot(y, kde=False, norm_hist=True)\nax = sns.kdeplot(X_train[\"Zip-code\"], color='r', ax=ax)\nax.set_xlabel(\"Rating\")\nax.legend(labels=['Zipcode', 'Rating']);\n\n/usr/local/lib/python3.9/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\nThe distribution of the encoded Zipcode feature roughly follows the distribution of the actual ratings, meaning that movie-watchers differed enough in their ratings from zipcode to zipcode that our target encoding was able to capture useful information.\nIf you would like to use KFold encoding, take a look at http://contrib.scikit-learn.org/category_encoders/wrapper.html#category_encoders.wrapper.NestedCVWrapper"
  },
  {
    "objectID": "04_Clean_feature_engineering.html#feature-engineering",
    "href": "04_Clean_feature_engineering.html#feature-engineering",
    "title": "4  Data cleaning and feature engineering",
    "section": "4.7 Feature Engineering",
    "text": "4.7 Feature Engineering\nWe’ll see how adding a few synthetic features to a dataset can improve the predictive performance of a random forest model.\nThe Concrete dataset contains a variety of concrete formulations and the resulting product’s compressive strength, which is a measure of how much load that kind of concrete can bear. The task for this dataset is to predict a concrete’s compressive strength given its formulation.\n\n!kaggle datasets download -d sinamhd9/concrete-comprehensive-strength\n!unzip -qq concrete-comprehensive-strength.zip\n\nDownloading concrete-comprehensive-strength.zip to /content\n  0% 0.00/32.9k [00:00&lt;?, ?B/s]\n100% 32.9k/32.9k [00:00&lt;00:00, 16.2MB/s]\n\n\n\ndf = pd.read_excel(\"Concrete_Data.xls\")\ndf.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nCement (component 1)(kg in a m^3 mixture)\nBlast Furnace Slag (component 2)(kg in a m^3 mixture)\nFly Ash (component 3)(kg in a m^3 mixture)\nWater (component 4)(kg in a m^3 mixture)\nSuperplasticizer (component 5)(kg in a m^3 mixture)\nCoarse Aggregate (component 6)(kg in a m^3 mixture)\nFine Aggregate (component 7)(kg in a m^3 mixture)\nAge (day)\nConcrete compressive strength(MPa, megapascals)\n\n\n\n\n0\n540.0\n0.0\n0.0\n162.0\n2.5\n1040.0\n676.0\n28\n79.986111\n\n\n1\n540.0\n0.0\n0.0\n162.0\n2.5\n1055.0\n676.0\n28\n61.887366\n\n\n2\n332.5\n142.5\n0.0\n228.0\n0.0\n932.0\n594.0\n270\n40.269535\n\n\n3\n332.5\n142.5\n0.0\n228.0\n0.0\n932.0\n594.0\n365\n41.052780\n\n\n4\n198.6\n132.4\n0.0\n192.0\n0.0\n978.4\n825.5\n360\n44.296075\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou can see here the various ingredients going into each variety of concrete. We’ll see in a moment how adding some additional synthetic features derived from these can help a model to learn important relationships among them. We’ll first establish a baseline by training the model on the un-augmented dataset. This will help us determine whether our new features are actually useful.\nEstablishing baselines like this is good practice at the start of the feature engineering process. A baseline score can help you decide whether your new features are worth keeping, or whether you should discard them and possibly try something else.\n\ndf.columns\n\nIndex(['Cement (component 1)(kg in a m^3 mixture)',\n       'Blast Furnace Slag (component 2)(kg in a m^3 mixture)',\n       'Fly Ash (component 3)(kg in a m^3 mixture)',\n       'Water  (component 4)(kg in a m^3 mixture)',\n       'Superplasticizer (component 5)(kg in a m^3 mixture)',\n       'Coarse Aggregate  (component 6)(kg in a m^3 mixture)',\n       'Fine Aggregate (component 7)(kg in a m^3 mixture)', 'Age (day)',\n       'Concrete compressive strength(MPa, megapascals) '],\n      dtype='object')\n\n\n\nX = df.copy()\ny = X.pop(df.columns[-1])\n\n# Train and score baseline model\nbaseline = RandomForestRegressor(criterion=\"absolute_error\", random_state=0)\nbaseline_score = cross_val_score(\n    baseline, X, y, cv=5, scoring=\"neg_mean_absolute_error\"\n)\nbaseline_score = -1 * baseline_score.mean()\n\nprint(f\"MAE Baseline Score: {baseline_score:.4}\")\n\nMAE Baseline Score: 8.397\n\n\nIf you ever cook at home, you might know that the ratio of ingredients in a recipe is usually a better predictor of how the recipe turns out than their absolute amounts. We might reason then that ratios of the features above would be a good predictor of CompressiveStrength. The cell below adds three new ratio features to the dataset.\n\nX = df.copy()\ny = X.pop(df.columns[-1])\n\n# Create synthetic features\nX[\"FCRatio\"] = X[df.columns[-2]] / X[df.columns[-3]]\nX[\"AggCmtRatio\"] = (X[df.columns[-3]] + X[df.columns[-2]]) / X[df.columns[0]]\nX[\"WtrCmtRatio\"] = X[df.columns[3]] / X[df.columns[0]]\n\n# Train and score model on dataset with additional ratio features\nmodel = RandomForestRegressor(criterion=\"absolute_error\", random_state=0)\nscore = cross_val_score(\n    model, X, y, cv=5, scoring=\"neg_mean_absolute_error\"\n)\nscore = -1 * score.mean()\n\nprint(f\"MAE Score with Ratio Features: {score:.4}\")\n\nMAE Score with Ratio Features: 7.732\n\n\nAnd sure enough, performance improved! This is evidence that these new ratio features exposed important information to the model that it wasn’t detecting before.\n\n4.7.1 Mathematical Transforms\nWe’ll use four datasets that having a range of feature types: US Traffic Accidents, 1985 Automobiles, and Customer Lifetime Value. The following hidden cell loads them up.\n\n!kaggle datasets download -d sobhanmoosavi/us-accidents\n!kaggle datasets download -d toramky/automobile-dataset\n!kaggle datasets download -d pankajjsh06/ibm-watson-marketing-customer-value-data\n\nDownloading us-accidents.zip to /content\n 96% 257M/269M [00:01&lt;00:00, 123MB/s]\n100% 269M/269M [00:01&lt;00:00, 147MB/s]\nDownloading automobile-dataset.zip to /content\n  0% 0.00/4.87k [00:00&lt;?, ?B/s]\n100% 4.87k/4.87k [00:00&lt;00:00, 4.82MB/s]\nDownloading ibm-watson-marketing-customer-value-data.zip to /content\n  0% 0.00/345k [00:00&lt;?, ?B/s]\n100% 345k/345k [00:00&lt;00:00, 87.6MB/s]\n\n\n\n!unzip -qq us-accidents.zip\n!unzip -qq automobile-dataset.zip\n!unzip -qq ibm-watson-marketing-customer-value-data.zip\n\n\naccidents = pd.read_csv(\"US_Accidents_Dec21_updated.csv\")\nautos = pd.read_csv(\"Automobile_data.csv\")\nconcrete = pd.read_excel(\"Concrete_Data.xls\")\ncustomer = pd.read_csv(\"WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv\")\n\nParserError: ignored\n\n\nRelationships among numerical features are often expressed through mathematical formulas, which you’ll frequently come across as part of your domain research. In Pandas, you can apply arithmetic operations to columns just as if they were ordinary numbers.\nIn the Automobile dataset are features describing a car’s engine. Research yields a variety of formulas for creating potentially useful new features. The “stroke ratio”, for instance, is a measure of how efficient an engine is versus how performant:\n\nautos.replace(\"?\", np.nan, inplace = True)\navg_bore=autos['bore'].astype('float').mean(axis=0)\nautos[\"bore\"].replace(np.nan, avg_bore, inplace=True)\navg_stroke = autos[\"stroke\"].astype(\"float\").mean(axis=0)\nautos[\"stroke\"].replace(np.nan, avg_stroke, inplace=True)\n\n\nautos[[\"bore\", \"stroke\"]] = autos[[\"bore\", \"stroke\"]].astype(\"float\")\nautos[\"stroke_ratio\"] = autos.stroke/ autos.bore\nautos[[\"stroke\", \"bore\", \"stroke_ratio\"]].head()\n\nData visualization can suggest transformations, often a “reshaping” of a feature through powers or logarithms. The distribution of WindSpeed in US Accidents is highly skewed, for instance. In this case the logarithm is effective at normalizing it:\n\naccidents.columns\n\n\naccidents[\"Wind_Speed(mph)\"].describe()\n\n\n# If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log\naccidents[\"LogWindSpeed\"] = accidents[\"Wind_Speed(mph)\"].apply(np.log1p)\n\n# Plot a comparison\nfig, axs = plt.subplots(1, 2, figsize=(8, 4))\nsns.kdeplot(accidents[\"Wind_Speed(mph)\"], shade=True, ax=axs[0])\nsns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1]);\n\n\n\n4.7.2 Counts\nFeatures describing the presence or absence of something often come in sets, the set of risk factors for a disease, say. You can aggregate such features by creating a count. These features will be binary (1 for Present, 0 for Absent) or boolean (True or False). In Python, booleans can be added up just as if they were integers.\nIn Traffic Accidents are several features indicating whether some roadway object was near the accident. This will create a count of the total number of roadway features nearby using the sum() method:\n\nroadway_features = [\"Amenity\", \"Bump\", \"Crossing\", \"Give_Way\",\n    \"Junction\", \"No_Exit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\",\n    \"Traffic_Calming\", \"Traffic_Signal\"]\naccidents[\"RoadwayFeatures\"] = accidents[roadway_features].sum(axis=1)\n\naccidents[roadway_features + [\"RoadwayFeatures\"]].head(20)\n\n\n\n4.7.3 Group Transforms\nFinally we have Group transforms, which aggregate information across multiple rows grouped by some category. With a group transform you can create features like: “the average income of a person’s state of residence,” or “the proportion of movies released on a weekday, by genre.” If you had discovered a category interaction, a group transform over that categry could be something good to investigate.\nUsing an aggregation function, a group transform combines two features: a categorical feature that provides the grouping and another feature whose values you wish to aggregate. For an “average income by state”, you would choose State for the grouping feature, mean() for the aggregation function, and Income for the aggregated feature. To compute this in Pandas, we use the groupby() and transform() methods:\n\ncustomer[\"AverageIncome\"] = (\n    customer.groupby(\"State\")  # for each state\n    [\"Income\"]                 # select the income\n    .transform(\"mean\")         # and compute its mean\n)\n\ncustomer[[\"State\", \"Income\", \"AverageIncome\"]].head(10)\n\nThe mean() function is a built-in dataframe method, which means we can pass it as a string to transform(). Other handy methods include max(), min(), median(), var(), std(), and count(). Here’s how you could calculate the frequency with which each state occurs in the dataset:\n\ncustomer[\"StateFreq\"] = (\n    customer.groupby(\"State\")\n    [\"State\"]\n    .transform(\"count\")\n    / customer.State.count()\n)\n\ncustomer[[\"State\", \"StateFreq\"]].head(10)"
  },
  {
    "objectID": "04_Clean_feature_engineering.html#data-centric-ai-with-cleanlab",
    "href": "04_Clean_feature_engineering.html#data-centric-ai-with-cleanlab",
    "title": "4  Data cleaning and feature engineering",
    "section": "4.8 Data-centric AI with CleanLab",
    "text": "4.8 Data-centric AI with CleanLab\ncleanlab automatically finds and fixes errors in any ML dataset. This data-centric AI package facilitates machine learning with messy, real-world data by providing clean labels during training.\n\ndata_dict = make_data()\nfor key, val in data_dict.items():  # Map data_dict to variables in namespace\n    print(key)\n    exec(key + \"=val\")\n\ndata\ntrue_labels\nlabels\ntest_data\ntest_labels\nnoisy_test_labels\nps\npy_true\nnoise_matrix_true\nclass_names\n\n\n\ntrue_errors = np.where(true_labels != labels)[0]\nplot_data(data, circles=true_errors, title=\"A realistic, messy dataset with 4 classes\", alpha=0.3)\n\n\n\n\nThe figure above represents a toy dataset we’ll use to demonstrate various cleanlab functionality. In this data, the features X are 2-dimensional and examples are colored according to their given label above. The given label happens to be incorrect for some of the examples (circled in red) in this dataset!\n\n4.8.1 Use CleanLearning() for everything\n\n# For comparison, this is how you would have trained your model normally (without Cleanlab)\nyourFavoriteModel = LogisticRegression(random_state=SEED)\nyourFavoriteModel.fit(data, labels)\nprint(f\"Accuracy using yourFavoriteModel: {yourFavoriteModel.score(test_data, test_labels):.0%}\")\n\nAccuracy using yourFavoriteModel: 83%\n\n\n\nyourFavoriteModel = LogisticRegression(random_state=SEED)\n\n# CleanLearning: Machine Learning with cleaned data (given messy, real-world data)\ncl = cleanlab.classification.CleanLearning(yourFavoriteModel, seed=SEED)\n\n# Fit model to messy, real-world data, automatically training on cleaned data.\n_ = cl.fit(data, labels)\n\n\n# But CleanLearning can do anything yourFavoriteModel can do, but enhanced.\n# For example, CleanLearning gives you predictions (just like yourFavoriteModel)\n# but the magic is that CleanLearning was trained as if your data did not have label errors.\nprint(f\"Accuracy using yourFavoriteModel (+ CleanLearning): {cl.score(test_data, test_labels):.0%}\")\n\nAccuracy using yourFavoriteModel (+ CleanLearning): 86%\n\n\nNote! Accuracy refers to the accuracy with respect to the true error-free labels of a test set., i.e. what we actually care about in practice because that’s what real-world model performance is based on!\n\n\n4.8.2 Use CleanLearning() to find_label_issues() in one line of code\n\n# One line of code. Literally.\nissues = CleanLearning(yourFavoriteModel, seed=SEED).find_label_issues(data, labels)\nissues.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nis_label_issue\nlabel_quality\ngiven_label\npredicted_label\n\n\n\n\n0\nFalse\n0.695174\n0\n0\n\n\n1\nFalse\n0.522929\n0\n0\n\n\n2\nTrue\n0.013722\n3\n0\n\n\n3\nFalse\n0.675606\n0\n0\n\n\n4\nFalse\n0.646438\n0\n0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nVisualize the twenty examples with lowest label quality to see if Cleanlab works.\n\nlowest_quality_labels = issues[\"label_quality\"].argsort()[:20]\nplot_data(data, circles=lowest_quality_labels, title=\"The 20 lowest label quality examples\")\n\n\n\n\nAbove, the top 20 label issues circled in red are found automatically using cleanlab (no true labels given).\n\n\n4.8.3 Use cleanlab to find dataset-level and class-level issues\n\nDid you notice that the yellow and seafoam green class above are overlapping?\nHow can a model ever know (or learn) what’s ground truth inside the yellow distribution?\nIf these two classes were merged, the model can learn more accurately from 3 classes (versus 4).\n\ncleanlab automatically finds data-set level issues like this, in one line of code. Check this out!\n\ncleanlab.dataset.find_overlapping_classes(\n    labels=labels,\n    confident_joint=cl.confident_joint,  # cleanlab uses the confident_joint internally to quantify label noise (see cleanlab.count.compute_confident_joint)\n    class_names=class_names,\n)\n\n\n  \n    \n      \n\n\n\n\n\n\nClass Name A\nClass Name B\nClass Index A\nClass Index B\nNum Overlapping Examples\nJoint Probability\n\n\n\n\n0\nseafoam green\nyellow\n2\n3\n26\n0.104\n\n\n1\npurple\nseafoam green\n0\n2\n23\n0.092\n\n\n2\npurple\nyellow\n0\n3\n10\n0.040\n\n\n3\nblue\nseafoam green\n1\n2\n6\n0.024\n\n\n4\npurple\nblue\n0\n1\n5\n0.020\n\n\n5\nblue\nyellow\n1\n3\n1\n0.004\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThere are two things being happening here:\n\nDistribution Overlap: The green distribution has huge variance and overlaps with other distributions. Cleanlab handles this for you: read the theory behind cleanlab for overlapping classes here.\nLabel Issues: A ton of examples (which actually belong to the purple class) have been mislabeled as “green” in our dataset.\n\nNow, let’s see what happens if we merge classes “seafoam green” and “yellow”\n\nyourFavoriteModel1 = LogisticRegression(random_state=SEED)\nyourFavoriteModel1.fit(data, labels)\nprint(f\"[Original classes] Accuracy of yourFavoriteModel: {yourFavoriteModel1.score(test_data, test_labels):.0%}\")\n\nmerged_labels, merged_test_labels = np.array(labels), np.array(test_labels)\n\n# Merge classes: map all yellow-labeled examples to seafoam green\nmerged_labels[merged_labels == 3] = 2\nmerged_test_labels[merged_test_labels == 3] = 2\n\n# Re-run our comparison. Re-run your model on the newly labeled dataset.\nyourFavoriteModel2 = LogisticRegression(random_state=SEED)\nyourFavoriteModel2.fit(data, merged_labels)\nprint(f\"[Modified classes] Accuracy of yourFavoriteModel: {yourFavoriteModel2.score(test_data, merged_test_labels):.0%}\")\n\n# Re-run CleanLearning as well.\nyourFavoriteModel3 = LogisticRegression(random_state=SEED)\ncl3 = cleanlab.classification.CleanLearning(yourFavoriteModel, seed=SEED)\ncl3.fit(data, merged_labels)\nprint(f\"[Modified classes] Accuracy of yourFavoriteModel (+ CleanLearning): {cl3.score(test_data, merged_test_labels):.0%}\")\n\n[Original classes] Accuracy of yourFavoriteModel: 83%\n[Modified classes] Accuracy of yourFavoriteModel: 94%\n[Modified classes] Accuracy of yourFavoriteModel (+ CleanLearning): 96%\n\n\nWhile on one hand that’s a huge improvement, it’s important to remember that choosing among three classes is an easier task than choosing among four classes, so it’s not fair to directly compare these numbers. Instead, the big takeaway is… if you get to choose your classes, combining overlapping classes can make the learning task easier for your model!\n\n\n4.8.4 Clean your test set too if you’re doing ML with noisy labels!\nIf your test and training data were randomly split, then be aware that your test labels are likely noisy too! It is thus important to fix label issues in them before we can trust measures like test accuracy.\n\n# Fit your model on noisily labeled train data\nyourFavoriteModel = LogisticRegression(random_state=SEED)\nyourFavoriteModel.fit(data, labels)\n\n# Get predicted probabilities for test data (these are out-of-sample)\nmy_test_pred_probs = yourFavoriteModel.predict_proba(test_data)\nmy_test_preds = my_test_pred_probs.argmax(axis=1)  # predicted labels\n\n# Find label issues in the test data\nissues_test = CleanLearning(yourFavoriteModel, seed=SEED).find_label_issues(\n    labels=noisy_test_labels, pred_probs=my_test_pred_probs)\n\nissues_test.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nis_label_issue\nlabel_quality\ngiven_label\npredicted_label\n\n\n\n\n0\nFalse\n0.702970\n0\n0\n\n\n1\nTrue\n0.009418\n1\n0\n\n\n2\nFalse\n0.788533\n0\n0\n\n\n3\nTrue\n0.207881\n2\n0\n\n\n4\nFalse\n0.713672\n0\n0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# You should inspect issues_test and fix issues to ensure high-quality test data labels.\ncorrected_test_labels = test_labels  # Here we'll pretend you have done this perfectly :)\n\n# Fit more robust version of model on noisily labeled training data\ncl = CleanLearning(yourFavoriteModel, seed=SEED).fit(data, labels)\ncl_test_preds = cl.predict(test_data)\n\nprint(f\" Noisy Test Accuracy (on given test labels) using yourFavoriteModel: {accuracy_score(noisy_test_labels, my_test_preds):.0%}\")\nprint(f\" Noisy Test Accuracy (on given test labels) using yourFavoriteModel (+ CleanLearning): {accuracy_score(noisy_test_labels, cl_test_preds):.0%}\")\nprint(f\"Actual Test Accuracy (on corrected test labels) using yourFavoriteModel: {accuracy_score(corrected_test_labels, my_test_preds):.0%}\")\nprint(f\"Actual Test Accuracy (on corrected test labels) using yourFavoriteModel (+ CleanLearning): {accuracy_score(corrected_test_labels, cl_test_preds):.0%}\")\n\n Noisy Test Accuracy (on given test labels) using yourFavoriteModel: 69%\n Noisy Test Accuracy (on given test labels) using yourFavoriteModel (+ CleanLearning): 71%\nActual Test Accuracy (on corrected test labels) using yourFavoriteModel: 83%\nActual Test Accuracy (on corrected test labels) using yourFavoriteModel (+ CleanLearning): 86%\n\n\n\n\n4.8.5 One score to rule them all – use cleanlab’s overall dataset health score\nThis score can be fairly compared across datasets or across versions of a dataset to track overall dataset quality (a.k.a. dataset health) over time.\n\n# One line of code.\nhealth = cleanlab.dataset.health_summary(\n    labels, confident_joint=cl.confident_joint\n    # cleanlab uses the confident_joint internally to quantify label noise (see cleanlab.count.compute_confident_joint)\n)\n\n--------------------------------------------------------\n|  Generating a Cleanlab Dataset Health Summary        |\n|   for your dataset with 250 examples and 4 classes.  |\n|  Note, Cleanlab is not a medical doctor... yet.      |\n--------------------------------------------------------\n\nOverall Class Quality and Noise across your dataset (below)\n------------------------------------------------------------ \n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nClass Index\nLabel Issues\nInverse Label Issues\nLabel Noise\nInverse Label Noise\nLabel Quality Score\n\n\n\n\n0\n2\n32\n23\n0.507937\n0.425926\n0.492063\n\n\n1\n3\n15\n22\n0.306122\n0.392857\n0.693878\n\n\n2\n0\n16\n22\n0.190476\n0.244444\n0.809524\n\n\n3\n1\n8\n4\n0.148148\n0.080000\n0.851852\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nClass Overlap. In some cases, you may want to merge classes in the top rows (below)\n-----------------------------------------------------------------------------------\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nClass Index A\nClass Index B\nNum Overlapping Examples\nJoint Probability\n\n\n\n\n0\n2\n3\n26\n0.104\n\n\n1\n0\n2\n23\n0.092\n\n\n2\n0\n3\n10\n0.040\n\n\n3\n1\n2\n6\n0.024\n\n\n4\n0\n1\n5\n0.020\n\n\n5\n1\n3\n1\n0.004\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n * Overall, about 28% (71 of the 250) labels in your dataset have potential issues.\n ** The overall label health score for this dataset is: 0.72.\n\nGenerated with &lt;3 from Cleanlab.\n\n\n\n\n# One line of code.\nhealth = cleanlab.dataset.overall_label_health_score(\n    labels, confident_joint=cl.confident_joint\n    # cleanlab uses the confident_joint internally to quantify label noise (see cleanlab.count.compute_confident_joint)\n)\n\n * Overall, about 28% (71 of the 250) labels in your dataset have potential issues.\n ** The overall label health score for this dataset is: 0.72.\n\n\nBecause we know the true labels (we created this toy dataset), we can compare with ground truth.\n\nlabel_acc = sum(labels != true_labels) / len(labels)\nprint(f\"Percentage of label issues guessed by cleanlab {1 - health:.0%}\")\nprint(f\"Percentage of (ground truth) label errors): {label_acc:.0%}\")\n\noffset = (1 - label_acc) - health\noffset\n\ncleanlab seems to be overestimating. Since data points that fall in between two overlapping distributions are often impossible to label and are counted as issues.\nFor more details, see https://docs.cleanlab.ai/stable/tutorials/indepth_overview.html#"
  },
  {
    "objectID": "05_Feature_selection_extraction.html#setup",
    "href": "05_Feature_selection_extraction.html#setup",
    "title": "5  Feature selection and extraction",
    "section": "5.1 Setup",
    "text": "5.1 Setup\n\n!pip install Boruta -qq\n!pip install opentsne -qq\n!pip install umap-learn -qq\n\n\n# Scientific computing\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# Preprocessing and datasets\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_digits\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\n# Modeling\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\n\n# Feature selection\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.inspection import permutation_importance\n\nfrom boruta import BorutaPy\n\n# Feature extraction\nfrom sklearn.decomposition import PCA\nfrom openTSNE import TSNE as oTSNE\nimport umap\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import FeatureAgglomeration"
  },
  {
    "objectID": "05_Feature_selection_extraction.html#feature-selection",
    "href": "05_Feature_selection_extraction.html#feature-selection",
    "title": "5  Feature selection and extraction",
    "section": "5.2 Feature selection",
    "text": "5.2 Feature selection\nThe classes in the sklearn.feature_selection module can be used for feature selection/extraction methods on datasets, either to improve estimators’ accuracy scores or to boost their performance on very high-dimensional datasets.\n\n5.2.1 Removing low variance features\nSuppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is 0.8 * (1 - 0.8)\n\nX = pd.DataFrame([[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]])\nX\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0\n0\n1\n\n\n1\n0\n1\n0\n\n\n2\n1\n0\n0\n\n\n3\n0\n1\n1\n\n\n4\n0\n1\n0\n\n\n5\n0\n1\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nsel = VarianceThreshold(threshold=(.8 * (1 - .8))).set_output(transform=\"pandas\")\nsel.fit_transform(X)\n\n\n  \n    \n      \n\n\n\n\n\n\nx1\nx2\n\n\n\n\n0\n0\n1\n\n\n1\n1\n0\n\n\n2\n0\n0\n\n\n3\n1\n1\n\n\n4\n1\n0\n\n\n5\n1\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAs expected, VarianceThreshold() has removed the first column, which has a probability of containing a zero.\n\n\n5.2.2 Univariate feature selection\nScikit-learn exposes feature selection routines as objects that implement the transform() method. For instance, we can perform a \\(\\chi^2\\) test to the samples to retrieve only the two best features as follows:\n\nX, y = load_iris(return_X_y=True, as_frame=True) # Load the iris data set\nX\n\n\n  \n    \n      \n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n\n150 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nselecter = SelectKBest(chi2, k=2).set_output(transform=\"pandas\")\nX_new = selecter.fit_transform(X, y)\nX_new\n\n\n  \n    \n      \n\n\n\n\n\n\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n1.4\n0.2\n\n\n1\n1.4\n0.2\n\n\n2\n1.3\n0.2\n\n\n3\n1.5\n0.2\n\n\n4\n1.4\n0.2\n\n\n...\n...\n...\n\n\n145\n5.2\n2.3\n\n\n146\n5.0\n1.9\n\n\n147\n5.2\n2.0\n\n\n148\n5.4\n2.3\n\n\n149\n5.1\n1.8\n\n\n\n\n\n150 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nplt.scatter(X_new.iloc[:,0], X_new.iloc[:,1], c=y)\nplt.show()\n\n\n\n\nThese objects take as input a scoring function that returns univariate scores/p-values (or only scores for SelectKBest() and SelectPercentile()):\n\nFor regression: r_regression, f_regression, mutual_info_regression\nFor classification: chi2, f_classif, mutual_info_classif\n\nThe methods based on F-test estimate the degree of linear dependency between two random variables. For example, we create a dataset with two informative features among a hundred. To simplify our example, we do not include either redundant or repeated features. In addition, We will explicitly not shuffle the dataset to ensure that the informative features will correspond to the three two columns of X.\n\n# Generate synthetic dataset\n\ndata, target = make_classification(\n    n_samples=5000,\n    n_features=100,\n    n_informative=2,\n    n_redundant=0,\n    n_repeated=0,\n    random_state=0,\n    shuffle = False  # So that we know the informative freatures are X[:, n_informative + n_redundant + n_repeated:].\n)\n\nWe will create two machine learning pipelines.\n\nThe former will be a random forest that will use all available features.\nThe latter will also be a random forest, but we will add a feature selection step to train this classifier.\n\n\n# Let’s create the model without any feature selection\nmodel_without_selection = RandomForestClassifier(n_jobs=2)\n\n\n# Then, let’s create a pipeline where the first stage will make the feature selection processing.\nmodel_with_selection = make_pipeline(\n    SelectKBest(score_func=f_classif, k=2), # Feature slection\n    RandomForestClassifier(n_jobs=2),       # Model\n)\n\nWe will measure the average time spent to train each pipeline and make it predict. Besides, we will compute the testing score of the model. We will collect these results via cross-validation.\n\n# Let’s start with the random forest without feature selection. We will store the results into a dataframe.\ncv_results_without_selection = cross_validate(model_without_selection, data, target)\ncv_results_without_selection = pd.DataFrame(cv_results_without_selection)\n\n\n# Now, we will repeat the process for the pipeline incorporating the feature selection.\ncv_results_with_selection = cross_validate(model_with_selection, data, target, return_estimator=True)\ncv_results_with_selection = pd.DataFrame(cv_results_with_selection)\n\nTo analyze the results, we will merge the results from the two pipeline in a single pandas dataframe.\n\ncv_results = pd.concat(\n    [cv_results_without_selection, cv_results_with_selection],\n    axis=1,\n    keys=[\"Without feature selection\", \"With feature selection\"],\n)\n\n# swap the level of the multi-index of the columns\ncv_results = cv_results.swaplevel(axis=\"columns\")\n\nLet’s first analyze the train and score time for each pipeline.\n\ncolor = {\"whiskers\": \"black\", \"medians\": \"black\", \"caps\": \"black\"}\ncv_results[\"fit_time\"].plot.box(color=color, vert=False)\nplt.xlabel(\"Elapsed time (s)\")\n_ = plt.title(\"Time to fit the model\")\n\n\n\n\n\ncv_results[\"score_time\"].plot.box(color=color, vert=False)\nplt.xlabel(\"Elapsed time (s)\")\n_ = plt.title(\"Time to make prediction\")\n\n\n\n\nWe can draw the same conclusions for both training and scoring elapsed time: selecting the most informative features speed-up our pipeline. Of course, such speed-up is beneficial only if the generalization performance in terms of metrics remain the same. Let’s check the testing score.\n\ncv_results[\"test_score\"].plot.box(color=color, vert=False)\nplt.xlabel(\"Accuracy score\")\n_ = plt.title(\"Test score via cross-validation\")\n\n\n\n\nWe can observe that the model’s generalization performance selecting a subset of features decreases compared with the model using all available features. Since we generated the dataset, we can infer that the decrease is because of the selection. The feature selection algorithm did not choose the two informative features.\n\nfor idx, pipeline in enumerate(cv_results_with_selection[\"estimator\"]):\n    print(\n        f\"Fold #{idx} - features selected are: \"\n        f\"{np.argsort(pipeline[0].scores_)[-2:]}\"\n    )\n\nFold #0 - features selected are: [30  1]\nFold #1 - features selected are: [10  1]\nFold #2 - features selected are: [10  1]\nFold #3 - features selected are: [30  1]\nFold #4 - features selected are: [61  1]\n\n\nWe see that the feature 1 is always selected while the other feature varies depending on the cross-validation fold.\nIf we would like to keep our score with similar generalization performance, we could choose another metric to perform the test or select more features. For instance, we could select the number of features based on a specific percentile of the highest scores.\n\n5.2.2.1 Mutual information\nThe Automobile dataset consists of 193 cars from the 1985 model year. The goal for this dataset is to predict a car’s price (the target) from 23 of the car’s features, such as make, body_style, and horsepower. In this example, we’ll rank the features with mutual information and investigate the results by data visualization. (The original dataset requires data cleaning, you could refer to https://skill-lync.com/student-projects/project-1-1299 for more details)\n\n# Uncpmment below if you are using Kaggle\n#!pip install gdown\n\n\n!gdown --fuzzy https://drive.google.com/file/d/1FoCRK2LQBo1hlPWK2fDDcOjjy-DnPiJw/view?usp=sharing\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1FoCRK2LQBo1hlPWK2fDDcOjjy-DnPiJw\nTo: /content/autos.csv\n  0% 0.00/21.6k [00:00&lt;?, ?B/s]100% 21.6k/21.6k [00:00&lt;00:00, 31.6MB/s]\n\n\n\ndf = pd.read_csv(\"autos.csv\") # Clean version\ndf.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nsymboling\nmake\nfuel_type\naspiration\nnum_of_doors\nbody_style\ndrive_wheels\nengine_location\nwheel_base\nlength\n...\nengine_size\nfuel_system\nbore\nstroke\ncompression_ratio\nhorsepower\npeak_rpm\ncity_mpg\nhighway_mpg\nprice\n\n\n\n\n0\n3\nalfa-romero\ngas\nstd\n2\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9\n111\n5000\n21\n27\n13495\n\n\n1\n3\nalfa-romero\ngas\nstd\n2\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9\n111\n5000\n21\n27\n16500\n\n\n2\n1\nalfa-romero\ngas\nstd\n2\nhatchback\nrwd\nfront\n94.5\n171.2\n...\n152\nmpfi\n2.68\n3.47\n9\n154\n5000\n19\n26\n16500\n\n\n3\n2\naudi\ngas\nstd\n4\nsedan\nfwd\nfront\n99.8\n176.6\n...\n109\nmpfi\n3.19\n3.40\n10\n102\n5500\n24\n30\n13950\n\n\n4\n2\naudi\ngas\nstd\n4\nsedan\n4wd\nfront\n99.4\n176.6\n...\n136\nmpfi\n3.19\n3.40\n8\n115\n5500\n18\n22\n17450\n\n\n\n\n\n5 rows × 25 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe scikit-learn algorithm for MI treats discrete features differently from continuous features. Consequently, you need to tell it which are which. As a rule of thumb, anything that have a float dtype is not discrete. Categoricals (object or categorial dtype) can be treated as discrete by giving them a label encoding\n\nX = df.copy()\ny = X.pop(\"price\")\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n# A way to perfrom label encoding see https://pandas.pydata.org/docs/reference/api/pandas.factorize.html\n    X[colname], _ = X[colname].factorize() \n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int\n\nScikit-learn has two mutual information metrics in its feature_selection module: one for real-valued targets (mutual_info_regression()) and one for categorical targets (mutual_info_classif()). Our target, price, is real-valued. The next cell computes the MI scores for our features and wraps them up in a nice dataframe.\n\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nmi_scores[::3]  # show a few features with their MI scores\n\ncurb_weight          1.436041\nhighway_mpg          0.948495\nlength               0.615407\nbore                 0.497058\nstroke               0.385846\nnum_of_cylinders     0.331445\ncompression_ratio    0.132048\nfuel_type            0.047279\nName: MI Scores, dtype: float64\n\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)\n\n\n\n\n\n# As we might expect, the high-scoring `curb_weight` feature exhibits a strong relationship with `price`, the target.\nsns.relplot(x=\"curb_weight\", y=\"price\", data=df)\n\n\n\n\nThe fuel_type feature has a fairly low MI score, but as we can see from the figure below, it clearly separates two price populations with different trends within the horsepower feature. This indicates that fuel_type contributes an interaction effect and might not be unimportant after all. Before deciding a feature is unimportant from its MI score, it’s good to investigate any possible interaction effects – domain knowledge can offer a lot of guidance here.\n\nsns.lmplot(x=\"horsepower\", y=\"price\", hue=\"fuel_type\", data=df)\n\n\n\n\n\n\n\n5.2.3 Sequential feature selection\nSequential Feature Selection is available in the SequentialFeatureSelector transformer. SFS can be either forward or backward:\n\nForward-SFS is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. Concretely, we initially start with zero feature and find the one feature that maximizes a cross-validated score when an estimator is trained on this single feature. Once that first feature is selected, we repeat the procedure by adding a new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the n_features_to_select parameter.\nBackward-SFS follows the same idea but works in the opposite direction: instead of starting with no feature and greedily adding features, we start with all the features and greedily remove features from the set. The direction parameter controls whether forward or backward SFS is used.\n\n\nIn general, forward and backward selection do not yield equivalent results. Also, one may be much faster than the other depending on the requested number of selected features: if we have 10 features and ask for 7 selected features, forward selection would need to perform 7 iterations while backward selection would only need to perform 3.\n\n\nX, y = load_iris(return_X_y=True, as_frame=True)\nknn = KNeighborsClassifier(n_neighbors=3)\n\n\nsfs = SequentialFeatureSelector(knn, n_features_to_select=2, direction='forward').set_output(transform=\"pandas\")\nsfs.fit(X, y)\n\nSequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),\n                          n_features_to_select=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SequentialFeatureSelectorSequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),\n                          n_features_to_select=2)estimator: KNeighborsClassifierKNeighborsClassifier(n_neighbors=3)KNeighborsClassifierKNeighborsClassifier(n_neighbors=3)\n\n\n\n df = sfs.transform(X)\n df\n\n\n  \n    \n      \n\n\n\n\n\n\nsepal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n0.2\n\n\n1\n4.9\n0.2\n\n\n2\n4.7\n0.2\n\n\n3\n4.6\n0.2\n\n\n4\n5.0\n0.2\n\n\n...\n...\n...\n\n\n145\n6.7\n2.3\n\n\n146\n6.3\n1.9\n\n\n147\n6.5\n2.0\n\n\n148\n6.2\n2.3\n\n\n149\n5.9\n1.8\n\n\n\n\n\n150 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nplt.scatter(df.iloc[:,0], df.iloc[:,1],c=y)\nplt.show()\n\n\n\n\n\n\n5.2.4 Feature selection from model\nSelectFromModel is a meta-transformer that can be used alongside any estimator that assigns importance to each feature through a specific attribute (such as coef_, feature_importances_) or via an importance_getter callable after fitting. The features are considered unimportant and removed if the corresponding importance of the feature values are below the provided threshold parameter.\nApart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. **Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”. In combination with the threshold criteria, one can use the max_features parameter to set a limit on the number of features to select.**\n\nX, y = load_iris(return_X_y=True, as_frame=True)\n\n\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False)\nmodel = SelectFromModel(lsvc).set_output(transform=\"pandas\")\n\n\nX_new = model.fit_transform(X,y) # We use threshold instead or max_features here\nX_new\n\n/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n\n\n1\n4.9\n3.0\n1.4\n\n\n2\n4.7\n3.2\n1.3\n\n\n3\n4.6\n3.1\n1.5\n\n\n4\n5.0\n3.6\n1.4\n\n\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n\n\n146\n6.3\n2.5\n5.0\n\n\n147\n6.5\n3.0\n5.2\n\n\n148\n6.2\n3.4\n5.4\n\n\n149\n5.9\n3.0\n5.1\n\n\n\n\n\n150 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n5.2.5 A Concret example\nThe following dataset is our old friend which is a record of neighborhoods in California district, predicting the median house value (target) given some information about the neighborhoods, as the average number of rooms, the latitude, the longitude or the median income of people in the neighborhoods (block).\n\nX, y = fetch_california_housing(as_frame=True, return_X_y=True)\n\n\n# To speed up the computation, we take the first 10,000 samples\nX = X[:10000]\ny = y[:10000]\nX.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe feature reads as follow:\n\nMedInc: median income in block\nHouseAge: median house age in block\nAveRooms: average number of rooms\nAveBedrms: average number of bedrooms\nPopulation: block population\nAveOccup: average house occupancy\nLatitude: house block latitude\nLongitude: house block longitude\nMedHouseVal: Median house value in 100k$ (target)\n\nTo assert the quality of our inspection technique, let’s add some random feature that won’t help the prediction (un-informative feature)\n\n# Adding random features\nrng = np.random.RandomState(0)\nbin_var = pd.Series(rng.randint(0, 2, X.shape[0]), name='rnd_bin')\nnum_var = pd.Series(np.arange(X.shape[0]), name='rnd_num')\nX_with_rnd_feat = pd.concat((X, bin_var, num_var), axis=1)\nX_with_rnd_feat\n\n\n  \n    \n      \n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nrnd_bin\nrnd_num\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n0\n0\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n1\n1\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n1\n2\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n0\n3\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n1\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\n4.0775\n10.0\n6.140900\n1.025440\n1275.0\n2.495108\n39.14\n-121.03\n0\n9995\n\n\n9996\n4.0848\n8.0\n6.350394\n1.091864\n1977.0\n2.594488\n39.13\n-121.07\n1\n9996\n\n\n9997\n3.6333\n7.0\n7.243455\n1.107330\n1143.0\n2.992147\n39.11\n-121.05\n0\n9997\n\n\n9998\n3.4630\n8.0\n6.363636\n1.166297\n1307.0\n2.898004\n39.08\n-121.04\n1\n9998\n\n\n9999\n3.0781\n7.0\n5.487500\n1.050000\n246.0\n3.075000\n39.09\n-121.00\n0\n9999\n\n\n\n\n\n10000 rows × 10 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X_with_rnd_feat, y, random_state=42)\n\nIn linear models, the target value is modeled as a linear combination of the features.\n\nmodel = RidgeCV()\n\nmodel.fit(X_train, y_train)\n\nprint(f'model score on training data: {model.score(X_train, y_train)}')\nprint(f'model score on testing data: {model.score(X_test, y_test)}')\n\nmodel score on training data: 0.6049524592207427\nmodel score on testing data: 0.5863921053581754\n\n\nOur linear model obtains a score of .60, so it explains a significant part of the target. Its coefficient should be somehow relevant. Let’s look at the coefficient learnt\n\ncoefs = pd.DataFrame(\n   model.coef_,\n   columns=['Coefficients'], index=X_train.columns\n)\n\ncoefs.plot(kind='barh', figsize=(9, 7))\nplt.title('Ridge model')\nplt.axvline(x=0, color='.5')\nplt.subplots_adjust(left=.3)\n\n\n\n\nThe AveBedrms have the higher coefficient. However, we can’t compare the magnitude of these coefficients directly, since they are not scaled. Indeed, Population is an integer which can be thousands, while AveBedrms is around 4 and Latitude is in degree.\nSo the Population coefficient is expressed in “100k$/habitant” while the AveBedrms is expressed in “100k$/nb of bedrooms” and the Latitude coefficient in “100k$/degree”. We see that changing population by one does not change the outcome, while as we go south (latitude increase) the price becomes cheaper. Also, adding a bedroom (keeping all other feature constant) shall rise the price of the house by 80k$.\nSo looking at the coefficient plot to gauge feature importance can be misleading as some of them vary on a small scale, while others vary a lot more, several decades. So before any interpretation, we need to scale each column (removing the mean and scaling the variance to 1).\n\nmodel = make_pipeline(StandardScaler(), RidgeCV())\n\nmodel.fit(X_train, y_train)\n\nprint(f'model score on training data: {model.score(X_train, y_train)}')\nprint(f'model score on testing data: {model.score(X_test, y_test)}')\n\nmodel score on training data: 0.6049222473801685\nmodel score on testing data: 0.586090835494786\n\n\n\ncoefs = pd.DataFrame(\n   model[1].coef_,\n   columns=['Coefficients'], index=X_train.columns\n)\n\ncoefs.plot(kind='barh', figsize=(9, 7))\nplt.title('Ridge model')\nplt.axvline(x=0, color='.5')\nplt.subplots_adjust(left=.3)\n\n\n\n\nNow that the coefficients have been scaled, we can safely compare them. The MedInc feature, with longitude and latitude are the three variables that most influence the model.\nThe plot above tells us about dependencies between a specific feature and the target when all other features remain constant, i.e., conditional dependencies. An increase of the HouseAge will induce an increase of the price when all other features remain constant. On the contrary, an increase of the AveRooms will induce an decrease of the price when all other features remain constant.\nWe can check the coefficient variability through cross-validation: it is a form of data perturbation.\n\ncv_model = cross_validate(\n   model, X_with_rnd_feat, y, cv=RepeatedKFold(n_splits=5, n_repeats=5),\n   return_estimator=True, n_jobs=2\n)\ncoefs = pd.DataFrame(\n   [model[1].coef_\n    for model in cv_model['estimator']],\n   columns=X_with_rnd_feat.columns\n)\nplt.figure(figsize=(9, 7))\nsns.boxplot(data=coefs, orient='h', color='cyan', saturation=0.5)\nplt.axvline(x=0, color='.5')\nplt.xlabel('Coefficient importance')\nplt.title('Coefficient importance and its variability')\nplt.subplots_adjust(left=.3)\n\n\n\n\nNow if we want to select the four features which are the most important according to the coefficients. The SelectFromModel() is meant just for that. SelectFromModel() accepts a threshold parameter and will select the features whose importance (defined by the coefficients) are above this threshold.\n\nmodel\n\nPipeline(steps=[('standardscaler', StandardScaler()), ('ridgecv', RidgeCV())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()), ('ridgecv', RidgeCV())])StandardScalerStandardScaler()RidgeCVRidgeCV()\n\n\n\nimportance = np.abs(model[1].coef_)\nthreshold = np.sort(importance)[-5] + 0.01\n\n\nsfm = SelectFromModel(model[1], threshold=threshold).fit(X, y)\nprint(f\"Features selected by SelectFromModel: {sfm.get_feature_names_out()}\")\n\nFeatures selected by SelectFromModel: ['MedInc' 'AveBedrms' 'Latitude' 'Longitude']\n\n\n\n5.2.5.1 Linear models with sparse coefficients (Lasso)\nIn it important to keep in mind that the associations extracted depend on the model. To illustrate this point we consider a Lasso model, that performs feature selection with a L1 penalty. Let us fit a Lasso model with a strong regularization parameters alpha\n\nmodel = make_pipeline(StandardScaler(), Lasso(alpha=.015))\n\nmodel.fit(X_train, y_train)\n\nprint(f'model score on training data: {model.score(X_train, y_train)}')\nprint(f'model score on testing data: {model.score(X_test, y_test)}')\n\nmodel score on training data: 0.5933235371761756\nmodel score on testing data: 0.5673786563118284\n\n\n\ncoefs = pd.DataFrame(\n   model[1].coef_,\n   columns=['Coefficients'], index=X_train.columns\n)\n\ncoefs.plot(kind='barh', figsize=(9, 7))\nplt.title('Lasso model, strong regularization')\nplt.axvline(x=0, color='.5')\nplt.subplots_adjust(left=.3)\n\n\n\n\nHere the model score is a bit lower, because of the strong regularization. However, it has zeroed out 3 coefficients, selecting a small number of variables to make its prediction.\n\n\n5.2.5.2 Randomforest with feature importance\nOn some algorithms, there are some feature importance methods, inherently built within the model. It is the case in RandomForest models. Let’s investigate the built-in feature_importances_ attribute.\n\nmodel = RandomForestRegressor()\n\nmodel.fit(X_train, y_train)\n\nprint(f'model score on training data: {model.score(X_train, y_train)}')\nprint(f'model score on testing data: {model.score(X_test, y_test)}')\n\nmodel score on training data: 0.9796643318943656\nmodel score on testing data: 0.8429137479202747\n\n\n\nimportances = model.feature_importances_\nindices = np.argsort(importances)\n\nfig, ax = plt.subplots()\nax.barh(range(len(importances)), importances[indices])\nax.set_yticks(range(len(importances)))\n_ = ax.set_yticklabels(np.array(X_train.columns)[indices])\n\n\n\n\nMedInc is still the most important feature. It also has a small bias toward high cardinality features, such as the noisy feature rnd_num, which are here predicted having 0.1 importance, more than HouseAge (which has low cardinality).\n\n\n5.2.5.3 Feature importance by permutation\nWe introduce here a new technique to evaluate the feature importance of any given fitted model. It basically shuffles a feature and sees how the model changes its prediction. Thus, the change in prediction will correspond to the feature importance.\n\n# Any model could be used here\n\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\nprint(f'model score on training data: {model.score(X_train, y_train)}')\nprint(f'model score on testing data: {model.score(X_test, y_test)}')\n\nmodel score on training data: 0.9795458070557226\nmodel score on testing data: 0.8448700648905965\n\n\n\nr = permutation_importance(model, X_test, y_test, n_repeats=30, random_state=42) # Notice that the model are already fitted\n\n\nfig, ax = plt.subplots()\n\nindices = r.importances_mean.argsort()\nplt.barh(range(len(indices)), r.importances_mean[indices], xerr=r.importances_std[indices])\n\nax.set_yticks(range(len(indices)))\n_ = ax.set_yticklabels(np.array(X_train.columns)[indices])\n\n\n\n\nWe see again that the feature MedInc, Latitude and Longitude are important for the model. We note that our random variable rnd_num is now less important than Latitude. Indeed, the feature importance built-in in RandomForest has bias for continuous data, such as AveOccup and rnd_num.\n\n\n5.2.5.4 Feature rejection using Boruta\n\n# define Boruta feature selection method\nmodel = RandomForestRegressor()\nfeat_selector = BorutaPy(model, n_estimators='auto', verbose=2, random_state=1)\n\n\n# find all relevant features \nfeat_selector.fit(X_train.to_numpy(), y_train.to_numpy())\n\nIteration:  1 / 100\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  2 / 100\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  3 / 100\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  4 / 100\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  5 / 100\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  6 / 100\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  7 / 100\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  8 / 100\nConfirmed:  9\nTentative:  0\nRejected:   1\n\n\nBorutaPy finished running.\n\nIteration:  9 / 100\nConfirmed:  9\nTentative:  0\nRejected:   1\n\n\nBorutaPy(estimator=RandomForestRegressor(n_estimators=44,\n                                         random_state=RandomState(MT19937) at 0x7F60C3A0E640),\n         n_estimators='auto',\n         random_state=RandomState(MT19937) at 0x7F60C3A0E640, verbose=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BorutaPyBorutaPy(estimator=RandomForestRegressor(n_estimators=44,\n                                         random_state=RandomState(MT19937) at 0x7F60C3A0E640),\n         n_estimators='auto',\n         random_state=RandomState(MT19937) at 0x7F60C3A0E640, verbose=2)estimator: RandomForestRegressorRandomForestRegressor(n_estimators=44,\n                      random_state=RandomState(MT19937) at 0x7F60C3A0E640)RandomForestRegressorRandomForestRegressor(n_estimators=44,\n                      random_state=RandomState(MT19937) at 0x7F60C3A0E640)\n\n\n\nX_train\n\n\n  \n    \n      \n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nrnd_bin\nrnd_num\n\n\n\n\n4901\n1.3287\n43.0\n4.036723\n1.090395\n1412.0\n3.988701\n34.01\n-118.25\n0\n4901\n\n\n4375\n2.3421\n48.0\n3.425532\n1.046809\n633.0\n2.693617\n34.10\n-118.28\n0\n4375\n\n\n6698\n3.6572\n26.0\n4.160797\n1.093023\n3001.0\n1.994020\n34.14\n-118.10\n0\n6698\n\n\n9805\n3.2750\n52.0\n8.357827\n1.543131\n582.0\n1.859425\n36.55\n-121.92\n1\n9805\n\n\n1101\n3.5189\n15.0\n5.489011\n1.027473\n1786.0\n2.453297\n39.82\n-121.68\n0\n1101\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5734\n5.1718\n31.0\n5.676417\n1.063985\n1359.0\n2.484461\n34.16\n-118.23\n0\n5734\n\n\n5191\n1.5256\n36.0\n4.897778\n1.097778\n702.0\n3.120000\n33.93\n-118.26\n1\n5191\n\n\n5390\n2.9344\n36.0\n3.986717\n1.079696\n1756.0\n3.332068\n34.03\n-118.38\n0\n5390\n\n\n860\n5.7192\n15.0\n6.395349\n1.067979\n1777.0\n3.178891\n37.58\n-121.96\n0\n860\n\n\n7270\n2.3900\n25.0\n3.928287\n1.235060\n1439.0\n5.733068\n33.98\n-118.23\n1\n7270\n\n\n\n\n\n7500 rows × 10 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# check selected features \nnp.array(X_train.columns)[feat_selector.support_]\n\narray(['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population',\n       'AveOccup', 'Latitude', 'Longitude', 'rnd_num'], dtype=object)\n\n\n\n# check ranking of features\nfeat_selector.ranking_\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 2, 1])\n\n\n\n# call transform() on X to filter it down to selected features\nX_filtered = feat_selector.transform(X_train.to_numpy())\nX_filtered.shape\n\n(7500, 9)"
  },
  {
    "objectID": "05_Feature_selection_extraction.html#dimensional-reduction",
    "href": "05_Feature_selection_extraction.html#dimensional-reduction",
    "title": "5  Feature selection and extraction",
    "section": "5.3 Dimensional reduction",
    "text": "5.3 Dimensional reduction\nWe now looked at our model-based method for feature engineering: principal component analysis (PCA). You could think of PCA as a partitioning of the variation in the data. PCA is a great tool to help you discover important relationships in the data and can also be used to create more informative features.\nThere are two ways you could use PCA for feature engineering.\n\nThe first way is to use it as a descriptive technique. Since the components tell you about the variation, you could compute the MI scores for the components and see what kind of variation is most predictive of your target. That could give you ideas for kinds of features to create – a product of 'Height' and 'Diameter' if 'Size' is important, say, or a ratio of 'Height' and 'Diameter' if Shape is important. You could even try clustering on one or more of the high-scoring components. Biplot will be useful in this case.\nThe second way is to use the components themselves as features. Because the components expose the variational structure of the data directly, they can often be more informative than the original features. Here are some use-cases:\n\n\nDimensionality reduction: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.\nAnomaly detection: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.\nNoise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.\nDecorrelation: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.\n\nPCA basically gives you direct access to the correlational structure of your data. You’ll no doubt come up with applications of your own!\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\n\ndf = pd.read_csv(\"autos.csv\")\ndf.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nsymboling\nmake\nfuel_type\naspiration\nnum_of_doors\nbody_style\ndrive_wheels\nengine_location\nwheel_base\nlength\n...\nengine_size\nfuel_system\nbore\nstroke\ncompression_ratio\nhorsepower\npeak_rpm\ncity_mpg\nhighway_mpg\nprice\n\n\n\n\n0\n3\nalfa-romero\ngas\nstd\n2\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9\n111\n5000\n21\n27\n13495\n\n\n1\n3\nalfa-romero\ngas\nstd\n2\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9\n111\n5000\n21\n27\n16500\n\n\n2\n1\nalfa-romero\ngas\nstd\n2\nhatchback\nrwd\nfront\n94.5\n171.2\n...\n152\nmpfi\n2.68\n3.47\n9\n154\n5000\n19\n26\n16500\n\n\n3\n2\naudi\ngas\nstd\n4\nsedan\nfwd\nfront\n99.8\n176.6\n...\n109\nmpfi\n3.19\n3.40\n10\n102\n5500\n24\n30\n13950\n\n\n4\n2\naudi\ngas\nstd\n4\nsedan\n4wd\nfront\n99.4\n176.6\n...\n136\nmpfi\n3.19\n3.40\n8\n115\n5500\n18\n22\n17450\n\n\n\n\n\n5 rows × 25 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe’ve selected four features that cover a range of properties. Each of these features also has a high MI score with the target, price. We’ll standardize the data since these features aren’t naturally on the same scale.\n\nfeatures = [\"highway_mpg\", \"engine_size\", \"horsepower\", \"curb_weight\"]\n\nX = df.copy()\ny = X.pop('price')\nX = X.loc[:, features]\n\n# Standardize\nX_scaled = (X - X.mean(axis=0)) / X.std(axis=0)\n\nNow we can fit scikit-learn’s PCA estimator and create the principal components. You can see here the first few rows of the transformed dataset.\n\n# Create principal components\npca = PCA()\nX_pca = pca.fit_transform(X_scaled)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca, columns=component_names)\n\nX_pca.head(10)\n\n\n  \n    \n      \n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\n\n\n\n\n0\n0.382486\n-0.400222\n0.124122\n0.169539\n\n\n1\n0.382486\n-0.400222\n0.124122\n0.169539\n\n\n2\n1.550890\n-0.107175\n0.598361\n-0.256081\n\n\n3\n-0.408859\n-0.425947\n0.243335\n0.013920\n\n\n4\n1.132749\n-0.814565\n-0.202885\n0.224138\n\n\n5\n0.547265\n-0.545141\n0.139969\n0.424955\n\n\n6\n0.869268\n-0.472834\n-0.294073\n0.090174\n\n\n7\n0.974373\n-0.449233\n-0.435749\n-0.019102\n\n\n8\n1.796553\n-1.050783\n-0.081821\n-0.296071\n\n\n9\n-0.306514\n-0.542020\n0.138605\n0.012612\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAfter fitting, the PCA instance contains the loadings in its components_ attribute. We’ll wrap the loadings up in a dataframe.\n\nloadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=X.columns,  # and the rows are the original features\n)\nloadings\n\n\n  \n    \n      \n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\n\n\n\n\nhighway_mpg\n-0.492347\n0.770892\n0.070142\n-0.397996\n\n\nengine_size\n0.503859\n0.626709\n0.019960\n0.594107\n\n\nhorsepower\n0.500448\n0.013788\n0.731093\n-0.463534\n\n\ncurb_weight\n0.503262\n0.113008\n-0.678369\n-0.523232\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# biplot \nxvector = pca.components_[0] \nyvector = -pca.components_[1]\n\nxs = X_pca.to_numpy()[:,0] \nys = -X_pca.to_numpy()[:,1]\n\nfig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(10, 9))\n\n# First Plot : Principal Component Scores \nx_min, x_max = -8, 8\ny_min, y_max = -3, 3\nax1.set_xlim(x_min, x_max)\nax1.set_ylim(y_min, y_max)\n\nfor i in range(len(xs)):\n    plt.plot(xs[i], ys[i], 'bo')\n\nax1.set_xlabel(\"1'st Principal Component Scores\")\nax1.set_ylabel(\"2'nd Principal Component Scores\")\n\n# Plot reference lines\nax1.hlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\nax1.vlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\n\n# Second Plot : Principal Component Loadings 'PCs' \nx_min, x_max = -1, 1          \ny_min, y_max = -1, 1\n\nax2 = ax1.twinx().twiny()\nax2.set_xlim(x_min, x_max)\nax2.set_ylim(y_min, y_max)\n\nfor i in range(len(xvector)):\n    ax2.arrow(0, 0, xvector[i],  yvector[i], color='red', width=0.005, head_width=0.02)\n    ax2.annotate(X.columns[i], (xvector[i]*1.05, yvector[i]*1.05), color='red', size=14)  \n\n    \nax2.set_xlabel(\"1'st Principal Component (Loading)\", color='red')\nax2.set_ylabel(\"2'nd Principal Component (Loading)\", color='red')\n\nText(0, 0.5, \"2'nd Principal Component (Loading)\")\n\n\n\n\n\nRecall that the signs and magnitudes of a component’s loadings tell us what kind of variation it’s captured. The first component (PC1) shows a contrast between large, powerful vehicles with poor gas milage, and smaller, more economical vehicles with good gas milage. We might call this the “Luxury/Economy” axis. The next figure shows that our four chosen features mostly vary along the Luxury/Economy axis.\n\n# Look at explained variance\nplot_variance(pca)\n\narray([&lt;Axes: title={'center': '% Explained Variance'}, xlabel='Component'&gt;,\n       &lt;Axes: title={'center': '% Cumulative Variance'}, xlabel='Component'&gt;],\n      dtype=object)\n\n\n\n\n\nLet’s also look at the MI scores of the components. Not surprisingly, PC1 is highly informative, though the remaining components, despite their small variance, still have a significant relationship with price. Examining those components could be worthwhile to find relationships not captured by the main Luxury/Economy axis.\n\nmi_scores = make_mi_scores(X_pca, y, discrete_features=False)\nmi_scores\n\nPC1    1.013739\nPC2    0.379649\nPC3    0.306207\nPC4    0.204905\nName: MI Scores, dtype: float64\n\n\nThe third component shows a contrast between horsepower and curb_weight – sports cars vs. wagons, it seems.\n\n# Show dataframe sorted by PC3\nidx = X_pca[\"PC3\"].sort_values(ascending=False).index\ncols = [\"make\", \"body_style\", \"horsepower\", \"curb_weight\"]\ndf.loc[idx, cols]\n\n\n  \n    \n      \n\n\n\n\n\n\nmake\nbody_style\nhorsepower\ncurb_weight\n\n\n\n\n118\nporsche\nhardtop\n207\n2756\n\n\n117\nporsche\nhardtop\n207\n2756\n\n\n119\nporsche\nconvertible\n207\n2800\n\n\n45\njaguar\nsedan\n262\n3950\n\n\n96\nnissan\nhatchback\n200\n3139\n\n\n...\n...\n...\n...\n...\n\n\n59\nmercedes-benz\nwagon\n123\n3750\n\n\n61\nmercedes-benz\nsedan\n123\n3770\n\n\n101\npeugot\nwagon\n95\n3430\n\n\n105\npeugot\nwagon\n95\n3485\n\n\n143\ntoyota\nwagon\n62\n3110\n\n\n\n\n\n193 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nTo express this contrast, let’s create a new ratio feature:\n\ndf[\"sports_or_wagon\"] = X.curb_weight / X.horsepower\nsns.regplot(x=\"sports_or_wagon\", y='price', data=df, order=2);"
  },
  {
    "objectID": "05_Feature_selection_extraction.html#manifold-learning",
    "href": "05_Feature_selection_extraction.html#manifold-learning",
    "title": "5  Feature selection and extraction",
    "section": "5.4 Manifold learning",
    "text": "5.4 Manifold learning\n\n5.4.0.1 t-SNE\n\ndigits = load_digits()\n\n\nX = digits.images.reshape(-1, digits.images.shape[1]*digits.images.shape[2])\nX.shape\n\n(1797, 64)\n\n\n\nfig, ax_array = plt.subplots(5, 5)\naxes = ax_array.flatten()\nfor i, ax in enumerate(axes):\n    ax.imshow(digits.images[i], cmap='gray_r')\nplt.setp(axes, xticks=[], yticks=[], frame_on=False)\nplt.tight_layout(h_pad=0.5, w_pad=0.01)\n\n\n\n\n\notsne = oTSNE(\n    n_components=2,\n    perplexity=30,\n    initialization='pca', \n    n_jobs=2,\n    random_state=0,\n    negative_gradient_method='auto', \n    verbose=True,\n)\n\n\nembedding = otsne.fit(X)\n\n--------------------------------------------------------------------------------\nTSNE(early_exaggeration=12, n_jobs=2, random_state=0, verbose=True)\n--------------------------------------------------------------------------------\n===&gt; Finding 90 nearest neighbors using Annoy approximate search using euclidean distance...\n   --&gt; Time elapsed: 0.79 seconds\n===&gt; Calculating affinity matrix...\n   --&gt; Time elapsed: 0.08 seconds\n===&gt; Calculating PCA-based initialization...\n   --&gt; Time elapsed: 0.01 seconds\n===&gt; Running optimization with exaggeration=12.00, lr=149.75 for 250 iterations...\nIteration   50, KL divergence 2.6162, 50 iterations in 0.5615 sec\nIteration  100, KL divergence 2.6249, 50 iterations in 0.4249 sec\nIteration  150, KL divergence 2.6110, 50 iterations in 0.5087 sec\nIteration  200, KL divergence 2.6054, 50 iterations in 0.4346 sec\nIteration  250, KL divergence 2.6028, 50 iterations in 0.4644 sec\n   --&gt; Time elapsed: 2.40 seconds\n===&gt; Running optimization with exaggeration=1.00, lr=1797.00 for 500 iterations...\nIteration   50, KL divergence 0.9203, 50 iterations in 0.4606 sec\nIteration  100, KL divergence 0.8272, 50 iterations in 0.4424 sec\nIteration  150, KL divergence 0.7951, 50 iterations in 0.4422 sec\nIteration  200, KL divergence 0.7783, 50 iterations in 0.4284 sec\nIteration  250, KL divergence 0.7685, 50 iterations in 0.4518 sec\nIteration  300, KL divergence 0.7625, 50 iterations in 1.4672 sec\nIteration  350, KL divergence 0.7580, 50 iterations in 1.2427 sec\nIteration  400, KL divergence 0.7539, 50 iterations in 0.4364 sec\nIteration  450, KL divergence 0.7514, 50 iterations in 0.4209 sec\nIteration  500, KL divergence 0.7492, 50 iterations in 1.7576 sec\n   --&gt; Time elapsed: 7.56 seconds\n\n\n\nplt.figure(figsize=(10, 8))\nplt.scatter(embedding[:, 0], embedding[:, 1], c=digits.target, cmap='Spectral', s=5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\nplt.title('tSNE of the Digits dataset', fontsize=24)\n\nText(0.5, 1.0, 'tSNE of the Digits dataset')\n\n\n\n\n\n\n\n5.4.0.2 UMAP\nUMAP is useful for generating visualisations, but if you want to make use of UMAP more generally for machine learning tasks it is important to be be able to train a model and then later pass new data to the model and have it transform that data into the learned space. For example if we use UMAP to learn a latent space and then train a classifier on data transformed into the latent space then the classifier is only useful for prediction if we can transform data for which we want a prediction into the latent space the classifier uses.\n\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, stratify=digits.target, random_state=42)\n\nNow to get a benchmark idea of what we are looking at let’s train a couple of different classifiers and then see how well they score on the test set. For this example let’s try a support vector classifier and a KNN classifier.\n\nsvc = SVC(gamma='auto').fit(X_train, y_train)\nknn = KNeighborsClassifier().fit(X_train, y_train)\nsvc.score(X_test, y_test), knn.score(X_test, y_test)\n\n(0.62, 0.9844444444444445)\n\n\nThe goal now is to make use of UMAP as a preprocessing step that one could potentially fit into a pipeline.\n\ntrans = umap.UMAP(n_neighbors=5, random_state=42).fit(X_train)\n\n\nplt.figure(figsize=(10, 8))\nplt.scatter(trans.embedding_[:, 0], trans.embedding_[:, 1], c=y_train, cmap='Spectral', s=5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\nplt.title('Umap of the Digits dataset', fontsize=24)\n\nText(0.5, 1.0, 'Umap of the Digits dataset')\n\n\n\n\n\nThis looks very promising! Most of the classes got very cleanly separated, and that gives us some hope that it could help with classifier performance. We can now train some new models (again an SVC and a KNN classifier) on the embedded training data. This looks exactly as before but now we pass it the embedded data.\n\nsvc = SVC(gamma='auto').fit(trans.embedding_, y_train)\nknn = KNeighborsClassifier().fit(trans.embedding_, y_train)\n\n\ntest_embedding = trans.transform(X_test)\n\nThe next important question is what the transform did to our test data. In principle we have a new two dimensional representation of the test-set, and ideally this should be based on the existing embedding of the training set\n\nplt.figure(figsize=(10, 8))\nplt.scatter(test_embedding[:, 0], test_embedding[:, 1], c=y_test, cmap='Spectral', s=5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\nplt.title('Umap of the Digits dataset', fontsize=24)\n\nText(0.5, 1.0, 'Umap of the Digits dataset')\n\n\n\n\n\nThe results look like what we should expect; the test data has been embedded into two dimensions in exactly the locations we should expect (by class) given the embedding of the training data visualised above. This means we can now try out models that were trained on the embedded training data by handing them the newly transformed test set.\n\ntrans.transform(X_test)\n\narray([[ 4.73177004e+00,  2.46851373e+00],\n       [ 1.91173019e+01,  3.47619963e+00],\n       [ 7.86290264e+00,  1.09972172e+01],\n       [-8.19267273e+00,  3.35794115e+00],\n       [ 7.09956217e+00,  1.42389803e+01],\n       [ 1.32900066e+01,  1.82322578e+01],\n       [ 2.65943050e-01, -8.95168245e-01],\n       [ 8.13252068e+00,  1.00992136e+01],\n       [ 5.73924875e+00,  2.07148552e+00],\n       [ 5.40985489e+00,  1.51770325e+01],\n       [ 6.47868681e+00,  1.46163197e+01],\n       [ 7.92969179e+00,  9.91152573e+00],\n       [ 1.17540598e+01, -5.85403776e+00],\n       [ 1.21744642e+01, -6.17784595e+00],\n       [ 6.87150419e-01, -2.17636895e+00],\n       [ 1.28335676e+01,  1.65336361e+01],\n       [ 6.92930698e+00,  1.49666691e+01],\n       [ 1.00950708e+01,  6.95953965e-01],\n       [ 3.05954218e+00,  4.18441391e+00],\n       [ 1.22419624e+01,  1.81537476e+01],\n       [ 8.41183376e+00,  1.01441069e+01],\n       [ 9.57836246e+00,  2.75005311e-01],\n       [ 1.93220310e+01,  2.32694030e+00],\n       [ 1.28659058e+01,  1.85482330e+01],\n       [ 1.96019783e+01,  2.01119971e+00],\n       [ 4.99640083e+00,  1.86033607e+00],\n       [ 1.13867264e+01, -6.41329575e+00],\n       [ 2.00881138e+01,  3.05871224e+00],\n       [ 3.00027013e+00, -8.01352322e-01],\n       [ 2.92265922e-01, -2.01319861e+00],\n       [ 1.41106904e+00,  9.47769833e+00],\n       [ 2.43231082e+00,  1.04742994e+01],\n       [ 4.24076748e+00,  3.97745585e+00],\n       [-6.14816606e-01, -1.45310473e+00],\n       [ 1.03815241e+01,  1.65336680e+00],\n       [ 1.33825254e+01,  1.69268723e+01],\n       [ 7.61123228e+00,  1.40565615e+01],\n       [ 5.55537558e+00,  2.05418849e+00],\n       [ 6.94975233e+00,  1.43994637e+01],\n       [ 8.03399944e+00,  1.41689272e+01],\n       [ 1.11499023e+01,  1.69359303e+00],\n       [ 1.93824215e+01,  1.83013022e+00],\n       [ 4.15738869e+00,  5.18572140e+00],\n       [ 3.60030508e+00,  5.01691294e+00],\n       [ 8.05832386e+00,  1.41819515e+01],\n       [ 3.83167720e+00,  4.41925764e+00],\n       [ 1.04558792e+01,  1.46102712e-01],\n       [ 1.96752357e+00,  9.15024090e+00],\n       [ 1.96036167e+01,  2.88023281e+00],\n       [ 4.75290745e-01, -3.02901536e-01],\n       [ 1.28543739e+01, -6.69067383e+00],\n       [ 7.68392706e+00,  1.35354586e+01],\n       [ 5.70900965e+00,  2.04589939e+00],\n       [ 1.27460394e+01, -7.56970310e+00],\n       [-8.09807491e+00,  3.45251179e+00],\n       [-8.20304966e+00,  3.34776187e+00],\n       [ 1.01384201e+01,  6.57227516e-01],\n       [ 1.05179510e+01,  1.68638420e+00],\n       [ 7.96865273e+00,  1.05273275e+01],\n       [ 4.90999174e+00,  1.58829975e+01],\n       [ 8.63133049e+00, -4.62174177e+00],\n       [ 7.42704582e+00,  1.51531487e+01],\n       [ 1.93333473e+01,  1.75126565e+00],\n       [ 7.81681252e+00,  1.01148701e+01],\n       [ 1.88073101e+01,  2.74154377e+00],\n       [ 1.29166088e+01, -6.52322578e+00],\n       [ 9.41298866e+00,  1.08080494e+00],\n       [ 6.16237783e+00, -9.58264709e-01],\n       [ 7.35584676e-01, -1.40368450e+00],\n       [ 7.49761915e+00,  9.53435230e+00],\n       [ 1.26329851e+01,  1.74112072e+01],\n       [ 6.09630585e+00, -2.83607483e-01],\n       [ 4.56159449e+00,  4.63957405e+00],\n       [ 5.21145630e+00,  1.55418911e+01],\n       [ 8.52060318e+00, -5.29649198e-01],\n       [ 1.76717103e+00,  9.06908798e+00],\n       [ 1.12594156e+01,  1.60219014e+00],\n       [ 1.06761303e+01,  5.61923206e-01],\n       [ 1.52136588e+00,  9.05504322e+00],\n       [ 1.22102089e+01,  1.73202934e+01],\n       [ 6.19346762e+00, -2.01016366e-01],\n       [ 1.95389805e+01,  2.73830152e+00],\n       [ 6.07237339e+00, -7.12067902e-01],\n       [ 1.18179951e+01, -7.24027014e+00],\n       [ 2.35402918e+00,  9.97527313e+00],\n       [ 8.05538595e-01, -1.55245471e+00],\n       [-5.48529863e-01, -1.44640124e+00],\n       [ 8.46719360e+00, -4.49677277e+00],\n       [ 2.63080096e+00, -4.21153259e+00],\n       [ 1.34470768e+01,  1.80865421e+01],\n       [ 7.93155432e+00,  1.32082472e+01],\n       [ 2.90374517e+00,  5.25399637e+00],\n       [ 1.21058340e+01, -5.75693417e+00],\n       [ 1.18059082e+01,  1.69832764e+01],\n       [-2.30677761e-02, -2.63910270e+00],\n       [ 7.50664759e+00,  1.45911045e+01],\n       [ 2.61786890e+00, -4.19906235e+00],\n       [ 6.76057696e-01, -2.03193378e+00],\n       [ 1.87298660e+01,  2.62258267e+00],\n       [ 7.26300764e+00,  1.50124683e+01],\n       [ 1.90552788e+01,  3.07881856e+00],\n       [ 6.09509850e+00, -7.56763577e-01],\n       [ 6.17576075e+00, -7.26652086e-01],\n       [ 7.98868608e+00,  9.83880329e+00],\n       [ 1.21262283e+01, -7.42956352e+00],\n       [ 9.36670208e+00,  9.97476220e-01],\n       [ 1.25175686e+01,  1.71377945e+01],\n       [ 6.26237488e+00,  1.43485527e+01],\n       [ 5.53839064e+00,  1.20771849e+00],\n       [ 1.95191860e+01,  3.09513426e+00],\n       [ 1.93319950e+01,  2.04565048e+00],\n       [ 7.90213203e+00,  1.01868477e+01],\n       [ 4.40916538e+00,  4.91170883e+00],\n       [ 1.59102690e+00,  5.91555834e+00],\n       [ 1.06758175e+01,  7.85464406e-01],\n       [ 8.50780201e+00, -4.53833485e+00],\n       [ 3.33708262e+00,  4.26565790e+00],\n       [ 1.03685141e+01,  1.33268273e+00],\n       [ 5.50983715e+00,  1.92333841e+00],\n       [ 9.89408302e+00,  7.60242999e-01],\n       [ 1.92521572e+01,  3.28466463e+00],\n       [ 1.93915577e+01,  1.80793393e+00],\n       [-9.59185779e-01, -1.43418705e+00],\n       [ 5.38180447e+00,  1.55832243e+01],\n       [-2.10883573e-01, -1.04512537e+00],\n       [ 6.69982815e+00, -7.31428787e-02],\n       [ 2.64970207e+00,  1.04629345e+01],\n       [ 2.70043421e+00, -4.25971699e+00],\n       [ 1.29440527e+01, -6.61230564e+00],\n       [ 9.27611649e-01, -9.55748022e-01],\n       [ 1.18965158e+01, -6.58824778e+00],\n       [ 1.23818903e+01,  1.64546566e+01],\n       [ 2.48696733e+00, -4.27661419e+00],\n       [ 9.49136913e-01, -8.76113117e-01],\n       [ 1.91355953e+01,  2.56769919e+00],\n       [ 2.03352299e+01,  3.30455041e+00],\n       [ 5.40721941e+00,  1.76240933e+00],\n       [ 1.20430918e+01,  1.79942932e+01],\n       [ 1.66636074e+00,  8.91286182e+00],\n       [ 6.12154388e+00,  1.49706059e+01],\n       [ 5.76479673e+00,  2.05096912e+00],\n       [ 1.26848288e+01,  1.63744678e+01],\n       [ 1.91420193e+01,  3.13646555e+00],\n       [ 7.81396389e+00,  1.00339603e+01],\n       [ 1.26516829e+01,  1.84274235e+01],\n       [ 3.53068799e-01, -9.54452991e-01],\n       [ 1.02022676e+01,  1.75975370e+00],\n       [ 4.81706905e+00,  2.32386041e+00],\n       [ 1.26537113e+01,  1.69497089e+01],\n       [ 1.23623486e+01,  1.78191795e+01],\n       [ 8.54515076e-01, -1.01536226e+00],\n       [ 7.05518866e+00,  1.40135460e+01],\n       [ 2.21469665e+00,  5.26067495e+00],\n       [ 2.56542659e+00,  5.00661182e+00],\n       [ 1.80082548e+00,  9.62335300e+00],\n       [ 1.07883806e+01,  1.61723030e+00],\n       [ 9.88454342e+00, -2.33417273e-01],\n       [ 8.17566204e+00,  9.67235184e+00],\n       [ 1.88418846e+01,  2.49431753e+00],\n       [ 1.28272562e+01, -6.51219225e+00],\n       [ 8.06352520e+00,  1.05998135e+01],\n       [ 1.97371750e+01,  3.02595925e+00],\n       [ 8.25599575e+00,  1.07834330e+01],\n       [ 1.17708435e+01,  1.67840595e+01],\n       [ 1.96602249e+01,  2.35182858e+00],\n       [-4.44723725e-01, -1.08014917e+00],\n       [ 1.22830667e+01,  1.63258533e+01],\n       [ 1.00864286e+01,  1.41382837e+00],\n       [ 9.63279915e+00,  1.22358000e+00],\n       [ 1.17074785e+01,  1.73078613e+01],\n       [ 4.91881818e-01, -1.86782694e+00],\n       [ 9.27524626e-01, -3.64588648e-01],\n       [ 1.03519487e+01,  1.71525097e+00],\n       [ 1.21878176e+01,  1.66502571e+01],\n       [ 2.54178166e+00, -4.13714075e+00],\n       [-7.94874549e-01, -1.32138753e+00],\n       [ 8.94740105e+00, -7.02543706e-02],\n       [ 1.60824668e+00,  9.57526588e+00],\n       [ 7.69076777e+00,  1.38979092e+01],\n       [ 1.19308853e+01, -5.87281370e+00],\n       [ 1.96669064e+01,  2.94271183e+00],\n       [ 4.00215197e+00,  5.47322416e+00],\n       [ 1.04111280e+01,  6.09881997e-01],\n       [ 7.43757534e+00,  1.38516932e+01],\n       [ 4.26444292e+00,  4.87076139e+00],\n       [ 2.90101218e+00,  8.00403118e+00],\n       [ 7.60269547e+00,  1.41803226e+01],\n       [ 1.09815300e+00, -4.20335007e+00],\n       [ 3.72613168e+00,  5.09122086e+00],\n       [ 2.86066222e+00,  1.06173820e+01],\n       [ 2.42815518e+00, -4.00461817e+00],\n       [ 2.61127090e+00,  5.14484406e+00],\n       [ 1.18339100e+01,  1.67339821e+01],\n       [ 1.22553959e+01, -6.08764076e+00],\n       [ 4.20773602e+00,  4.65283203e+00],\n       [ 8.62094879e+00, -4.78529274e-01],\n       [ 1.29443531e+01, -6.50885916e+00],\n       [ 5.15094805e+00,  3.08955741e+00],\n       [ 1.90184188e+00, -4.20560122e+00],\n       [ 7.78751183e+00,  1.03105097e+01],\n       [ 4.66492891e+00,  4.62675238e+00],\n       [ 7.20657921e+00,  1.00898695e+01],\n       [ 1.23361588e+01, -7.76199770e+00],\n       [ 7.19676876e+00,  1.45420494e+01],\n       [-8.28833485e+00,  3.26244545e+00],\n       [ 1.93086472e+01,  2.02614713e+00],\n       [ 8.56536388e+00,  1.08816519e+01],\n       [ 1.56364882e+00,  9.02052307e+00],\n       [ 1.93720913e+01,  3.13724089e+00],\n       [ 1.03784208e+01,  1.45391178e+00],\n       [ 1.01358051e+01, -4.34333980e-01],\n       [ 7.30056190e+00,  1.39617376e+01],\n       [ 7.31627369e+00,  1.48641815e+01],\n       [ 2.33456850e+00,  8.97941303e+00],\n       [ 2.09031320e+00,  9.15919304e+00],\n       [ 1.35915003e+01,  1.69954529e+01],\n       [ 2.61211252e+00,  9.67325783e+00],\n       [ 5.70830059e+00,  2.07967734e+00],\n       [ 2.03164749e+01,  2.60851431e+00],\n       [ 1.23774891e+01,  1.72216015e+01],\n       [ 9.85767186e-01, -8.03058326e-01],\n       [ 6.18093252e+00, -2.71137953e-01],\n       [ 1.33752537e+01,  1.74806652e+01],\n       [ 7.87491941e+00,  1.15665522e+01],\n       [ 1.19647760e+01,  1.65779037e+01],\n       [ 1.58233976e+00,  8.97871685e+00],\n       [ 1.11217403e+01, -7.53401709e+00],\n       [ 2.77137256e+00,  1.07307339e+01],\n       [ 1.98257885e+01,  3.34123826e+00],\n       [ 9.88493085e-01, -6.36084318e-01],\n       [-5.42631149e-01, -1.49713326e+00],\n       [ 6.00486565e+00, -1.04312032e-01],\n       [ 5.50934029e+00,  1.54812260e+01],\n       [ 6.79059935e+00,  1.43414955e+01],\n       [ 8.20319176e+00,  1.14464417e+01],\n       [ 1.51997113e+00,  9.93821526e+00],\n       [ 9.26871109e+00,  8.75526607e-01],\n       [ 1.15349522e+01, -7.81715679e+00],\n       [ 2.84447312e+00,  4.09177399e+00],\n       [ 3.01040769e+00,  1.07021465e+01],\n       [ 5.26517034e-01,  3.56633514e-01],\n       [ 2.23956490e+00,  1.05628624e+01],\n       [ 7.40757895e+00,  1.05235777e+01],\n       [ 7.57580614e+00,  1.39888668e+01],\n       [ 1.19394760e+01, -5.87033081e+00],\n       [ 1.19141283e+01, -7.70306826e+00],\n       [ 1.45862615e+00,  9.67681885e+00],\n       [ 5.70191717e+00,  1.50980463e+01],\n       [ 1.27075195e+01,  1.71650524e+01],\n       [ 1.90146370e+01,  2.91956520e+00],\n       [ 1.00382977e+01,  1.05415595e+00],\n       [ 1.20603743e+01, -6.73856401e+00],\n       [ 1.21463318e+01, -5.93729019e+00],\n       [ 2.98728800e+00,  5.01034117e+00],\n       [ 4.37886333e+00,  5.13170195e+00],\n       [ 5.71549463e+00,  9.21205699e-01],\n       [ 1.97396123e+00,  8.89696503e+00],\n       [ 1.93121815e+01,  3.62117839e+00],\n       [ 1.83073807e+00, -4.12922049e+00],\n       [ 1.35204191e+01,  1.69217930e+01],\n       [ 3.89947939e+00,  4.44158363e+00],\n       [ 2.01038971e+01,  3.17173958e+00],\n       [ 1.76283216e+00,  8.74818230e+00],\n       [ 1.39133549e+00,  9.86009979e+00],\n       [ 4.96045542e+00,  2.09638572e+00],\n       [ 5.37798691e+00,  2.55944920e+00],\n       [ 1.00153885e+01, -4.30421829e-01],\n       [ 2.58003759e+00,  5.65518475e+00],\n       [ 3.10385394e+00,  4.31488323e+00],\n       [ 8.06058502e+00,  1.08455706e+01],\n       [ 5.15572214e+00,  2.80767560e+00],\n       [ 7.04855144e-01, -2.10322872e-01],\n       [ 1.32493429e+01,  1.70429554e+01],\n       [ 1.25905075e+01, -6.49386692e+00],\n       [-8.13499737e+00,  3.41613173e+00],\n       [ 1.97236271e+01,  3.78322840e+00],\n       [ 8.44617176e+00, -4.47045088e+00],\n       [ 2.22543931e+00,  8.94437122e+00],\n       [ 7.39531803e+00,  9.88773346e+00],\n       [ 4.95809615e-01, -1.17228663e+00],\n       [ 3.56299803e-02, -2.51918101e+00],\n       [ 9.97978687e-01, -9.73300874e-01],\n       [ 7.88709497e+00,  1.02189322e+01],\n       [ 1.19830122e+01, -6.20376873e+00],\n       [ 7.27090883e+00,  1.48665085e+01],\n       [ 7.90857124e+00,  1.04754467e+01],\n       [ 7.78450203e+00,  1.34945431e+01],\n       [ 9.20992315e-01, -3.84849370e-01],\n       [ 2.06664467e+00,  8.91980267e+00],\n       [ 6.82216120e+00,  1.47176342e+01],\n       [ 7.96708155e+00,  9.88164520e+00],\n       [ 9.99245048e-01, -1.10285509e+00],\n       [ 4.92387438e+00,  3.11497521e+00],\n       [ 4.15050173e+00,  5.49935293e+00],\n       [ 1.85929432e+01,  2.93218923e+00],\n       [ 1.05673771e+01,  1.58918750e+00],\n       [ 1.17832613e+01, -6.88497448e+00],\n       [ 2.05436015e+00,  8.92279243e+00],\n       [-8.32482147e+00,  3.22605968e+00],\n       [-6.68644428e-01, -1.44689846e+00],\n       [ 1.26672497e+01, -6.95247126e+00],\n       [ 4.02511883e+00,  4.68911886e+00],\n       [ 1.83233964e+00,  1.02191486e+01],\n       [ 1.24769630e+01,  1.83115978e+01],\n       [ 6.54460144e+00,  8.17592907e+00],\n       [ 4.99710226e+00,  2.14675093e+00],\n       [ 8.82683563e+00, -3.46012086e-01],\n       [ 5.66530418e+00,  2.03878736e+00],\n       [ 1.12690344e+01,  1.65654433e+00],\n       [ 1.94616604e+01,  1.83572781e+00],\n       [ 4.22863197e+00,  4.24069071e+00],\n       [ 1.32883120e+01,  1.72658978e+01],\n       [ 2.04569340e+00,  9.79823208e+00],\n       [ 1.16519632e+01, -5.80133963e+00],\n       [ 2.73238510e-01, -1.97288191e+00],\n       [ 1.37230902e+01,  1.69941349e+01],\n       [ 2.34973192e+00,  9.78479385e+00],\n       [-8.32408810e+00,  3.22686195e+00],\n       [ 5.00070572e+00,  2.14611840e+00],\n       [ 1.94834633e+01,  3.31213975e+00],\n       [ 5.13502932e+00,  2.53799510e+00],\n       [ 3.51085663e+00,  4.86806011e+00],\n       [ 1.60497224e+00,  1.02030182e+01],\n       [ 1.77237749e+00,  9.68281555e+00],\n       [ 1.01137390e+01,  6.01305068e-01],\n       [ 1.24101944e+01,  1.69083271e+01],\n       [ 9.99769878e+00, -4.69175726e-01],\n       [ 1.92375374e+01,  2.82835078e+00],\n       [ 1.29148035e+01, -6.90968513e+00],\n       [ 4.79194498e+00,  1.79446292e+00],\n       [ 1.09801130e+01, -7.14352322e+00],\n       [ 4.43158436e+00,  4.53871965e+00],\n       [ 1.19143171e+01, -5.24801064e+00],\n       [ 8.03468132e+00,  1.08030205e+01],\n       [ 1.25794535e+01,  1.83805733e+01],\n       [ 1.23168669e+01,  1.66269989e+01],\n       [ 1.23458223e+01,  1.77778034e+01],\n       [ 9.14830303e+00,  2.24295601e-01],\n       [ 2.66571760e+00, -4.23472404e+00],\n       [ 5.79700947e+00,  2.10118103e+00],\n       [ 4.93734884e+00,  2.10275912e+00],\n       [ 8.59681225e+00, -6.13303065e-01],\n       [ 4.52130413e+00,  4.89938354e+00],\n       [ 1.25310907e+01,  1.77465248e+01],\n       [ 4.70883727e-01, -2.58479118e-01],\n       [ 7.51077795e+00,  1.44778681e+01],\n       [ 3.78150916e+00,  4.56379843e+00],\n       [ 5.95912027e+00,  3.97240400e-01],\n       [ 3.72255659e+00,  5.11259413e+00],\n       [ 1.19461737e+01,  1.67762375e+01],\n       [ 9.08414245e-01, -7.92203903e-01],\n       [ 2.96357393e+00,  1.00844154e+01],\n       [ 1.17556305e+01, -6.66913843e+00],\n       [ 2.02050533e+01,  3.61629868e+00],\n       [ 1.10305328e+01, -7.52728462e+00],\n       [ 1.96918893e+00,  9.84622478e+00],\n       [ 4.19275188e+00,  5.32430649e+00],\n       [ 7.87172747e+00,  1.34470758e+01],\n       [ 1.98914948e+01,  2.00160146e+00],\n       [ 8.35500717e+00,  9.87718010e+00],\n       [ 9.96109867e+00,  8.08443546e-01],\n       [ 1.18106031e+01, -5.74526215e+00],\n       [ 5.16102409e+00,  1.80734324e+00],\n       [ 1.06605225e+01,  5.83091080e-01],\n       [ 5.54406452e+00,  1.55050116e+01],\n       [-8.23605418e-01, -1.30636895e+00],\n       [ 1.74808240e+00, -4.07517624e+00],\n       [ 9.26349831e+00,  8.88854027e-01],\n       [ 1.09849749e+01, -7.48004770e+00],\n       [ 1.20971031e+01,  1.70955334e+01],\n       [ 7.80687809e+00,  1.46189804e+01],\n       [ 7.74820900e+00,  1.09906073e+01],\n       [ 2.86658764e+00,  8.10734367e+00],\n       [ 5.48915911e+00,  1.53481274e+01],\n       [ 3.66877079e+00,  4.72051382e+00],\n       [ 9.71720600e+00,  1.88182616e+00],\n       [ 1.29751072e+01, -6.48790073e+00],\n       [ 4.74673939e+00,  2.45048618e+00],\n       [ 6.65666580e+00,  1.45971031e+01],\n       [ 8.40965176e+00,  1.14738111e+01],\n       [-8.10725212e+00,  3.44368553e+00],\n       [ 7.50969601e+00,  1.05435114e+01],\n       [ 2.85458922e+00,  1.05950384e+01],\n       [ 9.97610331e-01, -4.11940002e+00],\n       [ 1.61350918e+00,  9.61839390e+00],\n       [ 7.60637164e-01, -2.08404779e+00],\n       [ 1.08878446e+00, -4.20332289e+00],\n       [ 1.71200943e+00,  1.01663733e+01],\n       [ 1.68586135e+00,  9.51459694e+00],\n       [ 9.96205997e+00, -4.65066135e-01],\n       [ 2.30960464e+00,  9.90717316e+00],\n       [ 7.35355377e-01, -1.99536717e+00],\n       [ 6.53025198e+00,  8.15707302e+00],\n       [ 2.84418404e-01, -4.09361899e-01],\n       [ 1.47742963e+00,  8.79598904e+00],\n       [ 1.94371700e+01,  2.68893600e+00],\n       [ 6.51380682e+00,  1.46814518e+01],\n       [ 1.20655746e+01, -6.61174297e+00],\n       [ 2.61204028e+00, -4.18903923e+00],\n       [ 1.27581778e+01,  1.74870110e+01],\n       [ 7.28155994e+00,  1.42150888e+01],\n       [ 7.64708996e+00,  1.37616825e+01],\n       [ 1.20456152e+01, -5.99717808e+00],\n       [ 5.90163994e+00, -5.59923708e-01],\n       [ 9.67985809e-01, -1.10858858e+00],\n       [ 1.89800892e+01,  3.22926378e+00],\n       [ 1.28227119e+01, -6.45035076e+00],\n       [ 2.41951394e+00,  1.07949047e+01],\n       [ 1.95769138e+01,  3.33323169e+00],\n       [ 1.95406742e+01,  3.64278507e+00],\n       [ 4.94027615e+00,  3.08518648e+00],\n       [ 1.16152763e+01, -7.42098475e+00],\n       [ 1.89538881e-01, -2.01126289e+00],\n       [ 3.81815052e+00,  4.59767532e+00],\n       [ 2.12944388e+00,  1.01413727e+01],\n       [ 5.31503677e+00,  1.56595840e+01],\n       [ 7.29801846e+00,  1.01420183e+01],\n       [ 2.09905934e+00,  5.28698969e+00],\n       [ 2.07446499e+01,  3.95429325e+00],\n       [ 8.20926762e+00,  1.09102173e+01],\n       [ 1.08266563e+01,  7.70246744e-01],\n       [ 6.07540846e+00, -7.29724586e-01],\n       [ 1.33210297e+01,  1.75505371e+01],\n       [ 1.24755239e+01,  1.77963142e+01],\n       [ 8.17131042e+00,  9.57447624e+00],\n       [ 1.85363064e+01,  2.59018946e+00],\n       [ 1.92491379e+01,  1.91278136e+00],\n       [ 1.18102551e+01,  1.75860405e+01],\n       [ 1.96883640e+01,  2.18472767e+00],\n       [ 9.52509582e-01, -1.43717289e+00],\n       [ 6.90265989e+00,  1.45218258e+01],\n       [ 5.67792988e+00,  1.57095594e+01],\n       [ 4.19719028e+00,  4.65386629e+00],\n       [ 1.18064594e+01,  1.68005753e+01],\n       [ 4.59421349e+00,  4.83855247e+00],\n       [ 1.28239794e+01, -6.72075176e+00],\n       [-1.79536175e-02, -1.28720593e+00],\n       [ 1.02263308e+01,  8.62763345e-01],\n       [ 7.40937901e+00,  1.00602980e+01],\n       [ 6.83069372e+00,  1.49400043e+01],\n       [-1.60397947e-01, -1.87085092e+00],\n       [ 1.31290855e+01,  1.73606892e+01],\n       [-8.06395912e+00,  3.48659539e+00],\n       [ 3.60813916e-01, -2.49946070e+00],\n       [ 3.95752883e+00,  5.46306419e+00],\n       [ 3.21658778e+00,  5.64122772e+00],\n       [ 1.89867115e+01,  2.76880908e+00],\n       [ 1.43908429e+00,  9.82541084e+00],\n       [ 8.73421192e+00, -4.49178368e-01],\n       [ 4.20121336e+00,  4.74164057e+00]], dtype=float32)\n\n\n\nsvc.score(trans.transform(X_test), y_test), knn.score(trans.transform(X_test), y_test)\n\n(0.98, 0.98)\n\n\nThe results are pretty good. While the accuracy of the KNN classifier did not improve there was not a lot of scope for improvement given the data. On the other hand the SVC has improved to have equal accuracy to the KNN classifier!\nFor more interesting datasets the larger dimensional embedding might have been a significant gain – it is certainly worth exploring as one of the parameters in a grid search across a pipeline that includes UMAP."
  },
  {
    "objectID": "05_Feature_selection_extraction.html#clustering",
    "href": "05_Feature_selection_extraction.html#clustering",
    "title": "5  Feature selection and extraction",
    "section": "5.5 Clustering",
    "text": "5.5 Clustering\nWhen used for feature engineering, we could attempt to discover groups of customers representing a market segment, for instance, or geographic areas that share similar weather patterns. Adding a feature of cluster labels can help machine learning models untangle complicated relationships of space or proximity.\n\n5.5.1 Cluster Labels as a feature\nApplied to a single real-valued feature, clustering acts like a traditional “binning” or “discretization” transform. On multiple features, it’s like “multi-dimensional binning” (sometimes called vector quantization).\nIt’s important to remember that this Cluster feature is categorical. Here, it’s shown with a label encoding (that is, as a sequence of integers) as a typical clustering algorithm would produce; depending on your model, a one-hot encoding may be more appropriate. The motivating idea for adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks. Our model can then just learn the simpler chunks one-by-one instead having to learn the complicated whole all at once. It’s a “divide and conquer” strategy.\nAs spatial features, California Housing’s 'Latitude' and 'Longitude' make natural candidates for k-means clustering. In this example we’ll cluster these with 'MedInc' (median income) to create economic segments in different regions of California. Since k-means clustering is sensitive to scale, it can be a good idea rescale or normalize data with extreme values. Our features are already roughly on the same scale, so we’ll leave them as-is.\n\ndf = fetch_california_housing(as_frame=True)['frame']\n\n\nX = df.loc[:, [\"MedInc\", \"Latitude\", \"Longitude\"]]\nX.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nMedInc\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n37.88\n-122.23\n\n\n1\n8.3014\n37.86\n-122.22\n\n\n2\n7.2574\n37.85\n-122.24\n\n\n3\n5.6431\n37.85\n-122.25\n\n\n4\n3.8462\n37.85\n-122.25\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# Create cluster feature\nkmeans = KMeans(n_clusters = 6, n_init = 10)\nX[\"Cluster\"] = kmeans.fit_predict(X)\nX[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")\n\nX.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nMedInc\nLatitude\nLongitude\nCluster\n\n\n\n\n0\n8.3252\n37.88\n-122.23\n5\n\n\n1\n8.3014\n37.86\n-122.22\n5\n\n\n2\n7.2574\n37.85\n-122.24\n5\n\n\n3\n5.6431\n37.85\n-122.25\n5\n\n\n4\n3.8462\n37.85\n-122.25\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNotice the differnece between predict() and transform() in the KMeans. predict() will predict the closest cluster each sample in X belongs to. transform() will transform data to a cluster-distance space where each dimension is the distance to the cluster centers.\nNow let’s look at a couple plots to see how effective this was. First, a scatter plot that shows the geographic distribution of the clusters. It seems like the algorithm has created separate segments for higher-income areas on the coasts.\n\nsns.relplot(\n    x=\"Longitude\", y=\"Latitude\", hue=\"Cluster\", data=X, height=6,\n);\n\n\n\n\nThe target in this dataset is MedHouseVal (median house value). These box-plots show the distribution of the target within each cluster. If the clustering is informative, these distributions should, for the most part, separate across MedHouseVal, which is indeed what we see.\n\nX[\"MedHouseVal\"] = df[\"MedHouseVal\"]\nsns.catplot(x=\"MedHouseVal\", y=\"Cluster\", data=X, kind=\"boxen\", height=6);\n\n\n\n\n\n\n5.5.2 Cluster distance as a feature\n\nX_digits, y_digits = load_digits(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)\n\nNow let’s fit a Logistic Regression model and evaluate it on the test set:\n\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\nlog_reg.fit(X_train, y_train)\n\nLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)\n\n\n\nlog_reg_score = log_reg.score(X_test, y_test)\nlog_reg_score\n\n0.9688888888888889\n\n\nOkay, that’s our baseline: 96.89% accuracy. Let’s see if we can do better by using K-Means as a preprocessing step. We will create a pipeline that will first cluster the training set into 30 clusters and replace the images with their distances to the 30 clusters, then apply a logistic regression model:\n\npipeline = Pipeline([\n    (\"kmeans\", KMeans(n_clusters=30, random_state=42, n_init=10)),\n    (\"log_reg\", LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)),\n])\npipeline.fit(X_train, y_train)\n\nPipeline(steps=[('kmeans', KMeans(n_clusters=30, n_init=10, random_state=42)),\n                ('log_reg',\n                 LogisticRegression(max_iter=5000, multi_class='ovr',\n                                    random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('kmeans', KMeans(n_clusters=30, n_init=10, random_state=42)),\n                ('log_reg',\n                 LogisticRegression(max_iter=5000, multi_class='ovr',\n                                    random_state=42))])KMeansKMeans(n_clusters=30, n_init=10, random_state=42)LogisticRegressionLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)\n\n\n\nX_train.shape\n\n(1347, 64)\n\n\n\npipeline_score = pipeline.score(X_test, y_test)\npipeline_score\n\n0.9733333333333334\n\n\nHow much did the error rate drop?\n\n1 - (1 - pipeline_score) / (1 - log_reg_score)\n\n0.1428571428571439\n\n\nWe reduced the error rate by over 14%!"
  },
  {
    "objectID": "05_Feature_selection_extraction.html#guideline-to-determine-the-optimal-number-of-features-or-threshold",
    "href": "05_Feature_selection_extraction.html#guideline-to-determine-the-optimal-number-of-features-or-threshold",
    "title": "5  Feature selection and extraction",
    "section": "5.6 Guideline to determine the optimal number of features or threshold?",
    "text": "5.6 Guideline to determine the optimal number of features or threshold?\nTo determine the optimal hyperparameter, we can use cross validation. For instance, in the above example, we chose the number of clusters k completely arbitrarily. However, we can surely do better. Since K-Means is just a preprocessing step in a classification pipeline, finding a good value for k is the best value of k is simply the one that results in the best classification performance.\n\n# The following cell may take close to 20 minutes to run, or more depending on your hardware!\nparam_grid = dict(kmeans__n_clusters=range(2, 60))\ngrid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)\ngrid_clf.fit(X_train, y_train)\n\n\ngrid_clf.best_params_\n\n\ngrid_clf.score(X_test, y_test)\n\nIn the same way, you can also use cross-validation to evaluate model performance with different numbers of top-ranked features or different numbers of features and choose the optimal number based on the performance metric (e.g., highest accuracy or lowest error).\n\n5.6.1 Using Clustering for Semi-Supervised Learning\nAnother use case for clustering is in semi-supervised learning, when we have plenty of unlabeled instances and very few labeled instances.\n\nX_digits, y_digits = load_digits(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)\n\nLet’s look at the performance of a logistic regression model when we only have 50 labeled instances:\n\nn_labeled = 50\n\n\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\nlog_reg.fit(X_train[:n_labeled], y_train[:n_labeled])\nlog_reg.score(X_test, y_test)\n\n0.8333333333333334\n\n\nThe model’s accuracy is just 83.33%. It’s much less than earlier of course. Let’s see how we can do better. First, let’s cluster the training set into 50 clusters, then for each cluster let’s find the image closest to the centroid. We will call these images the representative images:\n\nk = 50\nkmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\nX_digits_dist = kmeans.fit_transform(X_train)\nrepresentative_digit_idx = np.argmin(X_digits_dist, axis=0)\nX_representative_digits = X_train[representative_digit_idx]\n\nNow let’s plot these representative images and label them manually:\n\nplt.figure(figsize=(8, 2))\nfor index, X_representative_digit in enumerate(X_representative_digits):\n    plt.subplot(k // 10, 10, index + 1)\n    plt.imshow(X_representative_digit.reshape(8, 8), cmap=\"binary\", interpolation=\"bilinear\")\n    plt.axis('off')\nplt.show()\n\n\n\n\n\n# Assuming we manually label these digits\ny_train[representative_digit_idx]\n\narray([4, 8, 0, 6, 8, 3, 7, 7, 9, 2, 5, 5, 8, 5, 2, 1, 2, 9, 6, 1, 1, 6,\n       9, 0, 8, 3, 0, 7, 4, 1, 6, 5, 2, 4, 1, 8, 6, 3, 9, 2, 4, 2, 9, 4,\n       7, 6, 2, 3, 1, 1])\n\n\n\ny_representative_digits = y_train[representative_digit_idx]\n\nNow we have a dataset with just 50 labeled instances, but instead of being completely random instances, each of them is a representative image of its cluster. Let’s see if the performance is any better:\n\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\nlog_reg.fit(X_representative_digits, y_representative_digits)\nlog_reg.score(X_test, y_test)\n\n0.9222222222222223\n\n\nWe jumped from 83.3% accuracy to 92.2%, although we are still only training the model on 50 instances. Since it’s often costly and painful to label instances, especially when it has to be done manually by experts, it’s a good idea to make them label representative instances rather than just random instances.\nBut perhaps we can go one step further: what if we propagated the labels to all the other instances in the same cluster?\n\ny_train_propagated = np.empty(len(X_train), dtype=np.int32)\nfor i in range(k):\n    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]\n\n\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\nlog_reg.fit(X_train, y_train_propagated)\n\nLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)\n\n\n\nlog_reg.score(X_test, y_test)\n\n0.9333333333333333\n\n\nWe got a tiny little accuracy boost. Better than nothing, but we should probably have propagated the labels only to the instances closest to the centroid, because by propagating to the full cluster, we have certainly included some outliers. Let’s only propagate the labels to the 75th percentile closest to the centroid:\n\npercentile_closest = 75\n\nX_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\nfor i in range(k):\n    in_cluster = (kmeans.labels_ == i)\n    cluster_dist = X_cluster_dist[in_cluster]\n    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n    above_cutoff = (X_cluster_dist &gt; cutoff_distance)\n    X_cluster_dist[in_cluster & above_cutoff] = -1\n\n\npartially_propagated = (X_cluster_dist != -1)\nX_train_partially_propagated = X_train[partially_propagated]\ny_train_partially_propagated = y_train_propagated[partially_propagated]\n\n\nX_train.shape, X_train_partially_propagated.shape\n\n((1347, 64), (1003, 64))\n\n\n\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\nlog_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n\nLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)\n\n\n\nlog_reg.score(X_test, y_test)\n\n0.9355555555555556\n\n\nA bit better. With just 50 labeled instances (just 5 examples per class on average!), we got 93.5% performance, which is getting closer to the performance of logistic regression on the fully labeled digits dataset.\nOur propagated labels are actually pretty good: their accuracy is about 97.5%:\n\nnp.mean(y_train_partially_propagated == y_train[partially_propagated])\n\n0.9750747756729811\n\n\nYou could also do a few iterations of active learning:\n\nManually label the instances that the classifier is least sure about, if possible by picking them in distinct clusters.\nTrain a new model with these additional labels.\n\n\n\n5.6.2 Feature agglomeration\ncluster.FeatureAgglomeration applies Hierarchical clustering to group together features that behave similarly.\n\nX, y = load_iris(return_X_y=True)\n\n\n#set n_clusters to 2, the output will be two columns of agglomerated features (iris has 4 features)\nagglo = FeatureAgglomeration(n_clusters=2).fit_transform(X)\n\n\nplt.scatter(agglo[:,0], agglo[:,1],c=y)\nplt.show()"
  },
  {
    "objectID": "05_Feature_selection_extraction.html#references",
    "href": "05_Feature_selection_extraction.html#references",
    "title": "5  Feature selection and extraction",
    "section": "5.7 References",
    "text": "5.7 References\n\nhttps://www.kaggle.com/learn/feature-engineering\nhttps://inria.github.io/scikit-learn-mooc/python_scripts/dev_features_importance.html#\nhttps://scikit-learn.org/stable/modules/feature_selection.html\nhttps://scikit-learn.org/stable/modules/preprocessing.html#\nhttps://scikit-learn.org/stable/modules/unsupervised_reduction.html\nhttps://github.com/ageron/handson-ml3/blob/main/09_unsupervised_learning.ipynb"
  },
  {
    "objectID": "06_XAI.html#setup",
    "href": "06_XAI.html#setup",
    "title": "6  Explainable AI",
    "section": "6.1 Setup",
    "text": "6.1 Setup\n\n!pip install --upgrade mlxtend -qq\n!pip install imodels -qq\n!pip install dtreeviz -qq\n!pip install lime -qq\n!pip install shap -qq\n\n\n# Scientific computing\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline\n\n# Preprocessing and datasets\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import fetch_openml\n\n# Modeling\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree, DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_absolute_error\nimport tensorflow as tf\n\n# Interpretable Models\nfrom imodels import RuleFitRegressor\nfrom imodels import OneRClassifier, BayesianRuleListClassifier, FIGSClassifier, HSTreeClassifierCV\nfrom imodels.discretization import ExtraBasicDiscretizer\nfrom imodels.tree.viz_utils import extract_sklearn_tree_from_figs\n\n# Model-Agnostic Methods\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.inspection import partial_dependence\n\n# Local methods\nimport lime\nimport lime.lime_tabular\nfrom lime import lime_image\nimport shap  # package used to calculate Shap values\n\n# Helper functions\nfrom skimage.io import imread\nfrom skimage.segmentation import mark_boundaries\nfrom scipy.io.arff import loadarff\nimport graphviz\nimport dtreeviz\n\nimport logging\nimport warnings\nlogging.getLogger('matplotlib.font_manager').setLevel(level=logging.CRITICAL) # For dtreeviz\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) # For shap\n\n\nif not tf.config.list_physical_devices('GPU'):\n    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n    if \"google.colab\" in sys.modules:\n        print(\"Go to Runtime &gt; Change runtime and select a GPU hardware \"\n              \"accelerator.\")\n    if \"kaggle_secrets\" in sys.modules:\n        print(\"Go to Settings &gt; Accelerator and select GPU.\")\n\n\n!gdown --fuzzy https://drive.google.com/file/d/1GzAuz0gkk5arJPYC7NgnCrkVWS4rvTtW/view?usp=sharing\n!gdown --fuzzy https://drive.google.com/file/d/1GvSaZ1E45e45ns5IH8Y6EoVu8C6Vhffy/view?usp=sharing"
  },
  {
    "objectID": "06_XAI.html#decsion-rule-based-modeld-by-imodels",
    "href": "06_XAI.html#decsion-rule-based-modeld-by-imodels",
    "title": "6  Explainable AI",
    "section": "6.2 Decsion-Rule based modeld by imodels",
    "text": "6.2 Decsion-Rule based modeld by imodels\nimodels provides a simple interface for fitting and using state-of-the-art interpretable models, all compatible with scikit-learn. These models can often replace black-box models (e.g. random forests) with simpler models (e.g. rule lists) while improving interpretability and computational efficiency, all without sacrificing predictive accuracy!\n\nnp.random.seed(13)\n\ndef get_ames_data():\n    try:\n        housing = fetch_openml(name=\"house_prices\", as_frame=True, parser='auto')\n    except:\n        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\n    housing_target = housing['target'].values\n    housing_data_numeric = housing['data'].select_dtypes('number').drop(columns=['Id']).dropna(axis=1)\n    feature_names = housing_data_numeric.columns.values\n    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n        housing_data_numeric.values, housing_target, test_size=0.75)\n    return X_train_reg, X_test_reg, y_train_reg, y_test_reg, feature_names\n\ndef get_diabetes_data():\n    '''load (classification) data on diabetes\n    '''\n    data = loadarff(\"diabetes.arff\")\n    data_np = np.array(list(map(lambda x: np.array(list(x)), data[0])))\n    X = data_np[:, :-1].astype('float32')\n    y_text = data_np[:, -1].astype('str')\n    y = (y_text == 'tested_positive').astype(int)  # labels 0-1\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75)  # split\n    feature_names = [\"#Pregnant\", \"Glucose concentration test\", \"Blood pressure(mmHg)\",\n                     \"Triceps skin fold thickness(mm)\",\n                     \"2-Hour serum insulin (mu U/ml)\", \"Body mass index\", \"Diabetes pedigree function\", \"Age (years)\"]\n    return X_train, X_test, y_train, y_test, feature_names\n\ndef viz_classification_preds(probs, y_test):\n    '''look at prediction breakdown\n    '''\n    plt.subplot(121)\n    plt.hist(probs[:, 1][y_test == 0], label='Class 0')\n    plt.hist(probs[:, 1][y_test == 1], label='Class 1', alpha=0.8)\n    plt.ylabel('Count')\n    plt.xlabel('Predicted probability of class 1')\n    plt.legend()\n\n    plt.subplot(122)\n    preds = np.argmax(probs, axis=1)\n    plt.title('ROC curve')\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, preds)\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.plot(fpr, tpr)\n    plt.tight_layout()\n    plt.show()\n\nThe Ames dataset is a housing dataset that use seveal conditions to predict the housing price. The diabetes dataset has a binary-valued variable. We would like to investigated whether the patient shows signs of diabetes according to World Health Organization criteria.\n\nX_train_reg, X_test_reg, y_train_reg, y_test_reg, feat_names_reg = get_ames_data()\nX_train, X_test, y_train, y_test, feat_names = get_diabetes_data()\n\n\npd.DataFrame(X_train_reg, columns=feat_names_reg)\n\n\n  \n    \n      \n\n\n\n\n\n\nMSSubClass\nLotArea\nOverallQual\nOverallCond\nYearBuilt\nYearRemodAdd\nBsmtFinSF1\nBsmtFinSF2\nBsmtUnfSF\nTotalBsmtSF\n...\nGarageArea\nWoodDeckSF\nOpenPorchSF\nEnclosedPorch\n3SsnPorch\nScreenPorch\nPoolArea\nMiscVal\nMoSold\nYrSold\n\n\n\n\n0\n50\n6435\n6\n5\n1939\n1950\n0\n0\n972\n972\n...\n312\n0\n0\n0\n0\n0\n0\n0\n10\n2006\n\n\n1\n20\n10200\n5\n7\n1954\n2003\n320\n362\n404\n1086\n...\n490\n0\n0\n0\n0\n0\n0\n0\n5\n2010\n\n\n2\n20\n9503\n5\n5\n1958\n1983\n457\n374\n193\n1024\n...\n484\n316\n28\n0\n0\n0\n0\n0\n6\n2007\n\n\n3\n60\n9000\n8\n5\n2008\n2008\n0\n0\n768\n768\n...\n676\n0\n30\n0\n0\n0\n0\n0\n6\n2009\n\n\n4\n80\n19690\n6\n7\n1966\n1966\n0\n0\n697\n697\n...\n432\n586\n236\n0\n0\n0\n738\n0\n8\n2006\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n360\n20\n10656\n8\n5\n2006\n2007\n0\n0\n1638\n1638\n...\n870\n192\n80\n0\n0\n0\n0\n0\n11\n2007\n\n\n361\n20\n8450\n7\n5\n2000\n2001\n0\n0\n1349\n1349\n...\n539\n120\n55\n0\n0\n0\n0\n0\n12\n2007\n\n\n362\n50\n5790\n3\n6\n1915\n1950\n0\n0\n840\n840\n...\n379\n0\n0\n202\n0\n0\n0\n0\n5\n2010\n\n\n363\n60\n10029\n6\n5\n1988\n1989\n831\n0\n320\n1151\n...\n521\n0\n228\n0\n0\n192\n0\n0\n9\n2007\n\n\n364\n20\n14145\n7\n7\n1984\n1998\n213\n0\n995\n1208\n...\n440\n108\n45\n0\n0\n0\n0\n400\n5\n2006\n\n\n\n\n\n365 rows × 33 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ny_train_reg[:10]\n\narray([140200, 144900, 144000, 210000, 274970, 218000, 167500, 195400,\n        76000, 246578])\n\n\n\npd.DataFrame(X_train, columns=feat_names)\n\n\n  \n    \n      \n\n\n\n\n\n\n#Pregnant\nGlucose concentration test\nBlood pressure(mmHg)\nTriceps skin fold thickness(mm)\n2-Hour serum insulin (mu U/ml)\nBody mass index\nDiabetes pedigree function\nAge (years)\n\n\n\n\n0\n3.0\n158.0\n76.0\n36.0\n245.0\n31.600000\n0.851\n28.0\n\n\n1\n8.0\n186.0\n90.0\n35.0\n225.0\n34.500000\n0.423\n37.0\n\n\n2\n2.0\n85.0\n65.0\n0.0\n0.0\n39.599998\n0.930\n27.0\n\n\n3\n3.0\n187.0\n70.0\n22.0\n200.0\n36.400002\n0.408\n36.0\n\n\n4\n6.0\n93.0\n50.0\n30.0\n64.0\n28.700001\n0.356\n23.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n187\n0.0\n165.0\n76.0\n43.0\n255.0\n47.900002\n0.259\n26.0\n\n\n188\n8.0\n181.0\n68.0\n36.0\n495.0\n30.100000\n0.615\n60.0\n\n\n189\n0.0\n111.0\n65.0\n0.0\n0.0\n24.600000\n0.660\n31.0\n\n\n190\n3.0\n129.0\n92.0\n49.0\n155.0\n36.400002\n0.968\n32.0\n\n\n191\n1.0\n109.0\n56.0\n21.0\n135.0\n25.200001\n0.833\n23.0\n\n\n\n\n\n192 rows × 8 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ny_train[:10]\n\narray([1, 1, 0, 1, 0, 0, 0, 0, 1, 0])\n\n\nWe will now show how to fit different models. All models support the fit() and predict() method (classifiers also support predict_proba()).\nThe simplest way to visualize a fitted model m is usually just to call str(m) or print(m). Some models have custom methods that allow you to visualize them further. To pass feature names into a model for visualization, you can usually (i) pass in the feature_names argument to the fit() function or (ii) pass in a pandas dataframe with the feature names as column names.\n\n6.2.1 Rule lists\nRule list is nonoverlapping\n\n6.2.1.1 oneR\nFits a rule list restricted to use only one feature\n\n# fit a oneR model\nmodel = OneRClassifier()\nmodel.fit(X_train, y=y_train, feature_names=feat_names) # stores into m.rules_\nprobs = model.predict_proba(X_test)\npreds = model.predict(X_test)\n\n# print the rule list\nprint(\"Classifier Accuracy:\", np.mean(y_test == preds), \"\\n Learned interpretable model:\\n\", model)\n\n# look at prediction breakdown\nviz_classification_preds(probs, y_test)\n\nClassifier Accuracy: 0.6649305555555556 \n Learned interpretable model:\n &gt; ------------------------------\n&gt; Greedy Rule List\n&gt; ------------------------------\n↓\n24.11% risk (192 pts)\n    if #Pregnant ==&gt; 60.8% risk (51 pts)\n↓\n19.51% risk (141 pts)\n    if #Pregnant ==&gt; 30.5% risk (59 pts)\n↓\n15.38% risk (82 pts)\n    if ~#Pregnant ==&gt; 26.700000000000003% risk (30 pts)\n↓\n12.5% risk (52 pts)\n    if #Pregnant ==&gt; 20.0% risk (20 pts)\n\n\n\n\n\n\n\nmodel.rules_\n\n[{'col': '#Pregnant',\n  'index_col': 0,\n  'cutoff': 6.5,\n  'val': 0.24113475177304963,\n  'flip': False,\n  'val_right': 0.6078431372549019,\n  'num_pts': 192,\n  'num_pts_right': 51},\n {'col': '#Pregnant',\n  'index_col': 0,\n  'cutoff': 2.5,\n  'val': 0.1951219512195122,\n  'flip': False,\n  'val_right': 0.3050847457627119,\n  'num_pts': 141,\n  'num_pts_right': 59},\n {'col': '#Pregnant',\n  'index_col': 0,\n  'cutoff': 0.5,\n  'val': 0.15384615384615385,\n  'flip': True,\n  'val_right': 0.26666666666666666,\n  'num_pts': 82,\n  'num_pts_right': 30},\n {'col': '#Pregnant',\n  'index_col': 0,\n  'cutoff': 1.5,\n  'val': 0.125,\n  'flip': False,\n  'val_right': 0.2,\n  'num_pts': 52,\n  'num_pts_right': 20}]\n\n\n\n\n6.2.1.2 Bayesian rule lists\n\n# train classifier (allow more iterations for better accuracy; use BigDataRuleListClassifier for large datasets)\n# All numeric features must be discretized prior to fitting!\ndisc = ExtraBasicDiscretizer(feat_names, n_bins=3, strategy='uniform')\nX_train_disc = disc.fit_transform(pd.DataFrame(X_train, columns=feat_names))\nX_test_disc = disc.transform(pd.DataFrame(X_test, columns=feat_names))\n\nX_train_disc\n\n/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n\n\n\n  \n    \n      \n\n\n\n\n\n\n#Pregnant_0.0_to_4.666666666666667\n#Pregnant_4.666666666666667_to_9.333333333333334\n#Pregnant_9.333333333333334_to_14.0\nGlucose concentration test_44.0_to_95.66666666666666\nGlucose concentration test_95.66666666666666_to_147.33333333333331\nGlucose concentration test_147.33333333333331_to_199.0\nBlood pressure(mmHg)_0.0_to_40.666666666666664\nBlood pressure(mmHg)_40.666666666666664_to_81.33333333333333\nBlood pressure(mmHg)_81.33333333333333_to_122.0\nTriceps skin fold thickness(mm)_0.0_to_21.0\n...\n2-Hour serum insulin (mu U/ml)_330.0_to_495.0\nBody mass index_0.0_to_19.8000005086263\nBody mass index_19.8000005086263_to_39.6000010172526\nBody mass index_39.6000010172526_to_59.400001525878906\nDiabetes pedigree function_0.10199999809265137_to_0.874666690826416\nDiabetes pedigree function_0.874666690826416_to_1.6473333835601807\nDiabetes pedigree function_1.6473333835601807_to_2.4200000762939453\nAge (years)_21.0_to_36.333333333333336\nAge (years)_36.333333333333336_to_51.66666666666667\nAge (years)_51.66666666666667_to_67.0\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n1\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n2\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n3\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n4\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n187\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n188\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n189\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n190\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n191\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n192 rows × 24 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nmodel = BayesianRuleListClassifier(max_iter=3000, class1label=\"diabetes\", verbose=False)\nmodel.fit(X_train_disc.to_numpy(), y_train, feature_names=X_train_disc.columns)\nprobs = model.predict_proba(X_test_disc)\npreds = model.predict(X_test_disc.to_numpy(), threshold=0.5)\n\nprint(\"RuleListClassifier Accuracy:\", np.mean(y_test == preds), \"\\n Learned interpretable model:\\n\", model)\n\nviz_classification_preds(probs, y_test)\n\nRuleListClassifier Accuracy: 0.7309027777777778 \n Learned interpretable model:\n Trained RuleListClassifier for detecting diabetes\n==================================================\nIF Body mass index_39.6000010172526_to_59.400001525878906 &gt; 0.5 THEN probability of diabetes: 50.0% (30.6%-69.4%)\nELSE IF Glucose concentration test_147.33333333333331_to_199.0 &gt; 0.5 THEN probability of diabetes: 69.7% (53.3%-83.9%)\nELSE IF Triceps skin fold thickness(mm)_42.0_to_63.0 &gt; 0.5 THEN probability of diabetes: 38.5% (15.2%-65.1%)\nELSE IF #Pregnant_0.0_to_4.666666666666667 &gt; 0.5 THEN probability of diabetes: 11.0% (5.2%-18.5%)\nELSE IF 2-Hour serum insulin (mu U/ml)_0.0_to_165.0 &gt; 0.5 THEN probability of diabetes: 34.9% (21.6%-49.5%)\nELSE probability of diabetes: 77.8% (47.3%-96.8%)\n=================================================\n\n\n\n\n\n\n\n\n\n6.2.2 Rule sets\nRule sets are models that create a set of (potentially overlapping) rules.\n\n6.2.2.1 Rulefit\nIt fits a sparse linear model on rules extracted from decision trees\n\n# fit a rulefit model\nmodel = RuleFitRegressor(max_rules=10)\nmodel.fit(X_train_reg, y_train_reg, feature_names=feat_names_reg)\n\n# get test performance\npreds = model.predict(X_test_reg)\nprint(f'test mse: {metrics.mean_squared_error(y_test_reg, preds):0.2f}')\nprint(f'test r2: {metrics.r2_score(y_test_reg, preds):0.2f}')\n\n\n# inspect and print the rules\n#rules = model._get_rules()\n#rules = rules[rules.coef != 0].sort_values(\"support\", ascending=False)\n# 'rule' is how the feature is constructed\n# 'coef' is its weight in the final linear model\n# 'support' is the fraction of points it applies to\n#rules[['rule', 'coef', 'support']].style.background_gradient(cmap='viridis')\nmodel\n\ntest mse: 2224531388.26\ntest r2: 0.65\n\n\n&gt; ------------------------------\n&gt; RuleFit:\n&gt;    Predictions are made by summing the coefficients of each rule\n&gt; ------------------------------\n                                         rule      coef\n                                  OverallQual  17096.64\n                                    GrLivArea     30.09\n                                   GarageArea     21.71\n OverallQual &lt;= 7.5 and TotalBsmtSF &lt;= 1201.0  -2144.37\nGrLivArea &lt;= 1934.0 and TotalBsmtSF &lt;= 1199.0 -11512.20\n  GrLivArea &lt;= 1790.0 and YearBuilt &lt;= 1994.5  -5185.18\n   GrLivArea &gt; 1415.0 and TotalBsmtSF &gt; 984.0  13188.14\n     GrLivArea &gt; 1821.0 and OverallQual &gt; 6.5    185.65\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RuleFitRegressor&gt; ------------------------------\n&gt; RuleFit:\n&gt;    Predictions are made by summing the coefficients of each rule\n&gt; ------------------------------\n                                         rule      coef\n                                  OverallQual  17096.64\n                                    GrLivArea     30.09\n                                   GarageArea     21.71\n OverallQual &lt;= 7.5 and TotalBsmtSF &lt;= 1201.0  -2144.37\nGrLivArea &lt;= 1934.0 and TotalBsmtSF &lt;= 1199.0 -11512.20\n  GrLivArea &lt;= 1790.0 and YearBuilt &lt;= 1994.5  -5185.18\n   GrLivArea &gt; 1415.0 and TotalBsmtSF &gt; 984.0  13188.14\n     GrLivArea &gt; 1821.0 and OverallQual &gt; 6.5    185.65\n\n\n\n\nmodel._get_rules()\n\n\n  \n    \n      \n\n\n\n\n\n\nrule\ntype\ncoef\nsupport\nimportance\n\n\n\n\n0\nMSSubClass\nlinear\n-0.000000\n1.000000\n0.000000\n\n\n1\nLotArea\nlinear\n0.000000\n1.000000\n0.000000\n\n\n2\nOverallQual\nlinear\n17096.637090\n1.000000\n21985.927425\n\n\n3\nOverallCond\nlinear\n0.000000\n1.000000\n0.000000\n\n\n4\nYearBuilt\nlinear\n0.000000\n1.000000\n0.000000\n\n\n5\nYearRemodAdd\nlinear\n0.000000\n1.000000\n0.000000\n\n\n6\nBsmtFinSF1\nlinear\n0.000000\n1.000000\n0.000000\n\n\n7\nBsmtFinSF2\nlinear\n-0.000000\n1.000000\n0.000000\n\n\n8\nBsmtUnfSF\nlinear\n0.000000\n1.000000\n0.000000\n\n\n9\nTotalBsmtSF\nlinear\n0.000000\n1.000000\n0.000000\n\n\n10\n1stFlrSF\nlinear\n0.000000\n1.000000\n0.000000\n\n\n11\n2ndFlrSF\nlinear\n0.000000\n1.000000\n0.000000\n\n\n12\nLowQualFinSF\nlinear\n0.000000\n1.000000\n0.000000\n\n\n13\nGrLivArea\nlinear\n30.088170\n1.000000\n13783.356137\n\n\n14\nBsmtFullBath\nlinear\n0.000000\n1.000000\n0.000000\n\n\n15\nBsmtHalfBath\nlinear\n-0.000000\n1.000000\n0.000000\n\n\n16\nFullBath\nlinear\n0.000000\n1.000000\n0.000000\n\n\n17\nHalfBath\nlinear\n0.000000\n1.000000\n0.000000\n\n\n18\nBedroomAbvGr\nlinear\n0.000000\n1.000000\n0.000000\n\n\n19\nKitchenAbvGr\nlinear\n-0.000000\n1.000000\n0.000000\n\n\n20\nTotRmsAbvGrd\nlinear\n0.000000\n1.000000\n0.000000\n\n\n21\nFireplaces\nlinear\n0.000000\n1.000000\n0.000000\n\n\n22\nGarageCars\nlinear\n0.000000\n1.000000\n0.000000\n\n\n23\nGarageArea\nlinear\n21.705852\n1.000000\n4319.756022\n\n\n24\nWoodDeckSF\nlinear\n0.000000\n1.000000\n0.000000\n\n\n25\nOpenPorchSF\nlinear\n0.000000\n1.000000\n0.000000\n\n\n26\nEnclosedPorch\nlinear\n-0.000000\n1.000000\n0.000000\n\n\n27\n3SsnPorch\nlinear\n0.000000\n1.000000\n0.000000\n\n\n28\nScreenPorch\nlinear\n0.000000\n1.000000\n0.000000\n\n\n29\nPoolArea\nlinear\n0.000000\n1.000000\n0.000000\n\n\n30\nMiscVal\nlinear\n0.000000\n1.000000\n0.000000\n\n\n31\nMoSold\nlinear\n0.000000\n1.000000\n0.000000\n\n\n32\nYrSold\nlinear\n-0.000000\n1.000000\n0.000000\n\n\n33\nGrLivArea &lt;= 1790.0 and YearBuilt &lt;= 1994.5\nrule\n-5185.184696\n0.550685\n2579.237407\n\n\n34\nGrLivArea &lt;= 1934.0 and TotalBsmtSF &lt;= 1199.0\nrule\n-11512.201674\n0.583562\n5675.147066\n\n\n35\nOverallQual &lt;= 7.5 and TotalBsmtSF &lt;= 1201.0\nrule\n-2144.369052\n0.641096\n1028.608817\n\n\n36\nGrLivArea &gt; 1415.0 and TotalBsmtSF &gt; 984.0\nrule\n13188.139108\n0.312329\n6111.952092\n\n\n37\nGrLivArea &gt; 1821.0 and OverallQual &gt; 6.5\nrule\n185.651378\n0.156164\n67.393514\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nrules = model._get_rules()\nrules = rules[rules.coef != 0].sort_values(\"support\", ascending=False)\n# 'rule' is how the feature is constructed\n# 'coef' is its weight in the final linear model\n# 'support' is the fraction of points it applies to\nrules[['rule', 'coef', 'support']].style.background_gradient(cmap='viridis')\n\n\n\n\n\n\n \nrule\ncoef\nsupport\n\n\n\n\n2\nOverallQual\n17096.637090\n1.000000\n\n\n13\nGrLivArea\n30.088170\n1.000000\n\n\n23\nGarageArea\n21.705852\n1.000000\n\n\n35\nOverallQual &lt;= 7.5 and TotalBsmtSF &lt;= 1201.0\n-2144.369052\n0.641096\n\n\n34\nGrLivArea &lt;= 1934.0 and TotalBsmtSF &lt;= 1199.0\n-11512.201674\n0.583562\n\n\n33\nGrLivArea &lt;= 1790.0 and YearBuilt &lt;= 1994.5\n-5185.184696\n0.550685\n\n\n36\nGrLivArea &gt; 1415.0 and TotalBsmtSF &gt; 984.0\n13188.139108\n0.312329\n\n\n37\nGrLivArea &gt; 1821.0 and OverallQual &gt; 6.5\n185.651378\n0.156164\n\n\n\n\n\n\n\n\n6.2.3 Ruletree\n\n6.2.3.1 FIGSClassifier\n\n# specify a decision tree with a maximum depth\nfigs = FIGSClassifier(max_rules=7)\nfigs.fit(X_train, y_train, feature_names=feat_names)\n\n# calculate mse on the training data\nprobs = figs.predict_proba(X_test)\npreds = figs.predict(X_test)\n\nprint(\"Classifier Accuracy:\", np.mean(y_test == preds), \"\\n Learned interpretable model:\\n\", figs)\n\nviz_classification_preds(probs, y_test)\n\nClassifier Accuracy: 0.7152777777777778 \n Learned interpretable model:\n &gt; ------------------------------\n&gt; FIGS-Fast Interpretable Greedy-Tree Sums:\n&gt;   Predictions are made by summing the \"Val\" reached by traversing each tree.\n&gt;   For classifiers, a sigmoid function is then applied to the sum.\n&gt; ------------------------------\nGlucose concentration test &lt;= 99.500 (Tree #0 root)\n    Val: 0.068 (leaf)\n    Glucose concentration test &lt;= 168.500 (split)\n        #Pregnant &lt;= 6.500 (split)\n            Body mass index &lt;= 30.850 (split)\n                Val: 0.065 (leaf)\n                Blood pressure(mmHg) &lt;= 67.000 (split)\n                    Val: 0.705 (leaf)\n                    Val: 0.303 (leaf)\n            Val: 0.639 (leaf)\n        Blood pressure(mmHg) &lt;= 93.000 (split)\n            Val: 0.860 (leaf)\n            Val: -0.009 (leaf)\n\n    +\nDiabetes pedigree function &lt;= 0.404 (Tree #1 root)\n    Val: -0.088 (leaf)\n    Val: 0.106 (leaf)\n\n\n\n\n\n\n\nprint('Alternative visualization:')\nfigs.plot()\n\nAlternative visualization:\n\n\n\n\n\n\ndt = extract_sklearn_tree_from_figs(figs, tree_num=0, n_classes=2) # tree_num =  0 or 1\nviz_model = dtreeviz.model(dt,\n                           X_train=X_train, y_train=y_train,\n                           feature_names=feat_names,\n                           target_name='y', class_names=[0, 1])\n\n\nviz_model.view(scale=1.5)\n\n\n\n\n\nviz_model.view(orientation=\"LR\", scale=1.5)\n\n\n\n\n\nx_example = X_train[13]\nviz_model.view(x=x_example)\n\n\n\n\nSee https://github.com/parrt/dtreeviz/blob/master/notebooks/dtreeviz_sklearn_visualisations.ipynb for more information.\n\n\n6.2.3.2 HSTreeClassifier\n\n# specify a decision tree with a maximum depth\ndt = HSTreeClassifierCV(max_leaf_nodes=7)\ndt.fit(X_train, y_train, feature_names=feat_names)\n\n# calculate mse on the training data\nprobs = dt.predict_proba(X_test)\npreds = dt.predict(X_test)\n\nprint(\"Classifier Accuracy:\", np.mean(y_test == preds), \"\\n Learned interpretable model:\\n\", dt)\n\nviz_classification_preds(probs, y_test)\n\nClassifier Accuracy: 0.7291666666666666 \n Learned interpretable model:\n &gt; ------------------------------\n&gt; Decision Tree with Hierarchical Shrinkage\n&gt;   Prediction is made by looking at the value in the appropriate leaf of the tree\n&gt; ------------------------------\n|--- feature_1 &lt;= 99.50\n|   |--- weights: [0.84, 0.16] class: 0.0\n|--- feature_1 &gt;  99.50\n|   |--- feature_1 &lt;= 168.50\n|   |   |--- feature_0 &lt;= 6.50\n|   |   |   |--- feature_5 &lt;= 30.85\n|   |   |   |   |--- weights: [0.77, 0.23] class: 0.0\n|   |   |   |--- feature_5 &gt;  30.85\n|   |   |   |   |--- feature_2 &lt;= 67.00\n|   |   |   |   |   |--- weights: [0.53, 0.47] class: 0.0\n|   |   |   |   |--- feature_2 &gt;  67.00\n|   |   |   |   |   |--- weights: [0.66, 0.34] class: 0.0\n|   |   |--- feature_0 &gt;  6.50\n|   |   |   |--- feature_6 &lt;= 0.26\n|   |   |   |   |--- weights: [0.57, 0.43] class: 0.0\n|   |   |   |--- feature_6 &gt;  0.26\n|   |   |   |   |--- weights: [0.45, 0.55] class: 1.0\n|   |--- feature_1 &gt;  168.50\n|   |   |--- weights: [0.38, 0.62] class: 1.0\n\n\n\n\n\n\n\nprint('Alternative visualization:')\nfig = plt.figure(figsize=(15, 15))\nplot_tree(dt.estimator_, feature_names=feat_names)\nplt.show()\n\nAlternative visualization:"
  },
  {
    "objectID": "06_XAI.html#partial-depedency-plot-and-individual-conditional-expectation-plots",
    "href": "06_XAI.html#partial-depedency-plot-and-individual-conditional-expectation-plots",
    "title": "6  Explainable AI",
    "section": "6.3 Partial Depedency Plot and Individual Conditional Expectation plots",
    "text": "6.3 Partial Depedency Plot and Individual Conditional Expectation plots\nPartial dependence plots (PDP) and individual conditional expectation (ICE) plots can be used to visualize and analyze interaction between the target response 1 and a set of input features of interest.\nBoth PDPs and ICEs assume that the input features of interest are independent from the complement features, and this assumption is often violated in practice. Thus, in the case of correlated features, we will create absurd data points to compute the PDP/ICE.\n\n6.3.1 Partial dependence plots\nPartial dependence plots (PDP) show the dependence between the target response and a set of input features of interest, marginalizing over the values of all other input features (the ‘complement’ features). Intuitively, we can interpret the partial dependence as the expected target response as a function of the input features of interest.\nDue to the limits of human perception the size of the set of input feature of interest must be small (usually, one or two) thus the input features of interest are usually chosen among the most important features.\n\ncal_housing = fetch_california_housing()\nX = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\ny = cal_housing.target\n\ny -= y.mean()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n\n\n6.3.1.1 1-way partial dependence with different models\nNote that it is important to check that the model is accurate enough on a test set before plotting the partial dependence since there would be little use in explaining the impact of a given feature on the prediction function of a poor model.\n\nest = HistGradientBoostingRegressor(random_state=0) # Similar to lightgbm\nest.fit(X_train, y_train)\nprint(f\"Test R2 score: {est.score(X_test, y_test):.2f}\")\n\nTest R2 score: 0.85\n\n\nThe sklearn.inspection module provides a convenience function from_estimator to create one-way and two-way partial dependence plots.\n\nfeatures = [\"MedInc\", \"AveOccup\", \"HouseAge\", \"AveRooms\"]\n_, ax = plt.subplots(ncols=4, figsize=(15, 7))\ndisplay = PartialDependenceDisplay.from_estimator(\n    est,\n    X_train,\n    features,\n    grid_resolution=20,\n    ax = ax,\n    random_state=0,\n    pd_line_kw={\"color\": \"tab:orange\", \"linestyle\": \"--\"}\n)\ndisplay.figure_.suptitle(\n    \"Partial dependence of house value on non-location features\\n\"\n    \"for the California housing dataset, with HistGradientBoostingRegressor\"\n)\ndisplay.figure_.subplots_adjust(hspace=2)\n\n\n\n\nWe can clearly see on the PDPs (dashed orange line) that the median house price shows a linear relationship with the median income (left) and that the house price drops when the average occupants per household increases (middle). The right plots show that the house age in a district does not have a strong influence on the (median) house price; so does the average rooms per household.\nOne-way PDPs tell us about the interaction between the target response and an input feature of interest feature (e.g. linear, non-linear).\n\n\n6.3.1.2 2D Partial Dependence Plots\nPDPs with two features of interest enable us to visualize interactions among them. Another consideration is linked to the performance to compute the PDPs. With the tree-based algorithm, when only PDPs are requested, they can be computed on an efficient way using the ‘recursion’ method.\n\nfeatures = [\"AveOccup\", \"HouseAge\", (\"AveOccup\", \"HouseAge\")]\n_, ax = plt.subplots(ncols=3, figsize=(13, 6))\ndisplay = PartialDependenceDisplay.from_estimator(\n    est,\n    X_train,\n    features,\n    kind=\"average\",\n    grid_resolution=10,\n    ax=ax,\n)\n\ndisplay.figure_.suptitle(\n    \"Partial dependence of house value on non-location features\\n\"\n    \"for the California housing dataset, with Gradient Boosting\"\n)\ndisplay.figure_.subplots_adjust(wspace=0.4, hspace=0.3)\n\n\n\n\nThe left plot in the above figure shows the effect of the average occupancy on the median house price; we can clearly see a linear relationship among them when the average occupancy is inferior to 3 persons. Similarly, we could analyze the effect of the house age on the median house price (middle plot). Thus, these interpretations are marginal, considering a feature at a time.\nThe two-way partial dependence plot shows the dependence of median house price on joint values of house age and average occupants per household. We can clearly see an interaction between the two features: for an average occupancy greater than two, the house price is nearly independent of the house age, whereas for values less than two there is a strong dependence on age.\nIf you need the raw values of the partial dependence function rather than the plots, you can use the sklearn.inspection.partial_dependence() function.\n\n\n6.3.1.3 Another example\nLike permutation importance, partial dependence plots are calculated after a model has been fit. The model is fit on real data that has not been artificially manipulated in any way. Our example will use a model that predicts whether a soccer/football team will have the “Man of the Game” winner based on the team’s statistics. The “Man of the Game” award is given to the best player in the game.\nTeams may differ in many ways. How many passes they made, shots they took, goals they scored, etc. At first glance, it seems difficult to disentangle the effect of these features. To see how partial plots separate out the effect of each feature, we start by considering a single row of data. For example, that row of data might represent a team that had the ball 50% of the time, made 100 passes, took 10 shots and scored 1 goal.\nWe will use the fitted model to predict our outcome (probability their player won “man of the match”). But we repeatedly alter the value for one variable to make a series of predictions. We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time. Then predict again for 60%. And so on. We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis).\nIn this description, we used only a single row of data. Interactions between features may cause the plot for a single row to be atypical. So, we repeat that mental experiment with multiple rows from the original dataset, and we plot the average predicted outcome on the vertical axis.\n\ndata = pd.read_csv('FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\nX = data[feature_names]\nX\n\n\n  \n    \n      \n\n\n\n\n\n\nGoal Scored\nBall Possession %\nAttempts\nOn-Target\nOff-Target\nBlocked\nCorners\nOffsides\nFree Kicks\nSaves\nPass Accuracy %\nPasses\nDistance Covered (Kms)\nFouls Committed\nYellow Card\nYellow & Red\nRed\nGoals in PSO\n\n\n\n\n0\n5\n40\n13\n7\n3\n3\n6\n3\n11\n0\n78\n306\n118\n22\n0\n0\n0\n0\n\n\n1\n0\n60\n6\n0\n3\n3\n2\n1\n25\n2\n86\n511\n105\n10\n0\n0\n0\n0\n\n\n2\n0\n43\n8\n3\n3\n2\n0\n1\n7\n3\n78\n395\n112\n12\n2\n0\n0\n0\n\n\n3\n1\n57\n14\n4\n6\n4\n5\n1\n13\n3\n86\n589\n111\n6\n0\n0\n0\n0\n\n\n4\n0\n64\n13\n3\n6\n4\n5\n0\n14\n2\n86\n433\n101\n22\n1\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n123\n1\n46\n11\n1\n6\n4\n4\n3\n24\n5\n79\n479\n148\n14\n1\n0\n0\n0\n\n\n124\n2\n43\n12\n4\n3\n5\n4\n1\n5\n5\n88\n510\n108\n11\n1\n0\n0\n0\n\n\n125\n0\n57\n15\n5\n7\n3\n5\n0\n12\n2\n92\n698\n110\n5\n2\n0\n0\n0\n\n\n126\n4\n39\n8\n6\n1\n1\n2\n1\n14\n1\n75\n271\n99\n14\n2\n0\n0\n0\n\n\n127\n2\n61\n15\n3\n8\n4\n6\n1\n15\n3\n83\n547\n100\n13\n1\n0\n0\n0\n\n\n\n\n\n128 rows × 18 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\ntree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)\n\nOur first example uses a decision tree, which you can see below. In practice, you’ll use more sophistated models for real-world applications.\n\ntree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names)\ngraphviz.Source(tree_graph)\n\n\n\n\n\nfeature_names\n\n['Goal Scored',\n 'Ball Possession %',\n 'Attempts',\n 'On-Target',\n 'Off-Target',\n 'Blocked',\n 'Corners',\n 'Offsides',\n 'Free Kicks',\n 'Saves',\n 'Pass Accuracy %',\n 'Passes',\n 'Distance Covered (Kms)',\n 'Fouls Committed',\n 'Yellow Card',\n 'Yellow & Red',\n 'Red',\n 'Goals in PSO']\n\n\n\nPartialDependenceDisplay.from_estimator(tree_model, val_X, features=['Goal Scored'], feature_names=feature_names)\nplt.ylim(0,1)\n\n(0.0, 1.0)\n\n\n\n\n\nA few items are worth pointing out as you interpret this plot\n\nThe y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\nA blue shaded area indicates level of confidence\n\nFrom this particular graph, we see that scoring a goal substantially increases your chances of winning “Man of The Match.” But extra goals beyond that appear to have little impact on predictions.\nHere is another example plot:\n\nPartialDependenceDisplay.from_estimator(tree_model, val_X, features=['Distance Covered (Kms)'], feature_names=feature_names)\nplt.ylim(0,2)\n\n(0.0, 2.0)\n\n\n\n\n\nThis graph seems too simple to represent reality. But that’s because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model’s structure.\nYou can easily compare the structure or implications of different models. Here is the same plot with a Random Forest model.\n\n# Build Random Forest model\nrf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n\nPartialDependenceDisplay.from_estimator(rf_model, val_X, features=['Distance Covered (Kms)'], feature_names=feature_names)\n\n&lt;sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x7f495beb38e0&gt;\n\n\n\n\n\nThis model thinks you are more likely to win Man of the Match if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.\nIn general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model. Though this dataset is small enough that we would be careful in how we interpret any model.\nIf you are curious about interactions between features, 2D partial dependence plots are also useful. An example may clarify this.\nWe will again use the Decision Tree model for this graph. It will create an extremely simple plot, but you should be able to match what you see in the plot to the tree itself.\n\n# Similar to previous PDP plot\nPartialDependenceDisplay.from_estimator(tree_model, val_X, features=[('Goal Scored', 'Distance Covered (Kms)')], feature_names=feature_names)\n\n&lt;sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x7f495be2df70&gt;\n\n\n\n\n\nThis graph shows predictions for any combination of Goals Scored and Distance covered.\nFor example, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km. If they score 0 goals, distance covered doesn’t matter. Can you see this by tracing through the decision tree with 0 goals?\nBut distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot. Can you see this pattern in the decision tree too?\n\n\n\n6.3.2 Individual conditional expectation (ICE) plot\nDue to the limits of human perception, only one input feature of interest is supported for ICE plots.\nWhile the PDPs are good at showing the average effect of the target features, they can obscure a heterogeneous relationship created by interactions. When interactions are present the ICE plot will provide many more insights. For example, we could observe a linear relationship between the median income and the house price in the PD line. However, the ICE lines show that there are some exceptions, where the house price remains constant in some ranges of the median income. We will plot the partial dependence, both individual (ICE) and averaged one (PDP). We limit to only 50 ICE curves to not overcrowd the plot.\nThe sklearn.inspection module’s PartialDependenceDisplay.from_estimatorconvenience function can be used to create ICE plots by setting kind='individual'. But in ICE plots it might not be easy to see the average effect of the input feature of interest. Hence, it is recommended to use ICE plots alongside PDPs. They can be plotted together with kind='both'.\n\nfeatures = [\"MedInc\", \"AveOccup\", \"HouseAge\", \"AveRooms\"]\n_, ax = plt.subplots(ncols=4, figsize=(13, 6))\ndisplay = PartialDependenceDisplay.from_estimator(\n    est,\n    X_train,\n    features,\n    kind=\"both\",\n    subsample=50,\n    n_jobs=3,\n    grid_resolution=20,\n    random_state=0,\n    ax = ax, \n    ice_lines_kw={\"color\": \"tab:blue\", \"alpha\": 0.2, \"linewidth\": 0.5},\n    pd_line_kw={\"color\": \"tab:orange\", \"linestyle\": \"--\"},\n)\ndisplay.figure_.suptitle(\n    \"Partial dependence of house value on non-location features\\n\"\n    \"for the California housing dataset, with Gradient Boosting\"\n)\ndisplay.figure_.subplots_adjust(wspace=0.4, hspace=0.3)\n\n\n\n\nThe ICE curves (light blue lines) complement the analysis: we can see that there are some exceptions, where the house price remain constant with median income and average occupants. On the other hand, while the house age (top right) does not have a strong influence on the median house price on average, there seems to be a number of exceptions where the house price increase when between the ages 15-25. Similar exceptions can be observed for the average number of rooms (bottom left). Therefore, ICE plots show some individual effect which are attenuated by taking the averages.\nCheckout more information at https://scikit-learn.org/stable/modules/partial_dependence.html# or https://github.com/SauceCat/PDPbox"
  },
  {
    "objectID": "06_XAI.html#lime",
    "href": "06_XAI.html#lime",
    "title": "6  Explainable AI",
    "section": "6.4 LIME",
    "text": "6.4 LIME\nWe’ll use the Iris dataset, and we’ll train a random forest.\n\nnp.random.seed(1)\niris = load_iris()\ntrain, test, labels_train, labels_test = train_test_split(iris.data, iris.target, train_size=0.8)\n\n\nrf = RandomForestClassifier(n_estimators=500)\nrf.fit(train, labels_train)\nmetrics.accuracy_score(labels_test, rf.predict(test))\n\n0.9666666666666667\n\n\n\n6.4.1 Tabular data\n\n6.4.1.1 Create the explainer\nTabular explainers need a training set. The reason for this is because we compute statistics on each feature (column). If the feature is numerical, we compute the mean and std, and discretize it into quartiles. If the feature is categorical, we compute the frequency of each value. For this part, we’ll only look at numerical features.\nWe use these computed statistics for two things:\n\nTo scale the data, so that we can meaningfully compute distances when the attributes are not on the same scale\nTo sample perturbed instances - which we do by sampling from a Normal(0,1), multiplying by the std and adding back the mean.\n\n\nexplainer = lime.lime_tabular.LimeTabularExplainer(train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)\n\n\n\n6.4.1.2 Explaining an instance\nSince this is a multi-class classification problem, we set the top_labels parameter, so that we only explain the top class.\n\ni = np.random.randint(0, test.shape[0])\nexp = explainer.explain_instance(test[i], rf.predict_proba, num_features=2, top_labels=1)\n\nWe now explain a single instance:\n\nexp.show_in_notebook(show_table=True, show_all=True)\n\n\n\n        \n        \n        \n        \n        \n        \n\n\n\nfig = exp.as_pyplot_figure(label=0)\n\n\n\n\nNow, there is a lot going on here. First, note that the row we are explained is displayed on the right side, in table format. Since we had the show_all parameter set to false, only the features used in the explanation are displayed. The value column displays the original value for each feature.\nNote that LIME has discretized the features in the explanation. This is because we let discretize_continuous=True in the constructor (this is the default). Discretized features make for more intuitive explanations.\n\n\n\n6.4.2 Image data\n\ninet_model = tf.keras.applications.inception_v3.InceptionV3()\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n96112376/96112376 [==============================] - 0s 0us/step\n\n\n\n!wget https://raw.githubusercontent.com/marcotcr/lime/master/doc/notebooks/data/cat_mouse.jpg\n\n--2023-03-27 03:50:28--  https://raw.githubusercontent.com/marcotcr/lime/master/doc/notebooks/data/cat_mouse.jpg\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 248960 (243K) [image/jpeg]\nSaving to: ‘cat_mouse.jpg’\n\ncat_mouse.jpg         0%[                    ]       0  --.-KB/s               cat_mouse.jpg       100%[===================&gt;] 243.12K  --.-KB/s    in 0.003s  \n\n2023-03-27 03:50:28 (91.5 MB/s) - ‘cat_mouse.jpg’ saved [248960/248960]\n\n\n\n\ndef transform_img_fn(path_list):\n    out = []\n    for img_path in path_list:\n        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(299, 299))\n        x = tf.keras.preprocessing.image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n        x = tf.keras.applications.inception_v3.preprocess_input(x)\n        out.append(x)\n    return np.vstack(out)\n\n\nimages = transform_img_fn(['cat_mouse.jpg'])\n# I'm dividing by 2 and adding 0.5 because of how this Inception represents images\nplt.imshow(images[0] / 2 + 0.5)\npreds = inet_model.predict(images)\nfor x in tf.keras.applications.imagenet_utils.decode_predictions(preds)[0]:\n    print(x)\n\n1/1 [==============================] - 12s 12s/step\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n35363/35363 [==============================] - 0s 0us/step\n('n02133161', 'American_black_bear', 0.6371615)\n('n02105056', 'groenendael', 0.03181786)\n('n02104365', 'schipperke', 0.02994415)\n('n01883070', 'wombat', 0.028509395)\n('n01877812', 'wallaby', 0.025093386)\n\n\n\n\n\n\n6.4.2.1 Explanation\nNow let’s get an explanation\n\nexplainer = lime_image.LimeImageExplainer()\n\n\n# Hide color is the color for a superpixel turned OFF. Alternatively, if it is NONE, the superpixel will be replaced by the average of its pixels\nexplanation = explainer.explain_instance(images[0].astype('double'), inet_model.predict, top_labels=5, hide_color=0, num_samples=1000)\n\n\n\n\n1/1 [==============================] - 1s 1s/step\n1/1 [==============================] - 0s 54ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 44ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 46ms/step\n1/1 [==============================] - 0s 46ms/step\n1/1 [==============================] - 0s 44ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 48ms/step\n1/1 [==============================] - 0s 46ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 45ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 51ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 52ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 48ms/step\n1/1 [==============================] - 0s 46ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 46ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 45ms/step\n1/1 [==============================] - 0s 46ms/step\n1/1 [==============================] - 0s 47ms/step\n1/1 [==============================] - 0s 53ms/step\n1/1 [==============================] - 0s 52ms/step\n1/1 [==============================] - 0s 51ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 57ms/step\n1/1 [==============================] - 0s 54ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 57ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 49ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 58ms/step\n1/1 [==============================] - 0s 53ms/step\n\n\n\n\n6.4.2.2 Now let’s see the explanation for the classes\n\ntemp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=10, hide_rest=True)\nplt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n\n&lt;matplotlib.image.AxesImage at 0x7f4944ff6220&gt;\n\n\n\n\n\nWe can also see the ‘pros and cons’ (pros in green, cons in red)\n\ntemp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\nplt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n\n&lt;matplotlib.image.AxesImage at 0x7f495d2fac10&gt;\n\n\n\n\n\nAlternatively, we can also plot explanation weights onto a heatmap visualization. The colorbar shows the values of the weights.\n\n#Select the same class explained on the figures above.\nind =  explanation.top_labels[0]\n\n#Map each explanation weight to the corresponding superpixel\ndict_heatmap = dict(explanation.local_exp[ind])\nheatmap = np.vectorize(dict_heatmap.get)(explanation.segments) \n\n#Plot. The visualization makes more sense if a symmetrical colorbar is used.\nplt.imshow(heatmap, cmap = 'RdBu', vmin  = -heatmap.max(), vmax = heatmap.max())\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x7f495feeaca0&gt;\n\n\n\n\n\n\ndict_heatmap\n\n{25: 0.1266119733320075,\n 26: 0.12625454084766954,\n 20: 0.12550891124563643,\n 14: 0.11200991992818318,\n 17: 0.09093842626197812,\n 10: -0.05383741765004169,\n 34: -0.04666506465080401,\n 11: 0.03839457072387601,\n 33: 0.03631442598802239,\n 24: 0.03619821434663484,\n 45: 0.02873184783414671,\n 22: 0.02866525965253672,\n 12: 0.02705639125595404,\n 8: 0.025929842390449525,\n 27: -0.02401695011845878,\n 54: -0.022137392638820155,\n 29: -0.020931992891644612,\n 6: 0.02065014310337862,\n 31: 0.020590037005144224,\n 5: 0.020036001632855294,\n 41: -0.019769696586538182,\n 3: 0.01919291308718661,\n 32: 0.016896245705981867,\n 51: 0.015878670165086477,\n 35: -0.014445619649604592,\n 13: -0.014122936684619367,\n 0: 0.013491125214863297,\n 39: 0.013017245875456086,\n 38: -0.012823059854158792,\n 4: -0.01278698007178597,\n 37: 0.012529557102235748,\n 19: 0.011597288332063638,\n 52: 0.010744092391949366,\n 53: -0.010660751286737019,\n 15: 0.010452156377416876,\n 18: 0.007091472431529134,\n 36: -0.006765111561901239,\n 7: -0.006490874832987173,\n 23: -0.0059956691966935515,\n 50: -0.0057907960843224925,\n 21: 0.005479746317145224,\n 28: 0.004983914391121539,\n 48: 0.004792603255925444,\n 42: -0.004650885399392036,\n 46: -0.004480095737890147,\n 40: -0.004215050771676743,\n 1: 0.003401562283486891,\n 43: -0.00306510230290569,\n 44: -0.0027089154172046694,\n 30: -0.0025121327258090156,\n 49: -0.0021503267934872327,\n 16: -0.0014023601941624687,\n 2: -0.0011691877595369229,\n 9: -0.0008138065988610777,\n 47: 0.0005239851503219061}\n\n\nLet’s see the explanation for the wombat\n\ntemp, mask = explanation.get_image_and_mask(explanation.top_labels[3], positive_only=False, num_features=10, hide_rest=False)\nplt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n\n&lt;matplotlib.image.AxesImage at 0x7f495c4c7bb0&gt;\n\n\n\n\n\nFor more information, please refer to https://github.com/marcotcr/lime"
  },
  {
    "objectID": "06_XAI.html#shap",
    "href": "06_XAI.html#shap",
    "title": "6  Explainable AI",
    "section": "6.5 SHAP",
    "text": "6.5 SHAP\n\n6.5.1 The force plot\nAn example is helpful, and we’ll continue the soccer/football example from the partial dependence plots. In these part, we predicted whether a team would have a player win the Man of the Match award.\nWe could ask: * How much was a prediction driven by the fact that the team scored 3 goals?\nBut it’s easier to give a concrete, numeric answer if we restate this as: * How much was a prediction driven by the fact that the team scored 3 goals, instead of some baseline number of goals.\nOf course, each team has many features. So if we answer this question for number of goals, we could repeat the process for all other features.\n\ndata = pd.read_csv('FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n\nWe will look at SHAP values for a single row of the dataset (we arbitrarily chose row 5). For context, we’ll look at the raw predictions before looking at the SHAP values.\n\nrow_to_show = 5\ndata_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n#data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\nmy_model.predict_proba(data_for_prediction.values.reshape(1, -1))\n\n/usr/local/lib/python3.9/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n\n\narray([[0.29, 0.71]])\n\n\nThe team is 71% likely to have a player win the award. Now, we’ll move onto the code to get SHAP values for that single prediction.\n\n# Create object that can calculate shap values\nexplainer = shap.Explainer(my_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\n\nThe shap_values object above is a list with two arrays. The first array is the SHAP values for a negative outcome (don’t win the award), and the second array is the list of SHAP values for the positive outcome (wins the award). We typically think about predictions in terms of the prediction of a positive outcome, so we’ll pull out SHAP values for positive outcomes (pulling out shap_values[1]).\nIt’s cumbersome to review raw arrays, but the shap package has a nice way to visualize the results.\n\nshap.initjs()\nshap.plots.force(explainer.expected_value[1], shap_values[1], data_for_prediction) # You can use view output in full screen\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\nIf you look carefully at the code where we created the SHAP values, you’ll notice we reference Trees in shap.TreeExplainer(my_model). But the SHAP package has explainers for every type of model.\n\nshap.DeepExplainer works with Deep Learning models.\nshap.KernelExplainer works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.\n\n\n\n6.5.2 Summary Plots\nIn addition to this nice breakdown for each prediction, the Shap library offers great visualizations of groups of Shap values. We will focus on two of these visualizations. These visualizations have conceptual similarities to permutation importance and partial dependence plots.\n\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\nshap_values = explainer.shap_values(val_X)\n\n# Make plot. Index of [1] is explained in text below.\nshap.summary_plot(shap_values[1], val_X)\n\n/usr/local/lib/python3.9/dist-packages/shap/plots/_beeswarm.py:664: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n\n\n\n\n\nThe code isn’t too complex. But there are a few caveats.\n\nWhen plotting, we call shap_values[1]. For classification problems, there is a separate array of SHAP values for each possible outcome. In this case, we index in to get the SHAP values for the prediction of “True”.\nCalculating SHAP values can be slow. It isn’t a problem here, because this dataset is small. But you’ll want to be careful when running these to plot with reasonably sized datasets. The exception is when using an xgboost model, which SHAP has some optimizations for and which is thus much faster.\n\nThis provides a great overview of the model, but we might want to delve into a single feature. That’s where SHAP dependence contribution plots come into play.\n\n\n6.5.3 Dependence Contribution Plots\n\n# calculate shap values. This is what we will plot.\nshap_values = explainer.shap_values(X)\n\n# make plot.\nshap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index=\"Goal Scored\")\n\n\n\n\n\n\n6.5.4 Image data\nDeep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with DeepLIFT. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT.\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\ninput_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(tf.keras.layers.Dropout(0.25))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11490434/11490434 [==============================] - 0s 0us/step\nx_train shape: (60000, 28, 28, 1)\n60000 train samples\n10000 test samples\nEpoch 1/12\n469/469 [==============================] - 9s 10ms/step - loss: 0.2362 - accuracy: 0.9283 - val_loss: 0.0500 - val_accuracy: 0.9838\nEpoch 2/12\n469/469 [==============================] - 4s 9ms/step - loss: 0.0817 - accuracy: 0.9759 - val_loss: 0.0368 - val_accuracy: 0.9873\nEpoch 3/12\n469/469 [==============================] - 5s 10ms/step - loss: 0.0629 - accuracy: 0.9811 - val_loss: 0.0299 - val_accuracy: 0.9902\nEpoch 4/12\n469/469 [==============================] - 4s 9ms/step - loss: 0.0507 - accuracy: 0.9843 - val_loss: 0.0303 - val_accuracy: 0.9904\nEpoch 5/12\n469/469 [==============================] - 5s 10ms/step - loss: 0.0440 - accuracy: 0.9867 - val_loss: 0.0287 - val_accuracy: 0.9911\nEpoch 6/12\n469/469 [==============================] - 5s 10ms/step - loss: 0.0384 - accuracy: 0.9879 - val_loss: 0.0266 - val_accuracy: 0.9908\nEpoch 7/12\n469/469 [==============================] - 4s 9ms/step - loss: 0.0344 - accuracy: 0.9890 - val_loss: 0.0265 - val_accuracy: 0.9922\nEpoch 8/12\n469/469 [==============================] - 5s 10ms/step - loss: 0.0296 - accuracy: 0.9906 - val_loss: 0.0265 - val_accuracy: 0.9912\nEpoch 9/12\n469/469 [==============================] - 5s 10ms/step - loss: 0.0261 - accuracy: 0.9913 - val_loss: 0.0296 - val_accuracy: 0.9903\nEpoch 10/12\n469/469 [==============================] - 4s 9ms/step - loss: 0.0250 - accuracy: 0.9919 - val_loss: 0.0272 - val_accuracy: 0.9914\nEpoch 11/12\n469/469 [==============================] - 5s 10ms/step - loss: 0.0210 - accuracy: 0.9929 - val_loss: 0.0289 - val_accuracy: 0.9916\nEpoch 12/12\n469/469 [==============================] - 4s 9ms/step - loss: 0.0211 - accuracy: 0.9932 - val_loss: 0.0265 - val_accuracy: 0.9931\nTest loss: 0.02652600035071373\nTest accuracy: 0.9930999875068665\n\n\n\n# select a set of background examples to take an expectation over\nbackground = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]\n\n# explain predictions of the model on three images\ne = shap.DeepExplainer(model, background)\n# ...or pass tensors directly\n# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)\nshap_values = e.shap_values(x_test[1:5])\n\n/usr/local/lib/python3.9/dist-packages/shap/explainers/_deep/deep_tf.py:95: UserWarning: keras is no longer supported, please use tf.keras instead.\n/usr/local/lib/python3.9/dist-packages/shap/explainers/_deep/deep_tf.py:100: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n/usr/local/lib/python3.9/dist-packages/keras/backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\nWARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\nInstructions for updating:\nLambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n\n\n\nlen(shap_values)\n\n10\n\n\n\n# plot the feature attributions\n# 10 classes, thus we have 10 plots!\nshap.image_plot(shap_values, -x_test[1:5])\n\n\n\n\nThe plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model’s output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the ‘zero’ image the blank middle is important, while for the ‘four’ image the lack of a connection on top makes it a four instead of a nine."
  },
  {
    "objectID": "06_XAI.html#protodash-using-axi360",
    "href": "06_XAI.html#protodash-using-axi360",
    "title": "6  Explainable AI",
    "section": "6.6 Protodash using AXI360",
    "text": "6.6 Protodash using AXI360\nYou can find more examples here"
  },
  {
    "objectID": "06_XAI.html#counterfactual-instances",
    "href": "06_XAI.html#counterfactual-instances",
    "title": "6  Explainable AI",
    "section": "6.7 Counterfactual instances",
    "text": "6.7 Counterfactual instances\nYou can find more informations here and here"
  },
  {
    "objectID": "06_XAI.html#using-interpretable-features-for-model-debugging",
    "href": "06_XAI.html#using-interpretable-features-for-model-debugging",
    "title": "6  Explainable AI",
    "section": "6.8 Using Interpretable Features for Model Debugging",
    "text": "6.8 Using Interpretable Features for Model Debugging\nYou can find more informations here"
  },
  {
    "objectID": "06_XAI.html#references",
    "href": "06_XAI.html#references",
    "title": "6  Explainable AI",
    "section": "6.9 References",
    "text": "6.9 References\n\nhttps://www.kaggle.com/learn/machine-learning-explainability\nhttps://scikit-learn.org/stable/modules/partial_dependence.html#\nhttps://github.com/csinva/imodels\nhttps://github.com/marcotcr/lime\nhttps://github.com/slundberg/shap"
  },
  {
    "objectID": "07_Deploy.html#setup",
    "href": "07_Deploy.html#setup",
    "title": "7  Deploy and monitoring",
    "section": "7.1 Setup",
    "text": "7.1 Setup\n\n!pip install bentoml -qq\n!pip install pyngrok -qq\n!pip install PyYAML -U -qq\n!pip install streamlit -qq\n!pip install gradio -qq\n!pip install evidently -qq\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 972.5/972.5 KB 10.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.9/50.9 KB 2.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.3/135.3 KB 7.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 23.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.5/94.5 KB 5.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 20.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.4/66.4 KB 2.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.8/57.8 KB 4.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.6/200.6 KB 9.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 KB 3.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 KB 1.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.5/66.5 KB 3.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.8/158.8 KB 5.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 KB 2.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.6/264.6 KB 7.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 KB 7.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 761.3/761.3 KB 26.0 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n  Building wheel for pyngrok (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.7/9.7 MB 87.7 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.1/82.1 KB 12.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.3/184.3 KB 25.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164.8/164.8 KB 24.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 102.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 KB 8.1 MB/s eta 0:00:00\n  Building wheel for validators (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/15.7 MB 31.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.5/129.5 KB 18.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 144.1/144.1 KB 22.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.1/57.1 KB 6.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 KB 7.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.1/200.1 KB 27.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.5/71.5 KB 11.1 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.9/66.9 KB 9.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.6/69.6 KB 8.2 MB/s eta 0:00:00\n  Building wheel for ffmpy (setup.py) ... done\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbentoml 1.0.17 requires starlette&lt;0.26, but you have starlette 0.26.1 which is incompatible.\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 97.7 MB/s eta 0:00:00\n\n\nNotice that you may need to restart the kernel after the above installations.\n\n# Scientific computing\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline\n\n# Modeling\nfrom sklearn import datasets\nfrom sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn import ensemble\nimport tensorflow as tf\n\n# Deploy\nimport bentoml\nimport gradio as gr\n\n# Monitoring\nfrom evidently import ColumnMapping\nfrom evidently.report import Report\nfrom evidently.metric_preset import DataDriftPreset, TargetDriftPreset, RegressionPreset\n\n# Helper library\nfrom pyngrok import ngrok, conf\nimport getpass\n\n# Other system library\nfrom pathlib import Path\nimport requests\nimport os\nimport json\nimport sys\nimport zipfile\nimport io\nfrom datetime import datetime, time\n\nHere are some tips for this notebook https://amitness.com/2020/06/google-colaboratory-tips/ and https://stackoverflow.com/questions/59741453/is-there-a-general-way-to-run-web-applications-on-google-colab.\nngrok is a reverse proxy tool that opens secure tunnels from public URLs to localhost, perfect for exposing local web servers, building webhook integrations, enabling SSH access, testing chatbots, demoing from your own machine, and more. In this lab, we will use use https://pyngrok.readthedocs.io/en/latest/integrations.html. However, for production environment, it is recommended to use cloud service such as AWS, GCP or Azure, see here or https://pycaret.gitbook.io/docs/get-started/functions/deploy#deploy_model for more details.\n\nprint(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\nconf.get_default().auth_token = getpass.getpass()\n\nEnter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\n··········\n\n\n\n# Setup a tunnel to the port 8050\npublic_url = ngrok.connect(8050)\n\n\npublic_url\n\n&lt;NgrokTunnel: \"http://ebc0-35-234-170-255.ngrok-free.app\" -&gt; \"http://localhost:8050\"&gt;\n\n\n\nif not tf.config.list_physical_devices('GPU'):\n    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n    if \"google.colab\" in sys.modules:\n        print(\"Go to Runtime &gt; Change runtime and select a GPU hardware \"\n              \"accelerator.\")\n    if \"kaggle_secrets\" in sys.modules:\n        print(\"Go to Settings &gt; Accelerator and select GPU.\")"
  },
  {
    "objectID": "07_Deploy.html#deploying-tensorflow-models-to-tensorflow-serving-tfs-on-remote-server",
    "href": "07_Deploy.html#deploying-tensorflow-models-to-tensorflow-serving-tfs-on-remote-server",
    "title": "7  Deploy and monitoring",
    "section": "7.2 Deploying TensorFlow models to TensorFlow Serving (TFS) on remote server",
    "text": "7.2 Deploying TensorFlow models to TensorFlow Serving (TFS) on remote server\nYou could create your own microservice using any technology you want (e.g., using the Flask library), but why reinvent the wheel when you can just use TF Serving?\n\n7.2.1 Exporting SavedModels\nTensorFlow provides a simple tf.keras.models.save_model() function to export models to the SavedModel format. All you need to do is give it the model, specifying its name and version number, and the function will save the model’s computation graph and its weights:\n\n# Load and split the MNIST dataset\nmnist = tf.keras.datasets.mnist.load_data()\n(X_train_full, y_train_full), (X_test, y_test) = mnist\nX_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11490434/11490434 [==============================] - 1s 0us/step\n\n\nIt’s usually a good idea to include all the preprocessing layers in the final model you export so that it can ingest data in its natural form once it is deployed to production. This avoids having to take care of preprocessing separately within the application that uses the model. Bundling the preprocessing steps within the model also makes it simpler to update them later on and limits the risk of mismatch between a model and the preprocessing steps it requires!\n\n# Build & train an MNIST model (also handles image preprocessing)\n\ntf.random.set_seed(42)\ntf.keras.backend.clear_session()\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n    tf.keras.layers.Rescaling(scale=1 / 255),\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n              metrics=[\"accuracy\"])\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n\nEpoch 1/10\n1719/1719 [==============================] - 11s 3ms/step - loss: 0.6814 - accuracy: 0.8237 - val_loss: 0.3684 - val_accuracy: 0.9020\nEpoch 2/10\n1719/1719 [==============================] - 8s 4ms/step - loss: 0.3509 - accuracy: 0.9018 - val_loss: 0.2974 - val_accuracy: 0.9164\nEpoch 3/10\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.2992 - accuracy: 0.9157 - val_loss: 0.2617 - val_accuracy: 0.9266\nEpoch 4/10\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.2680 - accuracy: 0.9248 - val_loss: 0.2395 - val_accuracy: 0.9338\nEpoch 5/10\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.2449 - accuracy: 0.9321 - val_loss: 0.2214 - val_accuracy: 0.9380\nEpoch 6/10\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.2268 - accuracy: 0.9370 - val_loss: 0.2087 - val_accuracy: 0.9424\nEpoch 7/10\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.2117 - accuracy: 0.9409 - val_loss: 0.1945 - val_accuracy: 0.9488\nEpoch 8/10\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.1987 - accuracy: 0.9444 - val_loss: 0.1866 - val_accuracy: 0.9504\nEpoch 9/10\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.1872 - accuracy: 0.9476 - val_loss: 0.1768 - val_accuracy: 0.9534\nEpoch 10/10\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.1773 - accuracy: 0.9503 - val_loss: 0.1685 - val_accuracy: 0.9534\n\n\n&lt;keras.callbacks.History at 0x7f2b0c0efc70&gt;\n\n\n\nX_new = X_test[:3]  # pretend we have 3 new digit images to classify\nnp.round(model.predict(X_new), 2)\n\n1/1 [==============================] - 0s 81ms/step\n\n\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99, 0.  , 0.  ],\n       [0.  , 0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]],\n      dtype=float32)\n\n\nNow to version the model, you just need to create a subdirectory for each model version:\n\nmodel_name = \"my_mnist_model\"\nmodel_version = \"0001\"\nmodel_path = Path(model_name) / model_version\n\n\ntf.keras.models.save_model(\n    model,\n    model_path,\n    overwrite=True,\n    include_optimizer=True,\n    save_format=\"tf\",\n    signatures=None,\n    options=None\n)\n\nA SavedModel represents a version of your model. It is stored as a directory containing a saved_model.pb file, which defines the computation graph (represented as a serialized protocol buffer), and a variables subdirectory containing the variable values. For models containing a large number of weights, these variable values may be split across multiple files. A SavedModel also includes an assets subdirectory that may contain additional data, such as vocabulary files, class names, or some example instances for this model.\n\nfor root, dirs, files in os.walk(model_name):\n    indent = '    ' * root.count(os.sep)\n    print('{}{}/'.format(indent, os.path.basename(root)))\n    for filename in files:\n        print('{}{}'.format(indent + '    ', filename))\n\nmy_mnist_model/\n    0001/\n        keras_metadata.pb\n        fingerprint.pb\n        saved_model.pb\n        assets/\n        variables/\n            variables.data-00000-of-00001\n            variables.index\n\n\nAs you might expect, you can load a SavedModel using the tf.keras.models.load_model() function.\n\nsaved_model = tf.keras.models.load_model(model_path)\nnp.round(saved_model.predict(X_new), 2)\n\n1/1 [==============================] - 0s 51ms/step\n\n\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99, 0.  , 0.  ],\n       [0.  , 0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]],\n      dtype=float32)\n\n\nTensorFlow also comes with a small saved_model_cli command-line tool to inspect SavedModels:\n\n!saved_model_cli show --dir {model_path} --all\n\n2023-04-09 04:55:26.330899: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['flatten_input'] tensor_info:\n        dtype: DT_UINT8\n        shape: (-1, 28, 28)\n        name: serving_default_flatten_input:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['dense_1'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 10)\n        name: StatefulPartitionedCall:0\n  Method name is: tensorflow/serving/predict\nThe MetaGraph with tag set ['serve'] contains the following ops: {'Placeholder', 'StringJoin', 'VarHandleOp', 'StaticRegexFullMatch', 'DisableCopyOnRead', 'Softmax', 'NoOp', 'Cast', 'StatefulPartitionedCall', 'Mul', 'Const', 'AddV2', 'ShardedFilename', 'Select', 'MatMul', 'RestoreV2', 'Pack', 'BiasAdd', 'ReadVariableOp', 'AssignVariableOp', 'Identity', 'Reshape', 'SaveV2', 'Relu', 'MergeV2Checkpoints'}\n2023-04-09 04:55:29.198144: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n\nConcrete Functions:\n  Function Name: '__call__'\n    Option #1\n      Callable with:\n        Argument #1\n          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #2\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='inputs')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #3\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='inputs')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #4\n      Callable with:\n        Argument #1\n          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n\n  Function Name: '_default_save_signature'\n    Option #1\n      Callable with:\n        Argument #1\n          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n\n  Function Name: 'call_and_return_all_conditional_losses'\n    Option #1\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='inputs')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #2\n      Callable with:\n        Argument #1\n          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #3\n      Callable with:\n        Argument #1\n          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #4\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='inputs')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n\n\nA SavedModel contains one or more metagraphs. When you pass a tf.keras model, by default the function saves a simple SavedModel: it saves a single metagraph tagged “serve”, which contains two signature definitions, an initialization function (called _saved_model_init_op) and a default serving function (called serving_default). When saving a tf.keras model, the default serving function corresponds to the model’s call() function, which of course makes predictions.\n\n\n7.2.2 Serve your model with TensorFlow Serving (Server side)\nThere are many ways to install TF Serving: using the system’s package manager, using a Docker image, installing from source, and more. Since Colab/Kaggle runs on Ubuntu, we can use Ubuntu’s apt package manager like this:\n\nif \"google.colab\" in sys.modules or \"kaggle_secrets\" in sys.modules:\n    url = \"https://storage.googleapis.com/tensorflow-serving-apt\"\n    src = \"stable tensorflow-model-server tensorflow-model-server-universal\"\n    !echo 'deb {url} {src}' &gt; /etc/apt/sources.list.d/tensorflow-serving.list\n    !curl '{url}/tensorflow-serving.release.pub.gpg' | apt-key add -\n    !apt update -q && apt-get install -y tensorflow-model-server\n    %pip install -q -U tensorflow-serving-api\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  2943  100  2943    0     0  17414      0 --:--:-- --:--:-- --:--:-- 17414\nOK\nGet:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\nHit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\nHit:3 http://archive.ubuntu.com/ubuntu focal InRelease\nGet:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\nGet:5 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\nHit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\nHit:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\nGet:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\nGet:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]\nHit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\nHit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\nGet:12 https://storage.googleapis.com/tensorflow-serving-apt stable InRelease [3,026 B]\nGet:13 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2,060 kB]\nGet:14 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]\nGet:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,324 kB]\nGet:16 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.3 kB]\nGet:17 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,069 kB]\nGet:18 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,199 kB]\nGet:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [972 kB]\nGet:20 https://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 Packages [340 B]\nGet:21 https://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server-universal amd64 Packages [348 B]\nFetched 10.0 MB in 3s (3,259 kB/s)\nReading package lists...\nBuilding dependency tree...\nReading state information...\n24 packages can be upgraded. Run 'apt list --upgradable' to see them.\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following NEW packages will be installed:\n  tensorflow-model-server\n0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\nNeed to get 414 MB of archives.\nAfter this operation, 0 B of additional disk space will be used.\nGet:1 https://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 tensorflow-model-server all 2.11.1 [414 MB]\nFetched 414 MB in 13s (30.9 MB/s)\nSelecting previously unselected package tensorflow-model-server.\n(Reading database ... 122349 files and directories currently installed.)\nPreparing to unpack .../tensorflow-model-server_2.11.1_all.deb ...\nUnpacking tensorflow-model-server (2.11.1) ...\nSetting up tensorflow-model-server (2.11.1) ...\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 38.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 588.3/588.3 MB 2.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 439.2/439.2 KB 44.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 100.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 72.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 105.1 MB/s eta 0:00:00\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbentoml 1.0.17 requires starlette&lt;0.26, but you have starlette 0.26.1 which is incompatible.\n\n\n\nThe code above starts by adding TensorFlow’s package repository to Ubuntu’s list of package sources. Then it downloads TensorFlow’s public GPG key and adds it to the package manager’s key list so it can verify TensorFlow’s package signatures. Next, it uses apt to install the tensorflow-model-server package. Lastly, it installs the tensorflow-serving-api library, which we will need to communicate with the server.\n\nIf tensorflow_model_server is installed (e.g., if you are running this notebook in Colab/Kaggle), then the following 2 cells will start the server. If your OS is Windows, you may need to run the tensorflow_model_server command in a terminal, and replace ${MODEL_DIR} with the full path to the my_mnist_model directory. This is where we start running TensorFlow Serving and load our model. After it loads we can start making inference requests using REST. There are some important parameters:\n\nport: The port that you’ll use for gRPC requests.\nrest_api_port: The port that you’ll use for REST requests.\nmodel_name: You’ll use this in the URL of REST requests. It can be anything.\nmodel_base_path: This is the path to the directory where you’ve saved your model.\n\n\nos.environ[\"MODEL_DIR\"] = str(model_path.parent.absolute())\n\n\n%%bash --bg\nnohup tensorflow_model_server \\\n     --port=8500 \\\n     --rest_api_port=8050 \\\n     --model_name=my_mnist_model \\\n     --model_base_path=\"${MODEL_DIR}\" &gt; server.log 2&gt;&1\n\nThe %%bash  --bg magic command executes the cell as a bash script, running it in the background. The &gt;my_server.log  2&gt;&1 part redirects the standard output and standard error to the server.log file. And that’s it! TF Serving is now running in the background, and its logs are saved to server.log.\n\n!tail server.log\n\n[warn] getaddrinfo: address family for nodename not supported\n[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n\n\n\n\n7.2.3 Querying TF Serving through the REST API (client side)\nLet’s start by creating the query. It must contain the name of the function signature you want to call, and of course the input data. Since the request must use the JSON format, we have to convert the input images from a NumPy array to a Python list:\n\ninput_data_json = json.dumps({\n    \"signature_name\": \"serving_default\",\n    \"instances\": X_new.tolist(),\n})\n\nNote that the JSON format is 100% text-based:\n\nrepr(input_data_json)[:1500] + \"...\"\n\n'\\'{\"signature_name\": \"serving_default\", \"instances\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 84, 185, 159, 151, 60, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 222, 254, 254, 254, 254, 241, 198, 198, 198, 198, 198, 198, 198, 198, 170, 52, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 67, 114, 72, 114, 163, 227, 254, 225, 254, 254, 254, 250, 229, 254, 254, 140, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 66, 14, 67, 67, 67, 59, 21, 236, 254, 106, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 253, 209, 18, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 233, 255, 83, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 129, 254, 238, 44, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 249, 254, 62, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...'\n\n\nNow let’s send the input data to TF Serving by sending an HTTP POST request. This can be done easily using the requests library:\n\nSERVER_URL = 'http://localhost:8050/v1/models/my_mnist_model:predict'\nresponse = requests.post(SERVER_URL, data=input_data_json)\nresponse.raise_for_status() # raise an exception in case of error\nresponse = response.json()\n\n\nresponse\n\n{'predictions': [[2.062969e-05,\n   3.84173156e-08,\n   0.000472674757,\n   0.00438946532,\n   2.47851091e-07,\n   7.01864119e-05,\n   1.34316258e-09,\n   0.994873822,\n   7.89433e-06,\n   0.000165013989],\n  [0.00119448744,\n   0.000342271611,\n   0.983010888,\n   0.0104818037,\n   4.77538409e-09,\n   0.00223252643,\n   0.00219411566,\n   8.47914272e-09,\n   0.000543907867,\n   1.48177925e-09],\n  [7.8709978e-05,\n   0.975474656,\n   0.00764368149,\n   0.00409595482,\n   0.000257658307,\n   0.00130979472,\n   0.00135653175,\n   0.00625853334,\n   0.00292111211,\n   0.000603225373]]}\n\n\nThe response is a dictionary containing a single “predictions” key. The corresponding value is the list of predictions. This list is a Python list, so let’s convert it to a NumPy array and round the floats it contains to the second decimal:\n\ny_proba = np.array(response[\"predictions\"])\ny_proba.round(2)\n\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99, 0.  , 0.  ],\n       [0.  , 0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])\n\n\nFor more information, please refer to https://github.com/tensorflow/serving which include the usuage of gRPC.\n\n\n7.2.4 Deploying a new model version\nNow let’s create a new model version and export a SavedModel to the my_mnist_model/0002 directory, just like earlier:\n\n# Change the architecture\n\nnp.random.seed(42)\ntf.random.set_seed(42)\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n    tf.keras.layers.Rescaling(scale=1 / 255),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n              metrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train, epochs=10,\n                    validation_data=(X_valid, y_valid))\n\nEpoch 1/10\n1719/1719 [==============================] - 10s 5ms/step - loss: 0.7847 - accuracy: 0.7836 - val_loss: 0.3468 - val_accuracy: 0.9080\nEpoch 2/10\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.3279 - accuracy: 0.9055 - val_loss: 0.2753 - val_accuracy: 0.9238\nEpoch 3/10\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.2742 - accuracy: 0.9211 - val_loss: 0.2347 - val_accuracy: 0.9360\nEpoch 4/10\n1719/1719 [==============================] - 8s 4ms/step - loss: 0.2394 - accuracy: 0.9312 - val_loss: 0.2126 - val_accuracy: 0.9418\nEpoch 5/10\n1719/1719 [==============================] - 11s 7ms/step - loss: 0.2134 - accuracy: 0.9391 - val_loss: 0.1931 - val_accuracy: 0.9442\nEpoch 6/10\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.1934 - accuracy: 0.9448 - val_loss: 0.1759 - val_accuracy: 0.9532\nEpoch 7/10\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.1772 - accuracy: 0.9493 - val_loss: 0.1660 - val_accuracy: 0.9556\nEpoch 8/10\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.1640 - accuracy: 0.9528 - val_loss: 0.1617 - val_accuracy: 0.9550\nEpoch 9/10\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.1526 - accuracy: 0.9574 - val_loss: 0.1501 - val_accuracy: 0.9592\nEpoch 10/10\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.1431 - accuracy: 0.9597 - val_loss: 0.1411 - val_accuracy: 0.9600\n\n\n\nmodel_version = \"0002\"\nmodel_name = \"my_mnist_model\"\nmodel_path = os.path.join(model_name, model_version)\n\ntf.keras.models.save_model(\n    model,\n    model_path,\n    overwrite=True,\n    include_optimizer=True,\n    save_format=\"tf\",\n    signatures=None,\n    options=None\n)\n\n\nfor root, dirs, files in os.walk(model_name):\n    indent = '    ' * root.count(os.sep)\n    print('{}{}/'.format(indent, os.path.basename(root)))\n    for filename in files:\n        print('{}{}'.format(indent + '    ', filename))\n\nmy_mnist_model/\n    0002/\n        keras_metadata.pb\n        fingerprint.pb\n        saved_model.pb\n        assets/\n        variables/\n            variables.data-00000-of-00001\n            variables.index\n    0001/\n        keras_metadata.pb\n        fingerprint.pb\n        saved_model.pb\n        assets/\n        variables/\n            variables.data-00000-of-00001\n            variables.index\n\n\nAt regular intervals (the delay is configurable), TensorFlow Serving checks for new model versions. If it finds one, it will automatically handle the transition gracefully: by default, it will answer pending requests (if any) with the previous model version, while handling new requests with the new version. As soon as every pending request has been answered, the previous model version is unloaded. You can see this at work in the TF Serving logs:\n\nSERVER_URL = 'http://localhost:8050/v1/models/my_mnist_model:predict'\n            \nresponse = requests.post(SERVER_URL, data=input_data_json)\nresponse.raise_for_status()\nresponse = response.json()\n\n\ny_proba = np.array(response[\"predictions\"])\ny_proba.round(2)\n\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99, 0.  , 0.  ],\n       [0.  , 0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])\n\n\n\n!pgrep tensorflow\n\n6207\n\n\n\n!kill $(pgrep tensorflow)\n\n\n!pgrep tensorflow\n\nAs you can see, TF Serving makes it quite simple to deploy new models. Moreover, if you discover that version 2 does not work as well as you expected, then rolling back to version 1 is as simple as removing the my_mnist_model/0002 directory.\nYou can also refer to https://github.com/microsoft/ML-For-Beginners/blob/main/3-Web-App/1-Web-App/README.md or https://github.com/rodrigo-arenas/fast-ml-deploy which use Flask and FastAPI that may have more flexibility.\nIf you would like to deploy to GCP vertex AI, checkout here."
  },
  {
    "objectID": "07_Deploy.html#deploy-a-rest-api-server-using-bentoml-on-remote-server",
    "href": "07_Deploy.html#deploy-a-rest-api-server-using-bentoml-on-remote-server",
    "title": "7  Deploy and monitoring",
    "section": "7.3 Deploy a REST API server using BentoML on remote server",
    "text": "7.3 Deploy a REST API server using BentoML on remote server\nTo begin with BentoML, you will need to save your trained models with BentoML API in its model store (a local directory managed by BentoML). The model store is used for managing all your trained models locally as well as accessing them for serving.\n\n7.3.1 Train a classifier model using the iris dataset\n\n# Load training data\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Train the model\nclf = svm.SVC(gamma='scale')\nclf.fit(X_train, y_train)\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC()\n\n\n\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test,y_pred))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      1.00      1.00         9\n           2       1.00      1.00      1.00        11\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\n\n\n\nclf.predict([[5.9, 3.0, 5.1, 1.8]])\n\narray([2])\n\n\nSave the clf model with BentoML. We begin by saving a trained model instance to BentoML’s local model store. The local model store is used to version your models as well as control which models are packaged with your bento. It is noted that there are a wide range of models can be saved via BentoML.\n\nIt is possible to use pre-trained models directly with BentoML or import existing trained model files to BentoML. Learn more about it from Preparing Models.\n\n\n# Save model to the BentoML local model store\nsaved_model = bentoml.sklearn.save_model(\"iris_clf\", clf)\nprint(f\"Model saved: {saved_model}\")\n\nModel saved: Model(tag=\"iris_clf:ne2yncwwssscuasc\")\n\n\nModels saved can be accessed via bentoml models CLI command:\n\n!bentoml models list\n\n Tag                        Module           Size      Creation Time       \n iris_clf:ne2yncwwssscuasc  bentoml.sklearn  5.32 KiB  2023-04-09 05:07:22 \n\n\nTo verify that the saved model can be loaded correctly:\n\nloaded_model = bentoml.sklearn.load_model(\"iris_clf:latest\")\n# model = bentoml.sklearn.load_model(\"iris_clf:wewrqnwn2s6ucasc\") #we can instead load specific version of model\nloaded_model.predict([[5.9, 3.0, 5.1, 1.8]])\n\narray([2])\n\n\nIn BentoML, the recommended way of running ML model inference in serving is via Runner:\n\n# Create a Runner instance:\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n\n# Runner#init_local initializes the model in current process, this is meant for development and testing only:\niris_clf_runner.init_local()\n\n# This should yield the same result as the loaded model:\niris_clf_runner.predict.run([[5.9, 3.0, 5.1, 1.8]])\n\nWARNING:bentoml._internal.runner.runner:'Runner.init_local' is for debugging and testing only. Make sure to remove it before deploying to production.\n\n\narray([2])\n\n\nIn this example, bentoml.sklearn.get() creates a reference to the saved model in the model store, and to_runner() creates a Runner instance from the model. The Runner abstraction gives BentoServer more flexibility in terms of how to schedule the inference computation, how to dynamically batch inference calls and better take advantage of all hardware resource available.\n\n\n7.3.2 Create a BentoML Service for serving the model\nServices are the core components of BentoML, where the serving logic is defined. With the model saved in the model store, we can define the service by creating a Python file service.py with the following contents:\n\n%%writefile service.py\nimport numpy as np\nimport bentoml\nfrom bentoml.io import NumpyNdarray\n\n# Load the runner for the latest ScikitLearn model we just saved\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n\n# Create the iris_classifier service with the ScikitLearn runner\n# Multiple runners may be specified if needed in the runners array\n# When packaged as a bento, the runners here will included\nsvc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n\n# Create API function with pre- and post- processing logic with your new \"svc\" annotation\n@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\ndef classify(input_series: np.ndarray) -&gt; np.ndarray:\n    # Define pre-processing logic\n    result = iris_clf_runner.predict.run(input_series)\n    # Define post-processing logic\n    return result\n\nWriting service.py\n\n\nIn this example, we defined the input and output type to be numpy.ndarray. More options, such as pandas.DataFrame, JSON and PIL.image are also supported. The svc.api decorator adds a function to the bentoml.Service object’s APIs list. The input and output parameter takes an IO Descriptor object, which specifies the API function’s expected input/output types, and is used for generating HTTP endpoints. Inside the API function, users can define any business logic, feature fetching, and feature transformation code. Model inference calls are made directly through runner objects, that are passed into bentoml.Service(name=.., runners=[..]) call when creating the service object.\n\nBentoML Server runs the Service API in an ASGI web serving layer and puts Runners in a separate worker process pool managed by BentoML. The ASGI web serving layer will expose REST endpoints for inference APIs, such as POST /predict and common infrastructure APIs, such as GET /metrics for monitoring. You can use other ASGI app like FastAPI or WSGI app like Flask, see here.\n\nWe now have everything we need to serve our first request. Launch the server in debug mode by running the bentoml serve command in the current working directory. Using the --reload option allows the server to reflect any changes made to the service.py module without restarting:\n\n!nohup bentoml serve ./service.py:svc --reload --port 8050 &\n\nnohup: appending output to 'nohup.out'\n\n\nWe can then send requests to the newly started service with any HTTP client:\n\nrequests.post(\n    \"http://127.0.0.1:8050/classify\",\n    headers={\"content-type\": \"application/json\"},\n    data=\"[[5.9, 3, 5.1, 1.8]]\"\n    ).text\n\n'[2]'\n\n\n\n!pgrep bentoml\n\n12658\n\n\n\n!kill $(pgrep bentoml)\n\n\n\n7.3.3 Build and Deploy Bentos 🍱\nOnce we are happy with the service definition, we can build the model and service into a bento. Bento is the distribution format for a service. It is a self-contained archive that contains all the source code, model files and dependency specifications required to run the service. Checkout Building Bentos for more details.\nTo build a Bento, first create a file named bentofile.yaml in your project directory:\n\n%%writefile bentofile.yaml\nservice: \"service.py:svc\"  # A convention for locating your service: &lt;YOUR_SERVICE_PY&gt;:&lt;YOUR_SERVICE_ANNOTATION&gt;\ndescription: \"file: ./README.md\"\nlabels:\n    owner: nsysu-math608\n    stage: demo\ninclude:\n - \"*.py\"  # A pattern for matching which files to include in the bento\npython:\n  packages:\n   - scikit-learn  # Additional libraries to be included in the bento\n   - pandas\n  lock_packages: False\n\nWriting bentofile.yaml\n\n\n\n%%writefile README.md\nThis is a iris classifier build in math608\n\nWriting README.md\n\n\nNext, use the bentoml build CLI command in the same directory to build a bento.\n\n!bentoml build\n\nBuilding BentoML service \"iris_classifier:udz3qngwswfdyasc\" from build context \"/content\".\nPacking model \"iris_clf:ne2yncwwssscuasc\"\n\n██████╗░███████╗███╗░░██╗████████╗░█████╗░███╗░░░███╗██╗░░░░░\n██╔══██╗██╔════╝████╗░██║╚══██╔══╝██╔══██╗████╗░████║██║░░░░░\n██████╦╝█████╗░░██╔██╗██║░░░██║░░░██║░░██║██╔████╔██║██║░░░░░\n██╔══██╗██╔══╝░░██║╚████║░░░██║░░░██║░░██║██║╚██╔╝██║██║░░░░░\n██████╦╝███████╗██║░╚███║░░░██║░░░╚█████╔╝██║░╚═╝░██║███████╗\n╚═════╝░╚══════╝╚═╝░░╚══╝░░░╚═╝░░░░╚════╝░╚═╝░░░░░╚═╝╚══════╝\n\nSuccessfully built Bento(tag=\"iris_classifier:udz3qngwswfdyasc\").\n\nPossible next steps:\n\n * Containerize your Bento with `bentoml containerize`:\n    $ bentoml containerize iris_classifier:udz3qngwswfdyasc\n\n * Push to BentoCloud with `bentoml push`:\n    $ bentoml push iris_classifier:udz3qngwswfdyasc\n\n\nBentos built will be saved in the local bento store, which you can view using the bentoml list CLI command.\n\n!bentoml list\n\n Tag                     Size       Creation Time        Path                   \n iris_classifier:udz3q…  18.37 KiB  2023-04-09 05:16:05  ~/bentoml/bentos/iris… \n\n\nWe can serve bentos from the bento store using the bentoml serve --production CLI command. Using the --production option will serve the bento in production mode.\n\n%%bash --bg\nnohup bentoml serve iris_classifier:latest \\\n     --production \\\n     --port 8050 &gt; bentoml.log 2&gt;&1\n\nThis is another way to query the server\n\n!curl \\\n  -X POST \\\n  -H \"content-type: application/json\" \\\n  --data \"[[5.9,3,5.1,1.8]]\" \\\n  http://127.0.0.1:8050/classify\n\n[2]\n\n\nThe Bento directory contains all code, files, models and configs required for running this service. BentoML standarlizes this file structure which enables serving runtimes and deployment tools to be built on top of it. By default, Bentos are managed under the ~/bentoml/bentos directory:\n\npath =\"/root/bentoml/bentos/iris_classifier/\"\n\n\nfor root, dirs, files in os.walk(path):\n    indent = ' ' * root.count(os.sep)\n    print('{}{}/'.format(indent, os.path.basename(root)))\n    for filename in files:\n        print('{}{}'.format(indent + ' ', filename))\n\n     /\n      latest\n     udz3qngwswfdyasc/\n      bento.yaml\n      README.md\n      models/\n       iris_clf/\n        latest\n        ne2yncwwssscuasc/\n         saved_model.pkl\n         model.yaml\n      src/\n       service.py\n       __pycache__/\n        service.cpython-39.pyc\n      env/\n       docker/\n        entrypoint.sh\n        Dockerfile\n       python/\n        install.sh\n        requirements.txt\n        version.txt\n      apis/\n       openapi.yaml\n\n\n\n!pgrep bentoml\n\n14130\n\n\n\n!kill $(pgrep bentoml)\n\nFor more information, please refer to https://docs.bentoml.org/en/latest/index.html."
  },
  {
    "objectID": "07_Deploy.html#deploy-web-base-application-in-local-computer-using-streamit",
    "href": "07_Deploy.html#deploy-web-base-application-in-local-computer-using-streamit",
    "title": "7  Deploy and monitoring",
    "section": "7.4 Deploy web base application in local computer using streamit",
    "text": "7.4 Deploy web base application in local computer using streamit\nStreamlit’s simple and focused API lets you build incredibly rich and powerful tools. It contains a large number of elements and components that you can use.\nThere are a few ways to display data (tables, arrays, data frames) in Streamlit apps. Below, st.write() can be used to write anything from text, plots to tables. In addition, when you’ve got the data or model into the state that you want to explore, you can add in widgets like st.slider(), st.button() or st.selectbox(). Finally, Streamlit makes it easy to organize your widgets in a left panel sidebar with st.sidebar. Each element that’s passed to st.sidebar is pinned to the left, allowing users to focus on the content in your app while still having access to UI controls. For example, if you want to add a selectbox and a slider to a sidebar, use st.sidebar.slider and st.sidebar.selectbox instead of st.slider and st.selectbox:\n\n%%writefile iris-app.py\nimport streamlit as st\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestClassifier\n\nst.write(\"\"\"\n# Simple Iris Flower Prediction App\n\nThis app predicts the **Iris flower** type!\n\"\"\")\n\nst.sidebar.header('User Input Parameters')\n\ndef user_input_features():\n    sepal_length = st.sidebar.slider('Sepal length', 4.3, 7.9, 5.4)\n    sepal_width = st.sidebar.slider('Sepal width', 2.0, 4.4, 3.4)\n    petal_length = st.sidebar.slider('Petal length', 1.0, 6.9, 1.3)\n    petal_width = st.sidebar.slider('Petal width', 0.1, 2.5, 0.2)\n    data = {'sepal_length': sepal_length,\n            'sepal_width': sepal_width,\n            'petal_length': petal_length,\n            'petal_width': petal_width}\n    features = pd.DataFrame(data, index=[0])\n    return features\n\ndf = user_input_features()\n\nst.subheader('User Input parameters')\nst.write(df)\n\niris = datasets.load_iris()\nX = iris.data\nY = iris.target\n\nclf = RandomForestClassifier()\nclf.fit(X, Y)\n\nprediction = clf.predict(df)\nprediction_proba = clf.predict_proba(df)\n\nst.subheader('Class labels and their corresponding index number')\nst.write(iris.target_names)\n\nst.subheader('Prediction')\nst.write(iris.target_names[prediction])\n\nst.subheader('Prediction Probability')\nst.write(prediction_proba)\n\nWriting iris-app.py\n\n\n\n%%bash --bg \nstreamlit run iris-app.py --server.port 8050 &gt; debug.log 2&gt;&1\n\nAs soon as you run the script as shown above, a local Streamlit server will spin up and your app will open in a new tab in your default web browser. The app is your canvas, where you’ll draw charts, text, widgets, tables, and more.\n\n!tail debug.log\n\n\npublic_url\n\n&lt;NgrokTunnel: \"http://c5cb-35-234-170-255.ngrok-free.app\" -&gt; \"http://localhost:8050\"&gt;\n\n\nTry to click the above link to access the web app. For more information, please refer to https://github.com/streamlit/streamlit.\n\n!pgrep streamlit\n\n14947\n\n\n\n!kill $(pgrep streamlit)"
  },
  {
    "objectID": "07_Deploy.html#deploy-web-base-application-in-local-computer-using-gradio",
    "href": "07_Deploy.html#deploy-web-base-application-in-local-computer-using-gradio",
    "title": "7  Deploy and monitoring",
    "section": "7.5 Deploy web base application in local computer using Gradio",
    "text": "7.5 Deploy web base application in local computer using Gradio\nUI models are perfect to use with Gradio’s image input component, so in this section we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this.\n\n7.5.1 Setting up the Image Classification Model\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from Keras. You can use a different pretrained model or train your own.\n\n!wget https://hf.space/embed/abidlabs/keras-image-classifier/file/banana.jpg\n!wget https://hf.space/embed/abidlabs/keras-image-classifier/file/car.jpg\n\n--2023-04-09 05:21:23--  https://hf.space/embed/abidlabs/keras-image-classifier/file/banana.jpg\nResolving hf.space (hf.space)... 54.159.43.68, 18.204.155.216, 54.81.158.24, ...\nConnecting to hf.space (hf.space)|54.159.43.68|:443... connected.\nHTTP request sent, awaiting response... 307 Temporary Redirect\nLocation: https://abidlabs-keras-image-classifier.hf.space/file/banana.jpg [following]\n--2023-04-09 05:21:23--  https://abidlabs-keras-image-classifier.hf.space/file/banana.jpg\nResolving abidlabs-keras-image-classifier.hf.space (abidlabs-keras-image-classifier.hf.space)... 34.196.131.200, 54.156.168.251, 34.195.4.197\nConnecting to abidlabs-keras-image-classifier.hf.space (abidlabs-keras-image-classifier.hf.space)|34.196.131.200|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 28437 (28K) [image/jpeg]\nSaving to: ‘banana.jpg’\n\nbanana.jpg          100%[===================&gt;]  27.77K  --.-KB/s    in 0.09s   \n\n2023-04-09 05:21:23 (325 KB/s) - ‘banana.jpg’ saved [28437/28437]\n\n--2023-04-09 05:21:24--  https://hf.space/embed/abidlabs/keras-image-classifier/file/car.jpg\nResolving hf.space (hf.space)... 54.159.43.68, 18.204.155.216, 54.81.158.24, ...\nConnecting to hf.space (hf.space)|54.159.43.68|:443... connected.\nHTTP request sent, awaiting response... 307 Temporary Redirect\nLocation: https://abidlabs-keras-image-classifier.hf.space/file/car.jpg [following]\n--2023-04-09 05:21:24--  https://abidlabs-keras-image-classifier.hf.space/file/car.jpg\nResolving abidlabs-keras-image-classifier.hf.space (abidlabs-keras-image-classifier.hf.space)... 34.196.131.200, 54.156.168.251, 34.195.4.197\nConnecting to abidlabs-keras-image-classifier.hf.space (abidlabs-keras-image-classifier.hf.space)|34.196.131.200|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 79626 (78K) [image/jpeg]\nSaving to: ‘car.jpg’\n\ncar.jpg             100%[===================&gt;]  77.76K   457KB/s    in 0.2s    \n\n2023-04-09 05:21:24 (457 KB/s) - ‘car.jpg’ saved [79626/79626]\n\n\n\n\ninception_net = tf.keras.applications.MobileNetV2()\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n14536120/14536120 [==============================] - 1s 0us/step\n\n\n\n\n7.5.2 Defining a predict function\nNext, we will need to define a function that takes in the user input, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this text file.\nIn the case of our pretrained model, it will look like this:\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef classify_image(inp):\n    inp = inp.reshape((-1, 224, 224, 3))\n    inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n    prediction = inception_net.predict(inp).flatten()\n    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n    return confidences\n\nLet’s break this down. The function takes one parameter: * inp: the input image as a NumPy array\nThen, the function adds a batch dimension, passes it through the model, and returns: * confidences: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n\n7.5.3 Creating a Gradio Interface\nNow that we have our predictive function set up, we can create a Gradio Interface around it. In this case, the input component is a drag-and-drop image component. To create this input, we can use the gradio.inputs.Image class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.\nThe output component will be a “label”, which displays the top labels in a nice form. Since we don’t want to show all 1,000 class labels, we will customize it to show only the top 3 images.\nFinally, we’ll add one more parameter, the examples, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\ngr.Interface(fn=classify_image, \n             inputs=gr.Image(shape=(224, 224), label=\"Input image\"),\n             outputs=gr.Label(num_top_classes=3, label=\"Predition Probabilities\"),\n             examples=[\"banana.jpg\", \"car.jpg\"],\n             description=\"Please upload an image\",\n             title=\"Classification using MobileNet\",\n             ).launch(server_port=8050)\n\nColab notebook detected. To show errors in colab notebook, set debug=True in launch()\nNote: opening Chrome Inspector may crash demo inside Colab notebooks.\n\nTo create a public link, set `share=True` in `launch()`.\n\n\n\n\n\n\n\n\nGradio automatically produces sharable links with others, but you can also access the web app with our port as follows:\n\npublic_url\n\n&lt;NgrokTunnel: \"http://ebc0-35-234-170-255.ngrok-free.app\" -&gt; \"http://localhost:8050\"&gt;\n\n\nYou can see that there is a flaaged directory which collect data from users who try the model. To close the gradio, just call the close_all() function.\n\ngr.close_all()\n\nClosing server running on port: 8050\n\n\nFor more information, please refer to https://github.com/gradio-app/gradio."
  },
  {
    "objectID": "07_Deploy.html#deploy-web-base-applocation-using-tensorflow.js",
    "href": "07_Deploy.html#deploy-web-base-applocation-using-tensorflow.js",
    "title": "7  Deploy and monitoring",
    "section": "7.6 Deploy web base applocation using Tensorflow.js",
    "text": "7.6 Deploy web base applocation using Tensorflow.js\nTensorflow.js is a WebGL accelerated JavaScript library for training and deploying ML models. The TensorFlow.js project includes a tensorflowjs_converter tool that can convert a TensorFlow SavedModel or a Keras model file to the TensorFlow.js Layers format: this is a directory containing a set of sharded weight files in binary format and a model.json file that describes the model’s architecture and links to the weight files. This format is optimized to be downloaded efficiently on the web.\nUsers can then download the model and run predictions in the browser using the TensorFlow.js library. Here is a code snippet to give you an idea of what the JavaScript API looks like:\nimport * as tf from '@tensorflow/tfjs';\nconst model = await tf.loadLayersModel('https://example.com/tfjs/model.json');\nconst image = tf.fromPixels(webcamElement);\nconst prediction = model.predict(image);\nFor more information, please refer to https://github.com/tensorflow/tfjs."
  },
  {
    "objectID": "07_Deploy.html#deploy-mobile-application-using-tensorflow-lite",
    "href": "07_Deploy.html#deploy-mobile-application-using-tensorflow-lite",
    "title": "7  Deploy and monitoring",
    "section": "7.7 Deploy mobile application using Tensorflow Lite",
    "text": "7.7 Deploy mobile application using Tensorflow Lite\nOnce again, doing justice to this topic would require a whole book. If you want to learn more about TensorFlow Lite, check out the O’Reilly book Practical Deep Learning for Cloud, Mobile, and Edge or refer to https://www.tensorflow.org/lite."
  },
  {
    "objectID": "07_Deploy.html#monitoring-shift-with-evidently",
    "href": "07_Deploy.html#monitoring-shift-with-evidently",
    "title": "7  Deploy and monitoring",
    "section": "7.8 Monitoring shift with evidently",
    "text": "7.8 Monitoring shift with evidently\n\n7.8.1 The task at hand: bike demand forecasting\nWe took a Kaggle dataset on Bike Sharing Demand. Our goal is to predict the volume of bike rentals on an hourly basis. To do that, we have some data about the season, weather, and day of the week.\n\ncontent = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\").content\nwith zipfile.ZipFile(io.BytesIO(content)) as arc:\n    raw_data = pd.read_csv(arc.open(\"hour.csv\"), header=0, sep=',', parse_dates=['dteday'], index_col='dteday')\n\n\nraw_data.index = raw_data.apply(\n    lambda row: datetime.combine(row.name, time(hour=int(row['hr']))), axis = 1)\nraw_data.head()\n\n\n  \n    \n      \n\n\n\n\n\n\ninstant\nseason\nyr\nmnth\nhr\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n2011-01-01 00:00:00\n1\n1\n0\n1\n0\n0\n6\n0\n1\n0.24\n0.2879\n0.81\n0.0\n3\n13\n16\n\n\n2011-01-01 01:00:00\n2\n1\n0\n1\n1\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n8\n32\n40\n\n\n2011-01-01 02:00:00\n3\n1\n0\n1\n2\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n5\n27\n32\n\n\n2011-01-01 03:00:00\n4\n1\n0\n1\n3\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n3\n10\n13\n\n\n2011-01-01 04:00:00\n5\n1\n0\n1\n4\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n0\n1\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n7.8.2 Train a model\nWe trained a random forest model using data for the four weeks from January. Let’s imagine that in practice, we just started the data collection, and that was all the data available. The performance of the trained model looked acceptable, so we decided to give it a go.\nWe further assume that we only learn the ground truth (the actual demand) at the end of each week. That is a realistic assumption in real-world machine learning. Integrating and updating different data sources is not always straightforward. Even after the actual event has occurred! Maybe the daily usage data is stored locally and is only sent and merged in the database once per week.\nTo run it, we prepare our performance data as a Pandas DataFrame. It should include: * Model application logs—features that went into the model and corresponding prediction; and * Ground truth data—the actual number of bikes rented each hour as our “target.”\nOnce we train the model, we can take our training dataset and generated predictions and specify it as the “Reference” data. We can select this period directly from the DataFrame since it has datetime as an index:\n\nreference = raw_data.loc['2011-01-01 00:00:00':'2011-01-28 23:00:00']\n\ntarget = 'cnt'\nprediction = 'prediction'\nnumerical_features = ['temp', 'atemp', 'hum', 'windspeed', 'hr', 'weekday']\ncategorical_features = ['season', 'holiday', 'workingday']\n\n\nreference.head()\n\n\n  \n    \n      \n\n\n\n\n\n\ninstant\nseason\nyr\nmnth\nhr\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n2011-01-01 00:00:00\n1\n1\n0\n1\n0\n0\n6\n0\n1\n0.24\n0.2879\n0.81\n0.0\n3\n13\n16\n\n\n2011-01-01 01:00:00\n2\n1\n0\n1\n1\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n8\n32\n40\n\n\n2011-01-01 02:00:00\n3\n1\n0\n1\n2\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n5\n27\n32\n\n\n2011-01-01 03:00:00\n4\n1\n0\n1\n3\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n3\n10\n13\n\n\n2011-01-01 04:00:00\n5\n1\n0\n1\n4\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n0\n1\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nreference[numerical_features + categorical_features].shape\n\n(618, 9)\n\n\n\nregressor = ensemble.RandomForestRegressor(random_state = 0, n_estimators = 50)\n\n\nregressor.fit(reference[numerical_features + categorical_features], reference[target])\n\nRandomForestRegressor(n_estimators=50, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(n_estimators=50, random_state=0)\n\n\n\nref_prediction = regressor.predict(reference[numerical_features + categorical_features])\n\n\nreference['prediction'] = ref_prediction\n\nWe also map the columns to show Evidently what each column contains and perform a correct analysis:\n\ncolumn_mapping = ColumnMapping()\n\ncolumn_mapping.target = target\ncolumn_mapping.prediction = prediction\ncolumn_mapping.numerical_features = numerical_features\ncolumn_mapping.categorical_features = categorical_features\n\nBy default, Evidently uses the index as an x-axis in plots. In this case, it is datetime, so we do not need to add anything else explicitly. Otherwise, we would have to specify it in our column mapping.\nNext, we call a corresponding report for regression models.\n\nregression_perfomance = Report(metrics=[RegressionPreset()])\nregression_perfomance.run(current_data=reference, reference_data=None, column_mapping=column_mapping)\n\n\n# You can also specify the metrics see https://docs.evidentlyai.com/reference/all-metrics\n#the_report = Report(metrics=[\n#    RegressionQualityMetric(),\n#    RegressionErrorPlot(),\n#    RegressionErrorDistribution(),\n#    DataDriftPreset(stattest=anderson_stat_test, stattest_threshold=0.9),\n#])\n\nAnd display the results right in the Jupyter notebook.\n\nregression_perfomance.show()\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nWe also save it as a .html file to be able to share it easily.\n\n!mkdir reports\nregression_perfomance.save_html('reports/regression_performance_at_training.html')\n\nWe can see that the model has a fine quality given that we only trained on four weeks of data! The error is symmetric and distributed around zero. There is no obvious under- or over-estimation.\nWe will continue treating this dataset from model performance in training as our “reference.” It gives us a good feel of the quality we can expect from our model in production use. So, we can contrast the future performance against this benchmark.\n\n\n7.8.3 The first week in production\nObserving the model in production has straightforward goals. We want to detect if something goes wrong. Ideally, in advance. We also want to diagnose the root cause and get a quick understanding of how to address it. Maybe, the model degrades too fast, and we need to retrain it more often? Perhaps, the error is too high, and we need to adapt the model and rebuild it? Which new patterns are emerging?\nIn our case, we simply start by checking how well the model performs outside the training data. Our first week becomes what would have otherwise been a holdout dataset.\nFor demonstration purposes, we generated all predictions for several weeks ahead in a single batch. In reality, we would run the model sequentially as the data comes in.\nLet’s start by comparing the performance in the first week to what we have seen in training. The first 28 days are our Reference dataset; the next 7 are the Production.\n\ncurrent = raw_data.loc['2011-01-29 00:00:00':'2011-02-28 23:00:00']\ncurrent_prediction = regressor.predict(current[numerical_features + categorical_features])\ncurrent['prediction'] = current_prediction\n\n\nregression_perfomance = Report(metrics=[RegressionPreset()])\nregression_perfomance.run(current_data=current.loc['2011-01-29 00:00:00':'2011-02-07 23:00:00'], \n                          reference_data=reference,\n                          column_mapping=column_mapping)\n\nregression_perfomance.show()\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nThe error has slightly increased and is leaning towards underestimation. Let’s check if there is any statistical change in our target. To do that, we will generate the Target Drift report.\n\ntarget_drift = Report(metrics=[TargetDriftPreset()])\ntarget_drift.run(current_data=current.loc['2011-01-29 00:00:00':'2011-02-07 23:00:00'],\n                 reference_data=reference,\n                 column_mapping=column_mapping)\n\ntarget_drift.show()\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nWe can see that the distribution of the actual number of bikes rented remains sufficiently similar. To be more precise, the similarity hypothesis is not rejected. No drift is detected. The distributions of our predictions did not change much either.\nDespite this, a rational decision is to update your model by including the new week’s data. This way, the model can continue to learn, and we can probably improve the error. For the sake of demonstration, we’ll stick to see how fast things go really wrong.\n\n\n7.8.4 The second week: failing to keep up\nOnce again, we benchmark our new week against the reference dataset.\n\nregression_perfomance = Report(metrics=[RegressionPreset()])\nregression_perfomance.run(current_data=current.loc['2011-02-07 00:00:00':'2011-02-14 23:00:00'], \n                          reference_data=reference,\n                          column_mapping=column_mapping)\n\nregression_perfomance.show()\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nAt first glance, the model performance in the second week does not differ much. MAE remains almost the same. But, the skew towards under-estimation continues to grow. It seems that the error is not random! To know more, we move to the plots. We can see that the model catches overall daily trends just fine. So it learned something useful! But, at peak hours, the actual demand tends to be higher than predicted.\nIn the error distribution plot, we can see how it became “wider,” as we have more predictions with a high error. The shift to the left is visible, too. In some extreme instances, we have errors between 80 and 40 bikes that were unseen previously.\nLet’s check our target as well.\n\ntarget_drift = Report(metrics=[TargetDriftPreset()])\ntarget_drift.run(current_data=current.loc['2011-02-07 00:00:00':'2011-02-14 23:00:00'],\n                 reference_data=reference,\n                 column_mapping=column_mapping)\n\ntarget_drift.show()\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nThings are getting interesting!\nWe can see that the target distribution is now different: the similarity hypothesis is rejected. Literally, people are renting more bikes. And this is a statistically different change from our training period.\nBut, the distribution of our predictions does not keep up! That is an obvious example of model decay. Something new happens in the world, but it misses the patterns.\nIt is tempting to investigate further. Is there anything in the data that can explain this change? If there is some new signal, retraining would likely help the model to keep up. The Target Drift report has a section to help us explore the relationship between the features and the target (or model predictions). ‍When browsing through the individual features, we can inspect if we notice any new patterns. We know that predictions did not change, so we only look at the relations with the target. For example, there is a shift towards higher temperatures (measured in Celsius) with a corresponding increase in rented bikes.\nMaybe, it would pick up these patterns in retraining. But for now, we simply move on to the next week without any updates.\n\n\n7.8.5 Week 3: when things go south\n\nregression_perfomance = Report(metrics=[RegressionPreset()])\nregression_perfomance.run(current_data=current.loc['2011-02-15 00:00:00':'2011-02-21 23:00:00'], \n                          reference_data=reference,\n                          column_mapping=column_mapping)\n\nregression_perfomance.show()\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nOkay, now things do look bad. On week 3, we face a major quality drop. Both absolute and percentage error grew significantly. If we look at the plots, the model predictions are visibly scattered. We also face a new data segment with high demand that the model fails to predict. But even within the known range of target value, the model now makes errors. Things did change since the training. We can see that the model does not extrapolate well. The predicted demand stays within the same known range, while actual values are peaking.\nIf we zoom in on specific days, we might suggest that the error is higher on specific (active) hours of the day. We are doing just fine from 10 pm to 6 am!\nIn our example, we particularly want to understand the segment where the model underestimates the target function. The Error Bias table gives up more details. We sort it by the \"Range%\" field. If the values of a specific feature are significantly different in the group where the model under- or over-estimates, this feature will rank high. In our case, we can see that the extreme errors are dependent on the “temp” (temperature) and “atemp” (feels-like temperature) features.\nAfter this quick analysis, we have a more specific idea about model performance and its weaknesses. The model faces new, unusually high demand. Given how it was trained, it tends to underestimate it. On top of it, these errors are not at all random. At the very least, they are related to the temperature we observe. The higher it is, the larger the underestimation. It suggests new patterns that are related to the weather that the model could not learn before. Days got warmer, and the model went rogue.\nIf we run a target drift report, we will also see a relevant change in the linear correlations between the feature and the target. Temperature and humidity stand out.\n\ntarget_drift = Report(metrics=[TargetDriftPreset()])\ntarget_drift.run(current_data=current.loc['2011-02-15 00:00:00':'2011-02-21 23:00:00'],\n                 reference_data=reference,\n                 column_mapping=column_mapping)\n\ntarget_drift.show()\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nWe should retrain as soon as possible and do this often until we learn all the patterns. If we are not comfortable with frequent retraining, we might choose an algorithm that is more suitable for time series or is better in extrapolation.\n\n\n7.8.6 Data Drift\nIn practice, once we receive the ground truth, we can indeed course-correct quickly. Had we retrained the model after week one, it would have likely ended less dramatically. But what if we do not have the ground truth available? Can we catch such decay in advance?\nIn this case, we can analyze the data drift. We do not need actuals to calculate the error. Instead, our goal is to see if the input data has changed.\nOnce again, let’s compare the first week of production to our data in training. We can, of course, look at all our features. But we can also conclude that categorical features (like “season,” “holiday” and “workingday”) are not likely to change. Let’s look at numerical features only!\nWe specify these features so that the tool applies the correct statistical test. It would be Kolmogorov-Smirnov in this case.\n\ncolumn_mapping = ColumnMapping()\n\ncolumn_mapping.numerical_features = numerical_features\n\n\ndata_drift = Report(metrics = [DataDriftPreset()])\ndata_drift.run(current_data = current.loc['2011-01-29 00:00:00':'2011-02-07 23:00:00'],\n               reference_data = reference,\n               column_mapping=column_mapping)\n\ndata_drift.show()\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\nThe data drift report compares the distributions of each feature in the two datasets. It automatically picks an appropriate statistical test or metric based on the feature type and volume. It then returns p-values or distances and visually plots the distributions. You can also adjust the drift detection method or thresholds, or pass your own.\n\nOnce we show the report, it returns an answer. We can see already during the first week there is a statistical change in feature distributions.\nLet’s zoom in on our usual suspect—temperature. The report gives us two views on how the feature distributions evolve with time. We can notice how the observed temperature becomes higher day by day. The values clearly drift out of our green corridor (one standard deviation from the mean) that we saw in training. Looking at the steady growth, we can suspect an upward trend.\nAs we checked earlier, we did not detect drift in the model predictions after week one. Given that our model is not good at extrapolating, we should not really expect it. Such prediction drift might still happen and signal about things like broken input data. Otherwise, we would observe it if we had a more sensitive model. Regardless of this, the data drift alone provides excellent early monitoring to detect the change and react to it.\nFor more information please refer to https://github.com/evidentlyai/evidently, https://github.com/SeldonIO/alibi-detect, https://github.com/great-expectations/great_expectations or https://github.com/whylabs/whylogs."
  },
  {
    "objectID": "07_Deploy.html#references",
    "href": "07_Deploy.html#references",
    "title": "7  Deploy and monitoring",
    "section": "7.9 References",
    "text": "7.9 References\n\nhttps://github.com/ageron/handson-ml2/blob/master/19_training_and_deploying_at_scale.ipynb\nhttps://github.com/bentoml/BentoML\nhttps://github.com/streamlit/streamlit\nhttps://raw.githubusercontent.com/dataprofessor/code/master/streamlit/part2/iris-ml-app.py\nhttps://gradio.app/image-classification-in-tensorflow/\nhttps://evidentlyai.com/blog/tutorial-1-model-analytics-in-production"
  },
  {
    "objectID": "08_neural_nets_with_tensorflow.html#setup",
    "href": "08_neural_nets_with_tensorflow.html#setup",
    "title": "8  Introduction to Artificial Neural Networks - Tensorflow",
    "section": "8.1 Setup",
    "text": "8.1 Setup\n\n# Python ≥3.7 is recommended\nimport sys\nassert sys.version_info &gt;= (3, 7)\nimport os\nfrom pathlib import Path\nfrom time import strftime\n\n# Scikit-Learn ≥1.01 is recommended\nfrom packaging import version\nimport sklearn\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\n# Tensorflow ≥2.8.0 is recommended\nimport tensorflow as tf\nassert version.parse(tf.__version__) &gt;= version.parse(\"2.8.0\")\n\n# Common imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nif \"google.colab\" in sys.modules:  # extra code\n    %pip install -q -U tensorboard-plugin-profile\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 17.4 MB/s eta 0:00:00\n\n\n\nif not tf.config.list_physical_devices('GPU'):\n    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n    if \"google.colab\" in sys.modules:\n        print(\"Go to Runtime &gt; Change runtime and select a GPU hardware \"\n              \"accelerator.\")\n    if \"kaggle_secrets\" in sys.modules:\n        print(\"Go to Settings &gt; Accelerator and select GPU.\")"
  },
  {
    "objectID": "08_neural_nets_with_tensorflow.html#perceptrons",
    "href": "08_neural_nets_with_tensorflow.html#perceptrons",
    "title": "8  Introduction to Artificial Neural Networks - Tensorflow",
    "section": "8.2 Perceptrons",
    "text": "8.2 Perceptrons\nLet’s use the iris dataset from openml. This is a famous dataset that contains the sepal and petal length and width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica\n\n\n\nYou can find more information about the dataset here.\n\niris = load_iris(as_frame=True)\nprint(iris.data.shape)\niris.data.head()\n\n(150, 4)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nFor simplicity, here we perform binary classification based on two features.\n\n# Choose two features and setup a binary classification problem\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].to_numpy()\ny = (iris.target == 0)  # Iris setosa\n\n# Build Perceptron model\nper_clf = Perceptron(random_state=42)\nper_clf.fit(X, y)\n\n# Test on two new instances\nX_new = [[2, 0.5], [3, 1]]\ny_pred = per_clf.predict(X_new)  # predicts True and False for these 2 flowers\ny_pred\n\narray([ True, False])\n\n\n\n# Plot the decision boundary\n\na = -per_clf.coef_[0, 0] / per_clf.coef_[0, 1]\nb = -per_clf.intercept_ / per_clf.coef_[0, 1]\naxes = [0, 5, 0, 2]\nx0, x1 = np.meshgrid(\n    np.linspace(axes[0], axes[1], 500).reshape(-1, 1),\n    np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n)\nX_new = np.c_[x0.ravel(), x1.ravel()]\ny_predict = per_clf.predict(X_new)\nzz = y_predict.reshape(x0.shape)\ncustom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n\nplt.figure(figsize=(7, 3))\nplt.plot(X[y == 0, 0], X[y == 0, 1], \"bs\", label=\"Not Iris setosa\")\nplt.plot(X[y == 1, 0], X[y == 1, 1], \"yo\", label=\"Iris setosa\")\nplt.plot([axes[0], axes[1]], [a * axes[0] + b, a * axes[1] + b], \"k-\",\n         linewidth=3)\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"lower right\")\nplt.axis(axes)\nplt.show()"
  },
  {
    "objectID": "08_neural_nets_with_tensorflow.html#tensorflow-playground",
    "href": "08_neural_nets_with_tensorflow.html#tensorflow-playground",
    "title": "8  Introduction to Artificial Neural Networks - Tensorflow",
    "section": "8.3 Tensorflow Playground",
    "text": "8.3 Tensorflow Playground\nhttp://playground.tensorflow.org/\n\n8.3.1 Introduction\nThe Playground provides mainly 6 different types of datasets. 1. Classification: Circle, Exclusive or, Gaussian, spiral. 2. Regression: Plane, Multi Gaussian.\nSmall circle points are represented as data points that correspond to Positive (+) and Negative (-). Positive represented by blue, Negative represented by orange. These same colours are used in representing Data, Neuron, Weight values.\nThe datasets all have 2 input features and 1 output label. The 2 input features, X1 and X2, are represented by the coordinates.\n\nThe data points (represented by small circles) are initially colored orange or blue, which correspond to positive one and negative one.\nIn the hidden layers, the lines are colored by the weights of the connections between neurons. Blue shows a positive weight, which means the network is using that output of the neuron as given. An orange line shows that the network is assiging a negative weight.\nIn the output layer, the dots are colored orange or blue depending on their original values. The background color shows what the network is predicting for a particular area. The intensity of the color shows how confident that prediction is\n\n\n\n8.3.2 Try it\n\nLayers and patterns: try training the default neural network by clicking the “Run” button (top left). Notice how it quickly finds a good solution for the classification task. Notice that the neurons in the first hidden layer have learned simple patterns, while the neurons in the second hidden layer have learned to combine the simple patterns of the first hidden layer into more complex patterns). In general, the more layers, the more complex the patterns can be.\nActivation function: try replacing the Tanh activation function with the ReLU activation function, and train the network again. Notice that it finds a solution even faster.\nLocal minima: modify the network architecture to have just one hidden layer with three neurons. Train it multiple times (to reset the network weights, just add and remove a neuron). Notice that the training time varies a lot, and sometimes it even gets stuck in a local minimum.\nToo small: now remove one neuron to keep just 2. Notice that the neural network is now incapable of finding a good solution, even if you try multiple times. The model has too few parameters and it systematically underfits the training set.\nLarge enough: next, set the number of neurons to 8 and train the network several times. Notice that it is now consistently fast and never gets stuck. This highlights an important finding in neural network theory: large neural networks almost never get stuck in local minima, and even when they do these local optima are almost as good as the global optimum. However, they can still get stuck on long plateaus for a long time.\nDeep net and vanishing gradients: now change the dataset to be the spiral (bottom right dataset under “DATA”). Change the network architecture to have 4 hidden layers with 4 neurons each. Notice that training takes much longer, and often gets stuck on plateaus for long periods of time. Also notice that the neurons in the highest layers (i.e. on the right) tend to evolve faster than the neurons in the lowest layers (i.e. on the left). This problem, called the “vanishing gradients” problem, can be alleviated using better weight initialization and other techniques, better optimizers (such as AdaGrad or Adam), or using Batch Normalization."
  },
  {
    "objectID": "08_neural_nets_with_tensorflow.html#building-an-image-classifier-using-the-sequential-api",
    "href": "08_neural_nets_with_tensorflow.html#building-an-image-classifier-using-the-sequential-api",
    "title": "8  Introduction to Artificial Neural Networks - Tensorflow",
    "section": "8.4 Building an Image Classifier Using the Sequential API",
    "text": "8.4 Building an Image Classifier Using the Sequential API\nFirst let’s import TensorFlow and Keras.\n\ntf.__version__\n\n'2.12.0'\n\n\n\n\n\n\nsource: https://juejin.cn/post/7096480975512666142\n\nFirst, we need to load a dataset. We will tackle Fashion MNIST, which is a drop-in replacement of MNIST. It has the exact same format as MNIST (70,000 grayscale images of \\(28 \\times 28\\) pixels each, with 10 classes), but the images represent fashion items rather than handwritten digits, so each class is more diverse and the problem turns out to be significantly more challenging than MNIST. For example, a simple linear model reaches about 92% accuracy on MNIST, but only about 83% on Fashion MNIST.\nYou can find more built-in dataset here.\nLet’s start by loading the fashion MNIST dataset. tf.Keras has a number of functions to load popular datasets in tf.keras.datasets. The dataset is already split for you between a training set and a test set, but it can be useful to split the training set further to have a validation set:\n\nfashion_mnist = tf.keras.datasets.fashion_mnist.load_data() # use tf.keras not keras.io (multibackend keras)\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 [==============================] - 0s 0us/step\n\n\nThe training set contains 60,000 grayscale images, each 28x28 pixels. Notice that the order of Tensorflow is [batch, height, width, channel].\n\nX_train_full.shape\n\n(60000, 28, 28)\n\n\nEach pixel intensity is represented as a byte (0 to 255):\n\nX_train_full.dtype\n\ndtype('uint8')\n\n\nLet’s split the full training set into a validation set and a (smaller) training set. Now the validation set contains 5,000 images, and the test set contains 10,000 images. We also scale the pixel intensities down to the 0-1 range and convert them to floats, by dividing by 255.\n\nX_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\nX_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\nX_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255.\n\nThe labels are the class IDs (represented as uint8), from 0 to 9:\n\ny_train\n\narray([9, 0, 0, ..., 9, 0, 2], dtype=uint8)\n\n\nHere are the corresponding class names:\n\nclass_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n\nSo the first image in the training set is a ankle boot:\n\nclass_names[y_train[0]]\n\n'Ankle boot'\n\n\nLet’s take a look at a sample of the images in the dataset:\n\nn_rows = 4\nn_cols = 10\nplt.figure(figsize=(n_cols * 1.5, n_rows * 1.5))\nfor row in range(n_rows):\n    for col in range(n_cols):\n        index = n_cols * row + col\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n        plt.axis('off')\n        plt.title(class_names[y_train[index]])\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()\n\n\n\n\n\n8.4.1 Creating the Model Using the Sequential API\nNow let’s build the neural network! Here is a classification MLP with two hidden layers:\n\n# This is the simplest kind of Keras model, for neural networks that are just \n# composed of a single stack of layers, connected sequentially\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.InputLayer(input_shape=[28, 28]))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(300, activation=\"relu\"))\nmodel.add(tf.keras.layers.Dense(100, activation=\"relu\"))\nmodel.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n\n\nWe build the first layer and add it to the model. It is a Flatten layer whose role is simply to convert each input image into a 1D array: if it receives input data X, it computes X.reshape(-1, 1). This layer does not have any parameters, it is just there to do some simple preprocessing. Since it is the first layer in the model, you should specify the input_shape: this does not include the batch size, only the shape of the instances.\nNext we add a Dense hidden layer with 300 neurons. It will use the ReLU activation function. Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of weight bias terms (one per neuron in the next layer).\nNext we add a second Dense hidden layer with 100 neurons, also using the ReLU activation function.\nFinally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function.\n\nYou can find different activation functions here.\n\n[m for m in dir(tf.keras.activations) if not m.startswith(\"_\")], [m for m in dir(tf.keras.layers) if \"relu\" in m.lower()]\n\n\n# clear the session to reset the name counters\ntf.keras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# An alternative way to specify the sequential model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=[28, 28]),\n    tf.keras.layers.Dense(300, activation=\"relu\"),\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\n\nThe model’s summary() method displays all the model’s layers, including each layer’s name (which is automatically generated unless you set it when creating the layer), its output shape (None means the batch size can be anything), and its number of parameters. The summary ends with the total number of parameters, including trainable and non-trainable parameters. Here we only have trainable parameters.\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten (Flatten)           (None, 784)               0         \n                                                                 \n dense (Dense)               (None, 300)               235500    \n                                                                 \n dense_1 (Dense)             (None, 100)               30100     \n                                                                 \n dense_2 (Dense)             (None, 10)                1010      \n                                                                 \n=================================================================\nTotal params: 266,610\nTrainable params: 266,610\nNon-trainable params: 0\n_________________________________________________________________\n\n\nNote that Dense layers often have a lot of parameters. For example, the first hidden layer has 784×300 connection weights, plus 300 bias terms, which adds up to 235,500 parameters! This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data!\n\ntf.keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)\n\n\n\n\nYou can easily get a model’s list of layers, to fetch a layer by its index, or you can fetch it by name:\n\nmodel.layers\n\n[&lt;keras.layers.reshaping.flatten.Flatten at 0x7f4eabcf41c0&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f4eabc49c40&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f4eabc490a0&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f4eabc495e0&gt;]\n\n\n\nhidden1 = model.layers[1]\nhidden1.name\n\n'dense'\n\n\n\nmodel.get_layer('dense_1')\n\n&lt;keras.layers.core.dense.Dense at 0x7f4eabc490a0&gt;\n\n\nAll the parameters of a layer can be accessed using its get_weights() and set_weights() method. For a Dense layer, this includes both the connection weights and the bias terms:\n\nweights, biases = hidden1.get_weights()\n\n\nweights, biases\n\n(array([[-0.04090392,  0.05212015,  0.06563495, ..., -0.06813569,\n         -0.07277503,  0.06356135],\n        [-0.02273986, -0.03733981, -0.01904616, ...,  0.07051966,\n         -0.03919001, -0.02506971],\n        [ 0.06158407,  0.03164114,  0.04348575, ..., -0.07112216,\n          0.047318  ,  0.01152001],\n        ...,\n        [-0.05140251, -0.0742506 ,  0.0284429 , ...,  0.04885727,\n         -0.02155429, -0.05590397],\n        [-0.07400953,  0.05923583, -0.02156794, ...,  0.03335793,\n          0.01330438, -0.01484616],\n        [-0.05129209, -0.01325566,  0.00971705, ...,  0.05628417,\n         -0.04696462, -0.03978037]], dtype=float32),\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))\n\n\n\nweights.shape, biases.shape\n\n((784, 300), (300,))\n\n\nNotice that the Dense layer initialized the connection weights randomly (which is needed to break symmetry), and the biases were just initialized to zeros, which is fine. If you ever want to use a different initialization method, you can set kernel_initializer (kernel is another name for the matrix of connection weights) or bias_initializer when creating the layer.\nYou can find more information here.\n\n[name for name in dir(tf.keras.initializers) if not name.startswith(\"_\")]\n\n['Constant',\n 'GlorotNormal',\n 'GlorotUniform',\n 'HeNormal',\n 'HeUniform',\n 'Identity',\n 'Initializer',\n 'LecunNormal',\n 'LecunUniform',\n 'Ones',\n 'Orthogonal',\n 'RandomNormal',\n 'RandomUniform',\n 'TruncatedNormal',\n 'VarianceScaling',\n 'Zeros',\n 'constant',\n 'deserialize',\n 'get',\n 'glorot_normal',\n 'glorot_uniform',\n 'he_normal',\n 'he_uniform',\n 'identity',\n 'lecun_normal',\n 'lecun_uniform',\n 'ones',\n 'orthogonal',\n 'random_normal',\n 'random_uniform',\n 'serialize',\n 'truncated_normal',\n 'variance_scaling',\n 'zeros']\n\n\n\n\n8.4.2 Compiling the Model\nAfter a model is created, you must call its compile() method to specify the loss function and the optimizer to use.\nFirst, we use the \"sparse_categorical_crossentropy\" loss because we have sparse labels (i.e., for each instance there is just a target class index, from 0 to 9 in this case), and the classes are exclusive. If instead we had one target probability per class for each instance (such as one-hot vectors, e.g. [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would need to use the \"categorical_crossentropy\" loss instead. If we were doing binary classification, then we would use the \"sigmoid\" activation function in the output layer instead of the \"softmax\" activation function, and we would use the \"binary_crossentropy\" loss.\n\n#since this is a classifier, it’s useful to measure its \"accuracy\" during training\n\nmodel.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n              optimizer=tf.keras.optimizers.SGD(),\n              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n\nThis is equivalent to:\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"sgd\",\n              metrics=[\"accuracy\"])\nYou can easily convert between one-hot vector and class ID.\n\ntf.keras.utils.to_categorical([0, 5, 1, 0], num_classes=10)\n\narray([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)\n\n\n\nnp.argmax(\n    [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n     [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n     [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n     [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n    axis=1\n)\n\narray([0, 5, 1, 0])\n\n\n\n\n8.4.3 Training and Evaluating the Model\nNow the model is ready to be trained. For this we simply need to call its fit() method. We pass it the input features (X_train) and the target classes (y_train), as well as the number of epochs to train. We also pass a validation set (this is optional): Keras will measure the loss and the extra metrics on this set at the end of each epoch.\n\n# Instead of passing a validation set using the validation_data\n# argument, you could instead set validation_split to the ratio of\n# the training set that you want Keras to use for validation (e.g., 0.1).\nhistory = model.fit(X_train, y_train, epochs=30,\n                    validation_data=(X_valid, y_valid))\n\nEpoch 1/30\n1719/1719 [==============================] - 10s 3ms/step - loss: 0.7154 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.4979 - val_sparse_categorical_accuracy: 0.8282\nEpoch 2/30\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.4833 - sparse_categorical_accuracy: 0.8307 - val_loss: 0.4622 - val_sparse_categorical_accuracy: 0.8326\nEpoch 3/30\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.4374 - sparse_categorical_accuracy: 0.8464 - val_loss: 0.4222 - val_sparse_categorical_accuracy: 0.8514\nEpoch 4/30\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.4125 - sparse_categorical_accuracy: 0.8573 - val_loss: 0.3931 - val_sparse_categorical_accuracy: 0.8620\nEpoch 5/30\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3921 - sparse_categorical_accuracy: 0.8633 - val_loss: 0.3920 - val_sparse_categorical_accuracy: 0.8624\nEpoch 6/30\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.3761 - sparse_categorical_accuracy: 0.8683 - val_loss: 0.3939 - val_sparse_categorical_accuracy: 0.8632\nEpoch 7/30\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.3635 - sparse_categorical_accuracy: 0.8715 - val_loss: 0.3710 - val_sparse_categorical_accuracy: 0.8720\nEpoch 8/30\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3527 - sparse_categorical_accuracy: 0.8761 - val_loss: 0.3803 - val_sparse_categorical_accuracy: 0.8568\nEpoch 9/30\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.3422 - sparse_categorical_accuracy: 0.8801 - val_loss: 0.3510 - val_sparse_categorical_accuracy: 0.8742\nEpoch 10/30\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3324 - sparse_categorical_accuracy: 0.8814 - val_loss: 0.3536 - val_sparse_categorical_accuracy: 0.8756\nEpoch 11/30\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.3241 - sparse_categorical_accuracy: 0.8855 - val_loss: 0.3759 - val_sparse_categorical_accuracy: 0.8634\nEpoch 12/30\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3167 - sparse_categorical_accuracy: 0.8872 - val_loss: 0.3480 - val_sparse_categorical_accuracy: 0.8736\nEpoch 13/30\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.3096 - sparse_categorical_accuracy: 0.8907 - val_loss: 0.3302 - val_sparse_categorical_accuracy: 0.8816\nEpoch 14/30\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.3026 - sparse_categorical_accuracy: 0.8916 - val_loss: 0.3392 - val_sparse_categorical_accuracy: 0.8792\nEpoch 15/30\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.2966 - sparse_categorical_accuracy: 0.8940 - val_loss: 0.3374 - val_sparse_categorical_accuracy: 0.8802\nEpoch 16/30\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2897 - sparse_categorical_accuracy: 0.8960 - val_loss: 0.3331 - val_sparse_categorical_accuracy: 0.8790\nEpoch 17/30\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.2844 - sparse_categorical_accuracy: 0.8982 - val_loss: 0.3423 - val_sparse_categorical_accuracy: 0.8768\nEpoch 18/30\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2790 - sparse_categorical_accuracy: 0.8995 - val_loss: 0.3291 - val_sparse_categorical_accuracy: 0.8820\nEpoch 19/30\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2740 - sparse_categorical_accuracy: 0.9013 - val_loss: 0.3524 - val_sparse_categorical_accuracy: 0.8724\nEpoch 20/30\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.2688 - sparse_categorical_accuracy: 0.9035 - val_loss: 0.3194 - val_sparse_categorical_accuracy: 0.8848\nEpoch 21/30\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2639 - sparse_categorical_accuracy: 0.9049 - val_loss: 0.3208 - val_sparse_categorical_accuracy: 0.8834\nEpoch 22/30\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.2591 - sparse_categorical_accuracy: 0.9054 - val_loss: 0.3151 - val_sparse_categorical_accuracy: 0.8828\nEpoch 23/30\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2547 - sparse_categorical_accuracy: 0.9091 - val_loss: 0.3433 - val_sparse_categorical_accuracy: 0.8764\nEpoch 24/30\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.2496 - sparse_categorical_accuracy: 0.9112 - val_loss: 0.3240 - val_sparse_categorical_accuracy: 0.8832\nEpoch 25/30\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2454 - sparse_categorical_accuracy: 0.9119 - val_loss: 0.3186 - val_sparse_categorical_accuracy: 0.8828\nEpoch 26/30\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.2418 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.3136 - val_sparse_categorical_accuracy: 0.8852\nEpoch 27/30\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2368 - sparse_categorical_accuracy: 0.9149 - val_loss: 0.3232 - val_sparse_categorical_accuracy: 0.8854\nEpoch 28/30\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.2339 - sparse_categorical_accuracy: 0.9164 - val_loss: 0.3137 - val_sparse_categorical_accuracy: 0.8900\nEpoch 29/30\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2298 - sparse_categorical_accuracy: 0.9173 - val_loss: 0.3207 - val_sparse_categorical_accuracy: 0.8860\nEpoch 30/30\n1719/1719 [==============================] - 6s 3ms/step - loss: 0.2267 - sparse_categorical_accuracy: 0.9194 - val_loss: 0.3100 - val_sparse_categorical_accuracy: 0.8902\n\n\nThe fit() method returns a History object containing the training parameters (history.params), the list of epochs it went through (history.epoch), and most importantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set.\n\nhistory.params\n\n{'verbose': 1, 'epochs': 30, 'steps': 1719}\n\n\n\nhistory.history.keys()\n\ndict_keys(['loss', 'sparse_categorical_accuracy', 'val_loss', 'val_sparse_categorical_accuracy'])\n\n\n\npd.DataFrame(history.history).plot(\n    figsize=(8, 5), xlim=[0, 29], ylim=[0, 1], grid=True, xlabel=\"Epoch\",\n    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\nplt.legend(loc=\"lower left\")\nplt.show()\n\n\n\n\nYou can tell that the model has not quite converged yet, as the validation loss is still going down, so you should probably continue training. It’s as simple as calling the fit() method again, since Keras just continues training where it left off.\nOnce you are satisfied with your model’s validation accuracy, you should evaluate it on the test set to estimate the generalization error before you deploy the model to production. You can easily do this using the evaluate() method.\n\nmodel.evaluate(X_test, y_test)\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.3226 - sparse_categorical_accuracy: 0.8860\n\n\n[0.32261887192726135, 0.8859999775886536]\n\n\nIt is common to get slightly lower performance on the test set than on the validation set, because the hyperparameters are tuned on the validation set, not the test set (however, in this example, we did not do any hyperparameter tuning, so the lower accuracy is just bad luck).\n\n\n8.4.4 Using the Model to Make Predictions\nNext, we can use the model’s predict() method to make predictions on new instances. We will just use the first 3 instances of the test set:\n\nX_new = X_test[:3]\ny_proba = model.predict(X_new)\ny_proba.round(2)\n\n1/1 [==============================] - 0s 113ms/step\n\n\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.97],\n       [0.  , 0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n      dtype=float32)\n\n\nAs you can see, for each instance the model estimates one probability per class, from class 0 to class 9. For example, for the first image it estimates that the probability of class 9 (ankle boot) is 95%, the probability of class 7 (sneaker) is 4%, and the other classes are negligible. In other words, it “believes” it’s footwear, probably ankle boots, but it’s not entirely sure, it might be sneakers instead. If you only care about the class with the highest estimated probability then you can use the following code:\n\ny_pred = y_proba.argmax(axis=-1)\ny_pred\n\narray([9, 2, 1])\n\n\n\nnp.array(class_names)[y_pred]\n\narray(['Ankle boot', 'Pullover', 'Trouser'], dtype='&lt;U11')\n\n\n\ny_new = y_test[:3] # The classifier actually classified all three images correctly\ny_new\n\narray([9, 2, 1], dtype=uint8)\n\n\n\n\n8.4.5 Try different network architecture and hyperparameters\n\n# Sometimes applying BN before the activation function works better (there's a debate on this topic)\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape=[28, 28]),\n    tf.keras.layers.Dense(300, kernel_initializer=tf.keras.initializers.HeNormal),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Activation(\"swish\"),\n    tf.keras.layers.Dropout(rate=0.3),    \n    tf.keras.layers.Dense(100, kernel_initializer=tf.keras.initializers.HeNormal),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Activation(\"swish\"),\n    tf.keras.layers.Dropout(rate=0.3),    \n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\n\n\nmodel.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_1 (Flatten)         (None, 784)               0         \n                                                                 \n dense_3 (Dense)             (None, 300)               235500    \n                                                                 \n batch_normalization (BatchN  (None, 300)              1200      \n ormalization)                                                   \n                                                                 \n activation (Activation)     (None, 300)               0         \n                                                                 \n dropout (Dropout)           (None, 300)               0         \n                                                                 \n dense_4 (Dense)             (None, 100)               30100     \n                                                                 \n batch_normalization_1 (Batc  (None, 100)              400       \n hNormalization)                                                 \n                                                                 \n activation_1 (Activation)   (None, 100)               0         \n                                                                 \n dropout_1 (Dropout)         (None, 100)               0         \n                                                                 \n dense_5 (Dense)             (None, 10)                1010      \n                                                                 \n=================================================================\nTotal params: 268,210\nTrainable params: 267,410\nNon-trainable params: 800\n_________________________________________________________________\n\n\n\nlr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.9)\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\nn_epochs = 30\nhistory = model.fit(X_train, y_train, epochs=n_epochs,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[lr_scheduler])\n\nEpoch 1/30\n1719/1719 [==============================] - 10s 5ms/step - loss: 0.5526 - accuracy: 0.8029 - val_loss: 0.4146 - val_accuracy: 0.8486 - lr: 0.0500\nEpoch 2/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.4318 - accuracy: 0.8441 - val_loss: 0.3784 - val_accuracy: 0.8612 - lr: 0.0500\nEpoch 3/30\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.3955 - accuracy: 0.8555 - val_loss: 0.3812 - val_accuracy: 0.8642 - lr: 0.0500\nEpoch 4/30\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.3691 - accuracy: 0.8660 - val_loss: 0.3280 - val_accuracy: 0.8766 - lr: 0.0500\nEpoch 5/30\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.3551 - accuracy: 0.8705 - val_loss: 0.3447 - val_accuracy: 0.8682 - lr: 0.0500\nEpoch 6/30\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.3398 - accuracy: 0.8754 - val_loss: 0.3373 - val_accuracy: 0.8742 - lr: 0.0500\nEpoch 7/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.3249 - accuracy: 0.8811 - val_loss: 0.3297 - val_accuracy: 0.8808 - lr: 0.0500\nEpoch 8/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.3138 - accuracy: 0.8843 - val_loss: 0.3136 - val_accuracy: 0.8840 - lr: 0.0500\nEpoch 9/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.3078 - accuracy: 0.8859 - val_loss: 0.3263 - val_accuracy: 0.8766 - lr: 0.0500\nEpoch 10/30\n1719/1719 [==============================] - 12s 7ms/step - loss: 0.2978 - accuracy: 0.8896 - val_loss: 0.3302 - val_accuracy: 0.8770 - lr: 0.0500\nEpoch 11/30\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.2939 - accuracy: 0.8919 - val_loss: 0.3071 - val_accuracy: 0.8892 - lr: 0.0500\nEpoch 12/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.2828 - accuracy: 0.8945 - val_loss: 0.3982 - val_accuracy: 0.8634 - lr: 0.0500\nEpoch 13/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.2746 - accuracy: 0.8971 - val_loss: 0.3154 - val_accuracy: 0.8880 - lr: 0.0500\nEpoch 14/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.2706 - accuracy: 0.8998 - val_loss: 0.3416 - val_accuracy: 0.8774 - lr: 0.0500\nEpoch 15/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.2661 - accuracy: 0.9007 - val_loss: 0.3111 - val_accuracy: 0.8894 - lr: 0.0500\nEpoch 16/30\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.2592 - accuracy: 0.9032 - val_loss: 0.2984 - val_accuracy: 0.8900 - lr: 0.0500\nEpoch 17/30\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.2547 - accuracy: 0.9049 - val_loss: 0.3290 - val_accuracy: 0.8878 - lr: 0.0500\nEpoch 18/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.2477 - accuracy: 0.9068 - val_loss: 0.3126 - val_accuracy: 0.8872 - lr: 0.0500\nEpoch 19/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.2438 - accuracy: 0.9081 - val_loss: 0.2994 - val_accuracy: 0.8914 - lr: 0.0500\nEpoch 20/30\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.2407 - accuracy: 0.9102 - val_loss: 0.3374 - val_accuracy: 0.8800 - lr: 0.0500\nEpoch 21/30\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.2346 - accuracy: 0.9123 - val_loss: 0.3032 - val_accuracy: 0.8880 - lr: 0.0500\nEpoch 22/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.2105 - accuracy: 0.9206 - val_loss: 0.2887 - val_accuracy: 0.8958 - lr: 0.0250\nEpoch 23/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.2019 - accuracy: 0.9258 - val_loss: 0.3020 - val_accuracy: 0.8984 - lr: 0.0250\nEpoch 24/30\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.1984 - accuracy: 0.9243 - val_loss: 0.2939 - val_accuracy: 0.8960 - lr: 0.0250\nEpoch 25/30\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.1933 - accuracy: 0.9265 - val_loss: 0.3062 - val_accuracy: 0.8966 - lr: 0.0250\nEpoch 26/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.1947 - accuracy: 0.9268 - val_loss: 0.3093 - val_accuracy: 0.8924 - lr: 0.0250\nEpoch 27/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.1919 - accuracy: 0.9275 - val_loss: 0.3100 - val_accuracy: 0.8990 - lr: 0.0250\nEpoch 28/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.1743 - accuracy: 0.9354 - val_loss: 0.2958 - val_accuracy: 0.8992 - lr: 0.0125\nEpoch 29/30\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.1739 - accuracy: 0.9354 - val_loss: 0.2910 - val_accuracy: 0.9004 - lr: 0.0125\nEpoch 30/30\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.1685 - accuracy: 0.9367 - val_loss: 0.2951 - val_accuracy: 0.8994 - lr: 0.0125\n\n\n\nmodel.evaluate(X_test, y_test)\n\n\n# Perform MC Dropout \ny_probas = np.stack([model(X_test, training=True)\n                     for sample in range(100)])\ny_proba = y_probas.mean(axis=0)\ny_std = y_probas.std(axis=0)\ny_pred = np.argmax(y_proba, axis=1)\n\n\naccuracy = np.sum(y_pred == y_test) / len(y_test)\naccuracy"
  },
  {
    "objectID": "08_neural_nets_with_tensorflow.html#building-a-regression-mlp-using-the-sequential-api",
    "href": "08_neural_nets_with_tensorflow.html#building-a-regression-mlp-using-the-sequential-api",
    "title": "8  Introduction to Artificial Neural Networks - Tensorflow",
    "section": "8.5 Building a Regression MLP Using the Sequential API",
    "text": "8.5 Building a Regression MLP Using the Sequential API\nLet’s load, split and scale the California housing dataset (the original one, not the modified one as in our first lecture. This dataset is simpler than the one we used, since it contains only numerical features (there is no ocean_proximity feature, and there is no missing value.):\nhttps://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset\n\nhousing = fetch_california_housing()\n\nX_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n\n# Scale the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_valid = scaler.transform(X_valid)\nX_test = scaler.transform(X_test)\n\n\nhousing.data.shape, housing.target\n\n\npd.DataFrame(housing.data, columns=housing.feature_names)\n\nBuilding, training, evaluating and using a regression MLP using the Sequential API to make predictions is quite similar to what we did for classification. The main differences are the fact that the output layer has a single neuron (since we only want to predict a single value) and uses no activation function, and the loss function is the mean squared error.\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(input_shape=X_train.shape[1:]),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.summary()\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\nmodel.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\nhistory = model.fit(X_train, y_train, epochs=20,\n                    validation_data=(X_valid, y_valid))\n\n\nmse_test, rmse_test = model.evaluate(X_test, y_test)\nX_new = X_test[:3]\ny_pred = model.predict(X_new)\nrmse_test, y_pred"
  },
  {
    "objectID": "08_neural_nets_with_tensorflow.html#building-complex-models-using-the-functional-api-optional",
    "href": "08_neural_nets_with_tensorflow.html#building-complex-models-using-the-functional-api-optional",
    "title": "8  Introduction to Artificial Neural Networks - Tensorflow",
    "section": "8.6 Building Complex Models Using the Functional API (Optional)",
    "text": "8.6 Building Complex Models Using the Functional API (Optional)\nNot all neural network models are simply sequential. Some may have complex topologies. Some may have multiple inputs and/or multiple outputs. For example, a Wide & Deep neural network (see paper) connects all or part of the inputs directly to the output layer. This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path). In contrast, a regular MLP forces all the data to flow through the full stack of layers, thus simple patterns in the data may end up being distorted by this sequence of transformations.\n\n\n\n\nnormalization_layer = tf.keras.layers.Normalization()\nhidden_layer1 = tf.keras.layers.Dense(30, activation=\"relu\")\nhidden_layer2 = tf.keras.layers.Dense(30, activation=\"relu\")\nconcat_layer = tf.keras.layers.Concatenate()\noutput_layer = tf.keras.layers.Dense(1)\n\ninput_ = tf.keras.layers.Input(shape=X_train.shape[1:])\nnormalized = normalization_layer(input_)\nhidden1 = hidden_layer1(normalized)\nhidden2 = hidden_layer2(hidden1)\nconcat = concat_layer([normalized, hidden2])\noutput = output_layer(concat)\n\nmodel = tf.keras.Model(inputs=[input_], outputs=[output])\n\n\nFirst, we need to create an Input object. This is needed because we may have multiple inputs, as we will see later.\nNext, we create a Dense layer with 30 neurons and using the ReLU activation function. As soon as it is created, notice that we call it like a function, passing it the input. This is why this is called the Functional API. Note that we are just telling Keras how it should connect the layers together, no actual data is being processed yet.\nWe create a Concatenate() layer, and once again we immediately use it like a function, to concatenate the input layer and the output of the second hidden layer.\nLastly, we create a Keras Model, specifying which inputs and outputs to use.\n\n\nThe Normalization layer learns the feature means and standard deviations in the training data when you call the adapt() method. Yet when you display the model’s summary, these statistics are listed as non-trainable. This is because these parameters are not affected by gradient descent.\n\n\nmodel.summary()\n\n\ntf.keras.utils.plot_model(model, show_shapes=True)\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\nmodel.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\nnormalization_layer.adapt(X_train) # Normalize the input\n\nhistory = model.fit(X_train, y_train, epochs=20,\n                    validation_data=(X_valid, y_valid))\nmse_test = model.evaluate(X_test, y_test)\ny_pred = model.predict(X_new)\n\n\nmse_test, rmse_test = model.evaluate(X_test, y_test)\nX_new = X_test[:3]\ny_pred = model.predict(X_new)\nrmse_test, y_pred\n\nBut what if you want to send a subset of the features through the wide path, and a different subset (possibly overlapping) through the deep path? In this case, one solution is to use multiple inputs. For example, suppose we want to send 5 features through the deep path (features 0 to 4), and 6 features through the wide path (features 2 to 7).\n\n\n\n\ninput_wide = tf.keras.layers.Input(shape=[5])  # features 0 to 4\ninput_deep = tf.keras.layers.Input(shape=[6])  # features 2 to 7\nnorm_layer_wide = tf.keras.layers.Normalization()\nnorm_layer_deep = tf.keras.layers.Normalization()\nnorm_wide = norm_layer_wide(input_wide)\nnorm_deep = norm_layer_deep(input_deep)\nhidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\nhidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\nconcat = tf.keras.layers.concatenate([norm_wide, hidden2])\noutput = tf.keras.layers.Dense(1)(concat)\nmodel = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output])\n\n\nmodel.summary()\n\n\ntf.keras.utils.plot_model(model, show_shapes=True)\n\nOn the other hand, there are also many use cases in which you may want to have multiple outputs: 1. The task may demand it, for example you may want to locate and classify the main object in a picture. This is both a regression task (finding the coordinates of the object’s center, as well as its width and height) and a classification task. 2. Similarly, you may have multiple independent tasks to perform based on the same data. Sure, you could train one neural network per task, but in many cases you will get better results on all tasks by training a single neural network with one output per task. This is because the neural network can learn features in the data that are useful across tasks. For instance, you may want to build a multitask classification on pictures of facesm using one out put to classify the facial expression while another to identify whether they are wearing glasses or not. 3. Another use case is as a regularization technique (i.e., a training constraint whose objective is to reduce overfitting and thus improve the model’s ability to generalize). For example, you may want to add some auxiliary outputs in a neural network architecture to ensure that the underlying part of the network learns something useful on its own, without relying on the rest of thenetwork. Adding an auxiliary output for regularization:\nThe network we would like to build is thus like the following figure:\n\n\n\nAdding extra outputs is quite easy: just connect them to the appropriate layers and add them to your model’s list of outputs\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)\n\ninput_wide = tf.keras.layers.Input(shape=[5])  # features 0 to 4\ninput_deep = tf.keras.layers.Input(shape=[6])  # features 2 to 7\nnorm_layer_wide = tf.keras.layers.Normalization()\nnorm_layer_deep = tf.keras.layers.Normalization()\nnorm_wide = norm_layer_wide(input_wide)\nnorm_deep = norm_layer_deep(input_deep)\nhidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\nhidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\nconcat = tf.keras.layers.concatenate([norm_wide, hidden2])\noutput = tf.keras.layers.Dense(1)(concat)\naux_output = tf.keras.layers.Dense(1)(hidden2)\nmodel = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output, aux_output])\n\n\nmodel.summary()\n\n\ntf.keras.utils.plot_model(model, show_shapes=True)\n\nEach output will need its own loss function, so when we compile the model we should pass a list of losses. By default, Keras will compute all these losses and simply add them up to get the final loss used for training. However, we care much more about the main output than about the auxiliary output (as it is just used for regularization), so we want to give the main output’s loss a much greater weight.\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\nmodel.compile(loss=(\"mse\", \"mse\"), loss_weights=(0.9, 0.1), optimizer=optimizer,\n              metrics=[\"RootMeanSquaredError\"])\n\nNow when we train the model, we need to provide some labels for each output. In this example, the main output and the auxiliary output should try to predict the same thing, so they should use the same labels. So instead of passing y_train, we just need to pass (y_train, y_train) (and the same goes for y_valid and y_test):\n\nX_train_wide, X_train_deep = X_train[:, :5], X_train[:, 2:]\nX_valid_wide, X_valid_deep = X_valid[:, :5], X_valid[:, 2:]\nX_test_wide, X_test_deep = X_test[:, :5], X_test[:, 2:]\nX_new_wide, X_new_deep = X_test_wide[:3], X_test_deep[:3]\n\nnorm_layer_wide.adapt(X_train_wide)\nnorm_layer_deep.adapt(X_train_deep)\nhistory = model.fit(\n    (X_train_wide, X_train_deep), (y_train, y_train), epochs=20,\n    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid))\n)\n\nWhen we evaluate the model, Keras will return the total loss, as well as all the individual losses:\n\neval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\nweighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results\nweighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse\n\n\ny_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))"
  },
  {
    "objectID": "08_neural_nets_with_tensorflow.html#building-dynamic-models-using-the-subclassing-api-optional",
    "href": "08_neural_nets_with_tensorflow.html#building-dynamic-models-using-the-subclassing-api-optional",
    "title": "8  Introduction to Artificial Neural Networks - Tensorflow",
    "section": "8.7 Building Dynamic Models Using the Subclassing API (Optional)",
    "text": "8.7 Building Dynamic Models Using the Subclassing API (Optional)\nSome models involve loops, varying shapes, conditional branching, and other dynamic behaviors. For such cases, or simply if you prefer a more imperative programming style, the Subclassing API is for you.\nSimply subclass the Model class, create the layers you need in the constructor, and use them to perform the computations you want in the call() method. For example, creating an instance of the following WideAndDeepModel class gives us an equivalent model to the one we just built with the Functional API.\n\nclass WideAndDeepModel(tf.keras.Model):\n    def __init__(self, units=30, activation=\"relu\", **kwargs):\n        super().__init__(**kwargs)  # needed to support naming the model\n        self.norm_layer_wide = tf.keras.layers.Normalization()\n        self.norm_layer_deep = tf.keras.layers.Normalization()\n        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)\n        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)\n        self.main_output = tf.keras.layers.Dense(1)\n        self.aux_output = tf.keras.layers.Dense(1)\n        \n    def call(self, inputs):\n        input_wide, input_deep = inputs\n        norm_wide = self.norm_layer_wide(input_wide)\n        norm_deep = self.norm_layer_deep(input_deep)\n        hidden1 = self.hidden1(norm_deep)\n        hidden2 = self.hidden2(hidden1)\n        concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n        output = self.main_output(concat)\n        aux_output = self.aux_output(hidden2)\n        return output, aux_output\n\nmodel = WideAndDeepModel(30, activation=\"relu\", name=\"my_cool_model\")\n\nThis example looks very much like the Functional API, except we do not need to create the inputs, we just use the input argument to the call() method, and we separate the creation of the layers in the constructor from their usage in the call() method. However, the big difference is that you can do pretty much anything you want in the call() method: for loops, if statements, low-level TensorFlow operations! This makes it a great API for researchers experimenting with new ideas.\nHowever, this extra flexibility comes at a cost: your model’s architecture is hidden within the call() method, so Keras cannot easily inspect it, it cannot save or clone it, and when you call the summary() method, you only get a list of layers, without any information on how they are connected to each other.Moreover, Keras cannot check types and shapes ahead of time, and it is easier to make mistakes!\n\ninput_shape=[(None, 5), (None, 6)]\nmodel.build(input_shape) # We have to tell keras the input shape to count the number of parameters\nmodel.summary()\n\n\ntf.keras.utils.plot_model(model, show_shapes=True)\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\nmodel.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=optimizer,\n              metrics=[\"RootMeanSquaredError\"])\n\n\nmodel.norm_layer_wide.adapt(X_train_wide)\nmodel.norm_layer_deep.adapt(X_train_deep)\nhistory = model.fit(\n    (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,\n    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)))\neval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\nweighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results\ny_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))"
  },
  {
    "objectID": "08_neural_nets_with_tensorflow.html#saving-and-restoring",
    "href": "08_neural_nets_with_tensorflow.html#saving-and-restoring",
    "title": "8  Introduction to Artificial Neural Networks - Tensorflow",
    "section": "8.8 Saving and Restoring",
    "text": "8.8 Saving and Restoring\nWhen you set save_format=\"tf\", Keras saves the model using TensorFlow’s Saved‐Model format: this is a directory (with the given name) containing several files and subdirectories. In particular, the saved_model.pb file contains the model’s architecture and logic in the form of a serialized computation graph, so you don’t need to deploy the model’s source code in order to use it in production; the SavedModel is sufficient. The keras_metadata.pb file contains extra information needed by Keras. The variables subdirectory contains all the parameter values (including the connection weights, the biases, the normalization statistics, and the optimizer’s parameters), possibly split across multiple files if the model is very large. Lastly, the assets directory may contain extra files, such as data samples, feature names, class names, and so on. By default, the assets directory is empty. Since the optimizer is also saved, including its hyperparameters and any state it may have, after loading the model you can continue training if you want.\n\nIf you set save_format=\"h5\" or use a filename that ends with .h5, .hdf5, or .keras, then Keras will save the model to a single file using a Keras-specific format based on the HDF5 format. However, most TensorFlow deployment tools require the SavedModel format instead.\n\n\nmodel.save(\"my_keras_model\", save_format=\"tf\")\n\n\nfor path in sorted(Path(\"my_keras_model\").glob(\"**/*\")):\n    print(path)\n\nYou can then load the model using tf.keras.models.load_model():\n\nmodel = tf.keras.models.load_model(\"my_keras_model\")\ny_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))\n\nYou can also use save_weights() and load_weights() to save and load only the parameter values. This includes the connection weights, biases, preprocessing stats, optimizer state, etc. The parameter values are saved in one or more files such as my_weights.data-00004-of-00052, plus an index file like my_weights.index.\n\nmodel.save_weights(\"my_weights\")\n\n\nmodel.load_weights(\"my_weights\") # Note that you have to get the model before loading weights here\n\n\nfor path in sorted(Path().glob(\"my_weights.*\")):\n    print(path)"
  },
  {
    "objectID": "08_neural_nets_with_tensorflow.html#using-callbacks-during-training",
    "href": "08_neural_nets_with_tensorflow.html#using-callbacks-during-training",
    "title": "8  Introduction to Artificial Neural Networks - Tensorflow",
    "section": "8.9 Using Callbacks during Training",
    "text": "8.9 Using Callbacks during Training\nBut what if training lasts several hours? In this case, you should not only save your model at the end of training, but also save checkpoints at regular intervals during training. The fit() method accepts a callbacks argument that lets you specify a list of objects that Keras will call during training at the start and end of training, at the start and end of each epoch and even before and after processing each batch. For example, the ModelCheckpoint callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch:\nhttps://keras.io/callbacks/\n\n# if you use a validation set during training, you can set\n# save_best_only=True when creating the ModelCheckpoint. In this case, it will only\n# save your model when its performance on the validation set is the best so far.\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_checkpoints\", save_best_only=True)\n\nhistory = model.fit(\n    (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,\n    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)),\n    callbacks=[checkpoint_cb])\n\nAnother way to implement early stopping is to simply use the EarlyStopping callback. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the patience argument), and it will optionally roll back to the best model. You can combine both callbacks to both save checkpoints of your model (in case your computer crashes), and actually interrupt training early when there is no more progress (to avoid wasting time and resources):\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n\nhistory = model.fit(\n    (X_train_wide, X_train_deep), (y_train, y_train), epochs=100,\n    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)),\n    callbacks=[checkpoint_cb, early_stopping_cb])\n\nThe number of epochs can be set to a large value since training will stop automatically when there is no more progress (just make sure the learning rate is not too small, or else it might keep making slow progress until the end). The EarlyStopping callback will store the weights of the best model in RAM, and it will restore them for you at the end of training.\nIf you need extra control, you can easily write your own custom callbacks. For example, the following custom callback will display the ratio between the validation loss and the training loss during training (e.g., to detect overfitting):\n\nclass PrintValTrainRatioCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        ratio = logs[\"val_loss\"] / logs[\"loss\"]\n        print(f\"Epoch={epoch}, val/train={ratio:.2f}\")\n\n\nval_train_ratio_cb = PrintValTrainRatioCallback()\nhistory = model.fit(\n    (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,\n    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)),\n    callbacks=[val_train_ratio_cb], verbose=0) # Note that we use verbose=0 here"
  },
  {
    "objectID": "08_neural_nets_with_tensorflow.html#tensorboard",
    "href": "08_neural_nets_with_tensorflow.html#tensorboard",
    "title": "8  Introduction to Artificial Neural Networks - Tensorflow",
    "section": "8.10 TensorBoard",
    "text": "8.10 TensorBoard\nNow let’s take a look at one more tool you should definitely have in your toolbox when using tf.keras: TensorBoard. TensorBoard is a great interactive visualization tool that you can use to view the learning curves during training, compare learning curves between multiple runs, visualize the computation graph, analyze training statistics, view images generated by your model, and more!\nTo use it, you must modify your program so that it outputs the data you want to visualize to special binary log files called event files. Each binary data record is called a summary. The TensorBoard server will monitor the log directory, and it will automatically pick up the changes and update the visualizations.\nIn general, you want to point the TensorBoard server to a root log directory, and configure your program so that it writes to a different subdirectory every time it runs. So let’s start by defining the root log directory we will use for our TensorBoard logs, plus a small function that will generate a subdirectory path based on the current date and time, so that it is different at every run.\n\nroot_logdir = os.path.join(os.curdir, \"my_logs\")\n\n\ndef get_run_logdir(root_logdir=\"my_logs\"):\n    return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n\nrun_logdir = get_run_logdir()\nrun_logdir\n\n\n# builds the first regression model we used earlier\n\nnorm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\nmodel = tf.keras.Sequential([\n    norm_layer,\n    tf.keras.layers.Dense(30, activation=\"relu\"),\n    tf.keras.layers.Dense(30, activation=\"relu\"),\n    tf.keras.layers.Dense(1)\n])\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\nmodel.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\nnorm_layer.adapt(X_train)\n\n\ntensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir, profile_batch=(100, 200))\nhistory = model.fit(X_train, y_train, epochs=20,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[tensorboard_cb])\n\n\nprint(\"my_logs\")\nfor path in sorted(Path(\"my_logs\").glob(\"**/*\")):\n    print(\"  \" * (len(path.parts) - 1) + path.parts[-1])\n\nTo start the TensorBoard server, one option is to open a terminal, if needed activate the virtualenv where you installed TensorBoard, go to this notebook’s directory, then type:\n$ tensorboard --logdir=./my_logs --port=6006\nYou can then open your web browser to localhost:6006 and use TensorBoard. Once you are done, press Ctrl-C in the terminal window, this will shutdown the TensorBoard server.\nAlternatively, you can load TensorBoard’s Jupyter extension and run it like this:\n\n%load_ext tensorboard\n%tensorboard --logdir=./my_logs --port=6006\n\n\nfrom tensorboard import notebook\nnotebook.list() # View open TensorBoard instances\n\nCheck out the other available logging options:\nhttps://www.tensorflow.org/tensorboard/get_started"
  },
  {
    "objectID": "08_neural_nets_with_tensorflow.html#references",
    "href": "08_neural_nets_with_tensorflow.html#references",
    "title": "8  Introduction to Artificial Neural Networks - Tensorflow",
    "section": "8.11 References",
    "text": "8.11 References\n\nhttps://github.com/ageron/handson-ml3/"
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_tensorflow.html#setup",
    "href": "09_Convolutional_NeuralNetworks_tensorflow.html#setup",
    "title": "9  Image processing with Convolutional Neural Networks - Tensorflow",
    "section": "9.1 Setup",
    "text": "9.1 Setup\n\n!apt-get install tree -qq\n!pip install --upgrade git+https://github.com/keras-team/keras-cv -qq\n!pip install git+https://github.com/divamgupta/image-segmentation-keras -qq\n!pip install git+https://github.com/cleanlab/cleanvision.git -qq\n!pip install cleanlab -qq\n!pip install scikeras -qq\n\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting git+https://github.com/divamgupta/image-segmentation-keras\n  Cloning https://github.com/divamgupta/image-segmentation-keras to /tmp/pip-req-build-zzw3x8ac\n  Running command git clone --filter=blob:none --quiet https://github.com/divamgupta/image-segmentation-keras /tmp/pip-req-build-zzw3x8ac\n  Resolved https://github.com/divamgupta/image-segmentation-keras to commit 750a44ca16c0ca3355c9486026377a239635df4d\n  Preparing metadata (setup.py) ... done\nCollecting h5py&lt;=2.10.0\n  Downloading h5py-2.10.0.tar.gz (301 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.1/301.1 kB 22.6 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: Keras&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from keras-segmentation==0.3.0) (2.12.0)\nCollecting imageio==2.5.0\n  Downloading imageio-2.5.0-py3-none-any.whl (3.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 92.1 MB/s eta 0:00:00\nRequirement already satisfied: imgaug&gt;=0.4.0 in /usr/local/lib/python3.10/dist-packages (from keras-segmentation==0.3.0) (0.4.0)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from keras-segmentation==0.3.0) (4.7.0.72)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from keras-segmentation==0.3.0) (4.65.0)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imageio==2.5.0-&gt;keras-segmentation==0.3.0) (9.5.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio==2.5.0-&gt;keras-segmentation==0.3.0) (1.22.4)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from h5py&lt;=2.10.0-&gt;keras-segmentation==0.3.0) (1.16.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug&gt;=0.4.0-&gt;keras-segmentation==0.3.0) (1.10.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug&gt;=0.4.0-&gt;keras-segmentation==0.3.0) (3.7.1)\nRequirement already satisfied: scikit-image&gt;=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug&gt;=0.4.0-&gt;keras-segmentation==0.3.0) (0.19.3)\nRequirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug&gt;=0.4.0-&gt;keras-segmentation==0.3.0) (2.0.1)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image&gt;=0.14.2-&gt;imgaug&gt;=0.4.0-&gt;keras-segmentation==0.3.0) (23.1)\nRequirement already satisfied: networkx&gt;=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image&gt;=0.14.2-&gt;imgaug&gt;=0.4.0-&gt;keras-segmentation==0.3.0) (3.1)\nRequirement already satisfied: PyWavelets&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image&gt;=0.14.2-&gt;imgaug&gt;=0.4.0-&gt;keras-segmentation==0.3.0) (1.4.1)\nRequirement already satisfied: tifffile&gt;=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image&gt;=0.14.2-&gt;imgaug&gt;=0.4.0-&gt;keras-segmentation==0.3.0) (2023.4.12)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;imgaug&gt;=0.4.0-&gt;keras-segmentation==0.3.0) (4.39.3)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;imgaug&gt;=0.4.0-&gt;keras-segmentation==0.3.0) (3.0.9)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;imgaug&gt;=0.4.0-&gt;keras-segmentation==0.3.0) (0.11.0)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;imgaug&gt;=0.4.0-&gt;keras-segmentation==0.3.0) (1.0.7)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;imgaug&gt;=0.4.0-&gt;keras-segmentation==0.3.0) (1.4.4)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;imgaug&gt;=0.4.0-&gt;keras-segmentation==0.3.0) (2.8.2)\nBuilding wheels for collected packages: keras-segmentation, h5py\n  Building wheel for keras-segmentation (setup.py) ... done\n  Created wheel for keras-segmentation: filename=keras_segmentation-0.3.0-py3-none-any.whl size=34600 sha256=aadaccbaef653b5efcf3662e28dceba20aa03fed84c0fd59136fe6b5d5e65d85\n  Stored in directory: /tmp/pip-ephem-wheel-cache-atuxzteb/wheels/c3/c0/74/d7b2d21081981b49c0aafed6ff4c00531781dbffd31391799c\n  Building wheel for h5py (setup.py) ... done\n  Created wheel for h5py: filename=h5py-2.10.0-cp310-cp310-linux_x86_64.whl size=5620021 sha256=cc4f9c1ae4db2cdf80e9b00669bd688acb166148c0d4800d418e5177eb06e1c3\n  Stored in directory: /root/.cache/pip/wheels/21/bc/58/0d0c6056e1339f40188d136cd838c6554d9c17545196dd9110\nSuccessfully built keras-segmentation h5py\nInstalling collected packages: imageio, h5py, keras-segmentation\n  Attempting uninstall: imageio\n    Found existing installation: imageio 2.25.1\n    Uninstalling imageio-2.25.1:\n      Successfully uninstalled imageio-2.25.1\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.8.0\n    Uninstalling h5py-3.8.0:\n      Successfully uninstalled h5py-3.8.0\nSuccessfully installed h5py-2.10.0 imageio-2.5.0 keras-segmentation-0.3.0\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n\n\nWe need to restart the environment after installing.\n\n# Python ≥3.7 is recommended\nimport sys\nassert sys.version_info &gt;= (3, 7)\nimport os\nfrom pathlib import Path\nfrom time import strftime\n\n# Scikit-Learn ≥1.01 is recommended\nfrom packaging import version\nimport sklearn\nfrom sklearn.datasets import load_sample_image\nfrom sklearn.datasets import load_sample_images\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_predict\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\n# Tensorflow ≥2.8.0 is recommended\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.utils import image_dataset_from_directory\nfrom tensorflow.keras import optimizers\nimport keras_cv\nfrom keras_cv import bounding_box\nfrom keras_cv import visualization\nfrom keras_segmentation.models.unet import vgg_unet\nassert version.parse(tf.__version__) &gt;= version.parse(\"2.8.0\")\n\n# Image augmentation\nimport albumentations as A\n\n# Data centric AI\nfrom cleanvision.imagelab import Imagelab\nfrom cleanlab.filter import find_label_issues\nfrom scikeras.wrappers import KerasClassifier, KerasRegressor\n\n# Common imports\nimport numpy as np\nimport os\nimport shutil\nimport pathlib\nimport resource\nfrom functools import partial\nimport tqdm\nimport cv2\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nif not tf.config.list_physical_devices('GPU'):\n    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n    if \"google.colab\" in sys.modules:\n        print(\"Go to Runtime &gt; Change runtime and select a GPU hardware \"\n              \"accelerator.\")\n    if \"kaggle_secrets\" in sys.modules:\n        print(\"Go to Settings &gt; Accelerator and select GPU.\")\n\nA couple utility functions to plot grayscale and RGB images:\n\ndef plot_image(image):\n    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n    plt.axis(\"off\")\n\ndef plot_color_image(image):\n    plt.imshow(image, interpolation=\"nearest\")\n    plt.axis(\"off\")\n\ndef plot_examples(id_iter, nrows=1, ncols=1):\n    for count, id in enumerate(id_iter):\n        plt.subplot(nrows, ncols, count + 1)\n        plt.imshow(X[id].reshape(28, 28), cmap=\"gray\")\n        plt.title(f\"id: {id} \\n label: {labels[id]}\")\n        plt.axis(\"off\")\n\n    plt.tight_layout(h_pad=2.0)\n\n# Class mapping for pascalvoc\nclass_ids = [\n    \"Aeroplane\",\n    \"Bicycle\",\n    \"Bird\",\n    \"Boat\",\n    \"Bottle\",\n    \"Bus\",\n    \"Car\",\n    \"Cat\",\n    \"Chair\",\n    \"Cow\",\n    \"Dining Table\",\n    \"Dog\",\n    \"Horse\",\n    \"Motorbike\",\n    \"Person\",\n    \"Potted Plant\",\n    \"Sheep\",\n    \"Sofa\",\n    \"Train\",\n    \"Tvmonitor\",\n    \"Total\",\n]\nclass_mapping = dict(zip(range(len(class_ids)), class_ids))\n\ndef visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n    inputs = next(iter(inputs.take(1)))\n    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n    visualization.plot_bounding_box_gallery(\n        images,\n        value_range=value_range,\n        rows=rows,\n        cols=cols,\n        y_true=bounding_boxes,\n        scale=5,\n        font_scale=0.7,\n        bounding_box_format=bounding_box_format,\n        class_mapping=class_mapping,\n    )\n\n\ndef unpackage_raw_tfds_inputs(inputs, bounding_box_format):\n    image = inputs[\"image\"]\n    boxes = keras_cv.bounding_box.convert_format(\n        inputs[\"objects\"][\"bbox\"],\n        images=image,\n        source=\"rel_yxyx\",\n        target=bounding_box_format,\n    )\n    bounding_boxes = {\n        \"classes\": tf.cast(inputs[\"objects\"][\"label\"], dtype=tf.float32),\n        \"boxes\": tf.cast(boxes, dtype=tf.float32),\n    }\n    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n\n\ndef load_pascal_voc(split, dataset, bounding_box_format):\n    ds = tfds.load(dataset, split=split, with_info=False, shuffle_files=True)\n    ds = ds.map(\n        lambda x: unpackage_raw_tfds_inputs(x, bounding_box_format=bounding_box_format),\n        num_parallel_calls=tf.data.AUTOTUNE,\n    )\n    return ds\n\ndef visualize_detections(model, dataset, bounding_box_format):\n    images, y_true = next(iter(dataset.take(1)))\n    y_pred = model.predict(images)\n    y_pred = bounding_box.to_ragged(y_pred)\n    visualization.plot_bounding_box_gallery(\n        images,\n        value_range=(0, 255),\n        bounding_box_format=bounding_box_format,\n        y_true=y_true,\n        y_pred=y_pred,\n        scale=4,\n        rows=2,\n        cols=4,\n        show=True,\n        font_scale=0.7,\n        class_mapping=class_mapping,\n    )"
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_tensorflow.html#what-is-a-convolution",
    "href": "09_Convolutional_NeuralNetworks_tensorflow.html#what-is-a-convolution",
    "title": "9  Image processing with Convolutional Neural Networks - Tensorflow",
    "section": "9.2 What is a Convolution?",
    "text": "9.2 What is a Convolution?\nA neuron’s weights can be represented as a small image the size of the receptive field. For example, below shows two possible sets of weights, called filters (or convolution kernels). In TensorFlow, each input image is typically represented as a 3D tensor of shape [height, width, channels]. A mini-batch is represented as a 4D tensor of shape [mini-batch size, height, width, channels]. The weights of a convolutional layer are represented as a 4D tensor of shape [fh, fw, fn', fn]. The bias terms of a convolutional layer are simply represented as a 1D tensor of shape [fn].\nLet’s look at a simple example. The following code loads two sample images, using Scikit-Learn’s load_sample_images() (which loads two color images, one of a Chinese temple, and the other of a flower). The pixel intensities (for each color channel) is represented as a byte from 0 to 255, so we scale these features simply by dividing by 255, to get floats ranging from 0 to 1. Then we create two 7 × 7 filters (one with a vertical white line in the middle, and the other with a horizontal white line in the middle).\n\n# Load sample images\nchina = load_sample_image(\"china.jpg\") / 255\nflower = load_sample_image(\"flower.jpg\") / 255\nimages = np.array([china, flower])\nbatch_size, height, width, channels = images.shape\n\n# Create 2 filters\nfilters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32) # [height, width, channel of inputs, channel of feature maps]\nfilters[:, 3, :, 0] = 1  # vertical line\nfilters[3, :, :, 1] = 1  # horizontal line\nimages.shape\n\n(2, 427, 640, 3)\n\n\n\nplot_image(filters[:, :, 0, 0])\nplt.show()\nplot_image(filters[:, :, 0, 1])\n\n\n\n\n\n\n\nNow if all neurons in a layer use the same vertical line filter (and the same bias term), and you feed the network with the image, the layer will output a feature maps. Here, we apply them to both images using the tf.nn.conv2d() function, which is part of TensorFlow’s low-level Deep Learning API. In this example, we use zero padding (padding=\"SAME\") and a stride of 1\nThe output is a 4D tensor. The dimensions are: batch size, height, width, channels. The first dimension (batch size) is 2 since there are 2 input images. The next two dimensions are the height and width of the output feature maps: since padding=\"SAME\" and strides=1, the output feature maps have the same height and width as the input images (in this case, 427×640). Lastly, this convolutional layer has 2 filters, so the last dimension is 2: there are 2 output feature maps per input image.\n\noutputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\noutputs.shape # [batches, height, width, channel of feature maps]\n\nTensorShape([2, 427, 640, 2])\n\n\n\ndef crop(images):\n    return images[150:220, 130:250] #crop for better visulization\n\nplot_image(crop(images[0, :, :, 0]))\nplt.show()\n\nfor feature_map_index, filename in enumerate([\"china_vertical\", \"china_horizontal\"]):\n    plot_image(crop(outputs[0, :, :, feature_map_index]))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\nNotice that the vertical white lines get enhanced in one feature map while the rest gets blurred. Similarly, the other feature map is what you get if all neurons use the same horizontal line filter; notice that the horizontal white lines get enhanced while the rest is blurred out. Thus, a layer full of neurons using the same filter outputs a feature map, which highlights the areas in an image that activate the filter the most. Of course you do not have to define the filters manually: instead, during training the convolutional layer will automatically learn the most useful filters for its task, and the layers above will learn to combine them into more complex patterns."
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_tensorflow.html#convolutional-layer",
    "href": "09_Convolutional_NeuralNetworks_tensorflow.html#convolutional-layer",
    "title": "9  Image processing with Convolutional Neural Networks - Tensorflow",
    "section": "9.3 Convolutional Layer",
    "text": "9.3 Convolutional Layer\nInstead of manually creating the variables, however, you can simply use the tf.keras.layers.Conv2D layer. The code below creates a Conv2D layer with 32 filters, each 7 × 7, using a stride of 1 (both horizontally and vertically), VALID padding, and applying the linear activation function to its outputs. As you can see, convolutional layers have quite a few hyperparameters: you must choose the number of filters, their height and width, the strides, and the padding type. As always, you can use cross-validation to find the right hyperparameter values, but this is very time-consuming. We will discuss common CNN architectures later, to give you some idea of what hyperparameter values work best in practice.\n\nimages = load_sample_images()[\"images\"]\nimages = tf.keras.layers.CenterCrop(height=70, width=120)(images) # Functional API\nimages = tf.keras.layers.Rescaling(scale=1 / 255)(images)\nimages.shape\n\nTensorShape([2, 70, 120, 3])\n\n\nLet’s call this layer, passing it the two test images:\n\nconv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7)\nfmaps = conv_layer(images)\nfmaps.shape\n\nTensorShape([2, 64, 114, 32])\n\n\n\nconv_layer.get_config()\n\n{'name': 'conv2d',\n 'trainable': True,\n 'dtype': 'float32',\n 'filters': 32,\n 'kernel_size': (7, 7),\n 'strides': (1, 1),\n 'padding': 'valid',\n 'data_format': 'channels_last',\n 'dilation_rate': (1, 1),\n 'groups': 1,\n 'activation': 'linear',\n 'use_bias': True,\n 'kernel_initializer': {'class_name': 'GlorotUniform',\n  'config': {'seed': None}},\n 'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n 'kernel_regularizer': None,\n 'bias_regularizer': None,\n 'activity_regularizer': None,\n 'kernel_constraint': None,\n 'bias_constraint': None}\n\n\nThe height and width have both shrunk by 6 pixels. This is due to the fact that the Conv2D layer does not use any zero-padding by default, which means that we lose a few pixels on the sides of the output feature maps, depending on the size of the filters. Since the filters are initialized randomly, they’ll initially detect random patterns. Let’s take a look at the 2 output features maps for each image:\n\nplt.figure(figsize=(15, 9))\nfor image_idx in (0, 1):\n    for fmap_idx in (0, 1):\n        plt.subplot(2, 2, image_idx * 2 + fmap_idx + 1)\n        plt.imshow(fmaps[image_idx, :, :, fmap_idx], cmap=\"gray\")\n        plt.axis(\"off\")\n\nplt.show()\n\n\n\n\nAs you can see, randomly generated filters typically act like edge detectors, which is great since that’s a useful tool in image processing, and that’s the type of filters that a convolutional layer typically starts with. Then, during training, it gradually learns improved filters to recognize useful patterns for the task.\nIf instead we set padding=\"same\", then the inputs are padded with enough zeros on all sides to ensure that the output feature maps end up with the same size as the inputs (hence the name of this option):\n\nconv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7, padding=\"same\")\nfmaps = conv_layer(images)\nfmaps.shape\n\nTensorShape([2, 70, 120, 32])\n\n\nIf the stride is greater than 1 (in any direction), then the output size will not be equal to the input size, even if padding=\"same\". For example, if you set strides=2 (or equivalently strides=(2, 2)), then the output feature maps will be 35 × 60:\n\nconv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7, padding=\"same\", strides=2)\nfmaps = conv_layer(images)\nfmaps.shape\n\nTensorShape([2, 35, 60, 32])\n\n\n\n# This utility function can be useful to compute the size of the\n# feature maps output by a convolutional layer. It also returns\n# the number of ignored rows or columns if padding=\"valid\", or the\n# number of zero-padded rows or columns if padding=\"same\".\n\n\ndef conv_output_size(input_size, kernel_size, strides=1, padding=\"valid\"):\n    if padding==\"valid\":\n        z = input_size - kernel_size + strides\n        output_size = z // strides\n        num_ignored = z % strides\n        return output_size, num_ignored\n    else:\n        output_size = (input_size - 1) // strides + 1\n        num_padded = (output_size - 1) * strides + kernel_size - input_size\n        return output_size, num_padded\n\nconv_output_size(np.array([70, 120]), kernel_size=7, strides=2, padding=\"same\")\n\n(array([35, 60]), array([5, 5]))\n\n\nJust like a Dense layer, a Conv2D layer holds all the layer’s weights, including the kernels and biases. The kernels are initialized randomly, while the biases are initialized to zero. These weights are accessible as TF variables via the weights attribute, or as NumPy arrays via the get_weights() method:\n\nkernels, biases = conv_layer.get_weights()\nkernels.shape, biases.shape\n\n((7, 7, 3, 32), (32,))\n\n\nYou can find other useful kernels here https://setosa.io/ev/image-kernels/"
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_tensorflow.html#pooling-layer",
    "href": "09_Convolutional_NeuralNetworks_tensorflow.html#pooling-layer",
    "title": "9  Image processing with Convolutional Neural Networks - Tensorflow",
    "section": "9.4 Pooling layer",
    "text": "9.4 Pooling layer\n\n9.4.1 Max pooling\nImplementing a max pooling layer in TensorFlow is quite easy. The following code creates a max pooling layer using a 2 × 2 kernel. The strides default to the kernel size, so this layer will use a stride of 2 (both horizontally and vertically). By default, it uses VALID padding (i.e., no padding at all):\n\nmax_pool = tf.keras.layers.MaxPool2D(pool_size=2)\n\n\noutput = max_pool(images)\n\n\nfig = plt.figure(figsize=(12, 8))\ngs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2, 1])\n\nax1 = fig.add_subplot(gs[0, 0])\nax1.set_title(\"Input\")\nax1.imshow(images[0])  # plot the 1st image\nax2 = fig.add_subplot(gs[0, 1])\nax2.set_title(\"Output\")\nax2.imshow(output[0])  # plot the output for the 1st image\nplt.show()\n\n\n\n\n\n\n9.4.2 Average pooling\nTo create an average pooling layer, just use AvgPool2D instead of MaxPool2D. As you might expect, it works exactly like a max pooling layer, except it computes the mean rather than the max.\n\navg_pool = tf.keras.layers.AvgPool2D(pool_size=2)\n\n\noutput = avg_pool(images)\n\n\nfig = plt.figure(figsize=(12, 8))\ngs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2, 1])\n\nax1 = fig.add_subplot(gs[0, 0])\nax1.set_title(\"Input\")\nax1.imshow(images[0])  # plot the 1st image\nax2 = fig.add_subplot(gs[0, 1])\nax2.set_title(\"Output\")\nax2.imshow(output[0])  # plot the output for the 1st image\nplt.show()\n\n\n\n\n\n\n9.4.3 Depthwise pooling\nNote that max pooling and average pooling can be performed along the depth dimension instead of the spatial dimensions, although it’s not as common. This can allow the CNN to learn to be invariant to various features. For example, it could learn multiple filters, each detecting a different rotation of the same pattern, and the depthwise max pooling layer would ensure that the output is the same regardless of the rotation. The CNN could similarly learn to be invariant to anything: thickness, brightness, skew, color, and so on.\nKeras does not include a depthwise max pooling layer, but it’s not too difficult to implement a custom layer for that:\n\nclass DepthPool(tf.keras.layers.Layer):\n    def __init__(self, pool_size=2, **kwargs):\n        super().__init__(**kwargs)\n        self.pool_size = pool_size\n    \n    def call(self, inputs):\n        shape = tf.shape(inputs)  # shape[-1] is the number of channels\n        groups = shape[-1] // self.pool_size  # number of channel groups\n        new_shape = tf.concat([shape[:-1], [groups, self.pool_size]], axis=0)\n        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)\n\n\ndepth_output = DepthPool(pool_size=3)(images)\n\nprint(depth_output.shape)\nplt.figure(figsize=(12, 8))\nplt.subplot(1, 2, 1)\nplt.title(\"Input\")\nplt.imshow(images[0])  # plot the 1st image\nplt.axis(\"off\")\nplt.subplot(1, 2, 2)\nplt.title(\"Output\")\nplt.imshow(depth_output[0, ..., 0], cmap=\"gray\")  # plot 1st image's output\nplt.axis(\"off\")\nplt.show()\n\n(2, 70, 120, 1)\n\n\n\n\n\n\n\n9.4.4 Global Average Pooling\nOne last type of pooling layer that you will often see in modern architectures is the global average pooling layer. It works very differently: all it does is compute the mean of each entire feature map (it’s like an average pooling layer using a pooling kernel with the same spatial dimensions as the inputs). This means that it just outputs a single number per feature map and per instance. Although this is of course extremely destructive (most of the information in the feature map is lost), it can be useful as the output layer. To create such a layer, simply use the tf.keras.layers.GlobalAvgPool2D class:\n\nglobal_avg_pool = tf.keras.layers.GlobalAvgPool2D()\n\n\nimages.shape\n\nTensorShape([2, 70, 120, 3])\n\n\n\n# It is the same as using low level API to perform reduction\nglobal_avg_pool = tf.keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))\nglobal_avg_pool(images)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[0.643388  , 0.59718215, 0.5825038 ],\n       [0.7630747 , 0.2601088 , 0.10848834]], dtype=float32)&gt;\n\n\nNow you know all the building blocks to create a convolutional neural network. Let’s see how to assemble them."
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_tensorflow.html#tackling-fashion-mnist-with-a-cnn",
    "href": "09_Convolutional_NeuralNetworks_tensorflow.html#tackling-fashion-mnist-with-a-cnn",
    "title": "9  Image processing with Convolutional Neural Networks - Tensorflow",
    "section": "9.5 Tackling Fashion MNIST With a CNN",
    "text": "9.5 Tackling Fashion MNIST With a CNN\nBefore delving into the code, you can go through https://poloclub.github.io/cnn-explainer/ to make sure you understand every piece of CNN.\nTypical CNN architectures stack a few convolutional layers (each one generally followed by a ReLU layer), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on. The image gets smaller and smaller as it progresses through the network, but it also typically gets deeper and deeper (i.e.,with more feature maps) thanks to the convolutional layers. At the top of the stack, a regular feedforward neural network is added, composed of a few fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a softmax layer that outputs estimated class probabilities).\n\n\n\nHere is how you can implement a simple CNN to tackle the fashion MNIST dataset\n\nmnist = tf.keras.datasets.fashion_mnist.load_data()\n(X_train_full, y_train_full), (X_test, y_test) = mnist\nX_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255\nX_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255\nX_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\nX_train.shape, X_valid.shape, X_test.shape\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 [==============================] - 0s 0us/step\n\n\n((55000, 28, 28, 1), (5000, 28, 28, 1), (10000, 28, 28, 1))\n\n\n\ntf.random.set_seed(42)  \nDefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")\nmodel = tf.keras.Sequential([\n    DefaultConv2D(filters=32, kernel_size=7, input_shape=[28, 28, 1]),\n    tf.keras.layers.MaxPool2D(),\n    DefaultConv2D(filters=64),\n    DefaultConv2D(filters=64),\n    tf.keras.layers.MaxPool2D(),\n    DefaultConv2D(filters=128),\n    DefaultConv2D(filters=128),\n    tf.keras.layers.MaxPool2D(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(units=32, activation=\"relu\", kernel_initializer=\"he_normal\"),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n])\n\nIn this code, we start by using the partial() function to define a thin wrapper around the Conv2D class, called DefaultConv2D: it simply avoids having to repeat the same hyperparameter values over and over again.\n\nThe first layer sets input_shape=[28, 28, 1], which means the images are 28 × 28 pixels, with a single color channel (i.e., grayscale).\nNext, we have a max pooling layer, which divides each spatial dimension by a factor of two (since pool_size=2).\nThen we repeat the same structure twice: convolutional layers followed by a max pooling layer. For larger images, we could repeat this structure several times (the number of repetitions is a hyperparameter you can tune).\nNote that the number of filters grows as we climb up the CNN towards the output layer (it is initially 32, then 64, then 128): it makes sense for it to grow in the image setting, since the number of low level features is often fairly low (e.g., small circles, horizontal lines, etc.), but there are many different ways to combine them into higher level features. It is a common practice to double the number of filters after each pooling layer: since a pooling layer divides each spatial dimension by a factor of 2, we can afford doubling the number of feature maps in the next layer, without fear of exploding the number of parameters, memory usage, or computational load.\nNext is the fully connected network, composed of 1 hidden dense layers and a dense output layer. Note that we must flatten its inputs, since a dense network expects a 1D array of features for each instance. We also add two dropout layers, with a dropout rate of 50% each, to reduce overfitting.\n\n\nmodel.summary()\n\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_23 (Conv2D)          (None, 28, 28, 32)        1600      \n                                                                 \n max_pooling2d_13 (MaxPoolin  (None, 14, 14, 32)       0         \n g2D)                                                            \n                                                                 \n conv2d_24 (Conv2D)          (None, 14, 14, 64)        18496     \n                                                                 \n conv2d_25 (Conv2D)          (None, 14, 14, 64)        36928     \n                                                                 \n max_pooling2d_14 (MaxPoolin  (None, 7, 7, 64)         0         \n g2D)                                                            \n                                                                 \n conv2d_26 (Conv2D)          (None, 7, 7, 128)         73856     \n                                                                 \n conv2d_27 (Conv2D)          (None, 7, 7, 128)         147584    \n                                                                 \n max_pooling2d_15 (MaxPoolin  (None, 3, 3, 128)        0         \n g2D)                                                            \n                                                                 \n flatten_4 (Flatten)         (None, 1152)              0         \n                                                                 \n dense_10 (Dense)            (None, 64)                73792     \n                                                                 \n dropout_6 (Dropout)         (None, 64)                0         \n                                                                 \n dense_11 (Dense)            (None, 32)                2080      \n                                                                 \n dropout_7 (Dropout)         (None, 32)                0         \n                                                                 \n dense_12 (Dense)            (None, 10)                330       \n                                                                 \n=================================================================\nTotal params: 354,666\nTrainable params: 354,666\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n\nEpoch 1/30\n1719/1719 [==============================] - 18s 8ms/step - loss: 1.0470 - accuracy: 0.6161 - val_loss: 0.5400 - val_accuracy: 0.8344\nEpoch 2/30\n1719/1719 [==============================] - 13s 7ms/step - loss: 0.6715 - accuracy: 0.7635 - val_loss: 0.4153 - val_accuracy: 0.8658\nEpoch 3/30\n1719/1719 [==============================] - 13s 7ms/step - loss: 0.5936 - accuracy: 0.7930 - val_loss: 0.3833 - val_accuracy: 0.8806\nEpoch 4/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.5398 - accuracy: 0.8111 - val_loss: 0.3692 - val_accuracy: 0.8764\nEpoch 5/30\n1719/1719 [==============================] - 13s 7ms/step - loss: 0.4998 - accuracy: 0.8251 - val_loss: 0.3740 - val_accuracy: 0.8748\nEpoch 6/30\n1719/1719 [==============================] - 17s 10ms/step - loss: 0.4701 - accuracy: 0.8372 - val_loss: 0.3260 - val_accuracy: 0.8892\nEpoch 7/30\n1719/1719 [==============================] - 16s 9ms/step - loss: 0.4427 - accuracy: 0.8451 - val_loss: 0.3160 - val_accuracy: 0.8908\nEpoch 8/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.4219 - accuracy: 0.8521 - val_loss: 0.2874 - val_accuracy: 0.9014\nEpoch 9/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.3958 - accuracy: 0.8623 - val_loss: 0.2849 - val_accuracy: 0.9066\nEpoch 10/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.3818 - accuracy: 0.8667 - val_loss: 0.3108 - val_accuracy: 0.8972\nEpoch 11/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.3664 - accuracy: 0.8715 - val_loss: 0.2787 - val_accuracy: 0.9030\nEpoch 12/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.3619 - accuracy: 0.8742 - val_loss: 0.3005 - val_accuracy: 0.9004\nEpoch 13/30\n1719/1719 [==============================] - 12s 7ms/step - loss: 0.3500 - accuracy: 0.8773 - val_loss: 0.2992 - val_accuracy: 0.9088\nEpoch 14/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.3400 - accuracy: 0.8807 - val_loss: 0.2820 - val_accuracy: 0.9078\nEpoch 15/30\n1719/1719 [==============================] - 13s 7ms/step - loss: 0.3359 - accuracy: 0.8833 - val_loss: 0.3579 - val_accuracy: 0.9048\nEpoch 16/30\n1719/1719 [==============================] - 13s 7ms/step - loss: 0.3350 - accuracy: 0.8842 - val_loss: 0.2880 - val_accuracy: 0.9076\nEpoch 17/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.3248 - accuracy: 0.8901 - val_loss: 0.3157 - val_accuracy: 0.8962\nEpoch 18/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.3198 - accuracy: 0.8896 - val_loss: 0.3295 - val_accuracy: 0.9062\nEpoch 19/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.3058 - accuracy: 0.8958 - val_loss: 0.3126 - val_accuracy: 0.9074\nEpoch 20/30\n1719/1719 [==============================] - 14s 8ms/step - loss: 0.3021 - accuracy: 0.8959 - val_loss: 0.3221 - val_accuracy: 0.8972\nEpoch 21/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.2921 - accuracy: 0.8996 - val_loss: 0.3122 - val_accuracy: 0.9138\nEpoch 22/30\n1719/1719 [==============================] - 13s 7ms/step - loss: 0.2986 - accuracy: 0.8953 - val_loss: 0.2931 - val_accuracy: 0.9116\nEpoch 23/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.2828 - accuracy: 0.9005 - val_loss: 0.3284 - val_accuracy: 0.9086\nEpoch 24/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.2890 - accuracy: 0.9011 - val_loss: 0.3160 - val_accuracy: 0.9124\nEpoch 25/30\n1719/1719 [==============================] - 14s 8ms/step - loss: 0.2779 - accuracy: 0.9053 - val_loss: 0.3769 - val_accuracy: 0.8990\nEpoch 26/30\n1719/1719 [==============================] - 19s 11ms/step - loss: 0.2713 - accuracy: 0.9065 - val_loss: 0.3671 - val_accuracy: 0.8980\nEpoch 27/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.2864 - accuracy: 0.9028 - val_loss: 0.3430 - val_accuracy: 0.9148\nEpoch 28/30\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.2609 - accuracy: 0.9117 - val_loss: 0.3525 - val_accuracy: 0.9114\nEpoch 29/30\n1719/1719 [==============================] - 13s 7ms/step - loss: 0.2644 - accuracy: 0.9112 - val_loss: 0.3372 - val_accuracy: 0.9114\nEpoch 30/30\n1719/1719 [==============================] - 13s 7ms/step - loss: 0.2640 - accuracy: 0.9110 - val_loss: 0.3817 - val_accuracy: 0.9106\n\n\n\nscore = model.evaluate(X_test, y_test)\nX_new = X_test[:10] # pretend we have new images\ny_pred = model.predict(X_new)\n\nscore, y_pred\n\n313/313 [==============================] - 1s 4ms/step - loss: 0.3518 - accuracy: 0.9087\n1/1 [==============================] - 0s 270ms/step\n\n\n([0.35184580087661743, 0.9086999893188477],\n array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n         0.00000000e+00, 6.11583033e-34, 0.00000000e+00, 2.93755923e-16,\n         0.00000000e+00, 1.00000000e+00],\n        [2.29682566e-08, 0.00000000e+00, 9.95057583e-01, 1.22728333e-17,\n         1.12017071e-04, 0.00000000e+00, 4.83040046e-03, 0.00000000e+00,\n         2.82865502e-11, 0.00000000e+00],\n        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.07169106e-28,\n         3.38975873e-33, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n         0.00000000e+00, 0.00000000e+00],\n        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 5.98663190e-28,\n         1.62537842e-30, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n         0.00000000e+00, 0.00000000e+00],\n        [2.34796666e-02, 0.00000000e+00, 4.40896001e-05, 1.58730646e-08,\n         1.96278652e-05, 6.24068337e-15, 9.76455688e-01, 1.57429494e-17,\n         7.90122328e-07, 2.14440993e-14],\n        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.17844744e-34,\n         1.37916159e-38, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n         0.00000000e+00, 0.00000000e+00],\n        [1.58039376e-15, 0.00000000e+00, 1.19538361e-03, 3.84299897e-10,\n         9.98477161e-01, 0.00000000e+00, 3.27471091e-04, 0.00000000e+00,\n         9.29603522e-18, 0.00000000e+00],\n        [9.78829462e-07, 0.00000000e+00, 3.22155256e-06, 1.53601600e-12,\n         8.27561307e-05, 1.05881361e-37, 9.99913096e-01, 0.00000000e+00,\n         1.61168967e-10, 9.95916054e-31],\n        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n         0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n         0.00000000e+00, 0.00000000e+00],\n        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n         0.00000000e+00, 2.82005428e-27, 0.00000000e+00, 1.00000000e+00,\n         0.00000000e+00, 1.44178385e-11]], dtype=float32))\n\n\nThis CNN reaches over 90% accuracy on the test set. It’s not the state of the art, but it is pretty good and better than the dense network with similar number of parameters!"
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_tensorflow.html#training-a-convnet-from-scratch-on-a-small-dataset",
    "href": "09_Convolutional_NeuralNetworks_tensorflow.html#training-a-convnet-from-scratch-on-a-small-dataset",
    "title": "9  Image processing with Convolutional Neural Networks - Tensorflow",
    "section": "9.6 Training a convnet from scratch on a small dataset",
    "text": "9.6 Training a convnet from scratch on a small dataset\nHaving to train an image-classification model using very little data is a common situation, which you’ll likely encounter in practice if you ever do computer vision in a professional context. A “few” samples can mean anywhere from a few hundred to a few tens of thousands of images. As a practical example, we’ll focus on classifying images as dogs or cats in a dataset containing 5,000 pictures of cats and dogs (2,500 cats, 2,500 dogs). We’ll use 2,000 pictures for training, 1,000 for validation, and 2,000 for testing.\nIn this section, we’ll review one basic strategy to tackle this problem: training a new model from scratch using what little data you have. We’ll start by naively training a small convnet on the 2,000 training samples, without any regularization, to set a baseline for what can be achieved. This will get us to a classification accuracy of about 70%. At that point, the main issue will be overfitting. Then we’ll introduce data augmentation, a powerful technique for mitigating overfitting in computer vision. By using data augmentation, we’ll improve the model to reach an accuracy of 80–85%.\n\n9.6.1 The relevance of deep learning for small-data problems\nWhat qualifies as “enough samples” to train a model is relative— relative to the size and depth of the model you’re trying to train, for starters. It isn’t possible to train a convnet to solve a complex problem with just a few tens of samples, but a few hundred can potentially suffice if the model is small and well regularized and the task is simple.\nBecause convnets learn local, translation-invariant features, they’re highly data-efficient on perceptual problems. Training a convnet from scratch on a very small image dataset will yield reasonable results despite a relative lack of data, without the need for any custom feature engineering. You’ll see this in action in this section.\n\n\n9.6.2 Downloading the data\nThe Dogs vs. Cats dataset that we will use isn’t packaged with Keras. It was made available by Kaggle as part of a computer vision competition in late 2013, back when convnets weren’t mainstream. You can download the original dataset from www.kaggle.com/c/dogs-vs-cats/data.\nBut you can also use Kaggle API. First, you need to create a Kaggle API key and download it to your local machine. Just navigate to the Kaggle website in a web browser, log in, and go to the My Account page. In your account settings, you’ll find an API section. Clicking the Create New API Token button will generate a kaggle.json key file and will download it to your machine.\n\n\n\n\n# Upload the API’s key JSON file to your Colab\n# session by running the following code in a notebook cell:\nfrom google.colab import files\nfiles.upload()\n\nFinally, create a ~/.kaggle folder, and copy the key file to it. As a security best practice, you should also make sure that the file is only readable by the current user, yourself:\n\n!mkdir ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n\n\n# You can now download the data we’re about to use:\n!kaggle competitions download -c dogs-vs-cats\n\nDownloading dogs-vs-cats.zip to /content\n 98% 797M/812M [00:04&lt;00:00, 251MB/s]\n100% 812M/812M [00:04&lt;00:00, 191MB/s]\n\n\nThe first time you try to download the data, you may get a “403 Forbidden” error. That’s because you need to accept the terms associated with the dataset before you download it—you’ll have to go to www.kaggle.com/c/dogs-vs-cats/rules (while logged into your Kaggle account) and click the I Understand and Accept button. You only need to do this once.\n\n!unzip -qq dogs-vs-cats.zip\n\n\n!unzip -qq train.zip\n\nThe pictures in our dataset are medium-resolution color JPEGs. Unsurprisingly, the original dogs-versus-cats Kaggle competition, all the way back in 2013, was won by entrants who used convnets. The best entries achieved up to 95% accuracy. Even though we will train our models on less than 10% of the data that was available to the competitors, we will still get a resonable well performance.\nThis dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed). After downloading and uncompressing the data, we’ll create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 1,000 samples of each class. Why do this? Because many of the image datasets you’ll encounter in your career only contain a few thousand samples, not tens of thousands. Having more data available would make the problem easier, so it’s good practice to learn with a small dataset.\n\n!tree train\n\n串流輸出內容已截斷至最後 5000 行。\n├── dog.5502.jpg\n├── dog.5503.jpg\n├── dog.5504.jpg\n├── dog.5505.jpg\n├── dog.5506.jpg\n├── dog.5507.jpg\n├── dog.5508.jpg\n├── dog.5509.jpg\n├── dog.550.jpg\n├── dog.5510.jpg\n├── dog.5511.jpg\n├── dog.5512.jpg\n├── dog.5513.jpg\n├── dog.5514.jpg\n├── dog.5515.jpg\n├── dog.5516.jpg\n├── dog.5517.jpg\n├── dog.5518.jpg\n├── dog.5519.jpg\n├── dog.551.jpg\n├── dog.5520.jpg\n├── dog.5521.jpg\n├── dog.5522.jpg\n├── dog.5523.jpg\n├── dog.5524.jpg\n├── dog.5525.jpg\n├── dog.5526.jpg\n├── dog.5527.jpg\n├── dog.5528.jpg\n├── dog.5529.jpg\n├── dog.552.jpg\n├── dog.5530.jpg\n├── dog.5531.jpg\n├── dog.5532.jpg\n├── dog.5533.jpg\n├── dog.5534.jpg\n├── dog.5535.jpg\n├── dog.5536.jpg\n├── dog.5537.jpg\n├── dog.5538.jpg\n├── dog.5539.jpg\n├── dog.553.jpg\n├── dog.5540.jpg\n├── dog.5541.jpg\n├── dog.5542.jpg\n├── dog.5543.jpg\n├── dog.5544.jpg\n├── dog.5545.jpg\n├── dog.5546.jpg\n├── dog.5547.jpg\n├── dog.5548.jpg\n├── dog.5549.jpg\n├── dog.554.jpg\n├── dog.5550.jpg\n├── dog.5551.jpg\n├── dog.5552.jpg\n├── dog.5553.jpg\n├── dog.5554.jpg\n├── dog.5555.jpg\n├── dog.5556.jpg\n├── dog.5557.jpg\n├── dog.5558.jpg\n├── dog.5559.jpg\n├── dog.555.jpg\n├── dog.5560.jpg\n├── dog.5561.jpg\n├── dog.5562.jpg\n├── dog.5563.jpg\n├── dog.5564.jpg\n├── dog.5565.jpg\n├── dog.5566.jpg\n├── dog.5567.jpg\n├── dog.5568.jpg\n├── dog.5569.jpg\n├── dog.556.jpg\n├── dog.5570.jpg\n├── dog.5571.jpg\n├── dog.5572.jpg\n├── dog.5573.jpg\n├── dog.5574.jpg\n├── dog.5575.jpg\n├── dog.5576.jpg\n├── dog.5577.jpg\n├── dog.5578.jpg\n├── dog.5579.jpg\n├── dog.557.jpg\n├── dog.5580.jpg\n├── dog.5581.jpg\n├── dog.5582.jpg\n├── dog.5583.jpg\n├── dog.5584.jpg\n├── dog.5585.jpg\n├── dog.5586.jpg\n├── dog.5587.jpg\n├── dog.5588.jpg\n├── dog.5589.jpg\n├── dog.558.jpg\n├── dog.5590.jpg\n├── dog.5591.jpg\n├── dog.5592.jpg\n├── dog.5593.jpg\n├── dog.5594.jpg\n├── dog.5595.jpg\n├── dog.5596.jpg\n├── dog.5597.jpg\n├── dog.5598.jpg\n├── dog.5599.jpg\n├── dog.559.jpg\n├── dog.55.jpg\n├── dog.5600.jpg\n├── dog.5601.jpg\n├── dog.5602.jpg\n├── dog.5603.jpg\n├── dog.5604.jpg\n├── dog.5605.jpg\n├── dog.5606.jpg\n├── dog.5607.jpg\n├── dog.5608.jpg\n├── dog.5609.jpg\n├── dog.560.jpg\n├── dog.5610.jpg\n├── dog.5611.jpg\n├── dog.5612.jpg\n├── dog.5613.jpg\n├── dog.5614.jpg\n├── dog.5615.jpg\n├── dog.5616.jpg\n├── dog.5617.jpg\n├── dog.5618.jpg\n├── dog.5619.jpg\n├── dog.561.jpg\n├── dog.5620.jpg\n├── dog.5621.jpg\n├── dog.5622.jpg\n├── dog.5623.jpg\n├── dog.5624.jpg\n├── dog.5625.jpg\n├── dog.5626.jpg\n├── dog.5627.jpg\n├── dog.5628.jpg\n├── dog.5629.jpg\n├── dog.562.jpg\n├── dog.5630.jpg\n├── dog.5631.jpg\n├── dog.5632.jpg\n├── dog.5633.jpg\n├── dog.5634.jpg\n├── dog.5635.jpg\n├── dog.5636.jpg\n├── dog.5637.jpg\n├── dog.5638.jpg\n├── dog.5639.jpg\n├── dog.563.jpg\n├── dog.5640.jpg\n├── dog.5641.jpg\n├── dog.5642.jpg\n├── dog.5643.jpg\n├── dog.5644.jpg\n├── dog.5645.jpg\n├── dog.5646.jpg\n├── dog.5647.jpg\n├── dog.5648.jpg\n├── dog.5649.jpg\n├── dog.564.jpg\n├── dog.5650.jpg\n├── dog.5651.jpg\n├── dog.5652.jpg\n├── dog.5653.jpg\n├── dog.5654.jpg\n├── dog.5655.jpg\n├── dog.5656.jpg\n├── dog.5657.jpg\n├── dog.5658.jpg\n├── dog.5659.jpg\n├── dog.565.jpg\n├── dog.5660.jpg\n├── dog.5661.jpg\n├── dog.5662.jpg\n├── dog.5663.jpg\n├── dog.5664.jpg\n├── dog.5665.jpg\n├── dog.5666.jpg\n├── dog.5667.jpg\n├── dog.5668.jpg\n├── dog.5669.jpg\n├── dog.566.jpg\n├── dog.5670.jpg\n├── dog.5671.jpg\n├── dog.5672.jpg\n├── dog.5673.jpg\n├── dog.5674.jpg\n├── dog.5675.jpg\n├── dog.5676.jpg\n├── dog.5677.jpg\n├── dog.5678.jpg\n├── dog.5679.jpg\n├── dog.567.jpg\n├── dog.5680.jpg\n├── dog.5681.jpg\n├── dog.5682.jpg\n├── dog.5683.jpg\n├── dog.5684.jpg\n├── dog.5685.jpg\n├── dog.5686.jpg\n├── dog.5687.jpg\n├── dog.5688.jpg\n├── dog.5689.jpg\n├── dog.568.jpg\n├── dog.5690.jpg\n├── dog.5691.jpg\n├── dog.5692.jpg\n├── dog.5693.jpg\n├── dog.5694.jpg\n├── dog.5695.jpg\n├── dog.5696.jpg\n├── dog.5697.jpg\n├── dog.5698.jpg\n├── dog.5699.jpg\n├── dog.569.jpg\n├── dog.56.jpg\n├── dog.5700.jpg\n├── dog.5701.jpg\n├── dog.5702.jpg\n├── dog.5703.jpg\n├── dog.5704.jpg\n├── dog.5705.jpg\n├── dog.5706.jpg\n├── dog.5707.jpg\n├── dog.5708.jpg\n├── dog.5709.jpg\n├── dog.570.jpg\n├── dog.5710.jpg\n├── dog.5711.jpg\n├── dog.5712.jpg\n├── dog.5713.jpg\n├── dog.5714.jpg\n├── dog.5715.jpg\n├── dog.5716.jpg\n├── dog.5717.jpg\n├── dog.5718.jpg\n├── dog.5719.jpg\n├── dog.571.jpg\n├── dog.5720.jpg\n├── dog.5721.jpg\n├── dog.5722.jpg\n├── dog.5723.jpg\n├── dog.5724.jpg\n├── dog.5725.jpg\n├── dog.5726.jpg\n├── dog.5727.jpg\n├── dog.5728.jpg\n├── dog.5729.jpg\n├── dog.572.jpg\n├── dog.5730.jpg\n├── dog.5731.jpg\n├── dog.5732.jpg\n├── dog.5733.jpg\n├── dog.5734.jpg\n├── dog.5735.jpg\n├── dog.5736.jpg\n├── dog.5737.jpg\n├── dog.5738.jpg\n├── dog.5739.jpg\n├── dog.573.jpg\n├── dog.5740.jpg\n├── dog.5741.jpg\n├── dog.5742.jpg\n├── dog.5743.jpg\n├── dog.5744.jpg\n├── dog.5745.jpg\n├── dog.5746.jpg\n├── dog.5747.jpg\n├── dog.5748.jpg\n├── dog.5749.jpg\n├── dog.574.jpg\n├── dog.5750.jpg\n├── dog.5751.jpg\n├── dog.5752.jpg\n├── dog.5753.jpg\n├── dog.5754.jpg\n├── dog.5755.jpg\n├── dog.5756.jpg\n├── dog.5757.jpg\n├── dog.5758.jpg\n├── dog.5759.jpg\n├── dog.575.jpg\n├── dog.5760.jpg\n├── dog.5761.jpg\n├── dog.5762.jpg\n├── dog.5763.jpg\n├── dog.5764.jpg\n├── dog.5765.jpg\n├── dog.5766.jpg\n├── dog.5767.jpg\n├── dog.5768.jpg\n├── dog.5769.jpg\n├── dog.576.jpg\n├── dog.5770.jpg\n├── dog.5771.jpg\n├── dog.5772.jpg\n├── dog.5773.jpg\n├── dog.5774.jpg\n├── dog.5775.jpg\n├── dog.5776.jpg\n├── dog.5777.jpg\n├── dog.5778.jpg\n├── dog.5779.jpg\n├── dog.577.jpg\n├── dog.5780.jpg\n├── dog.5781.jpg\n├── dog.5782.jpg\n├── dog.5783.jpg\n├── dog.5784.jpg\n├── dog.5785.jpg\n├── dog.5786.jpg\n├── dog.5787.jpg\n├── dog.5788.jpg\n├── dog.5789.jpg\n├── dog.578.jpg\n├── dog.5790.jpg\n├── dog.5791.jpg\n├── dog.5792.jpg\n├── dog.5793.jpg\n├── dog.5794.jpg\n├── dog.5795.jpg\n├── dog.5796.jpg\n├── dog.5797.jpg\n├── dog.5798.jpg\n├── dog.5799.jpg\n├── dog.579.jpg\n├── dog.57.jpg\n├── dog.5800.jpg\n├── dog.5801.jpg\n├── dog.5802.jpg\n├── dog.5803.jpg\n├── dog.5804.jpg\n├── dog.5805.jpg\n├── dog.5806.jpg\n├── dog.5807.jpg\n├── dog.5808.jpg\n├── dog.5809.jpg\n├── dog.580.jpg\n├── dog.5810.jpg\n├── dog.5811.jpg\n├── dog.5812.jpg\n├── dog.5813.jpg\n├── dog.5814.jpg\n├── dog.5815.jpg\n├── dog.5816.jpg\n├── dog.5817.jpg\n├── dog.5818.jpg\n├── dog.5819.jpg\n├── dog.581.jpg\n├── dog.5820.jpg\n├── dog.5821.jpg\n├── dog.5822.jpg\n├── dog.5823.jpg\n├── dog.5824.jpg\n├── dog.5825.jpg\n├── dog.5826.jpg\n├── dog.5827.jpg\n├── dog.5828.jpg\n├── dog.5829.jpg\n├── dog.582.jpg\n├── dog.5830.jpg\n├── dog.5831.jpg\n├── dog.5832.jpg\n├── dog.5833.jpg\n├── dog.5834.jpg\n├── dog.5835.jpg\n├── dog.5836.jpg\n├── dog.5837.jpg\n├── dog.5838.jpg\n├── dog.5839.jpg\n├── dog.583.jpg\n├── dog.5840.jpg\n├── dog.5841.jpg\n├── dog.5842.jpg\n├── dog.5843.jpg\n├── dog.5844.jpg\n├── dog.5845.jpg\n├── dog.5846.jpg\n├── dog.5847.jpg\n├── dog.5848.jpg\n├── dog.5849.jpg\n├── dog.584.jpg\n├── dog.5850.jpg\n├── dog.5851.jpg\n├── dog.5852.jpg\n├── dog.5853.jpg\n├── dog.5854.jpg\n├── dog.5855.jpg\n├── dog.5856.jpg\n├── dog.5857.jpg\n├── dog.5858.jpg\n├── dog.5859.jpg\n├── dog.585.jpg\n├── dog.5860.jpg\n├── dog.5861.jpg\n├── dog.5862.jpg\n├── dog.5863.jpg\n├── dog.5864.jpg\n├── dog.5865.jpg\n├── dog.5866.jpg\n├── dog.5867.jpg\n├── dog.5868.jpg\n├── dog.5869.jpg\n├── dog.586.jpg\n├── dog.5870.jpg\n├── dog.5871.jpg\n├── dog.5872.jpg\n├── dog.5873.jpg\n├── dog.5874.jpg\n├── dog.5875.jpg\n├── dog.5876.jpg\n├── dog.5877.jpg\n├── dog.5878.jpg\n├── dog.5879.jpg\n├── dog.587.jpg\n├── dog.5880.jpg\n├── dog.5881.jpg\n├── dog.5882.jpg\n├── dog.5883.jpg\n├── dog.5884.jpg\n├── dog.5885.jpg\n├── dog.5886.jpg\n├── dog.5887.jpg\n├── dog.5888.jpg\n├── dog.5889.jpg\n├── dog.588.jpg\n├── dog.5890.jpg\n├── dog.5891.jpg\n├── dog.5892.jpg\n├── dog.5893.jpg\n├── dog.5894.jpg\n├── dog.5895.jpg\n├── dog.5896.jpg\n├── dog.5897.jpg\n├── dog.5898.jpg\n├── dog.5899.jpg\n├── dog.589.jpg\n├── dog.58.jpg\n├── dog.5900.jpg\n├── dog.5901.jpg\n├── dog.5902.jpg\n├── dog.5903.jpg\n├── dog.5904.jpg\n├── dog.5905.jpg\n├── dog.5906.jpg\n├── dog.5907.jpg\n├── dog.5908.jpg\n├── dog.5909.jpg\n├── dog.590.jpg\n├── dog.5910.jpg\n├── dog.5911.jpg\n├── dog.5912.jpg\n├── dog.5913.jpg\n├── dog.5914.jpg\n├── dog.5915.jpg\n├── dog.5916.jpg\n├── dog.5917.jpg\n├── dog.5918.jpg\n├── dog.5919.jpg\n├── dog.591.jpg\n├── dog.5920.jpg\n├── dog.5921.jpg\n├── dog.5922.jpg\n├── dog.5923.jpg\n├── dog.5924.jpg\n├── dog.5925.jpg\n├── dog.5926.jpg\n├── dog.5927.jpg\n├── dog.5928.jpg\n├── dog.5929.jpg\n├── dog.592.jpg\n├── dog.5930.jpg\n├── dog.5931.jpg\n├── dog.5932.jpg\n├── dog.5933.jpg\n├── dog.5934.jpg\n├── dog.5935.jpg\n├── dog.5936.jpg\n├── dog.5937.jpg\n├── dog.5938.jpg\n├── dog.5939.jpg\n├── dog.593.jpg\n├── dog.5940.jpg\n├── dog.5941.jpg\n├── dog.5942.jpg\n├── dog.5943.jpg\n├── dog.5944.jpg\n├── dog.5945.jpg\n├── dog.5946.jpg\n├── dog.5947.jpg\n├── dog.5948.jpg\n├── dog.5949.jpg\n├── dog.594.jpg\n├── dog.5950.jpg\n├── dog.5951.jpg\n├── dog.5952.jpg\n├── dog.5953.jpg\n├── dog.5954.jpg\n├── dog.5955.jpg\n├── dog.5956.jpg\n├── dog.5957.jpg\n├── dog.5958.jpg\n├── dog.5959.jpg\n├── dog.595.jpg\n├── dog.5960.jpg\n├── dog.5961.jpg\n├── dog.5962.jpg\n├── dog.5963.jpg\n├── dog.5964.jpg\n├── dog.5965.jpg\n├── dog.5966.jpg\n├── dog.5967.jpg\n├── dog.5968.jpg\n├── dog.5969.jpg\n├── dog.596.jpg\n├── dog.5970.jpg\n├── dog.5971.jpg\n├── dog.5972.jpg\n├── dog.5973.jpg\n├── dog.5974.jpg\n├── dog.5975.jpg\n├── dog.5976.jpg\n├── dog.5977.jpg\n├── dog.5978.jpg\n├── dog.5979.jpg\n├── dog.597.jpg\n├── dog.5980.jpg\n├── dog.5981.jpg\n├── dog.5982.jpg\n├── dog.5983.jpg\n├── dog.5984.jpg\n├── dog.5985.jpg\n├── dog.5986.jpg\n├── dog.5987.jpg\n├── dog.5988.jpg\n├── dog.5989.jpg\n├── dog.598.jpg\n├── dog.5990.jpg\n├── dog.5991.jpg\n├── dog.5992.jpg\n├── dog.5993.jpg\n├── dog.5994.jpg\n├── dog.5995.jpg\n├── dog.5996.jpg\n├── dog.5997.jpg\n├── dog.5998.jpg\n├── dog.5999.jpg\n├── dog.599.jpg\n├── dog.59.jpg\n├── dog.5.jpg\n├── dog.6000.jpg\n├── dog.6001.jpg\n├── dog.6002.jpg\n├── dog.6003.jpg\n├── dog.6004.jpg\n├── dog.6005.jpg\n├── dog.6006.jpg\n├── dog.6007.jpg\n├── dog.6008.jpg\n├── dog.6009.jpg\n├── dog.600.jpg\n├── dog.6010.jpg\n├── dog.6011.jpg\n├── dog.6012.jpg\n├── dog.6013.jpg\n├── dog.6014.jpg\n├── dog.6015.jpg\n├── dog.6016.jpg\n├── dog.6017.jpg\n├── dog.6018.jpg\n├── dog.6019.jpg\n├── dog.601.jpg\n├── dog.6020.jpg\n├── dog.6021.jpg\n├── dog.6022.jpg\n├── dog.6023.jpg\n├── dog.6024.jpg\n├── dog.6025.jpg\n├── dog.6026.jpg\n├── dog.6027.jpg\n├── dog.6028.jpg\n├── dog.6029.jpg\n├── dog.602.jpg\n├── dog.6030.jpg\n├── dog.6031.jpg\n├── dog.6032.jpg\n├── dog.6033.jpg\n├── dog.6034.jpg\n├── dog.6035.jpg\n├── dog.6036.jpg\n├── dog.6037.jpg\n├── dog.6038.jpg\n├── dog.6039.jpg\n├── dog.603.jpg\n├── dog.6040.jpg\n├── dog.6041.jpg\n├── dog.6042.jpg\n├── dog.6043.jpg\n├── dog.6044.jpg\n├── dog.6045.jpg\n├── dog.6046.jpg\n├── dog.6047.jpg\n├── dog.6048.jpg\n├── dog.6049.jpg\n├── dog.604.jpg\n├── dog.6050.jpg\n├── dog.6051.jpg\n├── dog.6052.jpg\n├── dog.6053.jpg\n├── dog.6054.jpg\n├── dog.6055.jpg\n├── dog.6056.jpg\n├── dog.6057.jpg\n├── dog.6058.jpg\n├── dog.6059.jpg\n├── dog.605.jpg\n├── dog.6060.jpg\n├── dog.6061.jpg\n├── dog.6062.jpg\n├── dog.6063.jpg\n├── dog.6064.jpg\n├── dog.6065.jpg\n├── dog.6066.jpg\n├── dog.6067.jpg\n├── dog.6068.jpg\n├── dog.6069.jpg\n├── dog.606.jpg\n├── dog.6070.jpg\n├── dog.6071.jpg\n├── dog.6072.jpg\n├── dog.6073.jpg\n├── dog.6074.jpg\n├── dog.6075.jpg\n├── dog.6076.jpg\n├── dog.6077.jpg\n├── dog.6078.jpg\n├── dog.6079.jpg\n├── dog.607.jpg\n├── dog.6080.jpg\n├── dog.6081.jpg\n├── dog.6082.jpg\n├── dog.6083.jpg\n├── dog.6084.jpg\n├── dog.6085.jpg\n├── dog.6086.jpg\n├── dog.6087.jpg\n├── dog.6088.jpg\n├── dog.6089.jpg\n├── dog.608.jpg\n├── dog.6090.jpg\n├── dog.6091.jpg\n├── dog.6092.jpg\n├── dog.6093.jpg\n├── dog.6094.jpg\n├── dog.6095.jpg\n├── dog.6096.jpg\n├── dog.6097.jpg\n├── dog.6098.jpg\n├── dog.6099.jpg\n├── dog.609.jpg\n├── dog.60.jpg\n├── dog.6100.jpg\n├── dog.6101.jpg\n├── dog.6102.jpg\n├── dog.6103.jpg\n├── dog.6104.jpg\n├── dog.6105.jpg\n├── dog.6106.jpg\n├── dog.6107.jpg\n├── dog.6108.jpg\n├── dog.6109.jpg\n├── dog.610.jpg\n├── dog.6110.jpg\n├── dog.6111.jpg\n├── dog.6112.jpg\n├── dog.6113.jpg\n├── dog.6114.jpg\n├── dog.6115.jpg\n├── dog.6116.jpg\n├── dog.6117.jpg\n├── dog.6118.jpg\n├── dog.6119.jpg\n├── dog.611.jpg\n├── dog.6120.jpg\n├── dog.6121.jpg\n├── dog.6122.jpg\n├── dog.6123.jpg\n├── dog.6124.jpg\n├── dog.6125.jpg\n├── dog.6126.jpg\n├── dog.6127.jpg\n├── dog.6128.jpg\n├── dog.6129.jpg\n├── dog.612.jpg\n├── dog.6130.jpg\n├── dog.6131.jpg\n├── dog.6132.jpg\n├── dog.6133.jpg\n├── dog.6134.jpg\n├── dog.6135.jpg\n├── dog.6136.jpg\n├── dog.6137.jpg\n├── dog.6138.jpg\n├── dog.6139.jpg\n├── dog.613.jpg\n├── dog.6140.jpg\n├── dog.6141.jpg\n├── dog.6142.jpg\n├── dog.6143.jpg\n├── dog.6144.jpg\n├── dog.6145.jpg\n├── dog.6146.jpg\n├── dog.6147.jpg\n├── dog.6148.jpg\n├── dog.6149.jpg\n├── dog.614.jpg\n├── dog.6150.jpg\n├── dog.6151.jpg\n├── dog.6152.jpg\n├── dog.6153.jpg\n├── dog.6154.jpg\n├── dog.6155.jpg\n├── dog.6156.jpg\n├── dog.6157.jpg\n├── dog.6158.jpg\n├── dog.6159.jpg\n├── dog.615.jpg\n├── dog.6160.jpg\n├── dog.6161.jpg\n├── dog.6162.jpg\n├── dog.6163.jpg\n├── dog.6164.jpg\n├── dog.6165.jpg\n├── dog.6166.jpg\n├── dog.6167.jpg\n├── dog.6168.jpg\n├── dog.6169.jpg\n├── dog.616.jpg\n├── dog.6170.jpg\n├── dog.6171.jpg\n├── dog.6172.jpg\n├── dog.6173.jpg\n├── dog.6174.jpg\n├── dog.6175.jpg\n├── dog.6176.jpg\n├── dog.6177.jpg\n├── dog.6178.jpg\n├── dog.6179.jpg\n├── dog.617.jpg\n├── dog.6180.jpg\n├── dog.6181.jpg\n├── dog.6182.jpg\n├── dog.6183.jpg\n├── dog.6184.jpg\n├── dog.6185.jpg\n├── dog.6186.jpg\n├── dog.6187.jpg\n├── dog.6188.jpg\n├── dog.6189.jpg\n├── dog.618.jpg\n├── dog.6190.jpg\n├── dog.6191.jpg\n├── dog.6192.jpg\n├── dog.6193.jpg\n├── dog.6194.jpg\n├── dog.6195.jpg\n├── dog.6196.jpg\n├── dog.6197.jpg\n├── dog.6198.jpg\n├── dog.6199.jpg\n├── dog.619.jpg\n├── dog.61.jpg\n├── dog.6200.jpg\n├── dog.6201.jpg\n├── dog.6202.jpg\n├── dog.6203.jpg\n├── dog.6204.jpg\n├── dog.6205.jpg\n├── dog.6206.jpg\n├── dog.6207.jpg\n├── dog.6208.jpg\n├── dog.6209.jpg\n├── dog.620.jpg\n├── dog.6210.jpg\n├── dog.6211.jpg\n├── dog.6212.jpg\n├── dog.6213.jpg\n├── dog.6214.jpg\n├── dog.6215.jpg\n├── dog.6216.jpg\n├── dog.6217.jpg\n├── dog.6218.jpg\n├── dog.6219.jpg\n├── dog.621.jpg\n├── dog.6220.jpg\n├── dog.6221.jpg\n├── dog.6222.jpg\n├── dog.6223.jpg\n├── dog.6224.jpg\n├── dog.6225.jpg\n├── dog.6226.jpg\n├── dog.6227.jpg\n├── dog.6228.jpg\n├── dog.6229.jpg\n├── dog.622.jpg\n├── dog.6230.jpg\n├── dog.6231.jpg\n├── dog.6232.jpg\n├── dog.6233.jpg\n├── dog.6234.jpg\n├── dog.6235.jpg\n├── dog.6236.jpg\n├── dog.6237.jpg\n├── dog.6238.jpg\n├── dog.6239.jpg\n├── dog.623.jpg\n├── dog.6240.jpg\n├── dog.6241.jpg\n├── dog.6242.jpg\n├── dog.6243.jpg\n├── dog.6244.jpg\n├── dog.6245.jpg\n├── dog.6246.jpg\n├── dog.6247.jpg\n├── dog.6248.jpg\n├── dog.6249.jpg\n├── dog.624.jpg\n├── dog.6250.jpg\n├── dog.6251.jpg\n├── dog.6252.jpg\n├── dog.6253.jpg\n├── dog.6254.jpg\n├── dog.6255.jpg\n├── dog.6256.jpg\n├── dog.6257.jpg\n├── dog.6258.jpg\n├── dog.6259.jpg\n├── dog.625.jpg\n├── dog.6260.jpg\n├── dog.6261.jpg\n├── dog.6262.jpg\n├── dog.6263.jpg\n├── dog.6264.jpg\n├── dog.6265.jpg\n├── dog.6266.jpg\n├── dog.6267.jpg\n├── dog.6268.jpg\n├── dog.6269.jpg\n├── dog.626.jpg\n├── dog.6270.jpg\n├── dog.6271.jpg\n├── dog.6272.jpg\n├── dog.6273.jpg\n├── dog.6274.jpg\n├── dog.6275.jpg\n├── dog.6276.jpg\n├── dog.6277.jpg\n├── dog.6278.jpg\n├── dog.6279.jpg\n├── dog.627.jpg\n├── dog.6280.jpg\n├── dog.6281.jpg\n├── dog.6282.jpg\n├── dog.6283.jpg\n├── dog.6284.jpg\n├── dog.6285.jpg\n├── dog.6286.jpg\n├── dog.6287.jpg\n├── dog.6288.jpg\n├── dog.6289.jpg\n├── dog.628.jpg\n├── dog.6290.jpg\n├── dog.6291.jpg\n├── dog.6292.jpg\n├── dog.6293.jpg\n├── dog.6294.jpg\n├── dog.6295.jpg\n├── dog.6296.jpg\n├── dog.6297.jpg\n├── dog.6298.jpg\n├── dog.6299.jpg\n├── dog.629.jpg\n├── dog.62.jpg\n├── dog.6300.jpg\n├── dog.6301.jpg\n├── dog.6302.jpg\n├── dog.6303.jpg\n├── dog.6304.jpg\n├── dog.6305.jpg\n├── dog.6306.jpg\n├── dog.6307.jpg\n├── dog.6308.jpg\n├── dog.6309.jpg\n├── dog.630.jpg\n├── dog.6310.jpg\n├── dog.6311.jpg\n├── dog.6312.jpg\n├── dog.6313.jpg\n├── dog.6314.jpg\n├── dog.6315.jpg\n├── dog.6316.jpg\n├── dog.6317.jpg\n├── dog.6318.jpg\n├── dog.6319.jpg\n├── dog.631.jpg\n├── dog.6320.jpg\n├── dog.6321.jpg\n├── dog.6322.jpg\n├── dog.6323.jpg\n├── dog.6324.jpg\n├── dog.6325.jpg\n├── dog.6326.jpg\n├── dog.6327.jpg\n├── dog.6328.jpg\n├── dog.6329.jpg\n├── dog.632.jpg\n├── dog.6330.jpg\n├── dog.6331.jpg\n├── dog.6332.jpg\n├── dog.6333.jpg\n├── dog.6334.jpg\n├── dog.6335.jpg\n├── dog.6336.jpg\n├── dog.6337.jpg\n├── dog.6338.jpg\n├── dog.6339.jpg\n├── dog.633.jpg\n├── dog.6340.jpg\n├── dog.6341.jpg\n├── dog.6342.jpg\n├── dog.6343.jpg\n├── dog.6344.jpg\n├── dog.6345.jpg\n├── dog.6346.jpg\n├── dog.6347.jpg\n├── dog.6348.jpg\n├── dog.6349.jpg\n├── dog.634.jpg\n├── dog.6350.jpg\n├── dog.6351.jpg\n├── dog.6352.jpg\n├── dog.6353.jpg\n├── dog.6354.jpg\n├── dog.6355.jpg\n├── dog.6356.jpg\n├── dog.6357.jpg\n├── dog.6358.jpg\n├── dog.6359.jpg\n├── dog.635.jpg\n├── dog.6360.jpg\n├── dog.6361.jpg\n├── dog.6362.jpg\n├── dog.6363.jpg\n├── dog.6364.jpg\n├── dog.6365.jpg\n├── dog.6366.jpg\n├── dog.6367.jpg\n├── dog.6368.jpg\n├── dog.6369.jpg\n├── dog.636.jpg\n├── dog.6370.jpg\n├── dog.6371.jpg\n├── dog.6372.jpg\n├── dog.6373.jpg\n├── dog.6374.jpg\n├── dog.6375.jpg\n├── dog.6376.jpg\n├── dog.6377.jpg\n├── dog.6378.jpg\n├── dog.6379.jpg\n├── dog.637.jpg\n├── dog.6380.jpg\n├── dog.6381.jpg\n├── dog.6382.jpg\n├── dog.6383.jpg\n├── dog.6384.jpg\n├── dog.6385.jpg\n├── dog.6386.jpg\n├── dog.6387.jpg\n├── dog.6388.jpg\n├── dog.6389.jpg\n├── dog.638.jpg\n├── dog.6390.jpg\n├── dog.6391.jpg\n├── dog.6392.jpg\n├── dog.6393.jpg\n├── dog.6394.jpg\n├── dog.6395.jpg\n├── dog.6396.jpg\n├── dog.6397.jpg\n├── dog.6398.jpg\n├── dog.6399.jpg\n├── dog.639.jpg\n├── dog.63.jpg\n├── dog.6400.jpg\n├── dog.6401.jpg\n├── dog.6402.jpg\n├── dog.6403.jpg\n├── dog.6404.jpg\n├── dog.6405.jpg\n├── dog.6406.jpg\n├── dog.6407.jpg\n├── dog.6408.jpg\n├── dog.6409.jpg\n├── dog.640.jpg\n├── dog.6410.jpg\n├── dog.6411.jpg\n├── dog.6412.jpg\n├── dog.6413.jpg\n├── dog.6414.jpg\n├── dog.6415.jpg\n├── dog.6416.jpg\n├── dog.6417.jpg\n├── dog.6418.jpg\n├── dog.6419.jpg\n├── dog.641.jpg\n├── dog.6420.jpg\n├── dog.6421.jpg\n├── dog.6422.jpg\n├── dog.6423.jpg\n├── dog.6424.jpg\n├── dog.6425.jpg\n├── dog.6426.jpg\n├── dog.6427.jpg\n├── dog.6428.jpg\n├── dog.6429.jpg\n├── dog.642.jpg\n├── dog.6430.jpg\n├── dog.6431.jpg\n├── dog.6432.jpg\n├── dog.6433.jpg\n├── dog.6434.jpg\n├── dog.6435.jpg\n├── dog.6436.jpg\n├── dog.6437.jpg\n├── dog.6438.jpg\n├── dog.6439.jpg\n├── dog.643.jpg\n├── dog.6440.jpg\n├── dog.6441.jpg\n├── dog.6442.jpg\n├── dog.6443.jpg\n├── dog.6444.jpg\n├── dog.6445.jpg\n├── dog.6446.jpg\n├── dog.6447.jpg\n├── dog.6448.jpg\n├── dog.6449.jpg\n├── dog.644.jpg\n├── dog.6450.jpg\n├── dog.6451.jpg\n├── dog.6452.jpg\n├── dog.6453.jpg\n├── dog.6454.jpg\n├── dog.6455.jpg\n├── dog.6456.jpg\n├── dog.6457.jpg\n├── dog.6458.jpg\n├── dog.6459.jpg\n├── dog.645.jpg\n├── dog.6460.jpg\n├── dog.6461.jpg\n├── dog.6462.jpg\n├── dog.6463.jpg\n├── dog.6464.jpg\n├── dog.6465.jpg\n├── dog.6466.jpg\n├── dog.6467.jpg\n├── dog.6468.jpg\n├── dog.6469.jpg\n├── dog.646.jpg\n├── dog.6470.jpg\n├── dog.6471.jpg\n├── dog.6472.jpg\n├── dog.6473.jpg\n├── dog.6474.jpg\n├── dog.6475.jpg\n├── dog.6476.jpg\n├── dog.6477.jpg\n├── dog.6478.jpg\n├── dog.6479.jpg\n├── dog.647.jpg\n├── dog.6480.jpg\n├── dog.6481.jpg\n├── dog.6482.jpg\n├── dog.6483.jpg\n├── dog.6484.jpg\n├── dog.6485.jpg\n├── dog.6486.jpg\n├── dog.6487.jpg\n├── dog.6488.jpg\n├── dog.6489.jpg\n├── dog.648.jpg\n├── dog.6490.jpg\n├── dog.6491.jpg\n├── dog.6492.jpg\n├── dog.6493.jpg\n├── dog.6494.jpg\n├── dog.6495.jpg\n├── dog.6496.jpg\n├── dog.6497.jpg\n├── dog.6498.jpg\n├── dog.6499.jpg\n├── dog.649.jpg\n├── dog.64.jpg\n├── dog.6500.jpg\n├── dog.6501.jpg\n├── dog.6502.jpg\n├── dog.6503.jpg\n├── dog.6504.jpg\n├── dog.6505.jpg\n├── dog.6506.jpg\n├── dog.6507.jpg\n├── dog.6508.jpg\n├── dog.6509.jpg\n├── dog.650.jpg\n├── dog.6510.jpg\n├── dog.6511.jpg\n├── dog.6512.jpg\n├── dog.6513.jpg\n├── dog.6514.jpg\n├── dog.6515.jpg\n├── dog.6516.jpg\n├── dog.6517.jpg\n├── dog.6518.jpg\n├── dog.6519.jpg\n├── dog.651.jpg\n├── dog.6520.jpg\n├── dog.6521.jpg\n├── dog.6522.jpg\n├── dog.6523.jpg\n├── dog.6524.jpg\n├── dog.6525.jpg\n├── dog.6526.jpg\n├── dog.6527.jpg\n├── dog.6528.jpg\n├── dog.6529.jpg\n├── dog.652.jpg\n├── dog.6530.jpg\n├── dog.6531.jpg\n├── dog.6532.jpg\n├── dog.6533.jpg\n├── dog.6534.jpg\n├── dog.6535.jpg\n├── dog.6536.jpg\n├── dog.6537.jpg\n├── dog.6538.jpg\n├── dog.6539.jpg\n├── dog.653.jpg\n├── dog.6540.jpg\n├── dog.6541.jpg\n├── dog.6542.jpg\n├── dog.6543.jpg\n├── dog.6544.jpg\n├── dog.6545.jpg\n├── dog.6546.jpg\n├── dog.6547.jpg\n├── dog.6548.jpg\n├── dog.6549.jpg\n├── dog.654.jpg\n├── dog.6550.jpg\n├── dog.6551.jpg\n├── dog.6552.jpg\n├── dog.6553.jpg\n├── dog.6554.jpg\n├── dog.6555.jpg\n├── dog.6556.jpg\n├── dog.6557.jpg\n├── dog.6558.jpg\n├── dog.6559.jpg\n├── dog.655.jpg\n├── dog.6560.jpg\n├── dog.6561.jpg\n├── dog.6562.jpg\n├── dog.6563.jpg\n├── dog.6564.jpg\n├── dog.6565.jpg\n├── dog.6566.jpg\n├── dog.6567.jpg\n├── dog.6568.jpg\n├── dog.6569.jpg\n├── dog.656.jpg\n├── dog.6570.jpg\n├── dog.6571.jpg\n├── dog.6572.jpg\n├── dog.6573.jpg\n├── dog.6574.jpg\n├── dog.6575.jpg\n├── dog.6576.jpg\n├── dog.6577.jpg\n├── dog.6578.jpg\n├── dog.6579.jpg\n├── dog.657.jpg\n├── dog.6580.jpg\n├── dog.6581.jpg\n├── dog.6582.jpg\n├── dog.6583.jpg\n├── dog.6584.jpg\n├── dog.6585.jpg\n├── dog.6586.jpg\n├── dog.6587.jpg\n├── dog.6588.jpg\n├── dog.6589.jpg\n├── dog.658.jpg\n├── dog.6590.jpg\n├── dog.6591.jpg\n├── dog.6592.jpg\n├── dog.6593.jpg\n├── dog.6594.jpg\n├── dog.6595.jpg\n├── dog.6596.jpg\n├── dog.6597.jpg\n├── dog.6598.jpg\n├── dog.6599.jpg\n├── dog.659.jpg\n├── dog.65.jpg\n├── dog.6600.jpg\n├── dog.6601.jpg\n├── dog.6602.jpg\n├── dog.6603.jpg\n├── dog.6604.jpg\n├── dog.6605.jpg\n├── dog.6606.jpg\n├── dog.6607.jpg\n├── dog.6608.jpg\n├── dog.6609.jpg\n├── dog.660.jpg\n├── dog.6610.jpg\n├── dog.6611.jpg\n├── dog.6612.jpg\n├── dog.6613.jpg\n├── dog.6614.jpg\n├── dog.6615.jpg\n├── dog.6616.jpg\n├── dog.6617.jpg\n├── dog.6618.jpg\n├── dog.6619.jpg\n├── dog.661.jpg\n├── dog.6620.jpg\n├── dog.6621.jpg\n├── dog.6622.jpg\n├── dog.6623.jpg\n├── dog.6624.jpg\n├── dog.6625.jpg\n├── dog.6626.jpg\n├── dog.6627.jpg\n├── dog.6628.jpg\n├── dog.6629.jpg\n├── dog.662.jpg\n├── dog.6630.jpg\n├── dog.6631.jpg\n├── dog.6632.jpg\n├── dog.6633.jpg\n├── dog.6634.jpg\n├── dog.6635.jpg\n├── dog.6636.jpg\n├── dog.6637.jpg\n├── dog.6638.jpg\n├── dog.6639.jpg\n├── dog.663.jpg\n├── dog.6640.jpg\n├── dog.6641.jpg\n├── dog.6642.jpg\n├── dog.6643.jpg\n├── dog.6644.jpg\n├── dog.6645.jpg\n├── dog.6646.jpg\n├── dog.6647.jpg\n├── dog.6648.jpg\n├── dog.6649.jpg\n├── dog.664.jpg\n├── dog.6650.jpg\n├── dog.6651.jpg\n├── dog.6652.jpg\n├── dog.6653.jpg\n├── dog.6654.jpg\n├── dog.6655.jpg\n├── dog.6656.jpg\n├── dog.6657.jpg\n├── dog.6658.jpg\n├── dog.6659.jpg\n├── dog.665.jpg\n├── dog.6660.jpg\n├── dog.6661.jpg\n├── dog.6662.jpg\n├── dog.6663.jpg\n├── dog.6664.jpg\n├── dog.6665.jpg\n├── dog.6666.jpg\n├── dog.6667.jpg\n├── dog.6668.jpg\n├── dog.6669.jpg\n├── dog.666.jpg\n├── dog.6670.jpg\n├── dog.6671.jpg\n├── dog.6672.jpg\n├── dog.6673.jpg\n├── dog.6674.jpg\n├── dog.6675.jpg\n├── dog.6676.jpg\n├── dog.6677.jpg\n├── dog.6678.jpg\n├── dog.6679.jpg\n├── dog.667.jpg\n├── dog.6680.jpg\n├── dog.6681.jpg\n├── dog.6682.jpg\n├── dog.6683.jpg\n├── dog.6684.jpg\n├── dog.6685.jpg\n├── dog.6686.jpg\n├── dog.6687.jpg\n├── dog.6688.jpg\n├── dog.6689.jpg\n├── dog.668.jpg\n├── dog.6690.jpg\n├── dog.6691.jpg\n├── dog.6692.jpg\n├── dog.6693.jpg\n├── dog.6694.jpg\n├── dog.6695.jpg\n├── dog.6696.jpg\n├── dog.6697.jpg\n├── dog.6698.jpg\n├── dog.6699.jpg\n├── dog.669.jpg\n├── dog.66.jpg\n├── dog.6700.jpg\n├── dog.6701.jpg\n├── dog.6702.jpg\n├── dog.6703.jpg\n├── dog.6704.jpg\n├── dog.6705.jpg\n├── dog.6706.jpg\n├── dog.6707.jpg\n├── dog.6708.jpg\n├── dog.6709.jpg\n├── dog.670.jpg\n├── dog.6710.jpg\n├── dog.6711.jpg\n├── dog.6712.jpg\n├── dog.6713.jpg\n├── dog.6714.jpg\n├── dog.6715.jpg\n├── dog.6716.jpg\n├── dog.6717.jpg\n├── dog.6718.jpg\n├── dog.6719.jpg\n├── dog.671.jpg\n├── dog.6720.jpg\n├── dog.6721.jpg\n├── dog.6722.jpg\n├── dog.6723.jpg\n├── dog.6724.jpg\n├── dog.6725.jpg\n├── dog.6726.jpg\n├── dog.6727.jpg\n├── dog.6728.jpg\n├── dog.6729.jpg\n├── dog.672.jpg\n├── dog.6730.jpg\n├── dog.6731.jpg\n├── dog.6732.jpg\n├── dog.6733.jpg\n├── dog.6734.jpg\n├── dog.6735.jpg\n├── dog.6736.jpg\n├── dog.6737.jpg\n├── dog.6738.jpg\n├── dog.6739.jpg\n├── dog.673.jpg\n├── dog.6740.jpg\n├── dog.6741.jpg\n├── dog.6742.jpg\n├── dog.6743.jpg\n├── dog.6744.jpg\n├── dog.6745.jpg\n├── dog.6746.jpg\n├── dog.6747.jpg\n├── dog.6748.jpg\n├── dog.6749.jpg\n├── dog.674.jpg\n├── dog.6750.jpg\n├── dog.6751.jpg\n├── dog.6752.jpg\n├── dog.6753.jpg\n├── dog.6754.jpg\n├── dog.6755.jpg\n├── dog.6756.jpg\n├── dog.6757.jpg\n├── dog.6758.jpg\n├── dog.6759.jpg\n├── dog.675.jpg\n├── dog.6760.jpg\n├── dog.6761.jpg\n├── dog.6762.jpg\n├── dog.6763.jpg\n├── dog.6764.jpg\n├── dog.6765.jpg\n├── dog.6766.jpg\n├── dog.6767.jpg\n├── dog.6768.jpg\n├── dog.6769.jpg\n├── dog.676.jpg\n├── dog.6770.jpg\n├── dog.6771.jpg\n├── dog.6772.jpg\n├── dog.6773.jpg\n├── dog.6774.jpg\n├── dog.6775.jpg\n├── dog.6776.jpg\n├── dog.6777.jpg\n├── dog.6778.jpg\n├── dog.6779.jpg\n├── dog.677.jpg\n├── dog.6780.jpg\n├── dog.6781.jpg\n├── dog.6782.jpg\n├── dog.6783.jpg\n├── dog.6784.jpg\n├── dog.6785.jpg\n├── dog.6786.jpg\n├── dog.6787.jpg\n├── dog.6788.jpg\n├── dog.6789.jpg\n├── dog.678.jpg\n├── dog.6790.jpg\n├── dog.6791.jpg\n├── dog.6792.jpg\n├── dog.6793.jpg\n├── dog.6794.jpg\n├── dog.6795.jpg\n├── dog.6796.jpg\n├── dog.6797.jpg\n├── dog.6798.jpg\n├── dog.6799.jpg\n├── dog.679.jpg\n├── dog.67.jpg\n├── dog.6800.jpg\n├── dog.6801.jpg\n├── dog.6802.jpg\n├── dog.6803.jpg\n├── dog.6804.jpg\n├── dog.6805.jpg\n├── dog.6806.jpg\n├── dog.6807.jpg\n├── dog.6808.jpg\n├── dog.6809.jpg\n├── dog.680.jpg\n├── dog.6810.jpg\n├── dog.6811.jpg\n├── dog.6812.jpg\n├── dog.6813.jpg\n├── dog.6814.jpg\n├── dog.6815.jpg\n├── dog.6816.jpg\n├── dog.6817.jpg\n├── dog.6818.jpg\n├── dog.6819.jpg\n├── dog.681.jpg\n├── dog.6820.jpg\n├── dog.6821.jpg\n├── dog.6822.jpg\n├── dog.6823.jpg\n├── dog.6824.jpg\n├── dog.6825.jpg\n├── dog.6826.jpg\n├── dog.6827.jpg\n├── dog.6828.jpg\n├── dog.6829.jpg\n├── dog.682.jpg\n├── dog.6830.jpg\n├── dog.6831.jpg\n├── dog.6832.jpg\n├── dog.6833.jpg\n├── dog.6834.jpg\n├── dog.6835.jpg\n├── dog.6836.jpg\n├── dog.6837.jpg\n├── dog.6838.jpg\n├── dog.6839.jpg\n├── dog.683.jpg\n├── dog.6840.jpg\n├── dog.6841.jpg\n├── dog.6842.jpg\n├── dog.6843.jpg\n├── dog.6844.jpg\n├── dog.6845.jpg\n├── dog.6846.jpg\n├── dog.6847.jpg\n├── dog.6848.jpg\n├── dog.6849.jpg\n├── dog.684.jpg\n├── dog.6850.jpg\n├── dog.6851.jpg\n├── dog.6852.jpg\n├── dog.6853.jpg\n├── dog.6854.jpg\n├── dog.6855.jpg\n├── dog.6856.jpg\n├── dog.6857.jpg\n├── dog.6858.jpg\n├── dog.6859.jpg\n├── dog.685.jpg\n├── dog.6860.jpg\n├── dog.6861.jpg\n├── dog.6862.jpg\n├── dog.6863.jpg\n├── dog.6864.jpg\n├── dog.6865.jpg\n├── dog.6866.jpg\n├── dog.6867.jpg\n├── dog.6868.jpg\n├── dog.6869.jpg\n├── dog.686.jpg\n├── dog.6870.jpg\n├── dog.6871.jpg\n├── dog.6872.jpg\n├── dog.6873.jpg\n├── dog.6874.jpg\n├── dog.6875.jpg\n├── dog.6876.jpg\n├── dog.6877.jpg\n├── dog.6878.jpg\n├── dog.6879.jpg\n├── dog.687.jpg\n├── dog.6880.jpg\n├── dog.6881.jpg\n├── dog.6882.jpg\n├── dog.6883.jpg\n├── dog.6884.jpg\n├── dog.6885.jpg\n├── dog.6886.jpg\n├── dog.6887.jpg\n├── dog.6888.jpg\n├── dog.6889.jpg\n├── dog.688.jpg\n├── dog.6890.jpg\n├── dog.6891.jpg\n├── dog.6892.jpg\n├── dog.6893.jpg\n├── dog.6894.jpg\n├── dog.6895.jpg\n├── dog.6896.jpg\n├── dog.6897.jpg\n├── dog.6898.jpg\n├── dog.6899.jpg\n├── dog.689.jpg\n├── dog.68.jpg\n├── dog.6900.jpg\n├── dog.6901.jpg\n├── dog.6902.jpg\n├── dog.6903.jpg\n├── dog.6904.jpg\n├── dog.6905.jpg\n├── dog.6906.jpg\n├── dog.6907.jpg\n├── dog.6908.jpg\n├── dog.6909.jpg\n├── dog.690.jpg\n├── dog.6910.jpg\n├── dog.6911.jpg\n├── dog.6912.jpg\n├── dog.6913.jpg\n├── dog.6914.jpg\n├── dog.6915.jpg\n├── dog.6916.jpg\n├── dog.6917.jpg\n├── dog.6918.jpg\n├── dog.6919.jpg\n├── dog.691.jpg\n├── dog.6920.jpg\n├── dog.6921.jpg\n├── dog.6922.jpg\n├── dog.6923.jpg\n├── dog.6924.jpg\n├── dog.6925.jpg\n├── dog.6926.jpg\n├── dog.6927.jpg\n├── dog.6928.jpg\n├── dog.6929.jpg\n├── dog.692.jpg\n├── dog.6930.jpg\n├── dog.6931.jpg\n├── dog.6932.jpg\n├── dog.6933.jpg\n├── dog.6934.jpg\n├── dog.6935.jpg\n├── dog.6936.jpg\n├── dog.6937.jpg\n├── dog.6938.jpg\n├── dog.6939.jpg\n├── dog.693.jpg\n├── dog.6940.jpg\n├── dog.6941.jpg\n├── dog.6942.jpg\n├── dog.6943.jpg\n├── dog.6944.jpg\n├── dog.6945.jpg\n├── dog.6946.jpg\n├── dog.6947.jpg\n├── dog.6948.jpg\n├── dog.6949.jpg\n├── dog.694.jpg\n├── dog.6950.jpg\n├── dog.6951.jpg\n├── dog.6952.jpg\n├── dog.6953.jpg\n├── dog.6954.jpg\n├── dog.6955.jpg\n├── dog.6956.jpg\n├── dog.6957.jpg\n├── dog.6958.jpg\n├── dog.6959.jpg\n├── dog.695.jpg\n├── dog.6960.jpg\n├── dog.6961.jpg\n├── dog.6962.jpg\n├── dog.6963.jpg\n├── dog.6964.jpg\n├── dog.6965.jpg\n├── dog.6966.jpg\n├── dog.6967.jpg\n├── dog.6968.jpg\n├── dog.6969.jpg\n├── dog.696.jpg\n├── dog.6970.jpg\n├── dog.6971.jpg\n├── dog.6972.jpg\n├── dog.6973.jpg\n├── dog.6974.jpg\n├── dog.6975.jpg\n├── dog.6976.jpg\n├── dog.6977.jpg\n├── dog.6978.jpg\n├── dog.6979.jpg\n├── dog.697.jpg\n├── dog.6980.jpg\n├── dog.6981.jpg\n├── dog.6982.jpg\n├── dog.6983.jpg\n├── dog.6984.jpg\n├── dog.6985.jpg\n├── dog.6986.jpg\n├── dog.6987.jpg\n├── dog.6988.jpg\n├── dog.6989.jpg\n├── dog.698.jpg\n├── dog.6990.jpg\n├── dog.6991.jpg\n├── dog.6992.jpg\n├── dog.6993.jpg\n├── dog.6994.jpg\n├── dog.6995.jpg\n├── dog.6996.jpg\n├── dog.6997.jpg\n├── dog.6998.jpg\n├── dog.6999.jpg\n├── dog.699.jpg\n├── dog.69.jpg\n├── dog.6.jpg\n├── dog.7000.jpg\n├── dog.7001.jpg\n├── dog.7002.jpg\n├── dog.7003.jpg\n├── dog.7004.jpg\n├── dog.7005.jpg\n├── dog.7006.jpg\n├── dog.7007.jpg\n├── dog.7008.jpg\n├── dog.7009.jpg\n├── dog.700.jpg\n├── dog.7010.jpg\n├── dog.7011.jpg\n├── dog.7012.jpg\n├── dog.7013.jpg\n├── dog.7014.jpg\n├── dog.7015.jpg\n├── dog.7016.jpg\n├── dog.7017.jpg\n├── dog.7018.jpg\n├── dog.7019.jpg\n├── dog.701.jpg\n├── dog.7020.jpg\n├── dog.7021.jpg\n├── dog.7022.jpg\n├── dog.7023.jpg\n├── dog.7024.jpg\n├── dog.7025.jpg\n├── dog.7026.jpg\n├── dog.7027.jpg\n├── dog.7028.jpg\n├── dog.7029.jpg\n├── dog.702.jpg\n├── dog.7030.jpg\n├── dog.7031.jpg\n├── dog.7032.jpg\n├── dog.7033.jpg\n├── dog.7034.jpg\n├── dog.7035.jpg\n├── dog.7036.jpg\n├── dog.7037.jpg\n├── dog.7038.jpg\n├── dog.7039.jpg\n├── dog.703.jpg\n├── dog.7040.jpg\n├── dog.7041.jpg\n├── dog.7042.jpg\n├── dog.7043.jpg\n├── dog.7044.jpg\n├── dog.7045.jpg\n├── dog.7046.jpg\n├── dog.7047.jpg\n├── dog.7048.jpg\n├── dog.7049.jpg\n├── dog.704.jpg\n├── dog.7050.jpg\n├── dog.7051.jpg\n├── dog.7052.jpg\n├── dog.7053.jpg\n├── dog.7054.jpg\n├── dog.7055.jpg\n├── dog.7056.jpg\n├── dog.7057.jpg\n├── dog.7058.jpg\n├── dog.7059.jpg\n├── dog.705.jpg\n├── dog.7060.jpg\n├── dog.7061.jpg\n├── dog.7062.jpg\n├── dog.7063.jpg\n├── dog.7064.jpg\n├── dog.7065.jpg\n├── dog.7066.jpg\n├── dog.7067.jpg\n├── dog.7068.jpg\n├── dog.7069.jpg\n├── dog.706.jpg\n├── dog.7070.jpg\n├── dog.7071.jpg\n├── dog.7072.jpg\n├── dog.7073.jpg\n├── dog.7074.jpg\n├── dog.7075.jpg\n├── dog.7076.jpg\n├── dog.7077.jpg\n├── dog.7078.jpg\n├── dog.7079.jpg\n├── dog.707.jpg\n├── dog.7080.jpg\n├── dog.7081.jpg\n├── dog.7082.jpg\n├── dog.7083.jpg\n├── dog.7084.jpg\n├── dog.7085.jpg\n├── dog.7086.jpg\n├── dog.7087.jpg\n├── dog.7088.jpg\n├── dog.7089.jpg\n├── dog.708.jpg\n├── dog.7090.jpg\n├── dog.7091.jpg\n├── dog.7092.jpg\n├── dog.7093.jpg\n├── dog.7094.jpg\n├── dog.7095.jpg\n├── dog.7096.jpg\n├── dog.7097.jpg\n├── dog.7098.jpg\n├── dog.7099.jpg\n├── dog.709.jpg\n├── dog.70.jpg\n├── dog.7100.jpg\n├── dog.7101.jpg\n├── dog.7102.jpg\n├── dog.7103.jpg\n├── dog.7104.jpg\n├── dog.7105.jpg\n├── dog.7106.jpg\n├── dog.7107.jpg\n├── dog.7108.jpg\n├── dog.7109.jpg\n├── dog.710.jpg\n├── dog.7110.jpg\n├── dog.7111.jpg\n├── dog.7112.jpg\n├── dog.7113.jpg\n├── dog.7114.jpg\n├── dog.7115.jpg\n├── dog.7116.jpg\n├── dog.7117.jpg\n├── dog.7118.jpg\n├── dog.7119.jpg\n├── dog.711.jpg\n├── dog.7120.jpg\n├── dog.7121.jpg\n├── dog.7122.jpg\n├── dog.7123.jpg\n├── dog.7124.jpg\n├── dog.7125.jpg\n├── dog.7126.jpg\n├── dog.7127.jpg\n├── dog.7128.jpg\n├── dog.7129.jpg\n├── dog.712.jpg\n├── dog.7130.jpg\n├── dog.7131.jpg\n├── dog.7132.jpg\n├── dog.7133.jpg\n├── dog.7134.jpg\n├── dog.7135.jpg\n├── dog.7136.jpg\n├── dog.7137.jpg\n├── dog.7138.jpg\n├── dog.7139.jpg\n├── dog.713.jpg\n├── dog.7140.jpg\n├── dog.7141.jpg\n├── dog.7142.jpg\n├── dog.7143.jpg\n├── dog.7144.jpg\n├── dog.7145.jpg\n├── dog.7146.jpg\n├── dog.7147.jpg\n├── dog.7148.jpg\n├── dog.7149.jpg\n├── dog.714.jpg\n├── dog.7150.jpg\n├── dog.7151.jpg\n├── dog.7152.jpg\n├── dog.7153.jpg\n├── dog.7154.jpg\n├── dog.7155.jpg\n├── dog.7156.jpg\n├── dog.7157.jpg\n├── dog.7158.jpg\n├── dog.7159.jpg\n├── dog.715.jpg\n├── dog.7160.jpg\n├── dog.7161.jpg\n├── dog.7162.jpg\n├── dog.7163.jpg\n├── dog.7164.jpg\n├── dog.7165.jpg\n├── dog.7166.jpg\n├── dog.7167.jpg\n├── dog.7168.jpg\n├── dog.7169.jpg\n├── dog.716.jpg\n├── dog.7170.jpg\n├── dog.7171.jpg\n├── dog.7172.jpg\n├── dog.7173.jpg\n├── dog.7174.jpg\n├── dog.7175.jpg\n├── dog.7176.jpg\n├── dog.7177.jpg\n├── dog.7178.jpg\n├── dog.7179.jpg\n├── dog.717.jpg\n├── dog.7180.jpg\n├── dog.7181.jpg\n├── dog.7182.jpg\n├── dog.7183.jpg\n├── dog.7184.jpg\n├── dog.7185.jpg\n├── dog.7186.jpg\n├── dog.7187.jpg\n├── dog.7188.jpg\n├── dog.7189.jpg\n├── dog.718.jpg\n├── dog.7190.jpg\n├── dog.7191.jpg\n├── dog.7192.jpg\n├── dog.7193.jpg\n├── dog.7194.jpg\n├── dog.7195.jpg\n├── dog.7196.jpg\n├── dog.7197.jpg\n├── dog.7198.jpg\n├── dog.7199.jpg\n├── dog.719.jpg\n├── dog.71.jpg\n├── dog.7200.jpg\n├── dog.7201.jpg\n├── dog.7202.jpg\n├── dog.7203.jpg\n├── dog.7204.jpg\n├── dog.7205.jpg\n├── dog.7206.jpg\n├── dog.7207.jpg\n├── dog.7208.jpg\n├── dog.7209.jpg\n├── dog.720.jpg\n├── dog.7210.jpg\n├── dog.7211.jpg\n├── dog.7212.jpg\n├── dog.7213.jpg\n├── dog.7214.jpg\n├── dog.7215.jpg\n├── dog.7216.jpg\n├── dog.7217.jpg\n├── dog.7218.jpg\n├── dog.7219.jpg\n├── dog.721.jpg\n├── dog.7220.jpg\n├── dog.7221.jpg\n├── dog.7222.jpg\n├── dog.7223.jpg\n├── dog.7224.jpg\n├── dog.7225.jpg\n├── dog.7226.jpg\n├── dog.7227.jpg\n├── dog.7228.jpg\n├── dog.7229.jpg\n├── dog.722.jpg\n├── dog.7230.jpg\n├── dog.7231.jpg\n├── dog.7232.jpg\n├── dog.7233.jpg\n├── dog.7234.jpg\n├── dog.7235.jpg\n├── dog.7236.jpg\n├── dog.7237.jpg\n├── dog.7238.jpg\n├── dog.7239.jpg\n├── dog.723.jpg\n├── dog.7240.jpg\n├── dog.7241.jpg\n├── dog.7242.jpg\n├── dog.7243.jpg\n├── dog.7244.jpg\n├── dog.7245.jpg\n├── dog.7246.jpg\n├── dog.7247.jpg\n├── dog.7248.jpg\n├── dog.7249.jpg\n├── dog.724.jpg\n├── dog.7250.jpg\n├── dog.7251.jpg\n├── dog.7252.jpg\n├── dog.7253.jpg\n├── dog.7254.jpg\n├── dog.7255.jpg\n├── dog.7256.jpg\n├── dog.7257.jpg\n├── dog.7258.jpg\n├── dog.7259.jpg\n├── dog.725.jpg\n├── dog.7260.jpg\n├── dog.7261.jpg\n├── dog.7262.jpg\n├── dog.7263.jpg\n├── dog.7264.jpg\n├── dog.7265.jpg\n├── dog.7266.jpg\n├── dog.7267.jpg\n├── dog.7268.jpg\n├── dog.7269.jpg\n├── dog.726.jpg\n├── dog.7270.jpg\n├── dog.7271.jpg\n├── dog.7272.jpg\n├── dog.7273.jpg\n├── dog.7274.jpg\n├── dog.7275.jpg\n├── dog.7276.jpg\n├── dog.7277.jpg\n├── dog.7278.jpg\n├── dog.7279.jpg\n├── dog.727.jpg\n├── dog.7280.jpg\n├── dog.7281.jpg\n├── dog.7282.jpg\n├── dog.7283.jpg\n├── dog.7284.jpg\n├── dog.7285.jpg\n├── dog.7286.jpg\n├── dog.7287.jpg\n├── dog.7288.jpg\n├── dog.7289.jpg\n├── dog.728.jpg\n├── dog.7290.jpg\n├── dog.7291.jpg\n├── dog.7292.jpg\n├── dog.7293.jpg\n├── dog.7294.jpg\n├── dog.7295.jpg\n├── dog.7296.jpg\n├── dog.7297.jpg\n├── dog.7298.jpg\n├── dog.7299.jpg\n├── dog.729.jpg\n├── dog.72.jpg\n├── dog.7300.jpg\n├── dog.7301.jpg\n├── dog.7302.jpg\n├── dog.7303.jpg\n├── dog.7304.jpg\n├── dog.7305.jpg\n├── dog.7306.jpg\n├── dog.7307.jpg\n├── dog.7308.jpg\n├── dog.7309.jpg\n├── dog.730.jpg\n├── dog.7310.jpg\n├── dog.7311.jpg\n├── dog.7312.jpg\n├── dog.7313.jpg\n├── dog.7314.jpg\n├── dog.7315.jpg\n├── dog.7316.jpg\n├── dog.7317.jpg\n├── dog.7318.jpg\n├── dog.7319.jpg\n├── dog.731.jpg\n├── dog.7320.jpg\n├── dog.7321.jpg\n├── dog.7322.jpg\n├── dog.7323.jpg\n├── dog.7324.jpg\n├── dog.7325.jpg\n├── dog.7326.jpg\n├── dog.7327.jpg\n├── dog.7328.jpg\n├── dog.7329.jpg\n├── dog.732.jpg\n├── dog.7330.jpg\n├── dog.7331.jpg\n├── dog.7332.jpg\n├── dog.7333.jpg\n├── dog.7334.jpg\n├── dog.7335.jpg\n├── dog.7336.jpg\n├── dog.7337.jpg\n├── dog.7338.jpg\n├── dog.7339.jpg\n├── dog.733.jpg\n├── dog.7340.jpg\n├── dog.7341.jpg\n├── dog.7342.jpg\n├── dog.7343.jpg\n├── dog.7344.jpg\n├── dog.7345.jpg\n├── dog.7346.jpg\n├── dog.7347.jpg\n├── dog.7348.jpg\n├── dog.7349.jpg\n├── dog.734.jpg\n├── dog.7350.jpg\n├── dog.7351.jpg\n├── dog.7352.jpg\n├── dog.7353.jpg\n├── dog.7354.jpg\n├── dog.7355.jpg\n├── dog.7356.jpg\n├── dog.7357.jpg\n├── dog.7358.jpg\n├── dog.7359.jpg\n├── dog.735.jpg\n├── dog.7360.jpg\n├── dog.7361.jpg\n├── dog.7362.jpg\n├── dog.7363.jpg\n├── dog.7364.jpg\n├── dog.7365.jpg\n├── dog.7366.jpg\n├── dog.7367.jpg\n├── dog.7368.jpg\n├── dog.7369.jpg\n├── dog.736.jpg\n├── dog.7370.jpg\n├── dog.7371.jpg\n├── dog.7372.jpg\n├── dog.7373.jpg\n├── dog.7374.jpg\n├── dog.7375.jpg\n├── dog.7376.jpg\n├── dog.7377.jpg\n├── dog.7378.jpg\n├── dog.7379.jpg\n├── dog.737.jpg\n├── dog.7380.jpg\n├── dog.7381.jpg\n├── dog.7382.jpg\n├── dog.7383.jpg\n├── dog.7384.jpg\n├── dog.7385.jpg\n├── dog.7386.jpg\n├── dog.7387.jpg\n├── dog.7388.jpg\n├── dog.7389.jpg\n├── dog.738.jpg\n├── dog.7390.jpg\n├── dog.7391.jpg\n├── dog.7392.jpg\n├── dog.7393.jpg\n├── dog.7394.jpg\n├── dog.7395.jpg\n├── dog.7396.jpg\n├── dog.7397.jpg\n├── dog.7398.jpg\n├── dog.7399.jpg\n├── dog.739.jpg\n├── dog.73.jpg\n├── dog.7400.jpg\n├── dog.7401.jpg\n├── dog.7402.jpg\n├── dog.7403.jpg\n├── dog.7404.jpg\n├── dog.7405.jpg\n├── dog.7406.jpg\n├── dog.7407.jpg\n├── dog.7408.jpg\n├── dog.7409.jpg\n├── dog.740.jpg\n├── dog.7410.jpg\n├── dog.7411.jpg\n├── dog.7412.jpg\n├── dog.7413.jpg\n├── dog.7414.jpg\n├── dog.7415.jpg\n├── dog.7416.jpg\n├── dog.7417.jpg\n├── dog.7418.jpg\n├── dog.7419.jpg\n├── dog.741.jpg\n├── dog.7420.jpg\n├── dog.7421.jpg\n├── dog.7422.jpg\n├── dog.7423.jpg\n├── dog.7424.jpg\n├── dog.7425.jpg\n├── dog.7426.jpg\n├── dog.7427.jpg\n├── dog.7428.jpg\n├── dog.7429.jpg\n├── dog.742.jpg\n├── dog.7430.jpg\n├── dog.7431.jpg\n├── dog.7432.jpg\n├── dog.7433.jpg\n├── dog.7434.jpg\n├── dog.7435.jpg\n├── dog.7436.jpg\n├── dog.7437.jpg\n├── dog.7438.jpg\n├── dog.7439.jpg\n├── dog.743.jpg\n├── dog.7440.jpg\n├── dog.7441.jpg\n├── dog.7442.jpg\n├── dog.7443.jpg\n├── dog.7444.jpg\n├── dog.7445.jpg\n├── dog.7446.jpg\n├── dog.7447.jpg\n├── dog.7448.jpg\n├── dog.7449.jpg\n├── dog.744.jpg\n├── dog.7450.jpg\n├── dog.7451.jpg\n├── dog.7452.jpg\n├── dog.7453.jpg\n├── dog.7454.jpg\n├── dog.7455.jpg\n├── dog.7456.jpg\n├── dog.7457.jpg\n├── dog.7458.jpg\n├── dog.7459.jpg\n├── dog.745.jpg\n├── dog.7460.jpg\n├── dog.7461.jpg\n├── dog.7462.jpg\n├── dog.7463.jpg\n├── dog.7464.jpg\n├── dog.7465.jpg\n├── dog.7466.jpg\n├── dog.7467.jpg\n├── dog.7468.jpg\n├── dog.7469.jpg\n├── dog.746.jpg\n├── dog.7470.jpg\n├── dog.7471.jpg\n├── dog.7472.jpg\n├── dog.7473.jpg\n├── dog.7474.jpg\n├── dog.7475.jpg\n├── dog.7476.jpg\n├── dog.7477.jpg\n├── dog.7478.jpg\n├── dog.7479.jpg\n├── dog.747.jpg\n├── dog.7480.jpg\n├── dog.7481.jpg\n├── dog.7482.jpg\n├── dog.7483.jpg\n├── dog.7484.jpg\n├── dog.7485.jpg\n├── dog.7486.jpg\n├── dog.7487.jpg\n├── dog.7488.jpg\n├── dog.7489.jpg\n├── dog.748.jpg\n├── dog.7490.jpg\n├── dog.7491.jpg\n├── dog.7492.jpg\n├── dog.7493.jpg\n├── dog.7494.jpg\n├── dog.7495.jpg\n├── dog.7496.jpg\n├── dog.7497.jpg\n├── dog.7498.jpg\n├── dog.7499.jpg\n├── dog.749.jpg\n├── dog.74.jpg\n├── dog.7500.jpg\n├── dog.7501.jpg\n├── dog.7502.jpg\n├── dog.7503.jpg\n├── dog.7504.jpg\n├── dog.7505.jpg\n├── dog.7506.jpg\n├── dog.7507.jpg\n├── dog.7508.jpg\n├── dog.7509.jpg\n├── dog.750.jpg\n├── dog.7510.jpg\n├── dog.7511.jpg\n├── dog.7512.jpg\n├── dog.7513.jpg\n├── dog.7514.jpg\n├── dog.7515.jpg\n├── dog.7516.jpg\n├── dog.7517.jpg\n├── dog.7518.jpg\n├── dog.7519.jpg\n├── dog.751.jpg\n├── dog.7520.jpg\n├── dog.7521.jpg\n├── dog.7522.jpg\n├── dog.7523.jpg\n├── dog.7524.jpg\n├── dog.7525.jpg\n├── dog.7526.jpg\n├── dog.7527.jpg\n├── dog.7528.jpg\n├── dog.7529.jpg\n├── dog.752.jpg\n├── dog.7530.jpg\n├── dog.7531.jpg\n├── dog.7532.jpg\n├── dog.7533.jpg\n├── dog.7534.jpg\n├── dog.7535.jpg\n├── dog.7536.jpg\n├── dog.7537.jpg\n├── dog.7538.jpg\n├── dog.7539.jpg\n├── dog.753.jpg\n├── dog.7540.jpg\n├── dog.7541.jpg\n├── dog.7542.jpg\n├── dog.7543.jpg\n├── dog.7544.jpg\n├── dog.7545.jpg\n├── dog.7546.jpg\n├── dog.7547.jpg\n├── dog.7548.jpg\n├── dog.7549.jpg\n├── dog.754.jpg\n├── dog.7550.jpg\n├── dog.7551.jpg\n├── dog.7552.jpg\n├── dog.7553.jpg\n├── dog.7554.jpg\n├── dog.7555.jpg\n├── dog.7556.jpg\n├── dog.7557.jpg\n├── dog.7558.jpg\n├── dog.7559.jpg\n├── dog.755.jpg\n├── dog.7560.jpg\n├── dog.7561.jpg\n├── dog.7562.jpg\n├── dog.7563.jpg\n├── dog.7564.jpg\n├── dog.7565.jpg\n├── dog.7566.jpg\n├── dog.7567.jpg\n├── dog.7568.jpg\n├── dog.7569.jpg\n├── dog.756.jpg\n├── dog.7570.jpg\n├── dog.7571.jpg\n├── dog.7572.jpg\n├── dog.7573.jpg\n├── dog.7574.jpg\n├── dog.7575.jpg\n├── dog.7576.jpg\n├── dog.7577.jpg\n├── dog.7578.jpg\n├── dog.7579.jpg\n├── dog.757.jpg\n├── dog.7580.jpg\n├── dog.7581.jpg\n├── dog.7582.jpg\n├── dog.7583.jpg\n├── dog.7584.jpg\n├── dog.7585.jpg\n├── dog.7586.jpg\n├── dog.7587.jpg\n├── dog.7588.jpg\n├── dog.7589.jpg\n├── dog.758.jpg\n├── dog.7590.jpg\n├── dog.7591.jpg\n├── dog.7592.jpg\n├── dog.7593.jpg\n├── dog.7594.jpg\n├── dog.7595.jpg\n├── dog.7596.jpg\n├── dog.7597.jpg\n├── dog.7598.jpg\n├── dog.7599.jpg\n├── dog.759.jpg\n├── dog.75.jpg\n├── dog.7600.jpg\n├── dog.7601.jpg\n├── dog.7602.jpg\n├── dog.7603.jpg\n├── dog.7604.jpg\n├── dog.7605.jpg\n├── dog.7606.jpg\n├── dog.7607.jpg\n├── dog.7608.jpg\n├── dog.7609.jpg\n├── dog.760.jpg\n├── dog.7610.jpg\n├── dog.7611.jpg\n├── dog.7612.jpg\n├── dog.7613.jpg\n├── dog.7614.jpg\n├── dog.7615.jpg\n├── dog.7616.jpg\n├── dog.7617.jpg\n├── dog.7618.jpg\n├── dog.7619.jpg\n├── dog.761.jpg\n├── dog.7620.jpg\n├── dog.7621.jpg\n├── dog.7622.jpg\n├── dog.7623.jpg\n├── dog.7624.jpg\n├── dog.7625.jpg\n├── dog.7626.jpg\n├── dog.7627.jpg\n├── dog.7628.jpg\n├── dog.7629.jpg\n├── dog.762.jpg\n├── dog.7630.jpg\n├── dog.7631.jpg\n├── dog.7632.jpg\n├── dog.7633.jpg\n├── dog.7634.jpg\n├── dog.7635.jpg\n├── dog.7636.jpg\n├── dog.7637.jpg\n├── dog.7638.jpg\n├── dog.7639.jpg\n├── dog.763.jpg\n├── dog.7640.jpg\n├── dog.7641.jpg\n├── dog.7642.jpg\n├── dog.7643.jpg\n├── dog.7644.jpg\n├── dog.7645.jpg\n├── dog.7646.jpg\n├── dog.7647.jpg\n├── dog.7648.jpg\n├── dog.7649.jpg\n├── dog.764.jpg\n├── dog.7650.jpg\n├── dog.7651.jpg\n├── dog.7652.jpg\n├── dog.7653.jpg\n├── dog.7654.jpg\n├── dog.7655.jpg\n├── dog.7656.jpg\n├── dog.7657.jpg\n├── dog.7658.jpg\n├── dog.7659.jpg\n├── dog.765.jpg\n├── dog.7660.jpg\n├── dog.7661.jpg\n├── dog.7662.jpg\n├── dog.7663.jpg\n├── dog.7664.jpg\n├── dog.7665.jpg\n├── dog.7666.jpg\n├── dog.7667.jpg\n├── dog.7668.jpg\n├── dog.7669.jpg\n├── dog.766.jpg\n├── dog.7670.jpg\n├── dog.7671.jpg\n├── dog.7672.jpg\n├── dog.7673.jpg\n├── dog.7674.jpg\n├── dog.7675.jpg\n├── dog.7676.jpg\n├── dog.7677.jpg\n├── dog.7678.jpg\n├── dog.7679.jpg\n├── dog.767.jpg\n├── dog.7680.jpg\n├── dog.7681.jpg\n├── dog.7682.jpg\n├── dog.7683.jpg\n├── dog.7684.jpg\n├── dog.7685.jpg\n├── dog.7686.jpg\n├── dog.7687.jpg\n├── dog.7688.jpg\n├── dog.7689.jpg\n├── dog.768.jpg\n├── dog.7690.jpg\n├── dog.7691.jpg\n├── dog.7692.jpg\n├── dog.7693.jpg\n├── dog.7694.jpg\n├── dog.7695.jpg\n├── dog.7696.jpg\n├── dog.7697.jpg\n├── dog.7698.jpg\n├── dog.7699.jpg\n├── dog.769.jpg\n├── dog.76.jpg\n├── dog.7700.jpg\n├── dog.7701.jpg\n├── dog.7702.jpg\n├── dog.7703.jpg\n├── dog.7704.jpg\n├── dog.7705.jpg\n├── dog.7706.jpg\n├── dog.7707.jpg\n├── dog.7708.jpg\n├── dog.7709.jpg\n├── dog.770.jpg\n├── dog.7710.jpg\n├── dog.7711.jpg\n├── dog.7712.jpg\n├── dog.7713.jpg\n├── dog.7714.jpg\n├── dog.7715.jpg\n├── dog.7716.jpg\n├── dog.7717.jpg\n├── dog.7718.jpg\n├── dog.7719.jpg\n├── dog.771.jpg\n├── dog.7720.jpg\n├── dog.7721.jpg\n├── dog.7722.jpg\n├── dog.7723.jpg\n├── dog.7724.jpg\n├── dog.7725.jpg\n├── dog.7726.jpg\n├── dog.7727.jpg\n├── dog.7728.jpg\n├── dog.7729.jpg\n├── dog.772.jpg\n├── dog.7730.jpg\n├── dog.7731.jpg\n├── dog.7732.jpg\n├── dog.7733.jpg\n├── dog.7734.jpg\n├── dog.7735.jpg\n├── dog.7736.jpg\n├── dog.7737.jpg\n├── dog.7738.jpg\n├── dog.7739.jpg\n├── dog.773.jpg\n├── dog.7740.jpg\n├── dog.7741.jpg\n├── dog.7742.jpg\n├── dog.7743.jpg\n├── dog.7744.jpg\n├── dog.7745.jpg\n├── dog.7746.jpg\n├── dog.7747.jpg\n├── dog.7748.jpg\n├── dog.7749.jpg\n├── dog.774.jpg\n├── dog.7750.jpg\n├── dog.7751.jpg\n├── dog.7752.jpg\n├── dog.7753.jpg\n├── dog.7754.jpg\n├── dog.7755.jpg\n├── dog.7756.jpg\n├── dog.7757.jpg\n├── dog.7758.jpg\n├── dog.7759.jpg\n├── dog.775.jpg\n├── dog.7760.jpg\n├── dog.7761.jpg\n├── dog.7762.jpg\n├── dog.7763.jpg\n├── dog.7764.jpg\n├── dog.7765.jpg\n├── dog.7766.jpg\n├── dog.7767.jpg\n├── dog.7768.jpg\n├── dog.7769.jpg\n├── dog.776.jpg\n├── dog.7770.jpg\n├── dog.7771.jpg\n├── dog.7772.jpg\n├── dog.7773.jpg\n├── dog.7774.jpg\n├── dog.7775.jpg\n├── dog.7776.jpg\n├── dog.7777.jpg\n├── dog.7778.jpg\n├── dog.7779.jpg\n├── dog.777.jpg\n├── dog.7780.jpg\n├── dog.7781.jpg\n├── dog.7782.jpg\n├── dog.7783.jpg\n├── dog.7784.jpg\n├── dog.7785.jpg\n├── dog.7786.jpg\n├── dog.7787.jpg\n├── dog.7788.jpg\n├── dog.7789.jpg\n├── dog.778.jpg\n├── dog.7790.jpg\n├── dog.7791.jpg\n├── dog.7792.jpg\n├── dog.7793.jpg\n├── dog.7794.jpg\n├── dog.7795.jpg\n├── dog.7796.jpg\n├── dog.7797.jpg\n├── dog.7798.jpg\n├── dog.7799.jpg\n├── dog.779.jpg\n├── dog.77.jpg\n├── dog.7800.jpg\n├── dog.7801.jpg\n├── dog.7802.jpg\n├── dog.7803.jpg\n├── dog.7804.jpg\n├── dog.7805.jpg\n├── dog.7806.jpg\n├── dog.7807.jpg\n├── dog.7808.jpg\n├── dog.7809.jpg\n├── dog.780.jpg\n├── dog.7810.jpg\n├── dog.7811.jpg\n├── dog.7812.jpg\n├── dog.7813.jpg\n├── dog.7814.jpg\n├── dog.7815.jpg\n├── dog.7816.jpg\n├── dog.7817.jpg\n├── dog.7818.jpg\n├── dog.7819.jpg\n├── dog.781.jpg\n├── dog.7820.jpg\n├── dog.7821.jpg\n├── dog.7822.jpg\n├── dog.7823.jpg\n├── dog.7824.jpg\n├── dog.7825.jpg\n├── dog.7826.jpg\n├── dog.7827.jpg\n├── dog.7828.jpg\n├── dog.7829.jpg\n├── dog.782.jpg\n├── dog.7830.jpg\n├── dog.7831.jpg\n├── dog.7832.jpg\n├── dog.7833.jpg\n├── dog.7834.jpg\n├── dog.7835.jpg\n├── dog.7836.jpg\n├── dog.7837.jpg\n├── dog.7838.jpg\n├── dog.7839.jpg\n├── dog.783.jpg\n├── dog.7840.jpg\n├── dog.7841.jpg\n├── dog.7842.jpg\n├── dog.7843.jpg\n├── dog.7844.jpg\n├── dog.7845.jpg\n├── dog.7846.jpg\n├── dog.7847.jpg\n├── dog.7848.jpg\n├── dog.7849.jpg\n├── dog.784.jpg\n├── dog.7850.jpg\n├── dog.7851.jpg\n├── dog.7852.jpg\n├── dog.7853.jpg\n├── dog.7854.jpg\n├── dog.7855.jpg\n├── dog.7856.jpg\n├── dog.7857.jpg\n├── dog.7858.jpg\n├── dog.7859.jpg\n├── dog.785.jpg\n├── dog.7860.jpg\n├── dog.7861.jpg\n├── dog.7862.jpg\n├── dog.7863.jpg\n├── dog.7864.jpg\n├── dog.7865.jpg\n├── dog.7866.jpg\n├── dog.7867.jpg\n├── dog.7868.jpg\n├── dog.7869.jpg\n├── dog.786.jpg\n├── dog.7870.jpg\n├── dog.7871.jpg\n├── dog.7872.jpg\n├── dog.7873.jpg\n├── dog.7874.jpg\n├── dog.7875.jpg\n├── dog.7876.jpg\n├── dog.7877.jpg\n├── dog.7878.jpg\n├── dog.7879.jpg\n├── dog.787.jpg\n├── dog.7880.jpg\n├── dog.7881.jpg\n├── dog.7882.jpg\n├── dog.7883.jpg\n├── dog.7884.jpg\n├── dog.7885.jpg\n├── dog.7886.jpg\n├── dog.7887.jpg\n├── dog.7888.jpg\n├── dog.7889.jpg\n├── dog.788.jpg\n├── dog.7890.jpg\n├── dog.7891.jpg\n├── dog.7892.jpg\n├── dog.7893.jpg\n├── dog.7894.jpg\n├── dog.7895.jpg\n├── dog.7896.jpg\n├── dog.7897.jpg\n├── dog.7898.jpg\n├── dog.7899.jpg\n├── dog.789.jpg\n├── dog.78.jpg\n├── dog.7900.jpg\n├── dog.7901.jpg\n├── dog.7902.jpg\n├── dog.7903.jpg\n├── dog.7904.jpg\n├── dog.7905.jpg\n├── dog.7906.jpg\n├── dog.7907.jpg\n├── dog.7908.jpg\n├── dog.7909.jpg\n├── dog.790.jpg\n├── dog.7910.jpg\n├── dog.7911.jpg\n├── dog.7912.jpg\n├── dog.7913.jpg\n├── dog.7914.jpg\n├── dog.7915.jpg\n├── dog.7916.jpg\n├── dog.7917.jpg\n├── dog.7918.jpg\n├── dog.7919.jpg\n├── dog.791.jpg\n├── dog.7920.jpg\n├── dog.7921.jpg\n├── dog.7922.jpg\n├── dog.7923.jpg\n├── dog.7924.jpg\n├── dog.7925.jpg\n├── dog.7926.jpg\n├── dog.7927.jpg\n├── dog.7928.jpg\n├── dog.7929.jpg\n├── dog.792.jpg\n├── dog.7930.jpg\n├── dog.7931.jpg\n├── dog.7932.jpg\n├── dog.7933.jpg\n├── dog.7934.jpg\n├── dog.7935.jpg\n├── dog.7936.jpg\n├── dog.7937.jpg\n├── dog.7938.jpg\n├── dog.7939.jpg\n├── dog.793.jpg\n├── dog.7940.jpg\n├── dog.7941.jpg\n├── dog.7942.jpg\n├── dog.7943.jpg\n├── dog.7944.jpg\n├── dog.7945.jpg\n├── dog.7946.jpg\n├── dog.7947.jpg\n├── dog.7948.jpg\n├── dog.7949.jpg\n├── dog.794.jpg\n├── dog.7950.jpg\n├── dog.7951.jpg\n├── dog.7952.jpg\n├── dog.7953.jpg\n├── dog.7954.jpg\n├── dog.7955.jpg\n├── dog.7956.jpg\n├── dog.7957.jpg\n├── dog.7958.jpg\n├── dog.7959.jpg\n├── dog.795.jpg\n├── dog.7960.jpg\n├── dog.7961.jpg\n├── dog.7962.jpg\n├── dog.7963.jpg\n├── dog.7964.jpg\n├── dog.7965.jpg\n├── dog.7966.jpg\n├── dog.7967.jpg\n├── dog.7968.jpg\n├── dog.7969.jpg\n├── dog.796.jpg\n├── dog.7970.jpg\n├── dog.7971.jpg\n├── dog.7972.jpg\n├── dog.7973.jpg\n├── dog.7974.jpg\n├── dog.7975.jpg\n├── dog.7976.jpg\n├── dog.7977.jpg\n├── dog.7978.jpg\n├── dog.7979.jpg\n├── dog.797.jpg\n├── dog.7980.jpg\n├── dog.7981.jpg\n├── dog.7982.jpg\n├── dog.7983.jpg\n├── dog.7984.jpg\n├── dog.7985.jpg\n├── dog.7986.jpg\n├── dog.7987.jpg\n├── dog.7988.jpg\n├── dog.7989.jpg\n├── dog.798.jpg\n├── dog.7990.jpg\n├── dog.7991.jpg\n├── dog.7992.jpg\n├── dog.7993.jpg\n├── dog.7994.jpg\n├── dog.7995.jpg\n├── dog.7996.jpg\n├── dog.7997.jpg\n├── dog.7998.jpg\n├── dog.7999.jpg\n├── dog.799.jpg\n├── dog.79.jpg\n├── dog.7.jpg\n├── dog.8000.jpg\n├── dog.8001.jpg\n├── dog.8002.jpg\n├── dog.8003.jpg\n├── dog.8004.jpg\n├── dog.8005.jpg\n├── dog.8006.jpg\n├── dog.8007.jpg\n├── dog.8008.jpg\n├── dog.8009.jpg\n├── dog.800.jpg\n├── dog.8010.jpg\n├── dog.8011.jpg\n├── dog.8012.jpg\n├── dog.8013.jpg\n├── dog.8014.jpg\n├── dog.8015.jpg\n├── dog.8016.jpg\n├── dog.8017.jpg\n├── dog.8018.jpg\n├── dog.8019.jpg\n├── dog.801.jpg\n├── dog.8020.jpg\n├── dog.8021.jpg\n├── dog.8022.jpg\n├── dog.8023.jpg\n├── dog.8024.jpg\n├── dog.8025.jpg\n├── dog.8026.jpg\n├── dog.8027.jpg\n├── dog.8028.jpg\n├── dog.8029.jpg\n├── dog.802.jpg\n├── dog.8030.jpg\n├── dog.8031.jpg\n├── dog.8032.jpg\n├── dog.8033.jpg\n├── dog.8034.jpg\n├── dog.8035.jpg\n├── dog.8036.jpg\n├── dog.8037.jpg\n├── dog.8038.jpg\n├── dog.8039.jpg\n├── dog.803.jpg\n├── dog.8040.jpg\n├── dog.8041.jpg\n├── dog.8042.jpg\n├── dog.8043.jpg\n├── dog.8044.jpg\n├── dog.8045.jpg\n├── dog.8046.jpg\n├── dog.8047.jpg\n├── dog.8048.jpg\n├── dog.8049.jpg\n├── dog.804.jpg\n├── dog.8050.jpg\n├── dog.8051.jpg\n├── dog.8052.jpg\n├── dog.8053.jpg\n├── dog.8054.jpg\n├── dog.8055.jpg\n├── dog.8056.jpg\n├── dog.8057.jpg\n├── dog.8058.jpg\n├── dog.8059.jpg\n├── dog.805.jpg\n├── dog.8060.jpg\n├── dog.8061.jpg\n├── dog.8062.jpg\n├── dog.8063.jpg\n├── dog.8064.jpg\n├── dog.8065.jpg\n├── dog.8066.jpg\n├── dog.8067.jpg\n├── dog.8068.jpg\n├── dog.8069.jpg\n├── dog.806.jpg\n├── dog.8070.jpg\n├── dog.8071.jpg\n├── dog.8072.jpg\n├── dog.8073.jpg\n├── dog.8074.jpg\n├── dog.8075.jpg\n├── dog.8076.jpg\n├── dog.8077.jpg\n├── dog.8078.jpg\n├── dog.8079.jpg\n├── dog.807.jpg\n├── dog.8080.jpg\n├── dog.8081.jpg\n├── dog.8082.jpg\n├── dog.8083.jpg\n├── dog.8084.jpg\n├── dog.8085.jpg\n├── dog.8086.jpg\n├── dog.8087.jpg\n├── dog.8088.jpg\n├── dog.8089.jpg\n├── dog.808.jpg\n├── dog.8090.jpg\n├── dog.8091.jpg\n├── dog.8092.jpg\n├── dog.8093.jpg\n├── dog.8094.jpg\n├── dog.8095.jpg\n├── dog.8096.jpg\n├── dog.8097.jpg\n├── dog.8098.jpg\n├── dog.8099.jpg\n├── dog.809.jpg\n├── dog.80.jpg\n├── dog.8100.jpg\n├── dog.8101.jpg\n├── dog.8102.jpg\n├── dog.8103.jpg\n├── dog.8104.jpg\n├── dog.8105.jpg\n├── dog.8106.jpg\n├── dog.8107.jpg\n├── dog.8108.jpg\n├── dog.8109.jpg\n├── dog.810.jpg\n├── dog.8110.jpg\n├── dog.8111.jpg\n├── dog.8112.jpg\n├── dog.8113.jpg\n├── dog.8114.jpg\n├── dog.8115.jpg\n├── dog.8116.jpg\n├── dog.8117.jpg\n├── dog.8118.jpg\n├── dog.8119.jpg\n├── dog.811.jpg\n├── dog.8120.jpg\n├── dog.8121.jpg\n├── dog.8122.jpg\n├── dog.8123.jpg\n├── dog.8124.jpg\n├── dog.8125.jpg\n├── dog.8126.jpg\n├── dog.8127.jpg\n├── dog.8128.jpg\n├── dog.8129.jpg\n├── dog.812.jpg\n├── dog.8130.jpg\n├── dog.8131.jpg\n├── dog.8132.jpg\n├── dog.8133.jpg\n├── dog.8134.jpg\n├── dog.8135.jpg\n├── dog.8136.jpg\n├── dog.8137.jpg\n├── dog.8138.jpg\n├── dog.8139.jpg\n├── dog.813.jpg\n├── dog.8140.jpg\n├── dog.8141.jpg\n├── dog.8142.jpg\n├── dog.8143.jpg\n├── dog.8144.jpg\n├── dog.8145.jpg\n├── dog.8146.jpg\n├── dog.8147.jpg\n├── dog.8148.jpg\n├── dog.8149.jpg\n├── dog.814.jpg\n├── dog.8150.jpg\n├── dog.8151.jpg\n├── dog.8152.jpg\n├── dog.8153.jpg\n├── dog.8154.jpg\n├── dog.8155.jpg\n├── dog.8156.jpg\n├── dog.8157.jpg\n├── dog.8158.jpg\n├── dog.8159.jpg\n├── dog.815.jpg\n├── dog.8160.jpg\n├── dog.8161.jpg\n├── dog.8162.jpg\n├── dog.8163.jpg\n├── dog.8164.jpg\n├── dog.8165.jpg\n├── dog.8166.jpg\n├── dog.8167.jpg\n├── dog.8168.jpg\n├── dog.8169.jpg\n├── dog.816.jpg\n├── dog.8170.jpg\n├── dog.8171.jpg\n├── dog.8172.jpg\n├── dog.8173.jpg\n├── dog.8174.jpg\n├── dog.8175.jpg\n├── dog.8176.jpg\n├── dog.8177.jpg\n├── dog.8178.jpg\n├── dog.8179.jpg\n├── dog.817.jpg\n├── dog.8180.jpg\n├── dog.8181.jpg\n├── dog.8182.jpg\n├── dog.8183.jpg\n├── dog.8184.jpg\n├── dog.8185.jpg\n├── dog.8186.jpg\n├── dog.8187.jpg\n├── dog.8188.jpg\n├── dog.8189.jpg\n├── dog.818.jpg\n├── dog.8190.jpg\n├── dog.8191.jpg\n├── dog.8192.jpg\n├── dog.8193.jpg\n├── dog.8194.jpg\n├── dog.8195.jpg\n├── dog.8196.jpg\n├── dog.8197.jpg\n├── dog.8198.jpg\n├── dog.8199.jpg\n├── dog.819.jpg\n├── dog.81.jpg\n├── dog.8200.jpg\n├── dog.8201.jpg\n├── dog.8202.jpg\n├── dog.8203.jpg\n├── dog.8204.jpg\n├── dog.8205.jpg\n├── dog.8206.jpg\n├── dog.8207.jpg\n├── dog.8208.jpg\n├── dog.8209.jpg\n├── dog.820.jpg\n├── dog.8210.jpg\n├── dog.8211.jpg\n├── dog.8212.jpg\n├── dog.8213.jpg\n├── dog.8214.jpg\n├── dog.8215.jpg\n├── dog.8216.jpg\n├── dog.8217.jpg\n├── dog.8218.jpg\n├── dog.8219.jpg\n├── dog.821.jpg\n├── dog.8220.jpg\n├── dog.8221.jpg\n├── dog.8222.jpg\n├── dog.8223.jpg\n├── dog.8224.jpg\n├── dog.8225.jpg\n├── dog.8226.jpg\n├── dog.8227.jpg\n├── dog.8228.jpg\n├── dog.8229.jpg\n├── dog.822.jpg\n├── dog.8230.jpg\n├── dog.8231.jpg\n├── dog.8232.jpg\n├── dog.8233.jpg\n├── dog.8234.jpg\n├── dog.8235.jpg\n├── dog.8236.jpg\n├── dog.8237.jpg\n├── dog.8238.jpg\n├── dog.8239.jpg\n├── dog.823.jpg\n├── dog.8240.jpg\n├── dog.8241.jpg\n├── dog.8242.jpg\n├── dog.8243.jpg\n├── dog.8244.jpg\n├── dog.8245.jpg\n├── dog.8246.jpg\n├── dog.8247.jpg\n├── dog.8248.jpg\n├── dog.8249.jpg\n├── dog.824.jpg\n├── dog.8250.jpg\n├── dog.8251.jpg\n├── dog.8252.jpg\n├── dog.8253.jpg\n├── dog.8254.jpg\n├── dog.8255.jpg\n├── dog.8256.jpg\n├── dog.8257.jpg\n├── dog.8258.jpg\n├── dog.8259.jpg\n├── dog.825.jpg\n├── dog.8260.jpg\n├── dog.8261.jpg\n├── dog.8262.jpg\n├── dog.8263.jpg\n├── dog.8264.jpg\n├── dog.8265.jpg\n├── dog.8266.jpg\n├── dog.8267.jpg\n├── dog.8268.jpg\n├── dog.8269.jpg\n├── dog.826.jpg\n├── dog.8270.jpg\n├── dog.8271.jpg\n├── dog.8272.jpg\n├── dog.8273.jpg\n├── dog.8274.jpg\n├── dog.8275.jpg\n├── dog.8276.jpg\n├── dog.8277.jpg\n├── dog.8278.jpg\n├── dog.8279.jpg\n├── dog.827.jpg\n├── dog.8280.jpg\n├── dog.8281.jpg\n├── dog.8282.jpg\n├── dog.8283.jpg\n├── dog.8284.jpg\n├── dog.8285.jpg\n├── dog.8286.jpg\n├── dog.8287.jpg\n├── dog.8288.jpg\n├── dog.8289.jpg\n├── dog.828.jpg\n├── dog.8290.jpg\n├── dog.8291.jpg\n├── dog.8292.jpg\n├── dog.8293.jpg\n├── dog.8294.jpg\n├── dog.8295.jpg\n├── dog.8296.jpg\n├── dog.8297.jpg\n├── dog.8298.jpg\n├── dog.8299.jpg\n├── dog.829.jpg\n├── dog.82.jpg\n├── dog.8300.jpg\n├── dog.8301.jpg\n├── dog.8302.jpg\n├── dog.8303.jpg\n├── dog.8304.jpg\n├── dog.8305.jpg\n├── dog.8306.jpg\n├── dog.8307.jpg\n├── dog.8308.jpg\n├── dog.8309.jpg\n├── dog.830.jpg\n├── dog.8310.jpg\n├── dog.8311.jpg\n├── dog.8312.jpg\n├── dog.8313.jpg\n├── dog.8314.jpg\n├── dog.8315.jpg\n├── dog.8316.jpg\n├── dog.8317.jpg\n├── dog.8318.jpg\n├── dog.8319.jpg\n├── dog.831.jpg\n├── dog.8320.jpg\n├── dog.8321.jpg\n├── dog.8322.jpg\n├── dog.8323.jpg\n├── dog.8324.jpg\n├── dog.8325.jpg\n├── dog.8326.jpg\n├── dog.8327.jpg\n├── dog.8328.jpg\n├── dog.8329.jpg\n├── dog.832.jpg\n├── dog.8330.jpg\n├── dog.8331.jpg\n├── dog.8332.jpg\n├── dog.8333.jpg\n├── dog.8334.jpg\n├── dog.8335.jpg\n├── dog.8336.jpg\n├── dog.8337.jpg\n├── dog.8338.jpg\n├── dog.8339.jpg\n├── dog.833.jpg\n├── dog.8340.jpg\n├── dog.8341.jpg\n├── dog.8342.jpg\n├── dog.8343.jpg\n├── dog.8344.jpg\n├── dog.8345.jpg\n├── dog.8346.jpg\n├── dog.8347.jpg\n├── dog.8348.jpg\n├── dog.8349.jpg\n├── dog.834.jpg\n├── dog.8350.jpg\n├── dog.8351.jpg\n├── dog.8352.jpg\n├── dog.8353.jpg\n├── dog.8354.jpg\n├── dog.8355.jpg\n├── dog.8356.jpg\n├── dog.8357.jpg\n├── dog.8358.jpg\n├── dog.8359.jpg\n├── dog.835.jpg\n├── dog.8360.jpg\n├── dog.8361.jpg\n├── dog.8362.jpg\n├── dog.8363.jpg\n├── dog.8364.jpg\n├── dog.8365.jpg\n├── dog.8366.jpg\n├── dog.8367.jpg\n├── dog.8368.jpg\n├── dog.8369.jpg\n├── dog.836.jpg\n├── dog.8370.jpg\n├── dog.8371.jpg\n├── dog.8372.jpg\n├── dog.8373.jpg\n├── dog.8374.jpg\n├── dog.8375.jpg\n├── dog.8376.jpg\n├── dog.8377.jpg\n├── dog.8378.jpg\n├── dog.8379.jpg\n├── dog.837.jpg\n├── dog.8380.jpg\n├── dog.8381.jpg\n├── dog.8382.jpg\n├── dog.8383.jpg\n├── dog.8384.jpg\n├── dog.8385.jpg\n├── dog.8386.jpg\n├── dog.8387.jpg\n├── dog.8388.jpg\n├── dog.8389.jpg\n├── dog.838.jpg\n├── dog.8390.jpg\n├── dog.8391.jpg\n├── dog.8392.jpg\n├── dog.8393.jpg\n├── dog.8394.jpg\n├── dog.8395.jpg\n├── dog.8396.jpg\n├── dog.8397.jpg\n├── dog.8398.jpg\n├── dog.8399.jpg\n├── dog.839.jpg\n├── dog.83.jpg\n├── dog.8400.jpg\n├── dog.8401.jpg\n├── dog.8402.jpg\n├── dog.8403.jpg\n├── dog.8404.jpg\n├── dog.8405.jpg\n├── dog.8406.jpg\n├── dog.8407.jpg\n├── dog.8408.jpg\n├── dog.8409.jpg\n├── dog.840.jpg\n├── dog.8410.jpg\n├── dog.8411.jpg\n├── dog.8412.jpg\n├── dog.8413.jpg\n├── dog.8414.jpg\n├── dog.8415.jpg\n├── dog.8416.jpg\n├── dog.8417.jpg\n├── dog.8418.jpg\n├── dog.8419.jpg\n├── dog.841.jpg\n├── dog.8420.jpg\n├── dog.8421.jpg\n├── dog.8422.jpg\n├── dog.8423.jpg\n├── dog.8424.jpg\n├── dog.8425.jpg\n├── dog.8426.jpg\n├── dog.8427.jpg\n├── dog.8428.jpg\n├── dog.8429.jpg\n├── dog.842.jpg\n├── dog.8430.jpg\n├── dog.8431.jpg\n├── dog.8432.jpg\n├── dog.8433.jpg\n├── dog.8434.jpg\n├── dog.8435.jpg\n├── dog.8436.jpg\n├── dog.8437.jpg\n├── dog.8438.jpg\n├── dog.8439.jpg\n├── dog.843.jpg\n├── dog.8440.jpg\n├── dog.8441.jpg\n├── dog.8442.jpg\n├── dog.8443.jpg\n├── dog.8444.jpg\n├── dog.8445.jpg\n├── dog.8446.jpg\n├── dog.8447.jpg\n├── dog.8448.jpg\n├── dog.8449.jpg\n├── dog.844.jpg\n├── dog.8450.jpg\n├── dog.8451.jpg\n├── dog.8452.jpg\n├── dog.8453.jpg\n├── dog.8454.jpg\n├── dog.8455.jpg\n├── dog.8456.jpg\n├── dog.8457.jpg\n├── dog.8458.jpg\n├── dog.8459.jpg\n├── dog.845.jpg\n├── dog.8460.jpg\n├── dog.8461.jpg\n├── dog.8462.jpg\n├── dog.8463.jpg\n├── dog.8464.jpg\n├── dog.8465.jpg\n├── dog.8466.jpg\n├── dog.8467.jpg\n├── dog.8468.jpg\n├── dog.8469.jpg\n├── dog.846.jpg\n├── dog.8470.jpg\n├── dog.8471.jpg\n├── dog.8472.jpg\n├── dog.8473.jpg\n├── dog.8474.jpg\n├── dog.8475.jpg\n├── dog.8476.jpg\n├── dog.8477.jpg\n├── dog.8478.jpg\n├── dog.8479.jpg\n├── dog.847.jpg\n├── dog.8480.jpg\n├── dog.8481.jpg\n├── dog.8482.jpg\n├── dog.8483.jpg\n├── dog.8484.jpg\n├── dog.8485.jpg\n├── dog.8486.jpg\n├── dog.8487.jpg\n├── dog.8488.jpg\n├── dog.8489.jpg\n├── dog.848.jpg\n├── dog.8490.jpg\n├── dog.8491.jpg\n├── dog.8492.jpg\n├── dog.8493.jpg\n├── dog.8494.jpg\n├── dog.8495.jpg\n├── dog.8496.jpg\n├── dog.8497.jpg\n├── dog.8498.jpg\n├── dog.8499.jpg\n├── dog.849.jpg\n├── dog.84.jpg\n├── dog.8500.jpg\n├── dog.8501.jpg\n├── dog.8502.jpg\n├── dog.8503.jpg\n├── dog.8504.jpg\n├── dog.8505.jpg\n├── dog.8506.jpg\n├── dog.8507.jpg\n├── dog.8508.jpg\n├── dog.8509.jpg\n├── dog.850.jpg\n├── dog.8510.jpg\n├── dog.8511.jpg\n├── dog.8512.jpg\n├── dog.8513.jpg\n├── dog.8514.jpg\n├── dog.8515.jpg\n├── dog.8516.jpg\n├── dog.8517.jpg\n├── dog.8518.jpg\n├── dog.8519.jpg\n├── dog.851.jpg\n├── dog.8520.jpg\n├── dog.8521.jpg\n├── dog.8522.jpg\n├── dog.8523.jpg\n├── dog.8524.jpg\n├── dog.8525.jpg\n├── dog.8526.jpg\n├── dog.8527.jpg\n├── dog.8528.jpg\n├── dog.8529.jpg\n├── dog.852.jpg\n├── dog.8530.jpg\n├── dog.8531.jpg\n├── dog.8532.jpg\n├── dog.8533.jpg\n├── dog.8534.jpg\n├── dog.8535.jpg\n├── dog.8536.jpg\n├── dog.8537.jpg\n├── dog.8538.jpg\n├── dog.8539.jpg\n├── dog.853.jpg\n├── dog.8540.jpg\n├── dog.8541.jpg\n├── dog.8542.jpg\n├── dog.8543.jpg\n├── dog.8544.jpg\n├── dog.8545.jpg\n├── dog.8546.jpg\n├── dog.8547.jpg\n├── dog.8548.jpg\n├── dog.8549.jpg\n├── dog.854.jpg\n├── dog.8550.jpg\n├── dog.8551.jpg\n├── dog.8552.jpg\n├── dog.8553.jpg\n├── dog.8554.jpg\n├── dog.8555.jpg\n├── dog.8556.jpg\n├── dog.8557.jpg\n├── dog.8558.jpg\n├── dog.8559.jpg\n├── dog.855.jpg\n├── dog.8560.jpg\n├── dog.8561.jpg\n├── dog.8562.jpg\n├── dog.8563.jpg\n├── dog.8564.jpg\n├── dog.8565.jpg\n├── dog.8566.jpg\n├── dog.8567.jpg\n├── dog.8568.jpg\n├── dog.8569.jpg\n├── dog.856.jpg\n├── dog.8570.jpg\n├── dog.8571.jpg\n├── dog.8572.jpg\n├── dog.8573.jpg\n├── dog.8574.jpg\n├── dog.8575.jpg\n├── dog.8576.jpg\n├── dog.8577.jpg\n├── dog.8578.jpg\n├── dog.8579.jpg\n├── dog.857.jpg\n├── dog.8580.jpg\n├── dog.8581.jpg\n├── dog.8582.jpg\n├── dog.8583.jpg\n├── dog.8584.jpg\n├── dog.8585.jpg\n├── dog.8586.jpg\n├── dog.8587.jpg\n├── dog.8588.jpg\n├── dog.8589.jpg\n├── dog.858.jpg\n├── dog.8590.jpg\n├── dog.8591.jpg\n├── dog.8592.jpg\n├── dog.8593.jpg\n├── dog.8594.jpg\n├── dog.8595.jpg\n├── dog.8596.jpg\n├── dog.8597.jpg\n├── dog.8598.jpg\n├── dog.8599.jpg\n├── dog.859.jpg\n├── dog.85.jpg\n├── dog.8600.jpg\n├── dog.8601.jpg\n├── dog.8602.jpg\n├── dog.8603.jpg\n├── dog.8604.jpg\n├── dog.8605.jpg\n├── dog.8606.jpg\n├── dog.8607.jpg\n├── dog.8608.jpg\n├── dog.8609.jpg\n├── dog.860.jpg\n├── dog.8610.jpg\n├── dog.8611.jpg\n├── dog.8612.jpg\n├── dog.8613.jpg\n├── dog.8614.jpg\n├── dog.8615.jpg\n├── dog.8616.jpg\n├── dog.8617.jpg\n├── dog.8618.jpg\n├── dog.8619.jpg\n├── dog.861.jpg\n├── dog.8620.jpg\n├── dog.8621.jpg\n├── dog.8622.jpg\n├── dog.8623.jpg\n├── dog.8624.jpg\n├── dog.8625.jpg\n├── dog.8626.jpg\n├── dog.8627.jpg\n├── dog.8628.jpg\n├── dog.8629.jpg\n├── dog.862.jpg\n├── dog.8630.jpg\n├── dog.8631.jpg\n├── dog.8632.jpg\n├── dog.8633.jpg\n├── dog.8634.jpg\n├── dog.8635.jpg\n├── dog.8636.jpg\n├── dog.8637.jpg\n├── dog.8638.jpg\n├── dog.8639.jpg\n├── dog.863.jpg\n├── dog.8640.jpg\n├── dog.8641.jpg\n├── dog.8642.jpg\n├── dog.8643.jpg\n├── dog.8644.jpg\n├── dog.8645.jpg\n├── dog.8646.jpg\n├── dog.8647.jpg\n├── dog.8648.jpg\n├── dog.8649.jpg\n├── dog.864.jpg\n├── dog.8650.jpg\n├── dog.8651.jpg\n├── dog.8652.jpg\n├── dog.8653.jpg\n├── dog.8654.jpg\n├── dog.8655.jpg\n├── dog.8656.jpg\n├── dog.8657.jpg\n├── dog.8658.jpg\n├── dog.8659.jpg\n├── dog.865.jpg\n├── dog.8660.jpg\n├── dog.8661.jpg\n├── dog.8662.jpg\n├── dog.8663.jpg\n├── dog.8664.jpg\n├── dog.8665.jpg\n├── dog.8666.jpg\n├── dog.8667.jpg\n├── dog.8668.jpg\n├── dog.8669.jpg\n├── dog.866.jpg\n├── dog.8670.jpg\n├── dog.8671.jpg\n├── dog.8672.jpg\n├── dog.8673.jpg\n├── dog.8674.jpg\n├── dog.8675.jpg\n├── dog.8676.jpg\n├── dog.8677.jpg\n├── dog.8678.jpg\n├── dog.8679.jpg\n├── dog.867.jpg\n├── dog.8680.jpg\n├── dog.8681.jpg\n├── dog.8682.jpg\n├── dog.8683.jpg\n├── dog.8684.jpg\n├── dog.8685.jpg\n├── dog.8686.jpg\n├── dog.8687.jpg\n├── dog.8688.jpg\n├── dog.8689.jpg\n├── dog.868.jpg\n├── dog.8690.jpg\n├── dog.8691.jpg\n├── dog.8692.jpg\n├── dog.8693.jpg\n├── dog.8694.jpg\n├── dog.8695.jpg\n├── dog.8696.jpg\n├── dog.8697.jpg\n├── dog.8698.jpg\n├── dog.8699.jpg\n├── dog.869.jpg\n├── dog.86.jpg\n├── dog.8700.jpg\n├── dog.8701.jpg\n├── dog.8702.jpg\n├── dog.8703.jpg\n├── dog.8704.jpg\n├── dog.8705.jpg\n├── dog.8706.jpg\n├── dog.8707.jpg\n├── dog.8708.jpg\n├── dog.8709.jpg\n├── dog.870.jpg\n├── dog.8710.jpg\n├── dog.8711.jpg\n├── dog.8712.jpg\n├── dog.8713.jpg\n├── dog.8714.jpg\n├── dog.8715.jpg\n├── dog.8716.jpg\n├── dog.8717.jpg\n├── dog.8718.jpg\n├── dog.8719.jpg\n├── dog.871.jpg\n├── dog.8720.jpg\n├── dog.8721.jpg\n├── dog.8722.jpg\n├── dog.8723.jpg\n├── dog.8724.jpg\n├── dog.8725.jpg\n├── dog.8726.jpg\n├── dog.8727.jpg\n├── dog.8728.jpg\n├── dog.8729.jpg\n├── dog.872.jpg\n├── dog.8730.jpg\n├── dog.8731.jpg\n├── dog.8732.jpg\n├── dog.8733.jpg\n├── dog.8734.jpg\n├── dog.8735.jpg\n├── dog.8736.jpg\n├── dog.8737.jpg\n├── dog.8738.jpg\n├── dog.8739.jpg\n├── dog.873.jpg\n├── dog.8740.jpg\n├── dog.8741.jpg\n├── dog.8742.jpg\n├── dog.8743.jpg\n├── dog.8744.jpg\n├── dog.8745.jpg\n├── dog.8746.jpg\n├── dog.8747.jpg\n├── dog.8748.jpg\n├── dog.8749.jpg\n├── dog.874.jpg\n├── dog.8750.jpg\n├── dog.8751.jpg\n├── dog.8752.jpg\n├── dog.8753.jpg\n├── dog.8754.jpg\n├── dog.8755.jpg\n├── dog.8756.jpg\n├── dog.8757.jpg\n├── dog.8758.jpg\n├── dog.8759.jpg\n├── dog.875.jpg\n├── dog.8760.jpg\n├── dog.8761.jpg\n├── dog.8762.jpg\n├── dog.8763.jpg\n├── dog.8764.jpg\n├── dog.8765.jpg\n├── dog.8766.jpg\n├── dog.8767.jpg\n├── dog.8768.jpg\n├── dog.8769.jpg\n├── dog.876.jpg\n├── dog.8770.jpg\n├── dog.8771.jpg\n├── dog.8772.jpg\n├── dog.8773.jpg\n├── dog.8774.jpg\n├── dog.8775.jpg\n├── dog.8776.jpg\n├── dog.8777.jpg\n├── dog.8778.jpg\n├── dog.8779.jpg\n├── dog.877.jpg\n├── dog.8780.jpg\n├── dog.8781.jpg\n├── dog.8782.jpg\n├── dog.8783.jpg\n├── dog.8784.jpg\n├── dog.8785.jpg\n├── dog.8786.jpg\n├── dog.8787.jpg\n├── dog.8788.jpg\n├── dog.8789.jpg\n├── dog.878.jpg\n├── dog.8790.jpg\n├── dog.8791.jpg\n├── dog.8792.jpg\n├── dog.8793.jpg\n├── dog.8794.jpg\n├── dog.8795.jpg\n├── dog.8796.jpg\n├── dog.8797.jpg\n├── dog.8798.jpg\n├── dog.8799.jpg\n├── dog.879.jpg\n├── dog.87.jpg\n├── dog.8800.jpg\n├── dog.8801.jpg\n├── dog.8802.jpg\n├── dog.8803.jpg\n├── dog.8804.jpg\n├── dog.8805.jpg\n├── dog.8806.jpg\n├── dog.8807.jpg\n├── dog.8808.jpg\n├── dog.8809.jpg\n├── dog.880.jpg\n├── dog.8810.jpg\n├── dog.8811.jpg\n├── dog.8812.jpg\n├── dog.8813.jpg\n├── dog.8814.jpg\n├── dog.8815.jpg\n├── dog.8816.jpg\n├── dog.8817.jpg\n├── dog.8818.jpg\n├── dog.8819.jpg\n├── dog.881.jpg\n├── dog.8820.jpg\n├── dog.8821.jpg\n├── dog.8822.jpg\n├── dog.8823.jpg\n├── dog.8824.jpg\n├── dog.8825.jpg\n├── dog.8826.jpg\n├── dog.8827.jpg\n├── dog.8828.jpg\n├── dog.8829.jpg\n├── dog.882.jpg\n├── dog.8830.jpg\n├── dog.8831.jpg\n├── dog.8832.jpg\n├── dog.8833.jpg\n├── dog.8834.jpg\n├── dog.8835.jpg\n├── dog.8836.jpg\n├── dog.8837.jpg\n├── dog.8838.jpg\n├── dog.8839.jpg\n├── dog.883.jpg\n├── dog.8840.jpg\n├── dog.8841.jpg\n├── dog.8842.jpg\n├── dog.8843.jpg\n├── dog.8844.jpg\n├── dog.8845.jpg\n├── dog.8846.jpg\n├── dog.8847.jpg\n├── dog.8848.jpg\n├── dog.8849.jpg\n├── dog.884.jpg\n├── dog.8850.jpg\n├── dog.8851.jpg\n├── dog.8852.jpg\n├── dog.8853.jpg\n├── dog.8854.jpg\n├── dog.8855.jpg\n├── dog.8856.jpg\n├── dog.8857.jpg\n├── dog.8858.jpg\n├── dog.8859.jpg\n├── dog.885.jpg\n├── dog.8860.jpg\n├── dog.8861.jpg\n├── dog.8862.jpg\n├── dog.8863.jpg\n├── dog.8864.jpg\n├── dog.8865.jpg\n├── dog.8866.jpg\n├── dog.8867.jpg\n├── dog.8868.jpg\n├── dog.8869.jpg\n├── dog.886.jpg\n├── dog.8870.jpg\n├── dog.8871.jpg\n├── dog.8872.jpg\n├── dog.8873.jpg\n├── dog.8874.jpg\n├── dog.8875.jpg\n├── dog.8876.jpg\n├── dog.8877.jpg\n├── dog.8878.jpg\n├── dog.8879.jpg\n├── dog.887.jpg\n├── dog.8880.jpg\n├── dog.8881.jpg\n├── dog.8882.jpg\n├── dog.8883.jpg\n├── dog.8884.jpg\n├── dog.8885.jpg\n├── dog.8886.jpg\n├── dog.8887.jpg\n├── dog.8888.jpg\n├── dog.8889.jpg\n├── dog.888.jpg\n├── dog.8890.jpg\n├── dog.8891.jpg\n├── dog.8892.jpg\n├── dog.8893.jpg\n├── dog.8894.jpg\n├── dog.8895.jpg\n├── dog.8896.jpg\n├── dog.8897.jpg\n├── dog.8898.jpg\n├── dog.8899.jpg\n├── dog.889.jpg\n├── dog.88.jpg\n├── dog.8900.jpg\n├── dog.8901.jpg\n├── dog.8902.jpg\n├── dog.8903.jpg\n├── dog.8904.jpg\n├── dog.8905.jpg\n├── dog.8906.jpg\n├── dog.8907.jpg\n├── dog.8908.jpg\n├── dog.8909.jpg\n├── dog.890.jpg\n├── dog.8910.jpg\n├── dog.8911.jpg\n├── dog.8912.jpg\n├── dog.8913.jpg\n├── dog.8914.jpg\n├── dog.8915.jpg\n├── dog.8916.jpg\n├── dog.8917.jpg\n├── dog.8918.jpg\n├── dog.8919.jpg\n├── dog.891.jpg\n├── dog.8920.jpg\n├── dog.8921.jpg\n├── dog.8922.jpg\n├── dog.8923.jpg\n├── dog.8924.jpg\n├── dog.8925.jpg\n├── dog.8926.jpg\n├── dog.8927.jpg\n├── dog.8928.jpg\n├── dog.8929.jpg\n├── dog.892.jpg\n├── dog.8930.jpg\n├── dog.8931.jpg\n├── dog.8932.jpg\n├── dog.8933.jpg\n├── dog.8934.jpg\n├── dog.8935.jpg\n├── dog.8936.jpg\n├── dog.8937.jpg\n├── dog.8938.jpg\n├── dog.8939.jpg\n├── dog.893.jpg\n├── dog.8940.jpg\n├── dog.8941.jpg\n├── dog.8942.jpg\n├── dog.8943.jpg\n├── dog.8944.jpg\n├── dog.8945.jpg\n├── dog.8946.jpg\n├── dog.8947.jpg\n├── dog.8948.jpg\n├── dog.8949.jpg\n├── dog.894.jpg\n├── dog.8950.jpg\n├── dog.8951.jpg\n├── dog.8952.jpg\n├── dog.8953.jpg\n├── dog.8954.jpg\n├── dog.8955.jpg\n├── dog.8956.jpg\n├── dog.8957.jpg\n├── dog.8958.jpg\n├── dog.8959.jpg\n├── dog.895.jpg\n├── dog.8960.jpg\n├── dog.8961.jpg\n├── dog.8962.jpg\n├── dog.8963.jpg\n├── dog.8964.jpg\n├── dog.8965.jpg\n├── dog.8966.jpg\n├── dog.8967.jpg\n├── dog.8968.jpg\n├── dog.8969.jpg\n├── dog.896.jpg\n├── dog.8970.jpg\n├── dog.8971.jpg\n├── dog.8972.jpg\n├── dog.8973.jpg\n├── dog.8974.jpg\n├── dog.8975.jpg\n├── dog.8976.jpg\n├── dog.8977.jpg\n├── dog.8978.jpg\n├── dog.8979.jpg\n├── dog.897.jpg\n├── dog.8980.jpg\n├── dog.8981.jpg\n├── dog.8982.jpg\n├── dog.8983.jpg\n├── dog.8984.jpg\n├── dog.8985.jpg\n├── dog.8986.jpg\n├── dog.8987.jpg\n├── dog.8988.jpg\n├── dog.8989.jpg\n├── dog.898.jpg\n├── dog.8990.jpg\n├── dog.8991.jpg\n├── dog.8992.jpg\n├── dog.8993.jpg\n├── dog.8994.jpg\n├── dog.8995.jpg\n├── dog.8996.jpg\n├── dog.8997.jpg\n├── dog.8998.jpg\n├── dog.8999.jpg\n├── dog.899.jpg\n├── dog.89.jpg\n├── dog.8.jpg\n├── dog.9000.jpg\n├── dog.9001.jpg\n├── dog.9002.jpg\n├── dog.9003.jpg\n├── dog.9004.jpg\n├── dog.9005.jpg\n├── dog.9006.jpg\n├── dog.9007.jpg\n├── dog.9008.jpg\n├── dog.9009.jpg\n├── dog.900.jpg\n├── dog.9010.jpg\n├── dog.9011.jpg\n├── dog.9012.jpg\n├── dog.9013.jpg\n├── dog.9014.jpg\n├── dog.9015.jpg\n├── dog.9016.jpg\n├── dog.9017.jpg\n├── dog.9018.jpg\n├── dog.9019.jpg\n├── dog.901.jpg\n├── dog.9020.jpg\n├── dog.9021.jpg\n├── dog.9022.jpg\n├── dog.9023.jpg\n├── dog.9024.jpg\n├── dog.9025.jpg\n├── dog.9026.jpg\n├── dog.9027.jpg\n├── dog.9028.jpg\n├── dog.9029.jpg\n├── dog.902.jpg\n├── dog.9030.jpg\n├── dog.9031.jpg\n├── dog.9032.jpg\n├── dog.9033.jpg\n├── dog.9034.jpg\n├── dog.9035.jpg\n├── dog.9036.jpg\n├── dog.9037.jpg\n├── dog.9038.jpg\n├── dog.9039.jpg\n├── dog.903.jpg\n├── dog.9040.jpg\n├── dog.9041.jpg\n├── dog.9042.jpg\n├── dog.9043.jpg\n├── dog.9044.jpg\n├── dog.9045.jpg\n├── dog.9046.jpg\n├── dog.9047.jpg\n├── dog.9048.jpg\n├── dog.9049.jpg\n├── dog.904.jpg\n├── dog.9050.jpg\n├── dog.9051.jpg\n├── dog.9052.jpg\n├── dog.9053.jpg\n├── dog.9054.jpg\n├── dog.9055.jpg\n├── dog.9056.jpg\n├── dog.9057.jpg\n├── dog.9058.jpg\n├── dog.9059.jpg\n├── dog.905.jpg\n├── dog.9060.jpg\n├── dog.9061.jpg\n├── dog.9062.jpg\n├── dog.9063.jpg\n├── dog.9064.jpg\n├── dog.9065.jpg\n├── dog.9066.jpg\n├── dog.9067.jpg\n├── dog.9068.jpg\n├── dog.9069.jpg\n├── dog.906.jpg\n├── dog.9070.jpg\n├── dog.9071.jpg\n├── dog.9072.jpg\n├── dog.9073.jpg\n├── dog.9074.jpg\n├── dog.9075.jpg\n├── dog.9076.jpg\n├── dog.9077.jpg\n├── dog.9078.jpg\n├── dog.9079.jpg\n├── dog.907.jpg\n├── dog.9080.jpg\n├── dog.9081.jpg\n├── dog.9082.jpg\n├── dog.9083.jpg\n├── dog.9084.jpg\n├── dog.9085.jpg\n├── dog.9086.jpg\n├── dog.9087.jpg\n├── dog.9088.jpg\n├── dog.9089.jpg\n├── dog.908.jpg\n├── dog.9090.jpg\n├── dog.9091.jpg\n├── dog.9092.jpg\n├── dog.9093.jpg\n├── dog.9094.jpg\n├── dog.9095.jpg\n├── dog.9096.jpg\n├── dog.9097.jpg\n├── dog.9098.jpg\n├── dog.9099.jpg\n├── dog.909.jpg\n├── dog.90.jpg\n├── dog.9100.jpg\n├── dog.9101.jpg\n├── dog.9102.jpg\n├── dog.9103.jpg\n├── dog.9104.jpg\n├── dog.9105.jpg\n├── dog.9106.jpg\n├── dog.9107.jpg\n├── dog.9108.jpg\n├── dog.9109.jpg\n├── dog.910.jpg\n├── dog.9110.jpg\n├── dog.9111.jpg\n├── dog.9112.jpg\n├── dog.9113.jpg\n├── dog.9114.jpg\n├── dog.9115.jpg\n├── dog.9116.jpg\n├── dog.9117.jpg\n├── dog.9118.jpg\n├── dog.9119.jpg\n├── dog.911.jpg\n├── dog.9120.jpg\n├── dog.9121.jpg\n├── dog.9122.jpg\n├── dog.9123.jpg\n├── dog.9124.jpg\n├── dog.9125.jpg\n├── dog.9126.jpg\n├── dog.9127.jpg\n├── dog.9128.jpg\n├── dog.9129.jpg\n├── dog.912.jpg\n├── dog.9130.jpg\n├── dog.9131.jpg\n├── dog.9132.jpg\n├── dog.9133.jpg\n├── dog.9134.jpg\n├── dog.9135.jpg\n├── dog.9136.jpg\n├── dog.9137.jpg\n├── dog.9138.jpg\n├── dog.9139.jpg\n├── dog.913.jpg\n├── dog.9140.jpg\n├── dog.9141.jpg\n├── dog.9142.jpg\n├── dog.9143.jpg\n├── dog.9144.jpg\n├── dog.9145.jpg\n├── dog.9146.jpg\n├── dog.9147.jpg\n├── dog.9148.jpg\n├── dog.9149.jpg\n├── dog.914.jpg\n├── dog.9150.jpg\n├── dog.9151.jpg\n├── dog.9152.jpg\n├── dog.9153.jpg\n├── dog.9154.jpg\n├── dog.9155.jpg\n├── dog.9156.jpg\n├── dog.9157.jpg\n├── dog.9158.jpg\n├── dog.9159.jpg\n├── dog.915.jpg\n├── dog.9160.jpg\n├── dog.9161.jpg\n├── dog.9162.jpg\n├── dog.9163.jpg\n├── dog.9164.jpg\n├── dog.9165.jpg\n├── dog.9166.jpg\n├── dog.9167.jpg\n├── dog.9168.jpg\n├── dog.9169.jpg\n├── dog.916.jpg\n├── dog.9170.jpg\n├── dog.9171.jpg\n├── dog.9172.jpg\n├── dog.9173.jpg\n├── dog.9174.jpg\n├── dog.9175.jpg\n├── dog.9176.jpg\n├── dog.9177.jpg\n├── dog.9178.jpg\n├── dog.9179.jpg\n├── dog.917.jpg\n├── dog.9180.jpg\n├── dog.9181.jpg\n├── dog.9182.jpg\n├── dog.9183.jpg\n├── dog.9184.jpg\n├── dog.9185.jpg\n├── dog.9186.jpg\n├── dog.9187.jpg\n├── dog.9188.jpg\n├── dog.9189.jpg\n├── dog.918.jpg\n├── dog.9190.jpg\n├── dog.9191.jpg\n├── dog.9192.jpg\n├── dog.9193.jpg\n├── dog.9194.jpg\n├── dog.9195.jpg\n├── dog.9196.jpg\n├── dog.9197.jpg\n├── dog.9198.jpg\n├── dog.9199.jpg\n├── dog.919.jpg\n├── dog.91.jpg\n├── dog.9200.jpg\n├── dog.9201.jpg\n├── dog.9202.jpg\n├── dog.9203.jpg\n├── dog.9204.jpg\n├── dog.9205.jpg\n├── dog.9206.jpg\n├── dog.9207.jpg\n├── dog.9208.jpg\n├── dog.9209.jpg\n├── dog.920.jpg\n├── dog.9210.jpg\n├── dog.9211.jpg\n├── dog.9212.jpg\n├── dog.9213.jpg\n├── dog.9214.jpg\n├── dog.9215.jpg\n├── dog.9216.jpg\n├── dog.9217.jpg\n├── dog.9218.jpg\n├── dog.9219.jpg\n├── dog.921.jpg\n├── dog.9220.jpg\n├── dog.9221.jpg\n├── dog.9222.jpg\n├── dog.9223.jpg\n├── dog.9224.jpg\n├── dog.9225.jpg\n├── dog.9226.jpg\n├── dog.9227.jpg\n├── dog.9228.jpg\n├── dog.9229.jpg\n├── dog.922.jpg\n├── dog.9230.jpg\n├── dog.9231.jpg\n├── dog.9232.jpg\n├── dog.9233.jpg\n├── dog.9234.jpg\n├── dog.9235.jpg\n├── dog.9236.jpg\n├── dog.9237.jpg\n├── dog.9238.jpg\n├── dog.9239.jpg\n├── dog.923.jpg\n├── dog.9240.jpg\n├── dog.9241.jpg\n├── dog.9242.jpg\n├── dog.9243.jpg\n├── dog.9244.jpg\n├── dog.9245.jpg\n├── dog.9246.jpg\n├── dog.9247.jpg\n├── dog.9248.jpg\n├── dog.9249.jpg\n├── dog.924.jpg\n├── dog.9250.jpg\n├── dog.9251.jpg\n├── dog.9252.jpg\n├── dog.9253.jpg\n├── dog.9254.jpg\n├── dog.9255.jpg\n├── dog.9256.jpg\n├── dog.9257.jpg\n├── dog.9258.jpg\n├── dog.9259.jpg\n├── dog.925.jpg\n├── dog.9260.jpg\n├── dog.9261.jpg\n├── dog.9262.jpg\n├── dog.9263.jpg\n├── dog.9264.jpg\n├── dog.9265.jpg\n├── dog.9266.jpg\n├── dog.9267.jpg\n├── dog.9268.jpg\n├── dog.9269.jpg\n├── dog.926.jpg\n├── dog.9270.jpg\n├── dog.9271.jpg\n├── dog.9272.jpg\n├── dog.9273.jpg\n├── dog.9274.jpg\n├── dog.9275.jpg\n├── dog.9276.jpg\n├── dog.9277.jpg\n├── dog.9278.jpg\n├── dog.9279.jpg\n├── dog.927.jpg\n├── dog.9280.jpg\n├── dog.9281.jpg\n├── dog.9282.jpg\n├── dog.9283.jpg\n├── dog.9284.jpg\n├── dog.9285.jpg\n├── dog.9286.jpg\n├── dog.9287.jpg\n├── dog.9288.jpg\n├── dog.9289.jpg\n├── dog.928.jpg\n├── dog.9290.jpg\n├── dog.9291.jpg\n├── dog.9292.jpg\n├── dog.9293.jpg\n├── dog.9294.jpg\n├── dog.9295.jpg\n├── dog.9296.jpg\n├── dog.9297.jpg\n├── dog.9298.jpg\n├── dog.9299.jpg\n├── dog.929.jpg\n├── dog.92.jpg\n├── dog.9300.jpg\n├── dog.9301.jpg\n├── dog.9302.jpg\n├── dog.9303.jpg\n├── dog.9304.jpg\n├── dog.9305.jpg\n├── dog.9306.jpg\n├── dog.9307.jpg\n├── dog.9308.jpg\n├── dog.9309.jpg\n├── dog.930.jpg\n├── dog.9310.jpg\n├── dog.9311.jpg\n├── dog.9312.jpg\n├── dog.9313.jpg\n├── dog.9314.jpg\n├── dog.9315.jpg\n├── dog.9316.jpg\n├── dog.9317.jpg\n├── dog.9318.jpg\n├── dog.9319.jpg\n├── dog.931.jpg\n├── dog.9320.jpg\n├── dog.9321.jpg\n├── dog.9322.jpg\n├── dog.9323.jpg\n├── dog.9324.jpg\n├── dog.9325.jpg\n├── dog.9326.jpg\n├── dog.9327.jpg\n├── dog.9328.jpg\n├── dog.9329.jpg\n├── dog.932.jpg\n├── dog.9330.jpg\n├── dog.9331.jpg\n├── dog.9332.jpg\n├── dog.9333.jpg\n├── dog.9334.jpg\n├── dog.9335.jpg\n├── dog.9336.jpg\n├── dog.9337.jpg\n├── dog.9338.jpg\n├── dog.9339.jpg\n├── dog.933.jpg\n├── dog.9340.jpg\n├── dog.9341.jpg\n├── dog.9342.jpg\n├── dog.9343.jpg\n├── dog.9344.jpg\n├── dog.9345.jpg\n├── dog.9346.jpg\n├── dog.9347.jpg\n├── dog.9348.jpg\n├── dog.9349.jpg\n├── dog.934.jpg\n├── dog.9350.jpg\n├── dog.9351.jpg\n├── dog.9352.jpg\n├── dog.9353.jpg\n├── dog.9354.jpg\n├── dog.9355.jpg\n├── dog.9356.jpg\n├── dog.9357.jpg\n├── dog.9358.jpg\n├── dog.9359.jpg\n├── dog.935.jpg\n├── dog.9360.jpg\n├── dog.9361.jpg\n├── dog.9362.jpg\n├── dog.9363.jpg\n├── dog.9364.jpg\n├── dog.9365.jpg\n├── dog.9366.jpg\n├── dog.9367.jpg\n├── dog.9368.jpg\n├── dog.9369.jpg\n├── dog.936.jpg\n├── dog.9370.jpg\n├── dog.9371.jpg\n├── dog.9372.jpg\n├── dog.9373.jpg\n├── dog.9374.jpg\n├── dog.9375.jpg\n├── dog.9376.jpg\n├── dog.9377.jpg\n├── dog.9378.jpg\n├── dog.9379.jpg\n├── dog.937.jpg\n├── dog.9380.jpg\n├── dog.9381.jpg\n├── dog.9382.jpg\n├── dog.9383.jpg\n├── dog.9384.jpg\n├── dog.9385.jpg\n├── dog.9386.jpg\n├── dog.9387.jpg\n├── dog.9388.jpg\n├── dog.9389.jpg\n├── dog.938.jpg\n├── dog.9390.jpg\n├── dog.9391.jpg\n├── dog.9392.jpg\n├── dog.9393.jpg\n├── dog.9394.jpg\n├── dog.9395.jpg\n├── dog.9396.jpg\n├── dog.9397.jpg\n├── dog.9398.jpg\n├── dog.9399.jpg\n├── dog.939.jpg\n├── dog.93.jpg\n├── dog.9400.jpg\n├── dog.9401.jpg\n├── dog.9402.jpg\n├── dog.9403.jpg\n├── dog.9404.jpg\n├── dog.9405.jpg\n├── dog.9406.jpg\n├── dog.9407.jpg\n├── dog.9408.jpg\n├── dog.9409.jpg\n├── dog.940.jpg\n├── dog.9410.jpg\n├── dog.9411.jpg\n├── dog.9412.jpg\n├── dog.9413.jpg\n├── dog.9414.jpg\n├── dog.9415.jpg\n├── dog.9416.jpg\n├── dog.9417.jpg\n├── dog.9418.jpg\n├── dog.9419.jpg\n├── dog.941.jpg\n├── dog.9420.jpg\n├── dog.9421.jpg\n├── dog.9422.jpg\n├── dog.9423.jpg\n├── dog.9424.jpg\n├── dog.9425.jpg\n├── dog.9426.jpg\n├── dog.9427.jpg\n├── dog.9428.jpg\n├── dog.9429.jpg\n├── dog.942.jpg\n├── dog.9430.jpg\n├── dog.9431.jpg\n├── dog.9432.jpg\n├── dog.9433.jpg\n├── dog.9434.jpg\n├── dog.9435.jpg\n├── dog.9436.jpg\n├── dog.9437.jpg\n├── dog.9438.jpg\n├── dog.9439.jpg\n├── dog.943.jpg\n├── dog.9440.jpg\n├── dog.9441.jpg\n├── dog.9442.jpg\n├── dog.9443.jpg\n├── dog.9444.jpg\n├── dog.9445.jpg\n├── dog.9446.jpg\n├── dog.9447.jpg\n├── dog.9448.jpg\n├── dog.9449.jpg\n├── dog.944.jpg\n├── dog.9450.jpg\n├── dog.9451.jpg\n├── dog.9452.jpg\n├── dog.9453.jpg\n├── dog.9454.jpg\n├── dog.9455.jpg\n├── dog.9456.jpg\n├── dog.9457.jpg\n├── dog.9458.jpg\n├── dog.9459.jpg\n├── dog.945.jpg\n├── dog.9460.jpg\n├── dog.9461.jpg\n├── dog.9462.jpg\n├── dog.9463.jpg\n├── dog.9464.jpg\n├── dog.9465.jpg\n├── dog.9466.jpg\n├── dog.9467.jpg\n├── dog.9468.jpg\n├── dog.9469.jpg\n├── dog.946.jpg\n├── dog.9470.jpg\n├── dog.9471.jpg\n├── dog.9472.jpg\n├── dog.9473.jpg\n├── dog.9474.jpg\n├── dog.9475.jpg\n├── dog.9476.jpg\n├── dog.9477.jpg\n├── dog.9478.jpg\n├── dog.9479.jpg\n├── dog.947.jpg\n├── dog.9480.jpg\n├── dog.9481.jpg\n├── dog.9482.jpg\n├── dog.9483.jpg\n├── dog.9484.jpg\n├── dog.9485.jpg\n├── dog.9486.jpg\n├── dog.9487.jpg\n├── dog.9488.jpg\n├── dog.9489.jpg\n├── dog.948.jpg\n├── dog.9490.jpg\n├── dog.9491.jpg\n├── dog.9492.jpg\n├── dog.9493.jpg\n├── dog.9494.jpg\n├── dog.9495.jpg\n├── dog.9496.jpg\n├── dog.9497.jpg\n├── dog.9498.jpg\n├── dog.9499.jpg\n├── dog.949.jpg\n├── dog.94.jpg\n├── dog.9500.jpg\n├── dog.9501.jpg\n├── dog.9502.jpg\n├── dog.9503.jpg\n├── dog.9504.jpg\n├── dog.9505.jpg\n├── dog.9506.jpg\n├── dog.9507.jpg\n├── dog.9508.jpg\n├── dog.9509.jpg\n├── dog.950.jpg\n├── dog.9510.jpg\n├── dog.9511.jpg\n├── dog.9512.jpg\n├── dog.9513.jpg\n├── dog.9514.jpg\n├── dog.9515.jpg\n├── dog.9516.jpg\n├── dog.9517.jpg\n├── dog.9518.jpg\n├── dog.9519.jpg\n├── dog.951.jpg\n├── dog.9520.jpg\n├── dog.9521.jpg\n├── dog.9522.jpg\n├── dog.9523.jpg\n├── dog.9524.jpg\n├── dog.9525.jpg\n├── dog.9526.jpg\n├── dog.9527.jpg\n├── dog.9528.jpg\n├── dog.9529.jpg\n├── dog.952.jpg\n├── dog.9530.jpg\n├── dog.9531.jpg\n├── dog.9532.jpg\n├── dog.9533.jpg\n├── dog.9534.jpg\n├── dog.9535.jpg\n├── dog.9536.jpg\n├── dog.9537.jpg\n├── dog.9538.jpg\n├── dog.9539.jpg\n├── dog.953.jpg\n├── dog.9540.jpg\n├── dog.9541.jpg\n├── dog.9542.jpg\n├── dog.9543.jpg\n├── dog.9544.jpg\n├── dog.9545.jpg\n├── dog.9546.jpg\n├── dog.9547.jpg\n├── dog.9548.jpg\n├── dog.9549.jpg\n├── dog.954.jpg\n├── dog.9550.jpg\n├── dog.9551.jpg\n├── dog.9552.jpg\n├── dog.9553.jpg\n├── dog.9554.jpg\n├── dog.9555.jpg\n├── dog.9556.jpg\n├── dog.9557.jpg\n├── dog.9558.jpg\n├── dog.9559.jpg\n├── dog.955.jpg\n├── dog.9560.jpg\n├── dog.9561.jpg\n├── dog.9562.jpg\n├── dog.9563.jpg\n├── dog.9564.jpg\n├── dog.9565.jpg\n├── dog.9566.jpg\n├── dog.9567.jpg\n├── dog.9568.jpg\n├── dog.9569.jpg\n├── dog.956.jpg\n├── dog.9570.jpg\n├── dog.9571.jpg\n├── dog.9572.jpg\n├── dog.9573.jpg\n├── dog.9574.jpg\n├── dog.9575.jpg\n├── dog.9576.jpg\n├── dog.9577.jpg\n├── dog.9578.jpg\n├── dog.9579.jpg\n├── dog.957.jpg\n├── dog.9580.jpg\n├── dog.9581.jpg\n├── dog.9582.jpg\n├── dog.9583.jpg\n├── dog.9584.jpg\n├── dog.9585.jpg\n├── dog.9586.jpg\n├── dog.9587.jpg\n├── dog.9588.jpg\n├── dog.9589.jpg\n├── dog.958.jpg\n├── dog.9590.jpg\n├── dog.9591.jpg\n├── dog.9592.jpg\n├── dog.9593.jpg\n├── dog.9594.jpg\n├── dog.9595.jpg\n├── dog.9596.jpg\n├── dog.9597.jpg\n├── dog.9598.jpg\n├── dog.9599.jpg\n├── dog.959.jpg\n├── dog.95.jpg\n├── dog.9600.jpg\n├── dog.9601.jpg\n├── dog.9602.jpg\n├── dog.9603.jpg\n├── dog.9604.jpg\n├── dog.9605.jpg\n├── dog.9606.jpg\n├── dog.9607.jpg\n├── dog.9608.jpg\n├── dog.9609.jpg\n├── dog.960.jpg\n├── dog.9610.jpg\n├── dog.9611.jpg\n├── dog.9612.jpg\n├── dog.9613.jpg\n├── dog.9614.jpg\n├── dog.9615.jpg\n├── dog.9616.jpg\n├── dog.9617.jpg\n├── dog.9618.jpg\n├── dog.9619.jpg\n├── dog.961.jpg\n├── dog.9620.jpg\n├── dog.9621.jpg\n├── dog.9622.jpg\n├── dog.9623.jpg\n├── dog.9624.jpg\n├── dog.9625.jpg\n├── dog.9626.jpg\n├── dog.9627.jpg\n├── dog.9628.jpg\n├── dog.9629.jpg\n├── dog.962.jpg\n├── dog.9630.jpg\n├── dog.9631.jpg\n├── dog.9632.jpg\n├── dog.9633.jpg\n├── dog.9634.jpg\n├── dog.9635.jpg\n├── dog.9636.jpg\n├── dog.9637.jpg\n├── dog.9638.jpg\n├── dog.9639.jpg\n├── dog.963.jpg\n├── dog.9640.jpg\n├── dog.9641.jpg\n├── dog.9642.jpg\n├── dog.9643.jpg\n├── dog.9644.jpg\n├── dog.9645.jpg\n├── dog.9646.jpg\n├── dog.9647.jpg\n├── dog.9648.jpg\n├── dog.9649.jpg\n├── dog.964.jpg\n├── dog.9650.jpg\n├── dog.9651.jpg\n├── dog.9652.jpg\n├── dog.9653.jpg\n├── dog.9654.jpg\n├── dog.9655.jpg\n├── dog.9656.jpg\n├── dog.9657.jpg\n├── dog.9658.jpg\n├── dog.9659.jpg\n├── dog.965.jpg\n├── dog.9660.jpg\n├── dog.9661.jpg\n├── dog.9662.jpg\n├── dog.9663.jpg\n├── dog.9664.jpg\n├── dog.9665.jpg\n├── dog.9666.jpg\n├── dog.9667.jpg\n├── dog.9668.jpg\n├── dog.9669.jpg\n├── dog.966.jpg\n├── dog.9670.jpg\n├── dog.9671.jpg\n├── dog.9672.jpg\n├── dog.9673.jpg\n├── dog.9674.jpg\n├── dog.9675.jpg\n├── dog.9676.jpg\n├── dog.9677.jpg\n├── dog.9678.jpg\n├── dog.9679.jpg\n├── dog.967.jpg\n├── dog.9680.jpg\n├── dog.9681.jpg\n├── dog.9682.jpg\n├── dog.9683.jpg\n├── dog.9684.jpg\n├── dog.9685.jpg\n├── dog.9686.jpg\n├── dog.9687.jpg\n├── dog.9688.jpg\n├── dog.9689.jpg\n├── dog.968.jpg\n├── dog.9690.jpg\n├── dog.9691.jpg\n├── dog.9692.jpg\n├── dog.9693.jpg\n├── dog.9694.jpg\n├── dog.9695.jpg\n├── dog.9696.jpg\n├── dog.9697.jpg\n├── dog.9698.jpg\n├── dog.9699.jpg\n├── dog.969.jpg\n├── dog.96.jpg\n├── dog.9700.jpg\n├── dog.9701.jpg\n├── dog.9702.jpg\n├── dog.9703.jpg\n├── dog.9704.jpg\n├── dog.9705.jpg\n├── dog.9706.jpg\n├── dog.9707.jpg\n├── dog.9708.jpg\n├── dog.9709.jpg\n├── dog.970.jpg\n├── dog.9710.jpg\n├── dog.9711.jpg\n├── dog.9712.jpg\n├── dog.9713.jpg\n├── dog.9714.jpg\n├── dog.9715.jpg\n├── dog.9716.jpg\n├── dog.9717.jpg\n├── dog.9718.jpg\n├── dog.9719.jpg\n├── dog.971.jpg\n├── dog.9720.jpg\n├── dog.9721.jpg\n├── dog.9722.jpg\n├── dog.9723.jpg\n├── dog.9724.jpg\n├── dog.9725.jpg\n├── dog.9726.jpg\n├── dog.9727.jpg\n├── dog.9728.jpg\n├── dog.9729.jpg\n├── dog.972.jpg\n├── dog.9730.jpg\n├── dog.9731.jpg\n├── dog.9732.jpg\n├── dog.9733.jpg\n├── dog.9734.jpg\n├── dog.9735.jpg\n├── dog.9736.jpg\n├── dog.9737.jpg\n├── dog.9738.jpg\n├── dog.9739.jpg\n├── dog.973.jpg\n├── dog.9740.jpg\n├── dog.9741.jpg\n├── dog.9742.jpg\n├── dog.9743.jpg\n├── dog.9744.jpg\n├── dog.9745.jpg\n├── dog.9746.jpg\n├── dog.9747.jpg\n├── dog.9748.jpg\n├── dog.9749.jpg\n├── dog.974.jpg\n├── dog.9750.jpg\n├── dog.9751.jpg\n├── dog.9752.jpg\n├── dog.9753.jpg\n├── dog.9754.jpg\n├── dog.9755.jpg\n├── dog.9756.jpg\n├── dog.9757.jpg\n├── dog.9758.jpg\n├── dog.9759.jpg\n├── dog.975.jpg\n├── dog.9760.jpg\n├── dog.9761.jpg\n├── dog.9762.jpg\n├── dog.9763.jpg\n├── dog.9764.jpg\n├── dog.9765.jpg\n├── dog.9766.jpg\n├── dog.9767.jpg\n├── dog.9768.jpg\n├── dog.9769.jpg\n├── dog.976.jpg\n├── dog.9770.jpg\n├── dog.9771.jpg\n├── dog.9772.jpg\n├── dog.9773.jpg\n├── dog.9774.jpg\n├── dog.9775.jpg\n├── dog.9776.jpg\n├── dog.9777.jpg\n├── dog.9778.jpg\n├── dog.9779.jpg\n├── dog.977.jpg\n├── dog.9780.jpg\n├── dog.9781.jpg\n├── dog.9782.jpg\n├── dog.9783.jpg\n├── dog.9784.jpg\n├── dog.9785.jpg\n├── dog.9786.jpg\n├── dog.9787.jpg\n├── dog.9788.jpg\n├── dog.9789.jpg\n├── dog.978.jpg\n├── dog.9790.jpg\n├── dog.9791.jpg\n├── dog.9792.jpg\n├── dog.9793.jpg\n├── dog.9794.jpg\n├── dog.9795.jpg\n├── dog.9796.jpg\n├── dog.9797.jpg\n├── dog.9798.jpg\n├── dog.9799.jpg\n├── dog.979.jpg\n├── dog.97.jpg\n├── dog.9800.jpg\n├── dog.9801.jpg\n├── dog.9802.jpg\n├── dog.9803.jpg\n├── dog.9804.jpg\n├── dog.9805.jpg\n├── dog.9806.jpg\n├── dog.9807.jpg\n├── dog.9808.jpg\n├── dog.9809.jpg\n├── dog.980.jpg\n├── dog.9810.jpg\n├── dog.9811.jpg\n├── dog.9812.jpg\n├── dog.9813.jpg\n├── dog.9814.jpg\n├── dog.9815.jpg\n├── dog.9816.jpg\n├── dog.9817.jpg\n├── dog.9818.jpg\n├── dog.9819.jpg\n├── dog.981.jpg\n├── dog.9820.jpg\n├── dog.9821.jpg\n├── dog.9822.jpg\n├── dog.9823.jpg\n├── dog.9824.jpg\n├── dog.9825.jpg\n├── dog.9826.jpg\n├── dog.9827.jpg\n├── dog.9828.jpg\n├── dog.9829.jpg\n├── dog.982.jpg\n├── dog.9830.jpg\n├── dog.9831.jpg\n├── dog.9832.jpg\n├── dog.9833.jpg\n├── dog.9834.jpg\n├── dog.9835.jpg\n├── dog.9836.jpg\n├── dog.9837.jpg\n├── dog.9838.jpg\n├── dog.9839.jpg\n├── dog.983.jpg\n├── dog.9840.jpg\n├── dog.9841.jpg\n├── dog.9842.jpg\n├── dog.9843.jpg\n├── dog.9844.jpg\n├── dog.9845.jpg\n├── dog.9846.jpg\n├── dog.9847.jpg\n├── dog.9848.jpg\n├── dog.9849.jpg\n├── dog.984.jpg\n├── dog.9850.jpg\n├── dog.9851.jpg\n├── dog.9852.jpg\n├── dog.9853.jpg\n├── dog.9854.jpg\n├── dog.9855.jpg\n├── dog.9856.jpg\n├── dog.9857.jpg\n├── dog.9858.jpg\n├── dog.9859.jpg\n├── dog.985.jpg\n├── dog.9860.jpg\n├── dog.9861.jpg\n├── dog.9862.jpg\n├── dog.9863.jpg\n├── dog.9864.jpg\n├── dog.9865.jpg\n├── dog.9866.jpg\n├── dog.9867.jpg\n├── dog.9868.jpg\n├── dog.9869.jpg\n├── dog.986.jpg\n├── dog.9870.jpg\n├── dog.9871.jpg\n├── dog.9872.jpg\n├── dog.9873.jpg\n├── dog.9874.jpg\n├── dog.9875.jpg\n├── dog.9876.jpg\n├── dog.9877.jpg\n├── dog.9878.jpg\n├── dog.9879.jpg\n├── dog.987.jpg\n├── dog.9880.jpg\n├── dog.9881.jpg\n├── dog.9882.jpg\n├── dog.9883.jpg\n├── dog.9884.jpg\n├── dog.9885.jpg\n├── dog.9886.jpg\n├── dog.9887.jpg\n├── dog.9888.jpg\n├── dog.9889.jpg\n├── dog.988.jpg\n├── dog.9890.jpg\n├── dog.9891.jpg\n├── dog.9892.jpg\n├── dog.9893.jpg\n├── dog.9894.jpg\n├── dog.9895.jpg\n├── dog.9896.jpg\n├── dog.9897.jpg\n├── dog.9898.jpg\n├── dog.9899.jpg\n├── dog.989.jpg\n├── dog.98.jpg\n├── dog.9900.jpg\n├── dog.9901.jpg\n├── dog.9902.jpg\n├── dog.9903.jpg\n├── dog.9904.jpg\n├── dog.9905.jpg\n├── dog.9906.jpg\n├── dog.9907.jpg\n├── dog.9908.jpg\n├── dog.9909.jpg\n├── dog.990.jpg\n├── dog.9910.jpg\n├── dog.9911.jpg\n├── dog.9912.jpg\n├── dog.9913.jpg\n├── dog.9914.jpg\n├── dog.9915.jpg\n├── dog.9916.jpg\n├── dog.9917.jpg\n├── dog.9918.jpg\n├── dog.9919.jpg\n├── dog.991.jpg\n├── dog.9920.jpg\n├── dog.9921.jpg\n├── dog.9922.jpg\n├── dog.9923.jpg\n├── dog.9924.jpg\n├── dog.9925.jpg\n├── dog.9926.jpg\n├── dog.9927.jpg\n├── dog.9928.jpg\n├── dog.9929.jpg\n├── dog.992.jpg\n├── dog.9930.jpg\n├── dog.9931.jpg\n├── dog.9932.jpg\n├── dog.9933.jpg\n├── dog.9934.jpg\n├── dog.9935.jpg\n├── dog.9936.jpg\n├── dog.9937.jpg\n├── dog.9938.jpg\n├── dog.9939.jpg\n├── dog.993.jpg\n├── dog.9940.jpg\n├── dog.9941.jpg\n├── dog.9942.jpg\n├── dog.9943.jpg\n├── dog.9944.jpg\n├── dog.9945.jpg\n├── dog.9946.jpg\n├── dog.9947.jpg\n├── dog.9948.jpg\n├── dog.9949.jpg\n├── dog.994.jpg\n├── dog.9950.jpg\n├── dog.9951.jpg\n├── dog.9952.jpg\n├── dog.9953.jpg\n├── dog.9954.jpg\n├── dog.9955.jpg\n├── dog.9956.jpg\n├── dog.9957.jpg\n├── dog.9958.jpg\n├── dog.9959.jpg\n├── dog.995.jpg\n├── dog.9960.jpg\n├── dog.9961.jpg\n├── dog.9962.jpg\n├── dog.9963.jpg\n├── dog.9964.jpg\n├── dog.9965.jpg\n├── dog.9966.jpg\n├── dog.9967.jpg\n├── dog.9968.jpg\n├── dog.9969.jpg\n├── dog.996.jpg\n├── dog.9970.jpg\n├── dog.9971.jpg\n├── dog.9972.jpg\n├── dog.9973.jpg\n├── dog.9974.jpg\n├── dog.9975.jpg\n├── dog.9976.jpg\n├── dog.9977.jpg\n├── dog.9978.jpg\n├── dog.9979.jpg\n├── dog.997.jpg\n├── dog.9980.jpg\n├── dog.9981.jpg\n├── dog.9982.jpg\n├── dog.9983.jpg\n├── dog.9984.jpg\n├── dog.9985.jpg\n├── dog.9986.jpg\n├── dog.9987.jpg\n├── dog.9988.jpg\n├── dog.9989.jpg\n├── dog.998.jpg\n├── dog.9990.jpg\n├── dog.9991.jpg\n├── dog.9992.jpg\n├── dog.9993.jpg\n├── dog.9994.jpg\n├── dog.9995.jpg\n├── dog.9996.jpg\n├── dog.9997.jpg\n├── dog.9998.jpg\n├── dog.9999.jpg\n├── dog.999.jpg\n├── dog.99.jpg\n└── dog.9.jpg\n\n0 directories, 25000 files\n\n\n\noriginal_dir = pathlib.Path(\"train\")\nnew_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n\ndef make_subset(subset_name, start_index, end_index):\n    for category in (\"cat\", \"dog\"):\n        dir = new_base_dir / subset_name / category\n        os.makedirs(dir)\n        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n        for fname in fnames:\n            shutil.copyfile(src=original_dir / fname,\n                            dst=dir / fname)\n\nmake_subset(\"train\", start_index=0, end_index=1000)\nmake_subset(\"validation\", start_index=1000, end_index=1500)\nmake_subset(\"test\", start_index=1500, end_index=2500)\n\n\n!tree cats_vs_dogs_small -L 2\n\ncats_vs_dogs_small\n├── test\n│   ├── cat\n│   └── dog\n├── train\n│   ├── cat\n│   └── dog\n└── validation\n    ├── cat\n    └── dog\n\n9 directories, 0 files\n\n\nWe now have 2,000 training images, 1,000 validation images, and 2,000 test images. Each split contains the same number of samples from each class: this is a balanced binary-classification problem, which means classification accuracy will be an appropriate measure of success.\n\n\n9.6.3 Building the model\nThe convnet will be a stack of alternated Conv2D (with relu activation) and MaxPooling2D layers. But because we’re dealing with bigger images and a more complex problem, we’ll make our model larger, accordingly: it will have two more Conv2D and MaxPooling2D stages. This serves both to augment the capacity of the model and to further reduce the size of the feature maps so they aren’t overly large when we reach the Flatten layer.\nHere, because we start from inputs of size 180 pixels × 180 pixels, we end up with feature maps of size 7 × 7 just before the Flatten layer. Because we’re looking at a binary-classification problem, we’ll end the model with a single unit (a Dense layer of size 1) and a sigmoid activation. This unit will encode the probability that the model is looking at one class or the other.\nOne last small difference: we will start the model with a Rescaling layer, which will rescale image inputs (whose values are originally in the [0, 255] range) to the [0, 1] range.\n\ninputs = tf.keras.Input(shape=(180, 180, 3))\nx = tf.keras.layers.Rescaling(1./255)(inputs)\nx = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=2)(x)\nx = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=2)(x)\nx = tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=2)(x)\nx = tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=2)(x)\nx = tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\nx = tf.keras.layers.Flatten()(x)\noutputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 180, 180, 3)]     0         \n                                                                 \n rescaling_1 (Rescaling)     (None, 180, 180, 3)       0         \n                                                                 \n conv2d_28 (Conv2D)          (None, 178, 178, 32)      896       \n                                                                 \n max_pooling2d_16 (MaxPoolin  (None, 89, 89, 32)       0         \n g2D)                                                            \n                                                                 \n conv2d_29 (Conv2D)          (None, 87, 87, 64)        18496     \n                                                                 \n max_pooling2d_17 (MaxPoolin  (None, 43, 43, 64)       0         \n g2D)                                                            \n                                                                 \n conv2d_30 (Conv2D)          (None, 41, 41, 128)       73856     \n                                                                 \n max_pooling2d_18 (MaxPoolin  (None, 20, 20, 128)      0         \n g2D)                                                            \n                                                                 \n conv2d_31 (Conv2D)          (None, 18, 18, 256)       295168    \n                                                                 \n max_pooling2d_19 (MaxPoolin  (None, 9, 9, 256)        0         \n g2D)                                                            \n                                                                 \n conv2d_32 (Conv2D)          (None, 7, 7, 256)         590080    \n                                                                 \n flatten_5 (Flatten)         (None, 12544)             0         \n                                                                 \n dense_13 (Dense)            (None, 1)                 12545     \n                                                                 \n=================================================================\nTotal params: 991,041\nTrainable params: 991,041\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n# For the compilation step, we’ll go with the nadam optimizer. Because we\n# ended the model with a single sigmoid unit, we’ll use binary crossentropy as the loss\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n\n\n\n9.6.4 Data preprocessing\nAs you know by now, data should be formatted into appropriately preprocessed floatingpoint tensors before being fed into the model. Currently, the data sits on a drive as JPEG files, so the steps for getting it into the model are roughly as follows:\n\nRead the picture files.\nDecode the JPEG content to RGB grids of pixels.\nConvert these into floating-point tensors.\nResize them to a shared size (we’ll use 180 × 180).\nPack them into batches (we’ll use batches of 32 images).\n\nIt may seem a bit daunting, but fortunately Keras has utilities to take care of these steps automatically. In particular, Keras features the utility function image_dataset_from_directory(), which lets you quickly set up a data pipeline that can automatically turn image files on disk into batches of preprocessed tensors. This is what we’ll use here.\nCalling image_dataset_from_directory(directory) will first list the subdirectories of directory and assume each one contains images from one of our classes. It will then index the image files in each subdirectory. Finally, it will create and return a tf.data.Dataset object configured to read these files, shuffle them, decode them to tensors, resize them to a shared size, and pack them into batches.\n\ntrain_dataset = image_dataset_from_directory(\n    new_base_dir / \"train\",\n    image_size=(180, 180),\n    batch_size=32)\nvalidation_dataset = image_dataset_from_directory(\n    new_base_dir / \"validation\",\n    image_size=(180, 180),\n    batch_size=32)\ntest_dataset = image_dataset_from_directory(\n    new_base_dir / \"test\",\n    image_size=(180, 180),\n    batch_size=32)\n\nFound 2000 files belonging to 2 classes.\nFound 1000 files belonging to 2 classes.\nFound 2000 files belonging to 2 classes.\n\n\nTensorFlow makes available the tf.data API to create efficient input pipelines for machine learning models. Its core class is tf.data.Dataset.\nA Dataset object is an iterator: you can use it in a for loop. It will typically return batches of input data and labels. You can pass a Dataset object directly to the fit() method of a Keras model. The Dataset class handles many key features that would otherwise be cumbersome to implement yourself—in particular, asynchronous data prefetching (preprocessing the next batch of data while the previous one is being handled by the model, which keeps execution flowing without interruptions).\nhttps://www.tensorflow.org/api_docs/python/tf/data/Dataset\nLet’s look at the output of one of these Dataset objects: it yields batches of 180 × 180 RGB images (shape (32, 180, 180, 3)) and integer labels (shape (32,)). There are 32 samples in each batch (the batch size).\n\nfor data_batch, labels_batch in train_dataset:\n    print(\"data batch shape:\", data_batch.shape)\n    print(\"labels batch shape:\", labels_batch.shape)\n    break\n\ndata batch shape: (32, 180, 180, 3)\nlabels batch shape: (32,)\n\n\n\n\n9.6.5 Fitting the model\nLet’s fit the model on our dataset. We’ll use the validation_data argument in fit() to monitor validation metrics on a separate Dataset object.\nNote that we’ll also use a ModelCheckpoint callback to save the model after each epoch. We’ll configure it with the path specifying where to save the file, as well as the arguments save_best_only=True and monitor=\"val_loss\": they tell the callback to only save a new file (overwriting any previous one) when the current value of the val_loss metric is lower than at any previous time during training. This guarantees that your saved file will always contain the state of the model corresponding to its bestperforming training epoch, in terms of its performance on the validation data. As a result, we won’t have to retrain a new model for a lower number of epochs if we start overfitting: we can just reload our saved file.\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\n        filepath=\"convnet_from_scratch.keras\",\n        save_best_only=True,\n        monitor=\"val_loss\")\n]\nhistory = model.fit(\n    train_dataset,\n    epochs=30,\n    validation_data=validation_dataset,\n    callbacks=callbacks)\n\nEpoch 1/30\n63/63 [==============================] - 10s 96ms/step - loss: 0.6954 - accuracy: 0.5155 - val_loss: 0.6925 - val_accuracy: 0.5250\nEpoch 2/30\n63/63 [==============================] - 4s 62ms/step - loss: 0.6850 - accuracy: 0.5680 - val_loss: 0.6672 - val_accuracy: 0.5780\nEpoch 3/30\n63/63 [==============================] - 5s 79ms/step - loss: 0.6513 - accuracy: 0.6145 - val_loss: 0.7753 - val_accuracy: 0.5990\nEpoch 4/30\n63/63 [==============================] - 4s 62ms/step - loss: 0.6254 - accuracy: 0.6655 - val_loss: 0.6971 - val_accuracy: 0.6010\nEpoch 5/30\n63/63 [==============================] - 5s 76ms/step - loss: 0.5667 - accuracy: 0.7145 - val_loss: 0.5896 - val_accuracy: 0.7150\nEpoch 6/30\n63/63 [==============================] - 4s 62ms/step - loss: 0.5186 - accuracy: 0.7550 - val_loss: 0.5844 - val_accuracy: 0.7060\nEpoch 7/30\n63/63 [==============================] - 4s 60ms/step - loss: 0.4710 - accuracy: 0.7795 - val_loss: 0.6676 - val_accuracy: 0.6700\nEpoch 8/30\n63/63 [==============================] - 5s 75ms/step - loss: 0.4173 - accuracy: 0.8065 - val_loss: 0.6441 - val_accuracy: 0.7050\nEpoch 9/30\n63/63 [==============================] - 4s 61ms/step - loss: 0.3676 - accuracy: 0.8320 - val_loss: 0.6050 - val_accuracy: 0.7430\nEpoch 10/30\n63/63 [==============================] - 6s 93ms/step - loss: 0.3073 - accuracy: 0.8625 - val_loss: 0.6773 - val_accuracy: 0.7340\nEpoch 11/30\n63/63 [==============================] - 4s 61ms/step - loss: 0.2443 - accuracy: 0.8955 - val_loss: 0.7916 - val_accuracy: 0.7190\nEpoch 12/30\n63/63 [==============================] - 5s 77ms/step - loss: 0.1617 - accuracy: 0.9365 - val_loss: 0.8033 - val_accuracy: 0.7640\nEpoch 13/30\n63/63 [==============================] - 4s 60ms/step - loss: 0.1082 - accuracy: 0.9560 - val_loss: 1.0225 - val_accuracy: 0.7350\nEpoch 14/30\n63/63 [==============================] - 4s 61ms/step - loss: 0.0771 - accuracy: 0.9715 - val_loss: 0.9666 - val_accuracy: 0.7490\nEpoch 15/30\n63/63 [==============================] - 5s 77ms/step - loss: 0.0511 - accuracy: 0.9825 - val_loss: 1.1158 - val_accuracy: 0.7680\nEpoch 16/30\n63/63 [==============================] - 4s 60ms/step - loss: 0.0686 - accuracy: 0.9750 - val_loss: 1.3073 - val_accuracy: 0.7400\nEpoch 17/30\n63/63 [==============================] - 5s 74ms/step - loss: 0.0659 - accuracy: 0.9770 - val_loss: 1.2265 - val_accuracy: 0.7400\nEpoch 18/30\n63/63 [==============================] - 4s 61ms/step - loss: 0.0685 - accuracy: 0.9795 - val_loss: 1.1100 - val_accuracy: 0.7410\nEpoch 19/30\n63/63 [==============================] - 4s 60ms/step - loss: 0.0247 - accuracy: 0.9945 - val_loss: 1.2864 - val_accuracy: 0.7450\nEpoch 20/30\n63/63 [==============================] - 5s 78ms/step - loss: 0.0125 - accuracy: 0.9955 - val_loss: 1.5210 - val_accuracy: 0.7230\nEpoch 21/30\n63/63 [==============================] - 4s 62ms/step - loss: 0.0213 - accuracy: 0.9920 - val_loss: 1.3596 - val_accuracy: 0.7310\nEpoch 22/30\n63/63 [==============================] - 4s 60ms/step - loss: 0.0050 - accuracy: 0.9990 - val_loss: 1.4320 - val_accuracy: 0.7590\nEpoch 23/30\n63/63 [==============================] - 5s 65ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.5110 - val_accuracy: 0.7570\nEpoch 24/30\n63/63 [==============================] - 4s 62ms/step - loss: 3.8918e-04 - accuracy: 1.0000 - val_loss: 1.5488 - val_accuracy: 0.7570\nEpoch 25/30\n63/63 [==============================] - 5s 78ms/step - loss: 2.6354e-04 - accuracy: 1.0000 - val_loss: 1.5863 - val_accuracy: 0.7570\nEpoch 26/30\n63/63 [==============================] - 4s 61ms/step - loss: 2.0452e-04 - accuracy: 1.0000 - val_loss: 1.6134 - val_accuracy: 0.7570\nEpoch 27/30\n63/63 [==============================] - 5s 78ms/step - loss: 1.6733e-04 - accuracy: 1.0000 - val_loss: 1.6389 - val_accuracy: 0.7560\nEpoch 28/30\n63/63 [==============================] - 6s 98ms/step - loss: 1.4225e-04 - accuracy: 1.0000 - val_loss: 1.6627 - val_accuracy: 0.7580\nEpoch 29/30\n63/63 [==============================] - 8s 107ms/step - loss: 1.2267e-04 - accuracy: 1.0000 - val_loss: 1.6835 - val_accuracy: 0.7590\nEpoch 30/30\n63/63 [==============================] - 4s 62ms/step - loss: 1.0639e-04 - accuracy: 1.0000 - val_loss: 1.7035 - val_accuracy: 0.7580\n\n\nLet’s plot the loss and accuracy of the model over the training and validation data during training\n\naccuracy = history.history[\"accuracy\"]\nval_accuracy = history.history[\"val_accuracy\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nepochs = range(1, len(accuracy) + 1)\nplt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\nplt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nThese plots are characteristic of overfitting. The training accuracy increases linearly over time, until it reaches nearly 100%, whereas the validation accuracy peaks at 74%. The validation loss reaches its minimum after only ten epochs and then stalls, whereas the training loss keeps decreasing linearly as training proceeds.\nLet’s check the test accuracy. We’ll reload the model from its saved file to evaluate it as it was before it started overfitting.\n\ntest_model = tf.keras.models.load_model(\"convnet_from_scratch.keras\")\ntest_loss, test_acc = test_model.evaluate(test_dataset)\nprint(f\"Test accuracy: {test_acc:.3f}\")\n\n63/63 [==============================] - 2s 31ms/step - loss: 0.5808 - accuracy: 0.7085\nTest accuracy: 0.709\n\n\nWe get a test accuracy of about 70%. Because we have relatively few training samples (2,000), overfitting will be our number one concern. You already know about a number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). We’re now going to work with a new one, specific to computer vision and used almost universally when processing images with deep learning models: data augmentation.\n\n\n9.6.6 Using data augmentation\nOverfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your model would be exposed to every possible aspect of the data distribution at hand: you would never overfit. Data augmentation takes the approach of generating more training data from existing training samples by augmenting the samples via a number of random transformations that yield believable-looking images.\nThe goal is that, at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data so it can generalize better. In Keras, this can be done by adding a number of data augmentation layers at the start of your model. Let’s get started with an example: the following Sequential model chains several random image transformations. In our model, we’d include it right before the Rescaling layer.\n\ndata_augmentation = tf.keras.Sequential(\n    [\n        tf.keras.layers.RandomFlip(\"horizontal\"),\n        tf.keras.layers.RandomRotation(0.1),\n        tf.keras.layers.RandomZoom(0.2),\n    ]\n)\n\nLet’s quickly go over this code: * RandomFlip(\"horizontal\")—Applies horizontal flipping to a random 50% of the images that go through it * RandomRotation(0.1)—Rotates the input images by a random value in the range [–10%, +10%] (these are fractions of a full circle—in degrees, the range would be [–36 degrees, +36 degrees]) * RandomZoom(0.2)—Zooms in or out of the image by a random factor in the range [-20%, +20%]\nhttps://www.tensorflow.org/guide/keras/preprocessing_layers#image_data_augmentation\n\nplt.figure(figsize=(10, 10))\nfor images, _ in train_dataset.take(1): #Sample 1 batch from the dataset\n    for i in range(9):\n      # During inference time, the output will be identical to input. \n      # Call the layer with training=True to flip the input.\n        augmented_images = data_augmentation(images, training=True)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")\n\n\n\n\nOn the other hand, we can also use albumentations for data augmentation:\n\n# Batch size set to 1\ntrain_dataset = image_dataset_from_directory(\n    new_base_dir / \"train\",\n    image_size=(180, 180),\n    batch_size=1)\nvalidation_dataset = image_dataset_from_directory(\n    new_base_dir / \"validation\",\n    image_size=(180, 180),\n    batch_size=1)\ntest_dataset = image_dataset_from_directory(\n    new_base_dir / \"test\",\n    image_size=(180, 180),\n    batch_size=1)\n\nFound 2000 files belonging to 2 classes.\nFound 1000 files belonging to 2 classes.\nFound 2000 files belonging to 2 classes.\n\n\n\nbatch_size = 32\nimage_size = 180\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ndef augment_train_data(train_ds):\n    transforms = A.Compose([\n            A.HorizontalFlip(),\n            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=20, p=.5),\n            ])\n    \n    def aug_fn(image):\n        data = {\"image\":image.squeeze()}\n        aug_data = transforms(**data)\n        aug_img = aug_data[\"image\"]\n        aug_img = tf.cast(aug_img/255.0, tf.float32)\n        return aug_img\n\n    def process_data(image, label):\n        aug_img = tf.numpy_function(func=aug_fn, inp=[image], Tout=tf.float32)\n        return aug_img, label\n    \n    def set_shapes(img, label, img_shape=(image_size,image_size,3)):\n        img.set_shape(img_shape)\n        label.set_shape([1,])\n        return img, label\n    \n    ds_alb = train_ds.map(partial(process_data), num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n    ds_alb = ds_alb.map(set_shapes, num_parallel_calls=AUTOTUNE)\n    ds_alb = ds_alb.batch(batch_size) # Return to original batch size here\n    return ds_alb\n\ndef augment_val_data(val_ds):\n    \n    def aug_fn(image):\n        aug_data = {\"image\":image.squeeze()}\n        aug_img = aug_data[\"image\"]\n        aug_img = tf.cast(aug_img/255.0, tf.float32)\n        return aug_img\n\n    def process_data(image, label):\n        aug_img = tf.numpy_function(func=aug_fn, inp=[image], Tout=tf.float32)\n        return aug_img, label\n    \n    def set_shapes(img, label, img_shape=(image_size, image_size,3)):\n        img.set_shape(img_shape)\n        label.set_shape([1,])\n        return img, label\n    \n    ds_alb = val_ds.map(partial(process_data), num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n    ds_alb = ds_alb.map(set_shapes, num_parallel_calls=AUTOTUNE).batch(batch_size)\n    return ds_alb\n\n\ntrain_dataset\n\n&lt;_BatchDataset element_spec=(TensorSpec(shape=(None, 180, 180, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))&gt;\n\n\n\ntrain_alb = augment_train_data(train_dataset)\nval_alb = augment_val_data(validation_dataset)\ntest_alb = augment_val_data(test_dataset)\n\n\ndef view_image(ds):\n    image, label = next(iter(ds)) # extract 1 batch from the dataset\n    image = image.numpy()\n    label = label.numpy()\n    \n    fig = plt.figure(figsize=(22, 22))\n    for i in range(20):\n        ax = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[])\n        ax.imshow(image[i])\n        ax.set_title(f\"Label: {label[i]}\")\n\n\nview_image(train_alb)\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nIf we train a new model using this data-augmentation configuration, the model will never see the same input twice. But the inputs it sees are still heavily intercorrelated because they come from a small number of original images—we can’t produce new information; we can only remix existing information. As such, this may not be enough to completely get rid of overfitting. To further fight overfitting, we’ll also add a Dropout layer to our model right before the densely connected classifier.\nOne last thing you should know about random image augmentation layers: just like Dropout, they’re inactive during inference (when we call predict() or evaluate()). During evaluation, our model will behave just the same as when it did not include data augmentation and dropout.\n\ninputs = tf.keras.Input(shape=(180, 180, 3))\n#x = data_augmentation(inputs)\n#x = tf.keras.layers.Rescaling(1./255)(x)\nx = inputs\nx = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=2)(x)\nx = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=2)(x)\nx = tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=2)(x)\nx = tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=2)(x)\nx = tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\nx = tf.keras.layers.Flatten()(x)\nx = tf.keras.layers.Dropout(0.5)(x)\noutputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(loss=\"binary_crossentropy\",\n              optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\n\n\nmodel.summary()\n\nLet’s train the model using data augmentation and dropout. Because we expect overfitting to occur much later during training, we will train for three times as many epochs—one hundred.\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\n        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n        save_best_only=True,\n        monitor=\"val_loss\")\n]\nhistory = model.fit(\n    train_alb, # Use the dataset from albumentations\n    epochs=100,\n    validation_data=val_alb,\n    callbacks=callbacks)\n\nEpoch 1/100\n63/63 [==============================] - 14s 174ms/step - loss: 0.6949 - accuracy: 0.5025 - val_loss: 0.6887 - val_accuracy: 0.5150\nEpoch 2/100\n63/63 [==============================] - 16s 262ms/step - loss: 0.6882 - accuracy: 0.5565 - val_loss: 0.6769 - val_accuracy: 0.5970\nEpoch 3/100\n63/63 [==============================] - 10s 165ms/step - loss: 0.6728 - accuracy: 0.6065 - val_loss: 0.6751 - val_accuracy: 0.5570\nEpoch 4/100\n63/63 [==============================] - 9s 141ms/step - loss: 0.6534 - accuracy: 0.6260 - val_loss: 0.6312 - val_accuracy: 0.6430\nEpoch 5/100\n63/63 [==============================] - 12s 191ms/step - loss: 0.6265 - accuracy: 0.6545 - val_loss: 0.6218 - val_accuracy: 0.6550\nEpoch 6/100\n63/63 [==============================] - 10s 160ms/step - loss: 0.6096 - accuracy: 0.6730 - val_loss: 0.6079 - val_accuracy: 0.6790\nEpoch 7/100\n63/63 [==============================] - 11s 170ms/step - loss: 0.5852 - accuracy: 0.6845 - val_loss: 0.5742 - val_accuracy: 0.7080\nEpoch 8/100\n63/63 [==============================] - 10s 153ms/step - loss: 0.5576 - accuracy: 0.7145 - val_loss: 0.5740 - val_accuracy: 0.6940\nEpoch 9/100\n63/63 [==============================] - 10s 155ms/step - loss: 0.5393 - accuracy: 0.7245 - val_loss: 0.5388 - val_accuracy: 0.7320\nEpoch 10/100\n63/63 [==============================] - 8s 133ms/step - loss: 0.5044 - accuracy: 0.7505 - val_loss: 0.5389 - val_accuracy: 0.7240\nEpoch 11/100\n63/63 [==============================] - 8s 131ms/step - loss: 0.4956 - accuracy: 0.7485 - val_loss: 0.5195 - val_accuracy: 0.7210\nEpoch 12/100\n63/63 [==============================] - 9s 138ms/step - loss: 0.4830 - accuracy: 0.7720 - val_loss: 0.5571 - val_accuracy: 0.7070\nEpoch 13/100\n63/63 [==============================] - 10s 158ms/step - loss: 0.4741 - accuracy: 0.7780 - val_loss: 0.5184 - val_accuracy: 0.7490\nEpoch 14/100\n63/63 [==============================] - 9s 141ms/step - loss: 0.4354 - accuracy: 0.7995 - val_loss: 0.5224 - val_accuracy: 0.7590\nEpoch 15/100\n63/63 [==============================] - 9s 142ms/step - loss: 0.4498 - accuracy: 0.7885 - val_loss: 0.4999 - val_accuracy: 0.7570\nEpoch 16/100\n63/63 [==============================] - 8s 128ms/step - loss: 0.3996 - accuracy: 0.8175 - val_loss: 0.4799 - val_accuracy: 0.7780\nEpoch 17/100\n63/63 [==============================] - 9s 142ms/step - loss: 0.4035 - accuracy: 0.8130 - val_loss: 0.5061 - val_accuracy: 0.7750\nEpoch 18/100\n63/63 [==============================] - 9s 149ms/step - loss: 0.3864 - accuracy: 0.8305 - val_loss: 0.5136 - val_accuracy: 0.7710\nEpoch 19/100\n63/63 [==============================] - 9s 143ms/step - loss: 0.3781 - accuracy: 0.8380 - val_loss: 0.5987 - val_accuracy: 0.7340\nEpoch 20/100\n63/63 [==============================] - 8s 123ms/step - loss: 0.3585 - accuracy: 0.8345 - val_loss: 0.5067 - val_accuracy: 0.7770\nEpoch 21/100\n63/63 [==============================] - 9s 136ms/step - loss: 0.3607 - accuracy: 0.8425 - val_loss: 0.4234 - val_accuracy: 0.8100\nEpoch 22/100\n63/63 [==============================] - 9s 138ms/step - loss: 0.3485 - accuracy: 0.8490 - val_loss: 0.4703 - val_accuracy: 0.8040\nEpoch 23/100\n63/63 [==============================] - 9s 137ms/step - loss: 0.3342 - accuracy: 0.8570 - val_loss: 0.4861 - val_accuracy: 0.8020\nEpoch 24/100\n63/63 [==============================] - 8s 130ms/step - loss: 0.3319 - accuracy: 0.8610 - val_loss: 0.4531 - val_accuracy: 0.8110\nEpoch 25/100\n63/63 [==============================] - 8s 130ms/step - loss: 0.3077 - accuracy: 0.8680 - val_loss: 0.4272 - val_accuracy: 0.8130\nEpoch 26/100\n63/63 [==============================] - 10s 153ms/step - loss: 0.2972 - accuracy: 0.8730 - val_loss: 0.4826 - val_accuracy: 0.8060\nEpoch 27/100\n63/63 [==============================] - 9s 139ms/step - loss: 0.2938 - accuracy: 0.8805 - val_loss: 0.5064 - val_accuracy: 0.7990\nEpoch 28/100\n63/63 [==============================] - 9s 138ms/step - loss: 0.2885 - accuracy: 0.8780 - val_loss: 0.4573 - val_accuracy: 0.8090\nEpoch 29/100\n63/63 [==============================] - 8s 128ms/step - loss: 0.2948 - accuracy: 0.8765 - val_loss: 0.4680 - val_accuracy: 0.8060\nEpoch 30/100\n63/63 [==============================] - 8s 132ms/step - loss: 0.2563 - accuracy: 0.8930 - val_loss: 0.4571 - val_accuracy: 0.8300\nEpoch 31/100\n63/63 [==============================] - 9s 137ms/step - loss: 0.2493 - accuracy: 0.8975 - val_loss: 0.6176 - val_accuracy: 0.7950\nEpoch 32/100\n63/63 [==============================] - 9s 143ms/step - loss: 0.2541 - accuracy: 0.8915 - val_loss: 0.4824 - val_accuracy: 0.8160\nEpoch 33/100\n63/63 [==============================] - 9s 144ms/step - loss: 0.2409 - accuracy: 0.8985 - val_loss: 0.4527 - val_accuracy: 0.8290\nEpoch 34/100\n63/63 [==============================] - 8s 121ms/step - loss: 0.2577 - accuracy: 0.8920 - val_loss: 0.4982 - val_accuracy: 0.8150\nEpoch 35/100\n63/63 [==============================] - 9s 134ms/step - loss: 0.2348 - accuracy: 0.8990 - val_loss: 0.4781 - val_accuracy: 0.8120\nEpoch 36/100\n63/63 [==============================] - 9s 139ms/step - loss: 0.2397 - accuracy: 0.9025 - val_loss: 0.4891 - val_accuracy: 0.8260\nEpoch 37/100\n63/63 [==============================] - 9s 141ms/step - loss: 0.2438 - accuracy: 0.8985 - val_loss: 0.4427 - val_accuracy: 0.8340\nEpoch 38/100\n63/63 [==============================] - 9s 137ms/step - loss: 0.2367 - accuracy: 0.9015 - val_loss: 0.4569 - val_accuracy: 0.8270\nEpoch 39/100\n63/63 [==============================] - 8s 127ms/step - loss: 0.2253 - accuracy: 0.9065 - val_loss: 0.4109 - val_accuracy: 0.8470\nEpoch 40/100\n63/63 [==============================] - 9s 137ms/step - loss: 0.2045 - accuracy: 0.9130 - val_loss: 0.4235 - val_accuracy: 0.8470\nEpoch 41/100\n63/63 [==============================] - 9s 144ms/step - loss: 0.2236 - accuracy: 0.9045 - val_loss: 0.4385 - val_accuracy: 0.8420\nEpoch 42/100\n63/63 [==============================] - 9s 143ms/step - loss: 0.1933 - accuracy: 0.9275 - val_loss: 0.5287 - val_accuracy: 0.8320\nEpoch 43/100\n63/63 [==============================] - 9s 136ms/step - loss: 0.2219 - accuracy: 0.9090 - val_loss: 0.4559 - val_accuracy: 0.8440\nEpoch 44/100\n63/63 [==============================] - 8s 133ms/step - loss: 0.1803 - accuracy: 0.9215 - val_loss: 0.5139 - val_accuracy: 0.8280\nEpoch 45/100\n63/63 [==============================] - 9s 139ms/step - loss: 0.1893 - accuracy: 0.9245 - val_loss: 0.4727 - val_accuracy: 0.8360\nEpoch 46/100\n63/63 [==============================] - 9s 143ms/step - loss: 0.1772 - accuracy: 0.9285 - val_loss: 0.4141 - val_accuracy: 0.8430\nEpoch 47/100\n63/63 [==============================] - 8s 124ms/step - loss: 0.1971 - accuracy: 0.9200 - val_loss: 0.5258 - val_accuracy: 0.8260\nEpoch 48/100\n63/63 [==============================] - 8s 127ms/step - loss: 0.1634 - accuracy: 0.9390 - val_loss: 0.4343 - val_accuracy: 0.8430\nEpoch 49/100\n63/63 [==============================] - 9s 140ms/step - loss: 0.1763 - accuracy: 0.9350 - val_loss: 0.5136 - val_accuracy: 0.8230\nEpoch 50/100\n63/63 [==============================] - 10s 152ms/step - loss: 0.1960 - accuracy: 0.9290 - val_loss: 0.4748 - val_accuracy: 0.8390\nEpoch 51/100\n63/63 [==============================] - 9s 140ms/step - loss: 0.1777 - accuracy: 0.9345 - val_loss: 0.4070 - val_accuracy: 0.8460\nEpoch 52/100\n63/63 [==============================] - 8s 121ms/step - loss: 0.1690 - accuracy: 0.9380 - val_loss: 0.3959 - val_accuracy: 0.8650\nEpoch 53/100\n63/63 [==============================] - 9s 150ms/step - loss: 0.1611 - accuracy: 0.9365 - val_loss: 0.4191 - val_accuracy: 0.8410\nEpoch 54/100\n63/63 [==============================] - 9s 140ms/step - loss: 0.1632 - accuracy: 0.9370 - val_loss: 0.4653 - val_accuracy: 0.8400\nEpoch 55/100\n63/63 [==============================] - 9s 147ms/step - loss: 0.1663 - accuracy: 0.9370 - val_loss: 0.4228 - val_accuracy: 0.8450\nEpoch 56/100\n63/63 [==============================] - 8s 130ms/step - loss: 0.1682 - accuracy: 0.9365 - val_loss: 0.4226 - val_accuracy: 0.8620\nEpoch 57/100\n63/63 [==============================] - 9s 145ms/step - loss: 0.1786 - accuracy: 0.9330 - val_loss: 0.4025 - val_accuracy: 0.8540\nEpoch 58/100\n63/63 [==============================] - 9s 142ms/step - loss: 0.1631 - accuracy: 0.9315 - val_loss: 0.5048 - val_accuracy: 0.8470\nEpoch 59/100\n63/63 [==============================] - 8s 127ms/step - loss: 0.1805 - accuracy: 0.9345 - val_loss: 0.3656 - val_accuracy: 0.8670\nEpoch 60/100\n63/63 [==============================] - 8s 127ms/step - loss: 0.1471 - accuracy: 0.9445 - val_loss: 0.4302 - val_accuracy: 0.8640\nEpoch 61/100\n63/63 [==============================] - 9s 141ms/step - loss: 0.1488 - accuracy: 0.9450 - val_loss: 0.3900 - val_accuracy: 0.8590\nEpoch 62/100\n63/63 [==============================] - 9s 141ms/step - loss: 0.1440 - accuracy: 0.9430 - val_loss: 0.3936 - val_accuracy: 0.8630\nEpoch 63/100\n63/63 [==============================] - 9s 142ms/step - loss: 0.1409 - accuracy: 0.9410 - val_loss: 0.4573 - val_accuracy: 0.8580\nEpoch 64/100\n63/63 [==============================] - 8s 128ms/step - loss: 0.1457 - accuracy: 0.9485 - val_loss: 0.4402 - val_accuracy: 0.8490\nEpoch 65/100\n63/63 [==============================] - 9s 145ms/step - loss: 0.1386 - accuracy: 0.9460 - val_loss: 0.4324 - val_accuracy: 0.8630\nEpoch 66/100\n63/63 [==============================] - 9s 145ms/step - loss: 0.1310 - accuracy: 0.9505 - val_loss: 0.3636 - val_accuracy: 0.8700\nEpoch 67/100\n63/63 [==============================] - 9s 143ms/step - loss: 0.1352 - accuracy: 0.9435 - val_loss: 0.3753 - val_accuracy: 0.8720\nEpoch 68/100\n63/63 [==============================] - 8s 124ms/step - loss: 0.1389 - accuracy: 0.9480 - val_loss: 0.4104 - val_accuracy: 0.8750\nEpoch 69/100\n63/63 [==============================] - 8s 128ms/step - loss: 0.1286 - accuracy: 0.9465 - val_loss: 0.4575 - val_accuracy: 0.8600\nEpoch 70/100\n63/63 [==============================] - 9s 143ms/step - loss: 0.1556 - accuracy: 0.9395 - val_loss: 0.4220 - val_accuracy: 0.8530\nEpoch 71/100\n63/63 [==============================] - 9s 141ms/step - loss: 0.1274 - accuracy: 0.9525 - val_loss: 0.4345 - val_accuracy: 0.8610\nEpoch 72/100\n63/63 [==============================] - 9s 144ms/step - loss: 0.1249 - accuracy: 0.9490 - val_loss: 0.4315 - val_accuracy: 0.8640\nEpoch 73/100\n63/63 [==============================] - 8s 131ms/step - loss: 0.1341 - accuracy: 0.9485 - val_loss: 0.4829 - val_accuracy: 0.8470\nEpoch 74/100\n63/63 [==============================] - 9s 141ms/step - loss: 0.1572 - accuracy: 0.9405 - val_loss: 0.3853 - val_accuracy: 0.8750\nEpoch 75/100\n63/63 [==============================] - 9s 140ms/step - loss: 0.1108 - accuracy: 0.9555 - val_loss: 0.4100 - val_accuracy: 0.8680\nEpoch 76/100\n63/63 [==============================] - 9s 142ms/step - loss: 0.1035 - accuracy: 0.9630 - val_loss: 0.5376 - val_accuracy: 0.8460\nEpoch 77/100\n63/63 [==============================] - 9s 136ms/step - loss: 0.1522 - accuracy: 0.9445 - val_loss: 0.3999 - val_accuracy: 0.8690\nEpoch 78/100\n63/63 [==============================] - 8s 128ms/step - loss: 0.1297 - accuracy: 0.9535 - val_loss: 0.4802 - val_accuracy: 0.8660\nEpoch 79/100\n63/63 [==============================] - 9s 141ms/step - loss: 0.1291 - accuracy: 0.9475 - val_loss: 0.5872 - val_accuracy: 0.8380\nEpoch 80/100\n63/63 [==============================] - 9s 140ms/step - loss: 0.1327 - accuracy: 0.9545 - val_loss: 0.4580 - val_accuracy: 0.8580\nEpoch 81/100\n63/63 [==============================] - 9s 141ms/step - loss: 0.1271 - accuracy: 0.9515 - val_loss: 0.3999 - val_accuracy: 0.8610\nEpoch 82/100\n63/63 [==============================] - 8s 130ms/step - loss: 0.1348 - accuracy: 0.9505 - val_loss: 0.3535 - val_accuracy: 0.8780\nEpoch 83/100\n63/63 [==============================] - 8s 125ms/step - loss: 0.0942 - accuracy: 0.9655 - val_loss: 0.4357 - val_accuracy: 0.8770\nEpoch 84/100\n63/63 [==============================] - 9s 137ms/step - loss: 0.1326 - accuracy: 0.9480 - val_loss: 0.4624 - val_accuracy: 0.8500\nEpoch 85/100\n63/63 [==============================] - 9s 142ms/step - loss: 0.1167 - accuracy: 0.9565 - val_loss: 0.4311 - val_accuracy: 0.8640\nEpoch 86/100\n63/63 [==============================] - 9s 142ms/step - loss: 0.1030 - accuracy: 0.9590 - val_loss: 0.3998 - val_accuracy: 0.8700\nEpoch 87/100\n63/63 [==============================] - 8s 125ms/step - loss: 0.1291 - accuracy: 0.9560 - val_loss: 0.4305 - val_accuracy: 0.8700\nEpoch 88/100\n63/63 [==============================] - 9s 140ms/step - loss: 0.1088 - accuracy: 0.9600 - val_loss: 0.4429 - val_accuracy: 0.8550\nEpoch 89/100\n63/63 [==============================] - 9s 141ms/step - loss: 0.1325 - accuracy: 0.9515 - val_loss: 0.4198 - val_accuracy: 0.8580\nEpoch 90/100\n63/63 [==============================] - 8s 131ms/step - loss: 0.1224 - accuracy: 0.9500 - val_loss: 0.5554 - val_accuracy: 0.8440\nEpoch 91/100\n63/63 [==============================] - 8s 126ms/step - loss: 0.1121 - accuracy: 0.9580 - val_loss: 0.3839 - val_accuracy: 0.8760\nEpoch 92/100\n63/63 [==============================] - 8s 132ms/step - loss: 0.1201 - accuracy: 0.9595 - val_loss: 0.4195 - val_accuracy: 0.8500\nEpoch 93/100\n63/63 [==============================] - 9s 142ms/step - loss: 0.1024 - accuracy: 0.9615 - val_loss: 0.4324 - val_accuracy: 0.8500\nEpoch 94/100\n63/63 [==============================] - 9s 142ms/step - loss: 0.1027 - accuracy: 0.9600 - val_loss: 0.3997 - val_accuracy: 0.8690\nEpoch 95/100\n63/63 [==============================] - 8s 125ms/step - loss: 0.1043 - accuracy: 0.9620 - val_loss: 0.4063 - val_accuracy: 0.8540\nEpoch 96/100\n63/63 [==============================] - 9s 140ms/step - loss: 0.1113 - accuracy: 0.9590 - val_loss: 0.4976 - val_accuracy: 0.8400\nEpoch 97/100\n63/63 [==============================] - 9s 137ms/step - loss: 0.0791 - accuracy: 0.9705 - val_loss: 0.3820 - val_accuracy: 0.8780\nEpoch 98/100\n63/63 [==============================] - 8s 126ms/step - loss: 0.1071 - accuracy: 0.9630 - val_loss: 0.4587 - val_accuracy: 0.8560\nEpoch 99/100\n63/63 [==============================] - 9s 140ms/step - loss: 0.1193 - accuracy: 0.9515 - val_loss: 0.3842 - val_accuracy: 0.8690\nEpoch 100/100\n63/63 [==============================] - 9s 142ms/step - loss: 0.1382 - accuracy: 0.9510 - val_loss: 0.4258 - val_accuracy: 0.8620\n\n\nLet’s plot the results again: Thanks to data augmentation and dropout, we start overfitting much later, around epochs 60–70 (compared to epoch 10 for the original model). The validation accuracy ends up consistently in the 80–85% range—a big improvement over our first try.\n\naccuracy = history.history[\"accuracy\"]\nval_accuracy = history.history[\"val_accuracy\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nepochs = range(1, len(accuracy) + 1)\nplt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\nplt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nLet’s check the test accuracy.\n\ntest_model = tf.keras.models.load_model(\n    \"convnet_from_scratch_with_augmentation.keras\")\ntest_loss, test_acc = test_model.evaluate(test_alb)\nprint(f\"Test accuracy: {test_acc:.3f}\")\n\n63/63 [==============================] - 4s 60ms/step - loss: 0.4295 - accuracy: 0.8550\nTest accuracy: 0.855\n\n\nWe get a test accuracy over 85%. It’s starting to look good! By further tuning the model’s configuration (such as the number of filters per convolution layer, or the number of layers in the model), we might be able to get an even better accuracy, likely up to 90%. But it would prove difficult to go any higher just by training our own convnet from scratch, because we have so little data to work with. As a next step to improve our accuracy on this problem, we’ll have to use a pretrained model as we will see later on."
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_tensorflow.html#object-detection-with-kerascv-optional",
    "href": "09_Convolutional_NeuralNetworks_tensorflow.html#object-detection-with-kerascv-optional",
    "title": "9  Image processing with Convolutional Neural Networks - Tensorflow",
    "section": "9.7 Object Detection with KerasCV (Optional)",
    "text": "9.7 Object Detection with KerasCV (Optional)\nKerasCV offers a complete set of production grade APIs to solve object detection problems. These APIs include object detection specific data augmentation techniques, Keras native COCO metrics, bounding box format conversion utilities, visualization tools.\nWhether you’re an object detection amateur or a well seasoned veteran, assembling an object detection pipeline from scratch is a massive undertaking. Luckily, all KerasCV object detection APIs are built as modular components. Whether you need a complete pipeline, just an object detection model, or even just a conversion utility to transform your boxes from xywh format to xyxy, KerasCV has you covered.\nTo get started, let’s sort out all of our imports and define global configuration parameters.\n\nBATCH_SIZE = 4\n\n\n9.7.1 Data loading\nTo get started, let’s discuss data loading and bounding box formatting. KerasCV has a predefined format for bounding boxes. To comply with this, you should package your bounding boxes into a dictionary matching the specification below:\nbounding_boxes = {\n    # num_boxes may be a Ragged dimension\n    'boxes': Tensor(shape=[batch, num_boxes, 4]),\n    'classes': Tensor(shape=[batch, num_boxes])\n}\nTo match the KerasCV API style, it is recommended that when writing a custom data loader, you also support a bounding_box_format argument. This makes it clear to those invoking your data loader what format the bounding boxes are in. In this example, we format our boxes to xywh format.\n\ntrain_ds = load_pascal_voc(\n    split=\"train\", dataset=\"voc/2007\", bounding_box_format=\"xywh\"\n)\neval_ds = load_pascal_voc(split=\"test\", dataset=\"voc/2007\", bounding_box_format=\"xywh\")\n\ntrain_ds = train_ds.shuffle(BATCH_SIZE * 4)\n\nDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/voc/2007/4.0.0...\n\n\n\n\n\n\n\n\n\n\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset voc downloaded and prepared to /root/tensorflow_datasets/voc/2007/4.0.0. Subsequent calls will reuse this data.\n\n\nWARNING:absl:`TensorInfo.dtype` is deprecated. Please change your code to use NumPy with the field `TensorInfo.np_dtype` or use TensorFlow with the field `TensorInfo.tf_dtype`.\nWARNING:absl:`TensorInfo.dtype` is deprecated. Please change your code to use NumPy with the field `TensorInfo.np_dtype` or use TensorFlow with the field `TensorInfo.tf_dtype`.\nWARNING:absl:`TensorInfo.dtype` is deprecated. Please change your code to use NumPy with the field `TensorInfo.np_dtype` or use TensorFlow with the field `TensorInfo.tf_dtype`.\n\n\nNext, let’s batch our data. In KerasCV object detection tasks it is recommended that users use ragged batches of inputs. This is due to the fact that images may be of different sizes in PascalVOC, as well as the fact that there may be different numbers of bounding boxes per image.\n\ntrain_ds, len(train_ds)\n\n(&lt;_ShuffleDataset element_spec={'images': TensorSpec(shape=(None, None, 3), dtype=tf.float32, name=None), 'bounding_boxes': {'classes': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'boxes': TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)}}&gt;,\n 2501)\n\n\n\neval_ds, len(eval_ds)\n\n(&lt;_ParallelMapDataset element_spec={'images': TensorSpec(shape=(None, None, 3), dtype=tf.float32, name=None), 'bounding_boxes': {'classes': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'boxes': TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)}}&gt;,\n 4952)\n\n\n\n\n9.7.2 Data augmentation\nOne of the most challenging tasks when constructing object detection pipelines is data augmentation. Image augmentation techniques must be aware of the underlying bounding boxes, and must update them accordingly.\nLuckily, KerasCV natively supports bounding box augmentation with its extensive library of data augmentation layers. The code below loads the Pascal VOC dataset, and performs on-the-fly bounding box friendly data augmentation inside of a tf.data pipeline.\n\naugmenter = tf.keras.Sequential(\n    layers=[\n        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xywh\"),\n        keras_cv.layers.JitteredResize(\n            target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xywh\"\n        ),\n    ]\n)\n\ntrain_ds = train_ds.apply(\n    tf.data.experimental.dense_to_ragged_batch(BATCH_SIZE)\n)\ntrain_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\nvisualize_dataset(\n    train_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2\n)\n\nWARNING:tensorflow:From dense_to_ragged_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.ragged_batch` instead.\nWARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'images': tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"RaggedFromVariant_2/RaggedTensorFromVariant:2\", shape=(None, 3), dtype=float32), row_splits=Tensor(\"RaggedFromVariant_2/RaggedTensorFromVariant:1\", shape=(None,), dtype=int64)), row_splits=Tensor(\"RaggedFromVariant_2/RaggedTensorFromVariant:0\", shape=(None,), dtype=int64)), 'bounding_boxes': {'classes': tf.RaggedTensor(values=Tensor(\"RaggedFromVariant_1/RaggedTensorFromVariant:1\", shape=(None,), dtype=float32), row_splits=Tensor(\"RaggedFromVariant_1/RaggedTensorFromVariant:0\", shape=(None,), dtype=int64)), 'boxes': tf.RaggedTensor(values=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:1\", shape=(None, 4), dtype=float32), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(None,), dtype=int64))}}. Consider rewriting this model with the Functional API.\n\n\n\n\n\nGreat! We now have a bounding box friendly data augmentation pipeline. Let’s format our evaluation dataset to match. Instead of using JitteredResize, let’s use the deterministic keras_cv.layers.Resizing() layer.\nDue to the fact that the resize operation differs between the train dataset, which uses JitteredResize() to resize images, and the inference dataset, which uses layers.Resizing(pad_to_aspect_ratio=True). it is good practice to visualize both datasets:\n\ninference_resizing = keras_cv.layers.Resizing(\n    640, 640, bounding_box_format=\"xywh\", pad_to_aspect_ratio=True\n)\neval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)\neval_ds = eval_ds.apply(tf.data.experimental.dense_to_ragged_batch(BATCH_SIZE))\n\n\nvisualize_dataset(\n    eval_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2\n)\n\n\n\n\nFinally, let’s unpackage our inputs from the preprocessing dictionary, and prepare to feed the inputs into our model. If training on GPU, you can omit the bounding_box.to_dense() call. If ommitted, the KerasCV RetinaNet label encoder will automatically correctly encode Ragged training targets.\nTo construct a ragged dataset in a tf.data pipeline, you can use the ragged_batch() method.\n\ndef dict_to_tuple(inputs):\n    return inputs[\"images\"], bounding_box.to_dense(\n        inputs[\"bounding_boxes\"], max_boxes=32\n    )\n\n\n\ntrain_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\neval_ds = eval_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n\n\ntrain_ds = train_ds.prefetch(tf.data.AUTOTUNE)\neval_ds = eval_ds.prefetch(tf.data.AUTOTUNE)\n\n\neval_ds\n\n&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 640, 640, 3), dtype=tf.float32, name=None), {'boxes': TensorSpec(shape=(None, 32, 4), dtype=tf.float32, name=None), 'classes': TensorSpec(shape=(None, 32), dtype=tf.float32, name=None)})&gt;\n\n\n\ntrain_ds\n\n&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 640, 640, 3), dtype=tf.float32, name=None), {'classes': TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), 'boxes': TensorSpec(shape=(None, 32, 4), dtype=tf.float32, name=None)})&gt;\n\n\nYou will always want to include a global_clipnorm when training object detection models. This is to remedy exploding gradient problems that frequently occur when training object detection models.\n\nbase_lr = 0.005\n# including a global_clipnorm is extremely important in object detection tasks\noptimizer = tf.keras.optimizers.SGD(\n    learning_rate=base_lr, momentum=0.9, global_clipnorm=10.0\n)\n\n\n\n9.7.3 Model creation\nNext, let’s use the KerasCV API to construct an untrained RetinaNet model. In this tutorial we using a pretrained ResNet50 backbone from the imagenet dataset.\nKerasCV makes it easy to construct a RetinaNet with any of the KerasCV backbones. Simply use one of the presets for the architecture you’d like!\n\nmodel = keras_cv.models.RetinaNet.from_preset(\n    \"resnet50_imagenet\",\n    num_classes=len(class_mapping),\n    # For more info on supported bounding box formats, visit\n    # https://keras.io/api/keras_cv/bounding_box/\n    bounding_box_format=\"xywh\",\n)\n\nDownloading data from https://storage.googleapis.com/keras-cv/models/resnet50/imagenet/classification-v0-notop.h5\n94657128/94657128 [==============================] - 7s 0us/step\n\n\nNow, we are going to compile our model. You may not be familiar with the “focal” or “smoothl1” losses. While not common in other models, these losses are more or less staples in the object detection world.\nIn short, “Focal Loss” places extra emphasis on difficult training examples. This is useful when training the classification loss, as the majority of the losses are assigned to the background class. “SmoothL1 Loss” is used to prevent exploding gradients that often occur when attempting to perform the box regression task.\nIn KerasCV you can use these losses simply by passing the strings “focal” and “smoothl1” to compile():\n\nmodel.compile(\n    classification_loss=\"focal\",\n    box_loss=\"smoothl1\",\n    optimizer=optimizer,\n    # We will use our custom callback to evaluate COCO metrics\n    metrics=None,\n)\n\n\nmodel.summary()\n\nModel: \"retina_net\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, None, None,  0           []                               \n                                 3)]                                                              \n                                                                                                  \n model (Functional)             {3: (None, None, No  23561152    ['input_2[0][0]']                \n                                ne, 512),                                                         \n                                 4: (None, None, No                                               \n                                ne, 1024),                                                        \n                                 5: (None, None, No                                               \n                                ne, 2048)}                                                        \n                                                                                                  \n feature_pyramid (FeaturePyrami  ((None, None, None,  7997440    ['model[0][0]',                  \n d)                              256),                            'model[0][1]',                  \n                                 (None, None, None,               'model[0][2]']                  \n                                 256),                                                            \n                                 (None, None, None,                                               \n                                 256),                                                            \n                                 (None, None, None,                                               \n                                 256),                                                            \n                                 (None, None, None,                                               \n                                 256))                                                            \n                                                                                                  \n tf.compat.v1.shape (TFOpLambda  (4,)                0           ['input_2[0][0]']                \n )                                                                                                \n                                                                                                  \n prediction_head_1 (PredictionH  (None, None, None,   1853220    ['feature_pyramid[0][0]',        \n ead)                           36)                               'feature_pyramid[0][1]',        \n                                                                  'feature_pyramid[0][2]',        \n                                                                  'feature_pyramid[0][3]',        \n                                                                  'feature_pyramid[0][4]']        \n                                                                                                  \n tf.__operators__.getitem (Slic  ()                  0           ['tf.compat.v1.shape[0][0]']     \n ingOpLambda)                                                                                     \n                                                                                                  \n prediction_head (PredictionHea  (None, None, None,   2205885    ['feature_pyramid[0][0]',        \n d)                             189)                              'feature_pyramid[0][1]',        \n                                                                  'feature_pyramid[0][2]',        \n                                                                  'feature_pyramid[0][3]',        \n                                                                  'feature_pyramid[0][4]']        \n                                                                                                  \n tf.reshape (TFOpLambda)        (None, None, 4)      0           ['prediction_head_1[0][0]',      \n                                                                  'tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n tf.reshape_2 (TFOpLambda)      (None, None, 4)      0           ['prediction_head_1[1][0]',      \n                                                                  'tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n tf.reshape_4 (TFOpLambda)      (None, None, 4)      0           ['prediction_head_1[2][0]',      \n                                                                  'tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n tf.reshape_6 (TFOpLambda)      (None, None, 4)      0           ['prediction_head_1[3][0]',      \n                                                                  'tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n tf.reshape_8 (TFOpLambda)      (None, None, 4)      0           ['prediction_head_1[4][0]',      \n                                                                  'tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n tf.reshape_1 (TFOpLambda)      (None, None, 21)     0           ['prediction_head[0][0]',        \n                                                                  'tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n tf.reshape_3 (TFOpLambda)      (None, None, 21)     0           ['prediction_head[1][0]',        \n                                                                  'tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n tf.reshape_5 (TFOpLambda)      (None, None, 21)     0           ['prediction_head[2][0]',        \n                                                                  'tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n tf.reshape_7 (TFOpLambda)      (None, None, 21)     0           ['prediction_head[3][0]',        \n                                                                  'tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n tf.reshape_9 (TFOpLambda)      (None, None, 21)     0           ['prediction_head[4][0]',        \n                                                                  'tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n box (Concatenate)              (None, None, 4)      0           ['tf.reshape[0][0]',             \n                                                                  'tf.reshape_2[0][0]',           \n                                                                  'tf.reshape_4[0][0]',           \n                                                                  'tf.reshape_6[0][0]',           \n                                                                  'tf.reshape_8[0][0]']           \n                                                                                                  \n classification (Concatenate)   (None, None, 21)     0           ['tf.reshape_1[0][0]',           \n                                                                  'tf.reshape_3[0][0]',           \n                                                                  'tf.reshape_5[0][0]',           \n                                                                  'tf.reshape_7[0][0]',           \n                                                                  'tf.reshape_9[0][0]']           \n                                                                                                  \n retina_net_label_encoder (Reti  multiple            0           []                               \n naNetLabelEncoder)                                                                               \n                                                                                                  \n anchor_generator (AnchorGenera  multiple            0           []                               \n tor)                                                                                             \n                                                                                                  \n res_net_backbone (ResNetBackbo  (None, None, None,   23561152   []                               \n ne)                            2048)                                                             \n                                                                                                  \n multi_class_non_max_suppressio  multiple            0           []                               \n n (MultiClassNonMaxSuppression                                                                   \n )                                                                                                \n                                                                                                  \n==================================================================================================\nTotal params: 35,617,697\nTrainable params: 35,564,577\nNon-trainable params: 53,120\n__________________________________________________________________________________________________\n\n\n\n\n9.7.4 Training our model\n\ntrain_ds, len(train_ds)\n\n(&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 640, 640, 3), dtype=tf.float32, name=None), {'classes': TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), 'boxes': TensorSpec(shape=(None, 32, 4), dtype=tf.float32, name=None)})&gt;,\n 626)\n\n\n\neval_ds, len(eval_ds)\n\n(&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 640, 640, 3), dtype=tf.float32, name=None), {'boxes': TensorSpec(shape=(None, 32, 4), dtype=tf.float32, name=None), 'classes': TensorSpec(shape=(None, 32), dtype=tf.float32, name=None)})&gt;,\n 1238)\n\n\n\nmodel.fit(\n    train_ds,\n    validation_data=eval_ds,\n    # Run for 10-35 epochs to achieve good scores.\n    epochs=10\n)\n\nEpoch 1/10\n626/626 [==============================] - 57s 91ms/step - loss: 0.9977 - box_loss: 0.4259 - classification_loss: 0.5718 - percent_boxes_matched_with_anchor: 0.9021 - val_loss: 0.9684 - val_box_loss: 0.4499 - val_classification_loss: 0.5185 - val_percent_boxes_matched_with_anchor: 0.8949\nEpoch 2/10\n626/626 [==============================] - 55s 87ms/step - loss: 0.8400 - box_loss: 0.3633 - classification_loss: 0.4767 - percent_boxes_matched_with_anchor: 0.9021 - val_loss: 0.8285 - val_box_loss: 0.3869 - val_classification_loss: 0.4416 - val_percent_boxes_matched_with_anchor: 0.8996\nEpoch 3/10\n626/626 [==============================] - 55s 87ms/step - loss: 0.7447 - box_loss: 0.3239 - classification_loss: 0.4208 - percent_boxes_matched_with_anchor: 0.9021 - val_loss: 0.9192 - val_box_loss: 0.3610 - val_classification_loss: 0.5582 - val_percent_boxes_matched_with_anchor: 0.9086\nEpoch 4/10\n626/626 [==============================] - 54s 87ms/step - loss: 0.6735 - box_loss: 0.2980 - classification_loss: 0.3755 - percent_boxes_matched_with_anchor: 0.9021 - val_loss: 0.7132 - val_box_loss: 0.3457 - val_classification_loss: 0.3675 - val_percent_boxes_matched_with_anchor: 0.9094\nEpoch 5/10\n626/626 [==============================] - 55s 87ms/step - loss: 0.6320 - box_loss: 0.2831 - classification_loss: 0.3489 - percent_boxes_matched_with_anchor: 0.9021 - val_loss: 0.7260 - val_box_loss: 0.3522 - val_classification_loss: 0.3737 - val_percent_boxes_matched_with_anchor: 0.9004\nEpoch 6/10\n626/626 [==============================] - 55s 87ms/step - loss: 0.5797 - box_loss: 0.2665 - classification_loss: 0.3133 - percent_boxes_matched_with_anchor: 0.9021 - val_loss: 0.6881 - val_box_loss: 0.3197 - val_classification_loss: 0.3684 - val_percent_boxes_matched_with_anchor: 0.8938\nEpoch 7/10\n626/626 [==============================] - 55s 87ms/step - loss: 0.5532 - box_loss: 0.2534 - classification_loss: 0.2998 - percent_boxes_matched_with_anchor: 0.9021 - val_loss: 0.6940 - val_box_loss: 0.3223 - val_classification_loss: 0.3718 - val_percent_boxes_matched_with_anchor: 0.9043\nEpoch 8/10\n626/626 [==============================] - 55s 87ms/step - loss: 0.5136 - box_loss: 0.2391 - classification_loss: 0.2745 - percent_boxes_matched_with_anchor: 0.9021 - val_loss: 0.6358 - val_box_loss: 0.2984 - val_classification_loss: 0.3375 - val_percent_boxes_matched_with_anchor: 0.9047\nEpoch 9/10\n626/626 [==============================] - 55s 87ms/step - loss: 0.4930 - box_loss: 0.2300 - classification_loss: 0.2630 - percent_boxes_matched_with_anchor: 0.9021 - val_loss: 0.6383 - val_box_loss: 0.3029 - val_classification_loss: 0.3354 - val_percent_boxes_matched_with_anchor: 0.8996\nEpoch 10/10\n626/626 [==============================] - 55s 87ms/step - loss: 0.4591 - box_loss: 0.2192 - classification_loss: 0.2399 - percent_boxes_matched_with_anchor: 0.9021 - val_loss: 0.5922 - val_box_loss: 0.2926 - val_classification_loss: 0.2996 - val_percent_boxes_matched_with_anchor: 0.9020\n\n\n&lt;keras.callbacks.History at 0x7f87d45f5510&gt;\n\n\n\n\n9.7.5 Inference and plotting results\n\nvisualization_ds = eval_ds.unbatch()\nvisualization_ds = visualization_ds.ragged_batch(16)\nvisualization_ds = visualization_ds.shuffle(8)\n\n\nvisualize_detections(model, dataset=visualization_ds, bounding_box_format=\"xywh\")\n\n1/1 [==============================] - 3s 3s/step\n\n\n\n\n\n\nmodel.prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression(\n    bounding_box_format=\"xywh\",\n    from_logits=True,\n    iou_threshold=0.5,\n    confidence_threshold=0.75,\n)\n\nvisualize_detections(model, dataset=visualization_ds, bounding_box_format=\"xywh\")\n\n1/1 [==============================] - 2s 2s/step"
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_tensorflow.html#image-segmentation-with-keras-optional",
    "href": "09_Convolutional_NeuralNetworks_tensorflow.html#image-segmentation-with-keras-optional",
    "title": "9  Image processing with Convolutional Neural Networks - Tensorflow",
    "section": "9.8 Image segmentation with Keras (Optional)",
    "text": "9.8 Image segmentation with Keras (Optional)\n\n9.8.1 Download the dataset\nFrom https://github.com/divamgupta/datasets/releases/\n\n!wget https://github.com/divamgupta/datasets/releases/download/seg/dataset1.zip -qq\n\n\n!unzip -qq dataset1.zip\n\nreplace __MACOSX/._dataset1? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n\n\n\n\n9.8.2 Initialize the model\n\nmodel = vgg_unet(n_classes=50 ,  input_height=320, input_width=640)\n\nDownloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58889256/58889256 [==============================] - 0s 0us/step\n\n\n\n\n9.8.3 Train the model\n\nmodel.train(\n    train_images =  \"dataset1/images_prepped_train/\",\n    train_annotations = \"dataset1/annotations_prepped_train/\",\n    checkpoints_path = \"/content/vgg_unet_1\" , epochs=5  \n)\n\nVerifying training dataset\n\n\n100%|██████████| 367/367 [00:02&lt;00:00, 156.06it/s]\n\n\nDataset verified! \nEpoch 1/5\n512/512 [==============================] - ETA: 0s - loss: 0.8406 - accuracy: 0.7546\nEpoch 1: saving model to /content/vgg_unet_1.00001\n512/512 [==============================] - 120s 192ms/step - loss: 0.8406 - accuracy: 0.7546\nEpoch 2/5\n512/512 [==============================] - ETA: 0s - loss: 0.4832 - accuracy: 0.8490\nEpoch 2: saving model to /content/vgg_unet_1.00002\n512/512 [==============================] - 100s 195ms/step - loss: 0.4832 - accuracy: 0.8490\nEpoch 3/5\n512/512 [==============================] - ETA: 0s - loss: 0.3920 - accuracy: 0.8755\nEpoch 3: saving model to /content/vgg_unet_1.00003\n512/512 [==============================] - 99s 194ms/step - loss: 0.3920 - accuracy: 0.8755\nEpoch 4/5\n512/512 [==============================] - ETA: 0s - loss: 0.3384 - accuracy: 0.8911\nEpoch 4: saving model to /content/vgg_unet_1.00004\n512/512 [==============================] - 96s 187ms/step - loss: 0.3384 - accuracy: 0.8911\nEpoch 5/5\n512/512 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.9050\nEpoch 5: saving model to /content/vgg_unet_1.00005\n512/512 [==============================] - 95s 185ms/step - loss: 0.2904 - accuracy: 0.9050\n\n\n\n\n9.8.4 Inference\n\nout = model.predict_segmentation(\n    inp=\"dataset1/images_prepped_test/0016E5_07965.png\",\n    out_fname=\"/content/out.png\"\n)\n\n1/1 [==============================] - 1s 1s/step\n\n\n\nin_img = cv2.imread(\"dataset1/images_prepped_test/0016E5_07965.png\")\nplt.imshow(in_img)\n\n&lt;matplotlib.image.AxesImage at 0x7fa158cf45b0&gt;\n\n\n\n\n\n\nplt.imshow(out)\n\n&lt;matplotlib.image.AxesImage at 0x7fa16014b310&gt;\n\n\n\n\n\nSee https://divamgupta.com/image-segmentation/2019/06/06/deep-learning-semantic-segmentation-keras.html for more information."
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_tensorflow.html#data-cleaning-with-cleanvision",
    "href": "09_Convolutional_NeuralNetworks_tensorflow.html#data-cleaning-with-cleanvision",
    "title": "9  Image processing with Convolutional Neural Networks - Tensorflow",
    "section": "9.9 Data cleaning with CleanVision",
    "text": "9.9 Data cleaning with CleanVision\nCleanVision is built to automatically detects various issues in image datasets. This data-centric AI package is designed as a quick first step for any computer vision project to find problems in your dataset, which you may want to address before applying machine learning. The following Issue Key column specifies the name for each type of issue in CleanVision code.\n\n\n\n\n\n\n\n\n\n\nIssue Type\nDescription\nIssue Key\n\n\n\n\n1\nLight\nImages that are too bright/washed out in the dataset\nlight\n\n\n2\nDark\nImages that are irregularly dark\ndark\n\n\n3\nOdd Aspect Ratio\nImages with an unusual aspect ratio (i.e. overly skinny/wide)\nodd_aspect_ratio\n\n\n4\nExact Duplicates\nImages that are exact duplicates of each other\nexact_duplicates\n\n\n5\nNear Duplicates\nImages that are almost visually identical to each other (e.g. same image with different filters)\nnear_duplicates\n\n\n6\nBlurry\nImages that are blurry or out of focus\nblurry\n\n\n7\nGrayscale\nImages that are grayscale (lacking color)\ngrayscale\n\n\n8\nLow Information\nImages that lack much information (e.g. a completely black image with a few white dots)\nlow_information\n\n\n\n\n!wget - nc 'https://cleanlab-public.s3.amazonaws.com/CleanVision/image_files.zip'\n!unzip -q image_files.zip\n\n--2023-04-29 05:47:34--  http://-/\nResolving - (-)... failed: Name or service not known.\nwget: unable to resolve host address ‘-’\n--2023-04-29 05:47:34--  http://nc/\nResolving nc (nc)... failed: No address associated with hostname.\nwget: unable to resolve host address ‘nc’\n--2023-04-29 05:47:34--  https://cleanlab-public.s3.amazonaws.com/CleanVision/image_files.zip\nResolving cleanlab-public.s3.amazonaws.com (cleanlab-public.s3.amazonaws.com)... 52.216.54.233, 54.231.201.145, 52.216.81.128, ...\nConnecting to cleanlab-public.s3.amazonaws.com (cleanlab-public.s3.amazonaws.com)|52.216.54.233|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 78293407 (75M) [application/zip]\nSaving to: ‘image_files.zip’\n\nimage_files.zip     100%[===================&gt;]  74.67M  56.9MB/s    in 1.3s    \n\n2023-04-29 05:47:36 (56.9 MB/s) - ‘image_files.zip’ saved [78293407/78293407]\n\nFINISHED --2023-04-29 05:47:36--\nTotal wall clock time: 1.8s\nDownloaded: 1 files, 75M in 1.3s (56.9 MB/s)\n\n\n\n# Path to your dataset, you can specify your own dataset path\ndataset_path = \"./image_files/\"\n\n# Initialize imagelab with your dataset\nimagelab = Imagelab(data_path=dataset_path)\n\n# Visualize a few sample images from the dataset\nimagelab.visualize(num_images=8)\n\nReading images from /content/image_files\nSample images from the dataset\n\n\n\n\n\n\n# Find issues\n# You can also specify issue types to detect, for example\n# issue_types = {\"dark\": {}}\n# imagelab.find_issues(issue_types)\nimagelab.find_issues()\n\nChecking for dark, light, odd_aspect_ratio, low_information, exact_duplicates, near_duplicates, blurry, grayscale images ...\n\n\n100%|██████████| 595/595 [00:08&lt;00:00, 67.68it/s]\n100%|██████████| 595/595 [00:03&lt;00:00, 178.71it/s]\n\n\nIssue checks completed. To see a detailed report of issues found, use imagelab.report().\n\n\n\n\n\nThe report() method helps you quickly understand the major issues detected in the dataset. It reports the number of images in the dataset that exhibit each type of issue, and shows example images corresponding to the most severe instances of each issue.\n\nimagelab.report()\n\nIssues found in order of severity in the dataset\n\n|    | issue_type       |   num_images |\n|---:|:-----------------|-------------:|\n|  0 | grayscale        |           20 |\n|  1 | near_duplicates  |           20 |\n|  2 | exact_duplicates |           19 |\n|  3 | dark             |           13 |\n|  4 | blurry           |           10 |\n|  5 | odd_aspect_ratio |            8 |\n|  6 | light            |            5 |\n|  7 | low_information  |            4 | \n\n\nTop 4 examples with grayscale issue in the dataset.\n\n\n\n\n\n\nTop 4 sets of images with near_duplicates issue\nSet: 0\n\n\n\n\n\nSet: 1\n\n\n\n\n\nSet: 2\n\n\n\n\n\nSet: 3\n\n\n\n\n\n\nTop 4 sets of images with exact_duplicates issue\nSet: 0\n\n\n\n\n\nSet: 1\n\n\n\n\n\nSet: 2\n\n\n\n\n\nSet: 3\n\n\n\n\n\n\nTop 4 examples with dark issue in the dataset.\n\n\n\n\n\n\nTop 4 examples with blurry issue in the dataset.\n\n\n\n\n\n\nTop 4 examples with odd_aspect_ratio issue in the dataset.\n\n\n\n\n\n\nTop 4 examples with light issue in the dataset.\n\n\n\n\n\n\nTop 4 examples with low_information issue in the dataset.\n\n\n\n\n\nThe main way to interface with your data is via the Imagelab class. This class can be used to understand the issues in your dataset at a high level (global overview) and low level (issues and quality scores for each image) as well as additional information about the dataset. It has three main attributes:\n\nImagelab.issue_summary\nImagelab.issues\nImagelab.info\n\n\n9.9.1 imagelab.issue_summary\nDataframe with global summary of all issue types detected in your dataset and the overall prevalence of each type.\nIn each row: - issue_type - name of the issue - num_images - number of images of that issue type found in the dataset\n\nimagelab.issue_summary\n\n\n  \n    \n      \n\n\n\n\n\n\nissue_type\nnum_images\n\n\n\n\n0\ngrayscale\n20\n\n\n1\nnear_duplicates\n20\n\n\n2\nexact_duplicates\n19\n\n\n3\ndark\n13\n\n\n4\nblurry\n10\n\n\n5\nodd_aspect_ratio\n8\n\n\n6\nlight\n5\n\n\n7\nlow_information\n4\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n9.9.2 imagelab.issues\nDataFrame assessing each image in your dataset, reporting which issues each image exhibits and a quality score for each type of issue.\n\nimagelab.issues.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nodd_aspect_ratio_score\nis_odd_aspect_ratio_issue\nlow_information_score\nis_low_information_issue\nlight_score\nis_light_issue\ngrayscale_score\nis_grayscale_issue\ndark_score\nis_dark_issue\nblurry_score\nis_blurry_issue\nis_exact_duplicates_issue\nis_near_duplicates_issue\n\n\n\n\n/content/image_files/image_0.png\n1.0\nFalse\n0.806332\nFalse\n0.925490\nFalse\n1\nFalse\n1.000000\nFalse\n0.373038\nFalse\nFalse\nFalse\n\n\n/content/image_files/image_1.png\n1.0\nFalse\n0.923116\nFalse\n0.906609\nFalse\n1\nFalse\n0.990676\nFalse\n0.345064\nFalse\nFalse\nFalse\n\n\n/content/image_files/image_10.png\n1.0\nFalse\n0.875129\nFalse\n0.995127\nFalse\n1\nFalse\n0.795937\nFalse\n0.534317\nFalse\nFalse\nFalse\n\n\n/content/image_files/image_100.png\n1.0\nFalse\n0.916140\nFalse\n0.889762\nFalse\n1\nFalse\n0.827587\nFalse\n0.494283\nFalse\nFalse\nFalse\n\n\n/content/image_files/image_101.png\n1.0\nFalse\n0.779338\nFalse\n0.960784\nFalse\n0\nTrue\n0.992157\nFalse\n0.471333\nFalse\nFalse\nFalse\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThere is a Boolean column for each issue type, showing whether each image exhibits that type of issue or not. For example, the rows where the is_dark_issue column contains True, those rows correspond to images that appear too dark. For the dark issue type (and more generally for other types of issues), there is a numeric column dark_score, which assesses how severe this issue is in each image. These quality scores lie between 0 and 1, where lower values indicate more severe instances of the issue (images which are darker in this example).\nOne use-case for imagelab.issues is to filter out all images exhibiting one particular type of issue and rank them by their quality score. Here’s how to get all blurry images ranked by their blurry_score, note lower scores indicate higher severity:\n\nblurry_images = imagelab.issues[imagelab.issues[\"is_blurry_issue\"] == True].sort_values(by=['blurry_score'])\nblurry_image_files = blurry_images.index.tolist()\n\n\nimagelab.visualize(image_files=blurry_image_files[:4])\n\n\n\n\nThe imagelab.visualize() also allows you can use to see examples of specific issues in your dataset. num_images and cell_size are optional arguments, that you can use to control number of examples of each issue type and size of each image in the grid respectively.\n\nissue_types = [\"grayscale\"]\nimagelab.visualize(issue_types=issue_types, num_images=8, cell_size=(3, 3))\n\n\nTop 8 examples with grayscale issue in the dataset.\n\n\n\n\n\n\n\n9.9.3 imagelab.info\nThis is a nested dictionary containing statistics about the images and other miscellaneous information stored while checking for issues in the dataset Possible keys in this dict are statistics and a key corresponding to each issue type\n\nimagelab.info.keys()\n\ndict_keys(['statistics', 'dark', 'light', 'odd_aspect_ratio', 'low_information', 'blurry', 'grayscale', 'exact_duplicates', 'near_duplicates'])\n\n\nimagelab.info['statistics'] is also a dict containing statistics calculated on images that are used for checking for issues in the dataset.\n\nimagelab.info['statistics'].keys()\n\ndict_keys(['brightness', 'aspect_ratio', 'entropy', 'blurriness', 'color_space'])\n\n\nimagelab.info can also be used to retrieve which images are near or exact duplicates of each other. issue.summary shows the number of exact duplicate images but does not show how many such sets of duplicates images exist in the dataset. To see the number of exact duplicate sets, you can use imagelab.info:\n\nimagelab.info['exact_duplicates']['num_sets']\n\n9\n\n\nYou can also get exactly which images are there in each (exact/near) duplicated set using imagelab.info.\n\nimagelab.info['exact_duplicates']['sets']\n\n[['/content/image_files/image_142.png', '/content/image_files/image_236.png'],\n ['/content/image_files/image_170.png', '/content/image_files/image_299.png'],\n ['/content/image_files/image_190.png', '/content/image_files/image_197.png'],\n ['/content/image_files/image_288.png', '/content/image_files/image_289.png'],\n ['/content/image_files/image_292.png',\n  '/content/image_files/image_348.png',\n  '/content/image_files/image_492.png'],\n ['/content/image_files/image_30.png', '/content/image_files/image_55.png'],\n ['/content/image_files/image_351.png', '/content/image_files/image_372.png'],\n ['/content/image_files/image_379.png', '/content/image_files/image_579.png'],\n ['/content/image_files/image_550.png', '/content/image_files/image_7.png']]\n\n\n\n\n9.9.4 Check for an issue with a different threshold\nYou can use the loaded imagelab instance to check for an issue type with a custom hyperparameter. Here is a table of hyperparameters that each issue type supports and their permissible values:\n\nthreshold- All images with scores below this threshold will be flagged as an issue.\nhash_size - This controls how much detail about an image we want to keep for getting perceptual hash. Higher sizes imply more detail.\nhash_type - Type of perceptual hash to use. Currently whash and phash are the supported hash types. Check here for more details on these hash types.\n\n\n\n\n\n\n\n\n\n\nIssue Key\nHyperparameters\n\n\n\n\n1\nlight\nthreshold (between 0 and 1)\n\n\n2\ndark\nthreshold (between 0 and 1)\n\n\n3\nodd_aspect_ratio\nthreshold (between 0 and 1)\n\n\n4\nexact_duplicates\nN/A\n\n\n5\nnear_duplicates\nhash_size (power of 2), hash_types (whash, phash)\n\n\n6\nblurry\nthreshold (between 0 and 1)\n\n\n7\ngrayscale\nthreshold (between 0 and 1)\n\n\n8\nlow_information\nthreshold (between 0 and 1)\n\n\n\n\nissue_types = {\"dark\": {\"threshold\": 0.2}}\nimagelab.find_issues(issue_types)\n\nimagelab.report(issue_types)\n\nChecking for dark images ...\nIssue checks completed. To see a detailed report of issues found, use imagelab.report().\nIssues found in order of severity in the dataset\n\n|    | issue_type   |   num_images |\n|---:|:-------------|-------------:|\n|  5 | dark         |            8 | \n\n\nTop 4 examples with dark issue in the dataset.\n\n\n\n\n\nNote the number of images with dark issue has reduced from the previous run!\n\n\n9.9.5 Save and load\nCleanVision also has a save and load functionality that you can use to save the results and load them at a later point in time to see results or run more checks. For saving, specify force=True to overwrite existing files:\n\nsave_path = \"./results\"\nimagelab.save(save_path)\n\nSaved Imagelab to folder: ./results\nThe data path and dataset must be not be changed to maintain consistent state when loading this Imagelab\n\n\n\n## For loading a saved instance, specify `dataset_path` \n## to help check for any inconsistencies between dataset paths in the previous and current run.\nimagelab = Imagelab.load(save_path, dataset_path)\n\nSuccessfully loaded Imagelab"
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_tensorflow.html#lable-issue-with-cleanlab",
    "href": "09_Convolutional_NeuralNetworks_tensorflow.html#lable-issue-with-cleanlab",
    "title": "9  Image processing with Convolutional Neural Networks - Tensorflow",
    "section": "9.10 Lable issue with Cleanlab",
    "text": "9.10 Lable issue with Cleanlab\n\nmnist = fetch_openml(\"mnist_784\")  # Fetch the MNIST dataset\n\nX = mnist.data.astype(\"float32\").to_numpy() # 2D array (images are flattened into 1D)\nX /= 255.0  # Scale the features to the [0, 1] range\n\nX = X.reshape(len(X), 28, 28, 1)  # reshape into [N, H, W, C] for Keras\nlabels = mnist.target.astype(\"int64\").to_numpy()  # 1D array of given labels\n\n/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n\n\n\n9.10.1 Ensure your classifier is scikit-learn compatible\nHere, we define a simple neural network with tf.keras\n\ndef build_model():\n    DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")\n    model = tf.keras.Sequential([\n        DefaultConv2D(filters=32, kernel_size=7, input_shape=[28, 28, 1]),\n        tf.keras.layers.MaxPool2D(),\n        DefaultConv2D(filters=64),\n        DefaultConv2D(filters=64),\n        tf.keras.layers.MaxPool2D(),\n        DefaultConv2D(filters=128),\n        DefaultConv2D(filters=128),\n        tf.keras.layers.MaxPool2D(),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(units=32, activation=\"relu\", kernel_initializer=\"he_normal\"),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(units=10, activation=\"softmax\")\n    ])\n    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n    return model\n\nAs some cleanlab features require scikit-learn compatibility, we adapt the above keras neural net accordingly. scikeras is a convenient package that helps with this:\n\nclf = KerasClassifier(\n    model=build_model,\n    epochs=10,\n    fit__batch_size=32\n)\n\n\n\n9.10.2 Compute out-of-sample predicted probabilities\nIf we’d like cleanlab to identify potential label errors in the whole dataset and not just the training set, we can consider using the entire dataset when computing the out-of-sample predicted probabilities, pred_probs, via cross-validation.\n\nnum_crossval_folds = 3  # for efficiency; values like 5 or 10 will generally work better\npred_probs = cross_val_predict(\n    clf,\n    X,\n    labels,\n    cv=num_crossval_folds,\n    method=\"predict_proba\",\n)\n\nEpoch 1/10\n1459/1459 [==============================] - 19s 8ms/step - loss: 1.4383 - accuracy: 0.4455\nEpoch 2/10\n1459/1459 [==============================] - 11s 7ms/step - loss: 0.8011 - accuracy: 0.7201\nEpoch 3/10\n1459/1459 [==============================] - 12s 8ms/step - loss: 0.5420 - accuracy: 0.8300\nEpoch 4/10\n1459/1459 [==============================] - 12s 8ms/step - loss: 0.4378 - accuracy: 0.8647\nEpoch 5/10\n1459/1459 [==============================] - 12s 8ms/step - loss: 0.3525 - accuracy: 0.8926\nEpoch 6/10\n1459/1459 [==============================] - 12s 8ms/step - loss: 0.3076 - accuracy: 0.9049\nEpoch 7/10\n1459/1459 [==============================] - 11s 8ms/step - loss: 0.2795 - accuracy: 0.9129\nEpoch 8/10\n1459/1459 [==============================] - 12s 8ms/step - loss: 0.2455 - accuracy: 0.9239\nEpoch 9/10\n1459/1459 [==============================] - 12s 8ms/step - loss: 0.2200 - accuracy: 0.9329\nEpoch 10/10\n1459/1459 [==============================] - 12s 8ms/step - loss: 0.1942 - accuracy: 0.9452\n730/730 [==============================] - 2s 2ms/step\nEpoch 1/10\n1459/1459 [==============================] - 18s 9ms/step - loss: 0.9713 - accuracy: 0.6592\nEpoch 2/10\n1459/1459 [==============================] - 12s 8ms/step - loss: 0.4769 - accuracy: 0.8387\nEpoch 3/10\n1459/1459 [==============================] - 20s 14ms/step - loss: 0.3473 - accuracy: 0.8917\nEpoch 4/10\n1459/1459 [==============================] - 16s 11ms/step - loss: 0.2899 - accuracy: 0.9132\nEpoch 5/10\n1459/1459 [==============================] - 15s 10ms/step - loss: 0.2572 - accuracy: 0.9245\nEpoch 6/10\n1459/1459 [==============================] - 12s 9ms/step - loss: 0.2157 - accuracy: 0.9390\nEpoch 7/10\n1459/1459 [==============================] - 12s 8ms/step - loss: 0.1945 - accuracy: 0.9457\nEpoch 8/10\n1459/1459 [==============================] - 13s 9ms/step - loss: 0.1667 - accuracy: 0.9559\nEpoch 9/10\n1459/1459 [==============================] - 13s 9ms/step - loss: 0.1533 - accuracy: 0.9603\nEpoch 10/10\n1459/1459 [==============================] - 14s 9ms/step - loss: 0.1462 - accuracy: 0.9615\n730/730 [==============================] - 2s 2ms/step\nEpoch 1/10\n1459/1459 [==============================] - 16s 9ms/step - loss: 1.1981 - accuracy: 0.5786\nEpoch 2/10\n1459/1459 [==============================] - 11s 8ms/step - loss: 0.5920 - accuracy: 0.7937\nEpoch 3/10\n1459/1459 [==============================] - 11s 8ms/step - loss: 0.4619 - accuracy: 0.8277\nEpoch 4/10\n1459/1459 [==============================] - 12s 8ms/step - loss: 0.4157 - accuracy: 0.8380\nEpoch 5/10\n1459/1459 [==============================] - 12s 8ms/step - loss: 0.3791 - accuracy: 0.8477\nEpoch 6/10\n1459/1459 [==============================] - 12s 8ms/step - loss: 0.3411 - accuracy: 0.8578\nEpoch 7/10\n1459/1459 [==============================] - 11s 8ms/step - loss: 0.3155 - accuracy: 0.8637\nEpoch 8/10\n1459/1459 [==============================] - 11s 8ms/step - loss: 0.2911 - accuracy: 0.8701\nEpoch 9/10\n1459/1459 [==============================] - 11s 8ms/step - loss: 0.2716 - accuracy: 0.8756\nEpoch 10/10\n1459/1459 [==============================] - 12s 8ms/step - loss: 0.2585 - accuracy: 0.8789\n730/730 [==============================] - 2s 2ms/step\n\n\nAn additional benefit of cross-validation is that it facilitates more reliable evaluation of our model than a single training/validation split.\n\npredicted_labels = pred_probs.argmax(axis=1)\nacc = accuracy_score(labels, predicted_labels)\nprint(f\"Cross-validated estimate of accuracy on held-out data: {acc}\")\n\nCross-validated estimate of accuracy on held-out data: 0.9587714285714286\n\n\n\n\n9.10.3 Use cleanlab to find label issues\nBased on the given labels and out-of-sample predicted probabilities, cleanlab can quickly help us identify label issues in our dataset. For a dataset with N examples from K classes, the labels should be a 1D array of length N and predicted probabilities should be a 2D (N x K) array. Here we request that the indices of the identified label issues be sorted by cleanlab’s self-confidence score, which measures the quality of each given label via the probability assigned to it in our model’s prediction.\n\nranked_label_issues = find_label_issues(\n    labels,\n    pred_probs,\n    return_indices_ranked_by=\"self_confidence\",\n)\n\nprint(f\"Cleanlab found {len(ranked_label_issues)} label issues.\")\nprint(f\"Top 15 most likely label errors: \\n {ranked_label_issues[:15]}\")\n\nCleanlab found 282 label issues.\nTop 15 most likely label errors: \n [26622 35616 10994 46857 62654 38230 43109 43454  8480 59701 15450  6848\n 53216  7768  9104]\n\n\nranked_label_issues() is a list of indices corresponding to examples that are worth inspecting more closely.\nLet’s look at the top 15 examples cleanlab thinks are most likely to be incorrectly labeled. We can see a few label errors and odd edge cases. Feel free to change the values below to display more/fewer examples.\n\nplot_examples(ranked_label_issues[range(15)], 3, 5)\n\n\n\n\nLet’s zoom into some specific examples from the above set:\nGiven label is 3 but looks more like a 9:\n\nplot_examples([10994])\n\n\n\n\nGiven label is 5 but looks more like a 3:\n\nplot_examples([43454])\n\n\n\n\nA very odd looking 2:\n\nplot_examples([8480])\n\n\n\n\ncleanlab has shortlisted the most likely label errors to speed up your data cleaning process. With this list, you can decide whether to fix label issues or prune some of these examples from the dataset."
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_tensorflow.html#references",
    "href": "09_Convolutional_NeuralNetworks_tensorflow.html#references",
    "title": "9  Image processing with Convolutional Neural Networks - Tensorflow",
    "section": "9.11 References",
    "text": "9.11 References\n\nhttps://github.com/ageron/handson-ml3/\nhttps://github.com/fchollet/deep-learning-with-python-notebooks_\nhttps://github.com/cleanlab/cleanlab\nhttps://github.com/cleanlab/cleanvision\nhttps://keras.io/guides/keras_cv/object_detection_keras_cv/\nhttps://github.com/divamgupta/image-segmentation-keras"
  },
  {
    "objectID": "10_Recurrent_Neural_Networks.html#setup",
    "href": "10_Recurrent_Neural_Networks.html#setup",
    "title": "10  Sequence Processing with RNNs and Attention - Tensforflow",
    "section": "10.1 Setup",
    "text": "10.1 Setup\nFirst, let’s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0.\n\nimport sys\nif \"google.colab\" in sys.modules or \"kaggle_secrets\" in sys.modules:\n    %pip install -q -U transformers\n    %pip install -q -U datasets\n    %pip install -q -U evaluate\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 37.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 19.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 83.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 474.6/474.6 kB 8.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 21.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.3/134.3 kB 14.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 38.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 7.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 10.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 20.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 6.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 3.4 MB/s eta 0:00:00\n\n\n\n# Python ≥3.7 is recommended\nassert sys.version_info &gt;= (3, 7)\nimport os\nimport shutil\nimport random\nfrom pathlib import Path\nfrom time import strftime\nimport string\n\n# Scikit-Learn ≥1.01 is recommended\nfrom packaging import version\nimport sklearn\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# Tensorflow ≥2.8.0 is recommended\nimport tensorflow as tf\nassert version.parse(tf.__version__) &gt;= version.parse(\"2.8.0\")\n\n# Huggingface transformer\nfrom transformers import pipeline\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification, AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import DataCollatorWithPadding\nfrom datasets import load_dataset\nimport evaluate\n\n# Common imports\nimport numpy as np\nimport pandas as pd\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nif not tf.config.list_physical_devices('GPU'):\n    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n    if \"google.colab\" in sys.modules:\n        print(\"Go to Runtime &gt; Change runtime and select a GPU hardware \"\n              \"accelerator.\")\n    if \"kaggle_secrets\" in sys.modules:\n        print(\"Go to Settings &gt; Accelerator and select GPU.\")"
  },
  {
    "objectID": "10_Recurrent_Neural_Networks.html#basic-rnns-for-forecasting-times-series",
    "href": "10_Recurrent_Neural_Networks.html#basic-rnns-for-forecasting-times-series",
    "title": "10  Sequence Processing with RNNs and Attention - Tensforflow",
    "section": "10.2 Basic RNNs for forecasting times series",
    "text": "10.2 Basic RNNs for forecasting times series\n\n10.2.1 Get the Dataset\nLet’s pretend you’ve just been hired as a data scientist by Chicago’s Transit Authority. Your first task is to build a model capable of forecasting the number of passengers that will ride on bus and rail the next day. You have access to daily ridership data since 2001. Let’s walk through together how you would handle this. Let’s download the organized ridership data from the ageron/data project. It originally comes from Chicago’s Transit Authority, and was downloaded from the Chicago’s Data Portal:\n\ntf.keras.utils.get_file(\n    \"ridership.tgz\",\n    \"https://github.com/ageron/data/raw/main/ridership.tgz\",\n    cache_dir=\".\",\n    extract=True\n)\n\nDownloading data from https://github.com/ageron/data/raw/main/ridership.tgz\n108512/108512 [==============================] - 0s 0us/step\n\n\n'./datasets/ridership.tgz'\n\n\nWe’ll start by loading and cleaning up the data:\n\npath = Path(\"datasets/ridership/CTA_-_Ridership_-_Daily_Boarding_Totals.csv\")\ndf = pd.read_csv(path, parse_dates=[\"service_date\"])\ndf.columns = [\"date\", \"day_type\", \"bus\", \"rail\", \"total\"]  # gives columns with shorter names\ndf = df.sort_values(\"date\").set_index(\"date\")\ndf = df.drop(\"total\", axis=1)  # no need for total, it's just bus + rail which may be non-informative\ndf = df.drop_duplicates()  # remove duplicated months (2011-10 and 2014-07)\n\nWe load the CSV file, set short column names, sort the rows by date, remove the redundant total column, and drop duplicate rows. Now let’s check what the first few rows look like:\n\ndf.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nday_type\nbus\nrail\n\n\ndate\n\n\n\n\n\n\n\n2001-01-01\nU\n297192\n126455\n\n\n2001-01-02\nW\n780827\n501952\n\n\n2001-01-03\nW\n824923\n536432\n\n\n2001-01-04\nW\n870021\n550011\n\n\n2001-01-05\nW\n890426\n557917\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nOn January 1st, 2001, 297,192 people boarded a bus in Chicago, and 126,455 boarded a train. The day_type column contains W for Weekdays, A for Saturdays, and U for Sundays or holidays. Now let’s plot the bus and rail ridership figures over a few months in 2019, to see what it looks like:\n\ndf[\"2019-03\":\"2019-05\"].plot(grid=True, marker=\".\", figsize=(8, 3.5));\n\n\n\n\nThis is a time series: data with values at different time steps, usually at regular intervals. More specifically, since there are multiple values per time step, this is called a multivariate time series. If we only looked at the bus column, it would be a univariate time series, with a single value per time step. Typical tasks are:\n\nPredicting future values (i.e., forecasting) is the most typical task when dealing with time series, and this is what we will focus on.\nOther tasks include imputation (filling in missing past values), classification, anomaly detection, and more.\n\n\n\n10.2.2 Computing Some Baselines\nWe can see that a similar pattern is clearly repeated every week. This is called a weekly seasonality. In fact, it’s so strong in this case that forecasting tomorrow’s ridership by just copying the values from a week earlier will yield reasonably good results. This is called naive forecasting: simply copying a past value to make our forecast. Naive forecasting is often a great baseline, and it can even be tricky to beat in some cases.\nTo visualize these naive forecasts, let’s overlay the two time series (for bus and rail) as well as the same time series lagged by one week (i.e., shifted toward the right) using dotted lines. We’ll also plot the difference between the two (i.e., the value at time \\(t\\) minus the value at time \\(t–7\\)); this is called differencing:\n\ndiff_7 = df[[\"bus\", \"rail\"]].diff(7)[\"2019-03\":\"2019-05\"]\n\nfig, axs = plt.subplots(2, 1, sharex=True, figsize=(8, 5))\ndf.plot(ax=axs[0], legend=False, marker=\".\")  # original time series\ndf.shift(7).plot(ax=axs[0], grid=True, legend=False, linestyle=\":\")  # lagged time series\ndiff_7.plot(ax=axs[1], grid=True, marker=\".\")  # 7-day difference time series\naxs[0].set_ylim([170_000, 900_000]);  # extra code – beautifies the plot\n\n\n\n\nNotice how closely the lagged time series track the actual time series. When a time series is correlated with a lagged version of itself, we say that the time series is autocorrelated. As you can see, most of the differences are fairly small, except at the end of May. Maybe there was a holiday at that time? Let’s check the day_type column:\n\nlist(df.loc[\"2019-05-25\":\"2019-05-27\"][\"day_type\"])\n\n['A', 'U', 'U']\n\n\nIndeed, there was a long weekend back then: the Monday was the Memorial Day holiday. We could use this column to improve our forecasts, but for now let’s just measure the mean absolute error over the three-month period we’re arbitrarily focusing on previously — March, April, and May 2019 — to get a rough idea:\n\ndiff_7.abs().mean()\n\nbus     43915.608696\nrail    42143.271739\ndtype: float64\n\n\nOur naive forecasts get an MAE of about 43,916 bus riders, and about 42,143 rail riders. It’s hard to tell at a glance how good or bad this is, so let’s put the forecast errors into perspective by dividing them by the target values:\n\ntargets = df[[\"bus\", \"rail\"]][\"2019-03\":\"2019-05\"]\n(diff_7 / targets).abs().mean()\n\nbus     0.082938\nrail    0.089948\ndtype: float64\n\n\nWhat we just computed is called the mean absolute percentage error (MAPE): it looks like our naive forecasts give us a MAPE of roughly 8.3% for bus and 9.0% for rail. It’s interesting to note that the MAE for the rail forecasts looks slightly better than the MAE for the bus forecasts, while the opposite is true for the MAPE. That’s because the bus ridership is larger than the rail ridership, so naturally the forecast errors are also larger, but when we put the errors into perspective, it turns out that the bus forecasts are actually slightly better than the rail forecasts.\nLooking at the time series, there doesn’t appear to be any significant monthly seasonality, but let’s check whether there’s any yearly seasonality. We’ll look at the data from 2001 to 2019. To reduce the risk of data snooping, we’ll ignore more recent data for now. Lets also plot a 12-month rolling average for each series to visualize long-term trends:\n\ndf_monthly = df.resample('M').mean()  # compute the mean for each month\nrolling_average_12_months = df_monthly[\"2001\":\"2019\"].rolling(window=12).mean()\n\nfig, ax = plt.subplots(figsize=(8, 4))\ndf_monthly[\"2001\":\"2019\"].plot(ax=ax, marker=\".\")\nrolling_average_12_months.plot(ax=ax, grid=True, legend=False);\n\nFutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  df_monthly = df.resample('M').mean()  # compute the mean for each month\n\n\n\n\n\nThere’s definitely some yearly seasonality as well, although it is noisier than the weekly seasonality, and more visible for the rail series than the bus series: we see peaks and troughs at roughly the same dates each year. Let’s check what we get if we plot the 12-month difference:\n\ndf_monthly.diff(12)[\"2001\":\"2019\"].plot(grid=True, marker=\".\", figsize=(8, 3));\n\n\n\n\nNotice how differencing not only removed the yearly seasonality, but it also removed the long-term trends. For example, the linear downward trend present in the time series from 2016 to 2019 became a roughly constant negative value in the differenced time series. In fact, differencing is a common technique used to remove trend and seasonality from a time series: it’s easier to study a stationary time series, meaning one whose statistical properties remain constant over time, without any seasonality or trends. Once you’re able to make accurate forecasts on the differenced time series, it’s easy to turn them into forecasts for the actual time series by just adding back the past values that were previously subtracted.\nYou may be thinking that we’re only trying to predict tomorrow’s ridership, so the long-term patterns matter much less than the short-term ones. But still, we may be able to improve performance slightly by taking long-term patterns into account. For example, daily bus ridership dropped by about 2,500 in October 2017, which represents about 570 fewer passengers each week, so if we were at the end of October 2017, it would make sense to forecast tomorrow’s ridership by copying the value from last week, minus 570. Accounting for the trend will make your forecasts a bit more accurate on average.\n\n\n10.2.3 The ARMA Model Family\nWe’ll start with the autoregressive moving average (ARMA) model, developed by Herman Wold in the 1930s: it computes its forecasts using a simple weighted sum of lagged values and corrects these forecasts by adding a moving average. Specifically, the moving average component is computed using a weighted sum of the last few forecast errors. This model assumes that the time series is stationary. If it is not, then differencing may help. Using differencing over a single time step will produce an approximation of the derivative of the time series. If the original time series has a quadratic trend instead of a linear trend, then a single round of differencing will not be enough. In this case, running d consecutive rounds of differencing computes an approximation of the d-th order derivative of the time series, so it will eliminate polynomial trends up to degree d. This hyperparameter d is called the order of integration. Differencing is the central contribution of the autoregressive integrated moving average (ARIMA) model, introduced in 1970 by George Box and Gwilym Jenkins in their book: this model runs d rounds of differencing to make the time series more stationary, then it applies a regular ARMA model. When making forecasts, it uses this ARMA model, then it adds back the terms that were subtracted by differencing.\nLet’s see how to fit a SARIMA model to the rail time series, and use it to make a forecast for tomorrow’s ridership. We’ll pretend today is the last day of May 2019, and we want to forecast the rail ridership for “tomorrow”, the 1st of June, 2019. For this, we can use the statsmodels library, which contains many different statistical models, including the ARMA model and its variants, implemented by the ARIMA class:\n\norigin, today = \"2019-01-01\", \"2019-05-31\"\nrail_series = df.loc[origin:today][\"rail\"].asfreq(\"D\") # Convert time series to specified frequency\nmodel = ARIMA(rail_series, order=(1, 0, 0), seasonal_order=(0, 1, 1, 7))\nmodel = model.fit()\ny_pred = model.forecast() \n\ny_pred[0]  # ARIMA forecast \n\n427758.62641035335\n\n\n\nprint(df[\"rail\"].loc[\"2019-06-01\"])  # target value\nprint(df[\"rail\"].loc[\"2019-05-25\"])  # naive forecast (value from one week earlier)\n\n379044\n426932\n\n\n\nWe start by importing the ARIMA class, then we take the rail ridership data from the start of 2019 up to “today”, and we use asfreq(\"D\") to set the time series’frequency to daily: this doesn’t change the data at all in this case, since it’s already daily, but without this the ARIMA class would have to guess the frequency, and it would display a warning.\nWe create an ARIMA instance, passing it all the data until “today”, and we set the model hyperparameters: order=(1, 0, 0) and seasonal_order=(0, 1, 1, 7) for the model (See API for more descriptions). Notice that the statsmodels API differs a bit from Scikit-Learn’s API, since we pass the data to the model at construction time, instead of passing it to the fit() method.\n\nThe forecast is 427,759 passengers, when in fact there were 379,044. Yikes, we’re 12.9% off - that’s pretty bad. It’s actually slightly worse than naive forecasting, which forecasts 426,932, off by 12.6%. But perhaps we were just unlucky that day? To check this, we can run the same code in a loop to make forecasts for every day in March, April, and May, and compute the MAE over that period:\n\norigin, start_date, end_date = \"2019-01-01\", \"2019-03-01\", \"2019-05-31\"\ntime_period = pd.date_range(start_date, end_date)\nrail_series = df.loc[origin:end_date][\"rail\"].asfreq(\"D\")\n\ny_preds = []\nfor today in time_period.shift(-1):\n    model = ARIMA(rail_series[origin:today],  # train on data from January up to \"today\", expanding window approach\n                  order=(1, 0, 0),\n                  seasonal_order=(0, 1, 1, 7))\n    model = model.fit()  # note that we retrain the model every day!\n    y_pred = model.forecast()[0]\n    y_preds.append(y_pred)\n\ny_preds = pd.Series(y_preds, index=time_period)\nmae = (y_preds - rail_series[time_period]).abs().mean()\nmae\n\n32040.720089453378\n\n\n\nfig, ax = plt.subplots(figsize=(8, 3))\nrail_series.loc[time_period].plot(label=\"True\", ax=ax, marker=\".\", grid=True)\nax.plot(y_preds, color=\"r\", marker=\".\", label=\"ARIMA Forecasts\")\nplt.legend();\n\n\n\n\nThat’s much better! The MAE is about 32,041, which is significantly lower than the MAE we got with naive forecasting (42,143). So although the model is not perfect, it still beats naive forecasting by a large margin, on average. There are approaches for selecting good hyperparameters, based on analyzing the autocorrelation function (ACF) and partial autocorrelation function (PACF), or minimizing the AIC or BIC metrics to penalize models that use too many parameters and reduce the risk of overfitting the data:\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\nplot_acf(df[\"2001\":\"2019\"][\"rail\"], ax=axs[0], lags=35)\naxs[0].grid()\nplot_pacf(df[\"2001\":\"2019\"][\"rail\"], ax=axs[1], lags=35, method=\"ywm\")\naxs[1].grid();\n\n\n\n\nRefer to https://www.statsmodels.org/devel/graphics.html#time-series-plots for more details.\n\n\n10.2.4 Preparing the Data for Machine Learning Models\nNow that we have two baselines, naive forecasting and ARIMA, let’s try to use the machine learning models we’ve covered so far to forecast this time series, starting with a basic linear model. Our goal will be to forecast tomorrow’s ridership based on the ridership of the past 8 weeks of data (56 days). The inputs to our model will therefore be sequences (usually a single sequence per day once the model is in production), each containing 56 values from time steps \\(t–55\\) to \\(t\\). For each input sequence, the model will output a single value: the forecast for time step \\(t+1\\). We will use every 56-day window from the past as training data, and the target for each window will be the value immediately following it.\ntf.Keras actually has a nice utility function called tf.keras.utils.timeseries_dataset_from_array() to help us prepare the training set. It takes a time series as input, and it builds a tf.data.Dataset containing all the windows of the desired length, as well as their corresponding targets. Here’s an example that takes a time series containing the numbers 0 to 5 and creates a dataset containing all the windows of length 3, with their corresponding targets, grouped into batches of size 2:\n\nmy_series = [0, 1, 2, 3, 4, 5]\nmy_dataset = tf.keras.utils.timeseries_dataset_from_array(\n    my_series,\n    targets=my_series[3:],  # the targets are 3 steps into the future\n    sequence_length=3,\n    batch_size=2\n)\nlist(my_dataset)\n\n[(&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n  array([[0, 1, 2],\n         [1, 2, 3]], dtype=int32)&gt;,\n  &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)&gt;),\n (&lt;tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[2, 3, 4]], dtype=int32)&gt;,\n  &lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)&gt;)]\n\n\nEach sample in the dataset is a window of length 3, along with its corresponding target (i.e., the value immediately after the window). The windows are [0, 1, 2], [1, 2, 3], and [2, 3, 4], and their respective targets are 3, 4, and 5. Since there are three windows in total, which is not a multiple of the batch size, the last batch only contains one window instead of two.\nAnother way to get the same result is to use the window() method of tf.data’s Dataset class. It’s more complex, but it gives you full control, which will come in handy later in this chapter, so let’s see how it works. The window() method returns a dataset of window datasets:\n\nfor window_dataset in tf.data.Dataset.range(6).window(4, shift=1):\n    for element in window_dataset:\n        print(f\"{element}\", end=\" \")\n    print()\n\n0 1 2 3 \n1 2 3 4 \n2 3 4 5 \n3 4 5 \n4 5 \n5 \n\n\nIn this example, the dataset contains six windows, each shifted by one step compared to the previous one, and the last three windows are smaller because they’ve reached the end of the series. In general you’ll want to get rid of these smaller windows by passing drop_remainder=True to the window() method. The window() method returns a nested dataset, analogous to a list of lists. This is useful when you want to transform each window by calling its dataset methods (e.g., to shuffle them or batch them). However, we cannot use a nested dataset directly for training, as our model will expect tensors as input, not datasets. Therefore, we must call the flat_map() method: it converts a nested dataset into a flat dataset (one that contains tensors, not datasets). For example, suppose {1, 2, 3} represents a dataset containing the sequence of tensors 1, 2, and 3. If you flatten the nested dataset {{1, 2}, {3, 4, 5, 6}}, you get back the flat dataset {1, 2, 3, 4, 5, 6}. Moreover, the flat_map() method takes a function as an argument, which allows you to transform each dataset in the nested dataset before flattening. For example, if you pass the function lambda ds: ds.batch(2) to flat_map(), then it will transform the nested dataset {{1, 2}, {3, 4, 5, 6}} into the flat dataset {[1, 2], [3, 4], [5, 6]}: it’s a dataset containing 3 tensors, each of size 2:\n\ndataset = tf.data.Dataset.range(6).window(4, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window_dataset: window_dataset.batch(4))\nfor window_tensor in dataset:\n    print(f\"{window_tensor}\")\n\n[0 1 2 3]\n[1 2 3 4]\n[2 3 4 5]\n\n\nSince each window dataset contains exactly four items, calling batch(4) on a window produces a single tensor of size 4. Great! We now have a dataset containing consecutive windows represented as tensors. Let’s create a little helper function to make it easier to extract windows from a dataset:\n\ndef to_windows(dataset, length):\n    dataset = dataset.window(length, shift=1, drop_remainder=True)\n    return dataset.flat_map(lambda window_ds: window_ds.batch(length))\n\nThe last step is to split each window into inputs and targets, using the map() method. We can also group the resulting windows into batches of size 2:\n\ndataset = to_windows(tf.data.Dataset.range(6), 4)\ndataset = dataset.map(lambda window: (window[:-1], window[-1]))\nlist(dataset.batch(2))\n\n[(&lt;tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n  array([[0, 1, 2],\n         [1, 2, 3]])&gt;,\n  &lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([3, 4])&gt;),\n (&lt;tf.Tensor: shape=(1, 3), dtype=int64, numpy=array([[2, 3, 4]])&gt;,\n  &lt;tf.Tensor: shape=(1,), dtype=int64, numpy=array([5])&gt;)]\n\n\nAs you can see, we now have the same output as we got earlier with the timeseries_dataset_from_array() function!\nBefore we continue looking at the data, let’s split the time series into three periods, for training, validation and testing. We won’t look at the test data for now:\n\nrail_train = df[\"rail\"][\"2016-01\":\"2018-12\"] / 1e6 # Normalize to have the unit of million\nrail_valid = df[\"rail\"][\"2019-01\":\"2019-05\"] / 1e6\nrail_test = df[\"rail\"][\"2019-06\":] / 1e6\n\nNext, let’s use timeseries_dataset_from_array() to create datasets for training and validation. Since gradient descent expects the instances in the training set to be independent and identically distributed (IID), argument shuffle=True to shuffle the training windows (but not their contents):\n\nseq_length = 56\ntf.random.set_seed(42)  # extra code – ensures reproducibility\ntrain_ds = tf.keras.utils.timeseries_dataset_from_array(\n    rail_train.to_numpy(),\n    targets=rail_train[seq_length:],\n    sequence_length=seq_length, # Sliding window approach\n    batch_size=32,\n    shuffle=True,\n    seed=42\n)\nvalid_ds = tf.keras.utils.timeseries_dataset_from_array(\n    rail_valid.to_numpy(),\n    targets=rail_valid[seq_length:],\n    sequence_length=seq_length,\n    batch_size=32\n)\n\n\n\n10.2.5 Forecasting Using a Linear Model\nLet’s try a basic linear model first. We will use the Huber loss, which usually works better than minimizing the MAE directly. We’ll also use early stopping:\n\ntf.random.set_seed(42)\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(1, input_shape=[seq_length])\n])\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_mae\", patience=50, restore_best_weights=True)\nopt = tf.keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=opt, metrics=[\"mae\"])\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 1)                 57        \n                                                                 \n=================================================================\nTotal params: 57\nTrainable params: 57\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nhistory = model.fit(train_ds, validation_data=valid_ds, epochs=500,\n                    callbacks=[early_stopping_cb])\n\nEpoch 1/500\n33/33 [==============================] - 5s 12ms/step - loss: 0.1395 - mae: 0.4360 - val_loss: 0.0117 - val_mae: 0.1257\nEpoch 2/500\n33/33 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.1306 - val_loss: 0.0106 - val_mae: 0.1097\nEpoch 3/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0068 - mae: 0.0827 - val_loss: 0.0067 - val_mae: 0.0844\nEpoch 4/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0064 - mae: 0.0797 - val_loss: 0.0058 - val_mae: 0.0797\nEpoch 5/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0064 - mae: 0.0803 - val_loss: 0.0068 - val_mae: 0.0843\nEpoch 6/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0059 - mae: 0.0770 - val_loss: 0.0053 - val_mae: 0.0737\nEpoch 7/500\n33/33 [==============================] - 1s 13ms/step - loss: 0.0054 - mae: 0.0718 - val_loss: 0.0051 - val_mae: 0.0713\nEpoch 8/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0058 - mae: 0.0770 - val_loss: 0.0050 - val_mae: 0.0753\nEpoch 9/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0049 - mae: 0.0692 - val_loss: 0.0053 - val_mae: 0.0737\nEpoch 10/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0049 - mae: 0.0674 - val_loss: 0.0042 - val_mae: 0.0656\nEpoch 11/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0050 - mae: 0.0691 - val_loss: 0.0042 - val_mae: 0.0682\nEpoch 12/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0046 - mae: 0.0645 - val_loss: 0.0039 - val_mae: 0.0614\nEpoch 13/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0044 - mae: 0.0631 - val_loss: 0.0052 - val_mae: 0.0775\nEpoch 14/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0044 - mae: 0.0632 - val_loss: 0.0046 - val_mae: 0.0722\nEpoch 15/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0048 - mae: 0.0695 - val_loss: 0.0043 - val_mae: 0.0694\nEpoch 16/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0046 - mae: 0.0672 - val_loss: 0.0036 - val_mae: 0.0608\nEpoch 17/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0046 - mae: 0.0663 - val_loss: 0.0034 - val_mae: 0.0553\nEpoch 18/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0041 - mae: 0.0607 - val_loss: 0.0034 - val_mae: 0.0571\nEpoch 19/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0042 - mae: 0.0627 - val_loss: 0.0032 - val_mae: 0.0544\nEpoch 20/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0044 - mae: 0.0654 - val_loss: 0.0044 - val_mae: 0.0692\nEpoch 21/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0047 - mae: 0.0696 - val_loss: 0.0031 - val_mae: 0.0521\nEpoch 22/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0037 - mae: 0.0573 - val_loss: 0.0030 - val_mae: 0.0512\nEpoch 23/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0035 - mae: 0.0546 - val_loss: 0.0031 - val_mae: 0.0537\nEpoch 24/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0037 - mae: 0.0570 - val_loss: 0.0029 - val_mae: 0.0504\nEpoch 25/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0035 - mae: 0.0554 - val_loss: 0.0028 - val_mae: 0.0486\nEpoch 26/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0036 - mae: 0.0558 - val_loss: 0.0030 - val_mae: 0.0513\nEpoch 27/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0036 - mae: 0.0553 - val_loss: 0.0034 - val_mae: 0.0569\nEpoch 28/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0035 - mae: 0.0574 - val_loss: 0.0032 - val_mae: 0.0565\nEpoch 29/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0033 - mae: 0.0530 - val_loss: 0.0030 - val_mae: 0.0516\nEpoch 30/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0037 - mae: 0.0574 - val_loss: 0.0028 - val_mae: 0.0492\nEpoch 31/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0034 - mae: 0.0543 - val_loss: 0.0026 - val_mae: 0.0455\nEpoch 32/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0033 - mae: 0.0516 - val_loss: 0.0026 - val_mae: 0.0452\nEpoch 33/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0032 - mae: 0.0523 - val_loss: 0.0028 - val_mae: 0.0508\nEpoch 34/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0033 - mae: 0.0524 - val_loss: 0.0029 - val_mae: 0.0513\nEpoch 35/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0031 - mae: 0.0500 - val_loss: 0.0026 - val_mae: 0.0467\nEpoch 36/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0031 - mae: 0.0515 - val_loss: 0.0026 - val_mae: 0.0456\nEpoch 37/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0033 - mae: 0.0537 - val_loss: 0.0029 - val_mae: 0.0515\nEpoch 38/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0032 - mae: 0.0512 - val_loss: 0.0028 - val_mae: 0.0498\nEpoch 39/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0031 - mae: 0.0501 - val_loss: 0.0025 - val_mae: 0.0424\nEpoch 40/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0031 - mae: 0.0494 - val_loss: 0.0024 - val_mae: 0.0421\nEpoch 41/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0030 - mae: 0.0500 - val_loss: 0.0025 - val_mae: 0.0426\nEpoch 42/500\n33/33 [==============================] - 0s 10ms/step - loss: 0.0030 - mae: 0.0490 - val_loss: 0.0025 - val_mae: 0.0436\nEpoch 43/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0033 - mae: 0.0535 - val_loss: 0.0033 - val_mae: 0.0551\nEpoch 44/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0031 - mae: 0.0522 - val_loss: 0.0024 - val_mae: 0.0415\nEpoch 45/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0031 - mae: 0.0488 - val_loss: 0.0025 - val_mae: 0.0426\nEpoch 46/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0030 - mae: 0.0504 - val_loss: 0.0024 - val_mae: 0.0418\nEpoch 47/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0028 - mae: 0.0465 - val_loss: 0.0024 - val_mae: 0.0413\nEpoch 48/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0030 - mae: 0.0500 - val_loss: 0.0024 - val_mae: 0.0417\nEpoch 49/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0029 - mae: 0.0470 - val_loss: 0.0024 - val_mae: 0.0407\nEpoch 50/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0030 - mae: 0.0492 - val_loss: 0.0024 - val_mae: 0.0416\nEpoch 51/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0029 - mae: 0.0473 - val_loss: 0.0023 - val_mae: 0.0406\nEpoch 52/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0028 - mae: 0.0464 - val_loss: 0.0024 - val_mae: 0.0407\nEpoch 53/500\n33/33 [==============================] - 0s 10ms/step - loss: 0.0031 - mae: 0.0497 - val_loss: 0.0023 - val_mae: 0.0398\nEpoch 54/500\n33/33 [==============================] - 1s 12ms/step - loss: 0.0028 - mae: 0.0467 - val_loss: 0.0023 - val_mae: 0.0398\nEpoch 55/500\n33/33 [==============================] - 0s 10ms/step - loss: 0.0030 - mae: 0.0495 - val_loss: 0.0027 - val_mae: 0.0467\nEpoch 56/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0030 - mae: 0.0497 - val_loss: 0.0024 - val_mae: 0.0425\nEpoch 57/500\n33/33 [==============================] - 1s 13ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0023 - val_mae: 0.0404\nEpoch 58/500\n33/33 [==============================] - 1s 12ms/step - loss: 0.0028 - mae: 0.0475 - val_loss: 0.0023 - val_mae: 0.0408\nEpoch 59/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0028 - mae: 0.0454 - val_loss: 0.0028 - val_mae: 0.0476\nEpoch 60/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0029 - mae: 0.0484 - val_loss: 0.0027 - val_mae: 0.0458\nEpoch 61/500\n33/33 [==============================] - 0s 10ms/step - loss: 0.0029 - mae: 0.0471 - val_loss: 0.0023 - val_mae: 0.0401\nEpoch 62/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0023 - val_mae: 0.0395\nEpoch 63/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0028 - mae: 0.0465 - val_loss: 0.0023 - val_mae: 0.0397\nEpoch 64/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0028 - mae: 0.0465 - val_loss: 0.0023 - val_mae: 0.0398\nEpoch 65/500\n33/33 [==============================] - 0s 10ms/step - loss: 0.0031 - mae: 0.0536 - val_loss: 0.0023 - val_mae: 0.0409\nEpoch 66/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0027 - mae: 0.0443 - val_loss: 0.0023 - val_mae: 0.0389\nEpoch 67/500\n33/33 [==============================] - 0s 10ms/step - loss: 0.0029 - mae: 0.0476 - val_loss: 0.0023 - val_mae: 0.0395\nEpoch 68/500\n33/33 [==============================] - 1s 18ms/step - loss: 0.0027 - mae: 0.0448 - val_loss: 0.0024 - val_mae: 0.0403\nEpoch 69/500\n33/33 [==============================] - 1s 18ms/step - loss: 0.0027 - mae: 0.0459 - val_loss: 0.0032 - val_mae: 0.0534\nEpoch 70/500\n33/33 [==============================] - 1s 14ms/step - loss: 0.0029 - mae: 0.0501 - val_loss: 0.0023 - val_mae: 0.0399\nEpoch 71/500\n33/33 [==============================] - 1s 18ms/step - loss: 0.0028 - mae: 0.0467 - val_loss: 0.0028 - val_mae: 0.0479\nEpoch 72/500\n33/33 [==============================] - 1s 10ms/step - loss: 0.0030 - mae: 0.0499 - val_loss: 0.0023 - val_mae: 0.0387\nEpoch 73/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0464 - val_loss: 0.0023 - val_mae: 0.0389\nEpoch 74/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0458 - val_loss: 0.0023 - val_mae: 0.0390\nEpoch 75/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0460 - val_loss: 0.0023 - val_mae: 0.0394\nEpoch 76/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0453 - val_loss: 0.0023 - val_mae: 0.0390\nEpoch 77/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0474 - val_loss: 0.0030 - val_mae: 0.0513\nEpoch 78/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0029 - mae: 0.0492 - val_loss: 0.0022 - val_mae: 0.0385\nEpoch 79/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0468 - val_loss: 0.0023 - val_mae: 0.0396\nEpoch 80/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0463 - val_loss: 0.0027 - val_mae: 0.0501\nEpoch 81/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0465 - val_loss: 0.0023 - val_mae: 0.0388\nEpoch 82/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0455 - val_loss: 0.0024 - val_mae: 0.0434\nEpoch 83/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0438 - val_loss: 0.0023 - val_mae: 0.0387\nEpoch 84/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0448 - val_loss: 0.0022 - val_mae: 0.0383\nEpoch 85/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0431 - val_loss: 0.0023 - val_mae: 0.0390\nEpoch 86/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0453 - val_loss: 0.0023 - val_mae: 0.0401\nEpoch 87/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0489 - val_loss: 0.0033 - val_mae: 0.0552\nEpoch 88/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0033 - mae: 0.0557 - val_loss: 0.0024 - val_mae: 0.0443\nEpoch 89/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0440 - val_loss: 0.0023 - val_mae: 0.0393\nEpoch 90/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0029 - mae: 0.0489 - val_loss: 0.0031 - val_mae: 0.0522\nEpoch 91/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0490 - val_loss: 0.0023 - val_mae: 0.0383\nEpoch 92/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0447 - val_loss: 0.0023 - val_mae: 0.0388\nEpoch 93/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0454 - val_loss: 0.0026 - val_mae: 0.0431\nEpoch 94/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0031 - mae: 0.0515 - val_loss: 0.0029 - val_mae: 0.0486\nEpoch 95/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0476 - val_loss: 0.0027 - val_mae: 0.0488\nEpoch 96/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0468 - val_loss: 0.0023 - val_mae: 0.0402\nEpoch 97/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0447 - val_loss: 0.0024 - val_mae: 0.0399\nEpoch 98/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0429 - val_loss: 0.0023 - val_mae: 0.0405\nEpoch 99/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0449 - val_loss: 0.0022 - val_mae: 0.0386\nEpoch 100/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0435 - val_loss: 0.0022 - val_mae: 0.0379\nEpoch 101/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0436 - val_loss: 0.0024 - val_mae: 0.0392\nEpoch 102/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0029 - mae: 0.0480 - val_loss: 0.0024 - val_mae: 0.0395\nEpoch 103/500\n33/33 [==============================] - 0s 9ms/step - loss: 0.0027 - mae: 0.0453 - val_loss: 0.0024 - val_mae: 0.0390\nEpoch 104/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0028 - mae: 0.0468 - val_loss: 0.0023 - val_mae: 0.0385\nEpoch 105/500\n33/33 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0478 - val_loss: 0.0024 - val_mae: 0.0391\nEpoch 106/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0026 - mae: 0.0433 - val_loss: 0.0023 - val_mae: 0.0384\nEpoch 107/500\n33/33 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0462 - val_loss: 0.0024 - val_mae: 0.0403\nEpoch 108/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0026 - mae: 0.0435 - val_loss: 0.0023 - val_mae: 0.0405\nEpoch 109/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0027 - mae: 0.0449 - val_loss: 0.0023 - val_mae: 0.0383\nEpoch 110/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0434 - val_loss: 0.0023 - val_mae: 0.0386\nEpoch 111/500\n33/33 [==============================] - 1s 20ms/step - loss: 0.0027 - mae: 0.0453 - val_loss: 0.0023 - val_mae: 0.0390\nEpoch 112/500\n33/33 [==============================] - 1s 14ms/step - loss: 0.0027 - mae: 0.0459 - val_loss: 0.0025 - val_mae: 0.0411\nEpoch 113/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0452 - val_loss: 0.0024 - val_mae: 0.0424\nEpoch 114/500\n33/33 [==============================] - 1s 21ms/step - loss: 0.0028 - mae: 0.0466 - val_loss: 0.0024 - val_mae: 0.0389\nEpoch 115/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0031 - mae: 0.0521 - val_loss: 0.0023 - val_mae: 0.0386\nEpoch 116/500\n33/33 [==============================] - 1s 19ms/step - loss: 0.0026 - mae: 0.0434 - val_loss: 0.0023 - val_mae: 0.0383\nEpoch 117/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0466 - val_loss: 0.0025 - val_mae: 0.0466\nEpoch 118/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0449 - val_loss: 0.0027 - val_mae: 0.0443\nEpoch 119/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0030 - mae: 0.0514 - val_loss: 0.0025 - val_mae: 0.0450\nEpoch 120/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0452 - val_loss: 0.0023 - val_mae: 0.0380\nEpoch 121/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0440 - val_loss: 0.0025 - val_mae: 0.0464\nEpoch 122/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0466 - val_loss: 0.0030 - val_mae: 0.0508\nEpoch 123/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0022 - val_mae: 0.0388\nEpoch 124/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0432 - val_loss: 0.0029 - val_mae: 0.0480\nEpoch 125/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0494 - val_loss: 0.0024 - val_mae: 0.0422\nEpoch 126/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0033 - mae: 0.0556 - val_loss: 0.0032 - val_mae: 0.0531\nEpoch 127/500\n33/33 [==============================] - 0s 9ms/step - loss: 0.0028 - mae: 0.0472 - val_loss: 0.0031 - val_mae: 0.0520\nEpoch 128/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0030 - mae: 0.0500 - val_loss: 0.0023 - val_mae: 0.0406\nEpoch 129/500\n33/33 [==============================] - 1s 15ms/step - loss: 0.0025 - mae: 0.0424 - val_loss: 0.0023 - val_mae: 0.0381\nEpoch 130/500\n33/33 [==============================] - 1s 15ms/step - loss: 0.0026 - mae: 0.0431 - val_loss: 0.0023 - val_mae: 0.0404\nEpoch 131/500\n33/33 [==============================] - 1s 19ms/step - loss: 0.0025 - mae: 0.0435 - val_loss: 0.0025 - val_mae: 0.0408\nEpoch 132/500\n33/33 [==============================] - 1s 27ms/step - loss: 0.0026 - mae: 0.0440 - val_loss: 0.0022 - val_mae: 0.0378\nEpoch 133/500\n33/33 [==============================] - 1s 19ms/step - loss: 0.0026 - mae: 0.0449 - val_loss: 0.0022 - val_mae: 0.0378\nEpoch 134/500\n33/33 [==============================] - 1s 18ms/step - loss: 0.0026 - mae: 0.0439 - val_loss: 0.0022 - val_mae: 0.0383\nEpoch 135/500\n33/33 [==============================] - 1s 24ms/step - loss: 0.0026 - mae: 0.0438 - val_loss: 0.0024 - val_mae: 0.0400\nEpoch 136/500\n33/33 [==============================] - 1s 15ms/step - loss: 0.0026 - mae: 0.0429 - val_loss: 0.0023 - val_mae: 0.0382\nEpoch 137/500\n33/33 [==============================] - 1s 13ms/step - loss: 0.0028 - mae: 0.0481 - val_loss: 0.0023 - val_mae: 0.0400\nEpoch 138/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0028 - mae: 0.0468 - val_loss: 0.0023 - val_mae: 0.0391\nEpoch 139/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0027 - mae: 0.0460 - val_loss: 0.0024 - val_mae: 0.0426\nEpoch 140/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0025 - mae: 0.0430 - val_loss: 0.0038 - val_mae: 0.0639\nEpoch 141/500\n33/33 [==============================] - 1s 16ms/step - loss: 0.0027 - mae: 0.0455 - val_loss: 0.0023 - val_mae: 0.0386\nEpoch 142/500\n33/33 [==============================] - 1s 16ms/step - loss: 0.0026 - mae: 0.0449 - val_loss: 0.0023 - val_mae: 0.0386\nEpoch 143/500\n33/33 [==============================] - 1s 11ms/step - loss: 0.0029 - mae: 0.0497 - val_loss: 0.0023 - val_mae: 0.0382\nEpoch 144/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0489 - val_loss: 0.0024 - val_mae: 0.0398\nEpoch 145/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0486 - val_loss: 0.0024 - val_mae: 0.0430\nEpoch 146/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0436 - val_loss: 0.0030 - val_mae: 0.0502\nEpoch 147/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0027 - val_mae: 0.0510\nEpoch 148/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0439 - val_loss: 0.0025 - val_mae: 0.0407\nEpoch 149/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0429 - val_loss: 0.0022 - val_mae: 0.0380\nEpoch 150/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0440 - val_loss: 0.0023 - val_mae: 0.0384\nEpoch 151/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0435 - val_loss: 0.0023 - val_mae: 0.0389\nEpoch 152/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0429 - val_loss: 0.0022 - val_mae: 0.0385\nEpoch 153/500\n33/33 [==============================] - 0s 9ms/step - loss: 0.0026 - mae: 0.0439 - val_loss: 0.0025 - val_mae: 0.0406\nEpoch 154/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0488 - val_loss: 0.0028 - val_mae: 0.0465\nEpoch 155/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0449 - val_loss: 0.0024 - val_mae: 0.0389\nEpoch 156/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0464 - val_loss: 0.0026 - val_mae: 0.0485\nEpoch 157/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0439 - val_loss: 0.0022 - val_mae: 0.0386\nEpoch 158/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0464 - val_loss: 0.0024 - val_mae: 0.0403\nEpoch 159/500\n33/33 [==============================] - 1s 12ms/step - loss: 0.0027 - mae: 0.0469 - val_loss: 0.0023 - val_mae: 0.0394\nEpoch 160/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0023 - val_mae: 0.0400\nEpoch 161/500\n33/33 [==============================] - 1s 13ms/step - loss: 0.0032 - mae: 0.0549 - val_loss: 0.0023 - val_mae: 0.0380\nEpoch 162/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0026 - mae: 0.0441 - val_loss: 0.0022 - val_mae: 0.0379\nEpoch 163/500\n33/33 [==============================] - 1s 12ms/step - loss: 0.0026 - mae: 0.0460 - val_loss: 0.0025 - val_mae: 0.0477\nEpoch 164/500\n33/33 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0458 - val_loss: 0.0023 - val_mae: 0.0381\nEpoch 165/500\n33/33 [==============================] - 0s 11ms/step - loss: 0.0026 - mae: 0.0433 - val_loss: 0.0022 - val_mae: 0.0377\nEpoch 166/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0488 - val_loss: 0.0026 - val_mae: 0.0418\nEpoch 167/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0439 - val_loss: 0.0029 - val_mae: 0.0486\nEpoch 168/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0484 - val_loss: 0.0023 - val_mae: 0.0380\nEpoch 169/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0443 - val_loss: 0.0022 - val_mae: 0.0379\nEpoch 170/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0471 - val_loss: 0.0024 - val_mae: 0.0401\nEpoch 171/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0434 - val_loss: 0.0024 - val_mae: 0.0434\nEpoch 172/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0454 - val_loss: 0.0033 - val_mae: 0.0560\nEpoch 173/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0432 - val_loss: 0.0023 - val_mae: 0.0416\nEpoch 174/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0462 - val_loss: 0.0024 - val_mae: 0.0398\nEpoch 175/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0462 - val_loss: 0.0024 - val_mae: 0.0393\nEpoch 176/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0472 - val_loss: 0.0026 - val_mae: 0.0481\nEpoch 177/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0025 - mae: 0.0427 - val_loss: 0.0026 - val_mae: 0.0421\nEpoch 178/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0466 - val_loss: 0.0023 - val_mae: 0.0411\nEpoch 179/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0431 - val_loss: 0.0023 - val_mae: 0.0378\nEpoch 180/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0482 - val_loss: 0.0023 - val_mae: 0.0392\nEpoch 181/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0448 - val_loss: 0.0024 - val_mae: 0.0399\nEpoch 182/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0423 - val_loss: 0.0022 - val_mae: 0.0384\nEpoch 183/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0025 - mae: 0.0423 - val_loss: 0.0022 - val_mae: 0.0378\nEpoch 184/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0025 - mae: 0.0427 - val_loss: 0.0029 - val_mae: 0.0496\nEpoch 185/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0031 - mae: 0.0527 - val_loss: 0.0028 - val_mae: 0.0464\nEpoch 186/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0468 - val_loss: 0.0023 - val_mae: 0.0384\nEpoch 187/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0441 - val_loss: 0.0026 - val_mae: 0.0429\nEpoch 188/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0474 - val_loss: 0.0028 - val_mae: 0.0463\nEpoch 189/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0029 - mae: 0.0484 - val_loss: 0.0027 - val_mae: 0.0446\nEpoch 190/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0451 - val_loss: 0.0028 - val_mae: 0.0465\nEpoch 191/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0492 - val_loss: 0.0023 - val_mae: 0.0418\nEpoch 192/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0032 - mae: 0.0530 - val_loss: 0.0023 - val_mae: 0.0404\nEpoch 193/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0456 - val_loss: 0.0027 - val_mae: 0.0515\nEpoch 194/500\n33/33 [==============================] - 0s 13ms/step - loss: 0.0029 - mae: 0.0504 - val_loss: 0.0023 - val_mae: 0.0391\nEpoch 195/500\n33/33 [==============================] - 1s 12ms/step - loss: 0.0025 - mae: 0.0428 - val_loss: 0.0028 - val_mae: 0.0462\nEpoch 196/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0026 - mae: 0.0443 - val_loss: 0.0030 - val_mae: 0.0579\nEpoch 197/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0025 - mae: 0.0444 - val_loss: 0.0025 - val_mae: 0.0464\nEpoch 198/500\n33/33 [==============================] - 0s 12ms/step - loss: 0.0031 - mae: 0.0540 - val_loss: 0.0033 - val_mae: 0.0556\nEpoch 199/500\n33/33 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0459 - val_loss: 0.0022 - val_mae: 0.0382\nEpoch 200/500\n33/33 [==============================] - 1s 14ms/step - loss: 0.0027 - mae: 0.0450 - val_loss: 0.0023 - val_mae: 0.0420\nEpoch 201/500\n33/33 [==============================] - 0s 10ms/step - loss: 0.0026 - mae: 0.0447 - val_loss: 0.0023 - val_mae: 0.0388\nEpoch 202/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0475 - val_loss: 0.0023 - val_mae: 0.0387\nEpoch 203/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0440 - val_loss: 0.0023 - val_mae: 0.0405\nEpoch 204/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0425 - val_loss: 0.0025 - val_mae: 0.0417\nEpoch 205/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0452 - val_loss: 0.0023 - val_mae: 0.0381\nEpoch 206/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0425 - val_loss: 0.0030 - val_mae: 0.0497\nEpoch 207/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0476 - val_loss: 0.0024 - val_mae: 0.0425\nEpoch 208/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0426 - val_loss: 0.0023 - val_mae: 0.0383\nEpoch 209/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0432 - val_loss: 0.0023 - val_mae: 0.0410\nEpoch 210/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0425 - val_loss: 0.0024 - val_mae: 0.0395\nEpoch 211/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0430 - val_loss: 0.0024 - val_mae: 0.0426\nEpoch 212/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0432 - val_loss: 0.0025 - val_mae: 0.0403\nEpoch 213/500\n33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0449 - val_loss: 0.0023 - val_mae: 0.0387\nEpoch 214/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0029 - mae: 0.0502 - val_loss: 0.0033 - val_mae: 0.0558\nEpoch 215/500\n33/33 [==============================] - 0s 7ms/step - loss: 0.0029 - mae: 0.0500 - val_loss: 0.0022 - val_mae: 0.0381\n\n\nThis model reaches a validation MAE of about 38,100 (your mileage may vary). That’s better than naive forecasting, but worse than the ARIMA model. Can we do better with an RNN? Let’s see!\n\n\n10.2.6 Forecasting Using a Simple RNN\nLet’s try the most basic RNN, containing a single recurrent layer with just one recurrent neuron:\n\ntf.random.set_seed(42)  # extra code – ensures reproducibility\nmodel = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(1, input_shape=[None, 1]) # 1*(1+1) + 1*1\n])\n\nmodel.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn (SimpleRNN)      (None, 1)                 3         \n                                                                 \n=================================================================\nTotal params: 3\nTrainable params: 3\nNon-trainable params: 0\n_________________________________________________________________\n\n\nAll recurrent layers in Keras expect 3D inputs of shape [batch size, time steps, dimensionality], where dimensionality is 1 for univariate time series and more for multivariate time series. Recall that the input_shape argument ignores the first dimension (i.e., the batch size), and since recurrent layers can accept input sequences of any length, we can set the second dimension to None, which means “any size”. Lastly, since we’re dealing with a univariate time series, we need the last dimension’s size to be 1. This is why we specified the input shape [None, 1]: it means “univariate sequences of any length”. Note that the datasets actually contain inputs of shape [batch size, timesteps], so we’re missing the last dimension, of size 1, but Keras is kind enough to add it for us in this case.\nSo that’s our first recurrent model! It’s a sequence-to-vector model. Since there’s a single output neuron, the output vector has a size of 1. We define another helper function for fit and evaluation:\n\ndef fit_and_evaluate(model, train_set, valid_set, learning_rate, epochs=500):\n    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_mae\", patience=50, restore_best_weights=True)\n    opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n    \n    model.compile(loss=tf.keras.losses.Huber(), optimizer=opt, metrics=[\"mae\"])\n    \n    history = model.fit(train_set, validation_data=valid_set, epochs=epochs,\n                        callbacks=[early_stopping_cb])\n    valid_loss, valid_mae = model.evaluate(valid_set)\n    return valid_mae * 1e6\n\n\nfit_and_evaluate(model, train_ds, valid_ds, learning_rate=0.02)\n\nEpoch 1/500\n33/33 [==============================] - 2s 45ms/step - loss: 0.8407 - mae: 1.3014 - val_loss: 0.0304 - val_mae: 0.1623\nEpoch 2/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0467 - mae: 0.2393 - val_loss: 0.0493 - val_mae: 0.2517\nEpoch 3/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0317 - mae: 0.1811 - val_loss: 0.0246 - val_mae: 0.1599\nEpoch 4/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0239 - mae: 0.1783 - val_loss: 0.0233 - val_mae: 0.1617\nEpoch 5/500\n33/33 [==============================] - 2s 64ms/step - loss: 0.0232 - mae: 0.1759 - val_loss: 0.0230 - val_mae: 0.1557\nEpoch 6/500\n33/33 [==============================] - 3s 80ms/step - loss: 0.0225 - mae: 0.1717 - val_loss: 0.0221 - val_mae: 0.1541\nEpoch 7/500\n33/33 [==============================] - 3s 80ms/step - loss: 0.0218 - mae: 0.1717 - val_loss: 0.0214 - val_mae: 0.1518\nEpoch 8/500\n33/33 [==============================] - 2s 58ms/step - loss: 0.0211 - mae: 0.1686 - val_loss: 0.0207 - val_mae: 0.1493\nEpoch 9/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0205 - mae: 0.1670 - val_loss: 0.0200 - val_mae: 0.1476\nEpoch 10/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0199 - mae: 0.1658 - val_loss: 0.0195 - val_mae: 0.1448\nEpoch 11/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0193 - mae: 0.1652 - val_loss: 0.0189 - val_mae: 0.1431\nEpoch 12/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0188 - mae: 0.1634 - val_loss: 0.0183 - val_mae: 0.1425\nEpoch 13/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0182 - mae: 0.1617 - val_loss: 0.0178 - val_mae: 0.1412\nEpoch 14/500\n33/33 [==============================] - 2s 68ms/step - loss: 0.0177 - mae: 0.1603 - val_loss: 0.0172 - val_mae: 0.1415\nEpoch 15/500\n33/33 [==============================] - 2s 65ms/step - loss: 0.0173 - mae: 0.1598 - val_loss: 0.0169 - val_mae: 0.1392\nEpoch 16/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0168 - mae: 0.1586 - val_loss: 0.0164 - val_mae: 0.1390\nEpoch 17/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0164 - mae: 0.1571 - val_loss: 0.0159 - val_mae: 0.1386\nEpoch 18/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0160 - mae: 0.1560 - val_loss: 0.0156 - val_mae: 0.1378\nEpoch 19/500\n33/33 [==============================] - 1s 37ms/step - loss: 0.0157 - mae: 0.1550 - val_loss: 0.0152 - val_mae: 0.1372\nEpoch 20/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0153 - mae: 0.1543 - val_loss: 0.0149 - val_mae: 0.1355\nEpoch 21/500\n33/33 [==============================] - 2s 63ms/step - loss: 0.0150 - mae: 0.1525 - val_loss: 0.0146 - val_mae: 0.1345\nEpoch 22/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0147 - mae: 0.1505 - val_loss: 0.0144 - val_mae: 0.1336\nEpoch 23/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0145 - mae: 0.1515 - val_loss: 0.0140 - val_mae: 0.1345\nEpoch 24/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0142 - mae: 0.1487 - val_loss: 0.0139 - val_mae: 0.1319\nEpoch 25/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0139 - mae: 0.1477 - val_loss: 0.0136 - val_mae: 0.1318\nEpoch 26/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0137 - mae: 0.1477 - val_loss: 0.0133 - val_mae: 0.1315\nEpoch 27/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0135 - mae: 0.1464 - val_loss: 0.0132 - val_mae: 0.1302\nEpoch 28/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0133 - mae: 0.1444 - val_loss: 0.0130 - val_mae: 0.1292\nEpoch 29/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0131 - mae: 0.1440 - val_loss: 0.0128 - val_mae: 0.1291\nEpoch 30/500\n33/33 [==============================] - 2s 67ms/step - loss: 0.0130 - mae: 0.1430 - val_loss: 0.0127 - val_mae: 0.1274\nEpoch 31/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0128 - mae: 0.1416 - val_loss: 0.0125 - val_mae: 0.1270\nEpoch 32/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0127 - mae: 0.1408 - val_loss: 0.0123 - val_mae: 0.1267\nEpoch 33/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0125 - mae: 0.1410 - val_loss: 0.0122 - val_mae: 0.1250\nEpoch 34/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0124 - mae: 0.1390 - val_loss: 0.0121 - val_mae: 0.1239\nEpoch 35/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0123 - mae: 0.1381 - val_loss: 0.0120 - val_mae: 0.1243\nEpoch 36/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0121 - mae: 0.1376 - val_loss: 0.0119 - val_mae: 0.1231\nEpoch 37/500\n33/33 [==============================] - 2s 46ms/step - loss: 0.0120 - mae: 0.1356 - val_loss: 0.0118 - val_mae: 0.1221\nEpoch 38/500\n33/33 [==============================] - 3s 76ms/step - loss: 0.0119 - mae: 0.1354 - val_loss: 0.0117 - val_mae: 0.1221\nEpoch 39/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0119 - mae: 0.1358 - val_loss: 0.0116 - val_mae: 0.1210\nEpoch 40/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0117 - mae: 0.1338 - val_loss: 0.0115 - val_mae: 0.1210\nEpoch 41/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0116 - mae: 0.1330 - val_loss: 0.0114 - val_mae: 0.1200\nEpoch 42/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0116 - mae: 0.1324 - val_loss: 0.0114 - val_mae: 0.1196\nEpoch 43/500\n33/33 [==============================] - 3s 74ms/step - loss: 0.0115 - mae: 0.1319 - val_loss: 0.0113 - val_mae: 0.1195\nEpoch 44/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0114 - mae: 0.1319 - val_loss: 0.0112 - val_mae: 0.1188\nEpoch 45/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0114 - mae: 0.1303 - val_loss: 0.0112 - val_mae: 0.1180\nEpoch 46/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0113 - mae: 0.1300 - val_loss: 0.0111 - val_mae: 0.1180\nEpoch 47/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0112 - mae: 0.1296 - val_loss: 0.0110 - val_mae: 0.1172\nEpoch 48/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0112 - mae: 0.1288 - val_loss: 0.0110 - val_mae: 0.1165\nEpoch 49/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0111 - mae: 0.1281 - val_loss: 0.0109 - val_mae: 0.1165\nEpoch 50/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0111 - mae: 0.1273 - val_loss: 0.0109 - val_mae: 0.1160\nEpoch 51/500\n33/33 [==============================] - 3s 80ms/step - loss: 0.0110 - mae: 0.1274 - val_loss: 0.0108 - val_mae: 0.1154\nEpoch 52/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0110 - mae: 0.1260 - val_loss: 0.0108 - val_mae: 0.1149\nEpoch 53/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0109 - mae: 0.1265 - val_loss: 0.0107 - val_mae: 0.1149\nEpoch 54/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0109 - mae: 0.1256 - val_loss: 0.0107 - val_mae: 0.1141\nEpoch 55/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0108 - mae: 0.1242 - val_loss: 0.0107 - val_mae: 0.1135\nEpoch 56/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0108 - mae: 0.1243 - val_loss: 0.0106 - val_mae: 0.1136\nEpoch 57/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0108 - mae: 0.1241 - val_loss: 0.0106 - val_mae: 0.1128\nEpoch 58/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0107 - mae: 0.1233 - val_loss: 0.0106 - val_mae: 0.1125\nEpoch 59/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0107 - mae: 0.1229 - val_loss: 0.0106 - val_mae: 0.1120\nEpoch 60/500\n33/33 [==============================] - 2s 67ms/step - loss: 0.0107 - mae: 0.1228 - val_loss: 0.0105 - val_mae: 0.1120\nEpoch 61/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0107 - mae: 0.1221 - val_loss: 0.0105 - val_mae: 0.1114\nEpoch 62/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0106 - mae: 0.1214 - val_loss: 0.0105 - val_mae: 0.1108\nEpoch 63/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0106 - mae: 0.1215 - val_loss: 0.0104 - val_mae: 0.1109\nEpoch 64/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0106 - mae: 0.1210 - val_loss: 0.0104 - val_mae: 0.1102\nEpoch 65/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0106 - mae: 0.1211 - val_loss: 0.0104 - val_mae: 0.1101\nEpoch 66/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0106 - mae: 0.1201 - val_loss: 0.0104 - val_mae: 0.1097\nEpoch 67/500\n33/33 [==============================] - 3s 81ms/step - loss: 0.0105 - mae: 0.1198 - val_loss: 0.0104 - val_mae: 0.1092\nEpoch 68/500\n33/33 [==============================] - 2s 58ms/step - loss: 0.0105 - mae: 0.1197 - val_loss: 0.0104 - val_mae: 0.1090\nEpoch 69/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0105 - mae: 0.1193 - val_loss: 0.0103 - val_mae: 0.1089\nEpoch 70/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0105 - mae: 0.1189 - val_loss: 0.0103 - val_mae: 0.1086\nEpoch 71/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0105 - mae: 0.1188 - val_loss: 0.0103 - val_mae: 0.1083\nEpoch 72/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0105 - mae: 0.1185 - val_loss: 0.0103 - val_mae: 0.1082\nEpoch 73/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0105 - mae: 0.1183 - val_loss: 0.0103 - val_mae: 0.1079\nEpoch 74/500\n33/33 [==============================] - 3s 75ms/step - loss: 0.0104 - mae: 0.1179 - val_loss: 0.0103 - val_mae: 0.1076\nEpoch 75/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0104 - mae: 0.1182 - val_loss: 0.0103 - val_mae: 0.1075\nEpoch 76/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1176 - val_loss: 0.0103 - val_mae: 0.1071\nEpoch 77/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1172 - val_loss: 0.0103 - val_mae: 0.1071\nEpoch 78/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1175 - val_loss: 0.0103 - val_mae: 0.1069\nEpoch 79/500\n33/33 [==============================] - 1s 43ms/step - loss: 0.0104 - mae: 0.1168 - val_loss: 0.0103 - val_mae: 0.1067\nEpoch 80/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0104 - mae: 0.1168 - val_loss: 0.0103 - val_mae: 0.1065\nEpoch 81/500\n33/33 [==============================] - 3s 76ms/step - loss: 0.0104 - mae: 0.1167 - val_loss: 0.0102 - val_mae: 0.1065\nEpoch 82/500\n33/33 [==============================] - 2s 64ms/step - loss: 0.0104 - mae: 0.1165 - val_loss: 0.0103 - val_mae: 0.1063\nEpoch 83/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1164 - val_loss: 0.0102 - val_mae: 0.1062\nEpoch 84/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1162 - val_loss: 0.0103 - val_mae: 0.1059\nEpoch 85/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1166 - val_loss: 0.0102 - val_mae: 0.1060\nEpoch 86/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1160 - val_loss: 0.0103 - val_mae: 0.1056\nEpoch 87/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1158 - val_loss: 0.0103 - val_mae: 0.1054\nEpoch 88/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1155 - val_loss: 0.0102 - val_mae: 0.1054\nEpoch 89/500\n33/33 [==============================] - 2s 73ms/step - loss: 0.0104 - mae: 0.1155 - val_loss: 0.0103 - val_mae: 0.1053\nEpoch 90/500\n33/33 [==============================] - 2s 59ms/step - loss: 0.0104 - mae: 0.1155 - val_loss: 0.0103 - val_mae: 0.1052\nEpoch 91/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1155 - val_loss: 0.0102 - val_mae: 0.1051\nEpoch 92/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1152 - val_loss: 0.0103 - val_mae: 0.1049\nEpoch 93/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1152 - val_loss: 0.0103 - val_mae: 0.1049\nEpoch 94/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0104 - mae: 0.1152 - val_loss: 0.0103 - val_mae: 0.1047\nEpoch 95/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1148 - val_loss: 0.0103 - val_mae: 0.1047\nEpoch 96/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1151 - val_loss: 0.0102 - val_mae: 0.1049\nEpoch 97/500\n33/33 [==============================] - 2s 44ms/step - loss: 0.0104 - mae: 0.1148 - val_loss: 0.0102 - val_mae: 0.1046\nEpoch 98/500\n33/33 [==============================] - 3s 76ms/step - loss: 0.0104 - mae: 0.1150 - val_loss: 0.0103 - val_mae: 0.1044\nEpoch 99/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1145 - val_loss: 0.0102 - val_mae: 0.1044\nEpoch 100/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1144 - val_loss: 0.0103 - val_mae: 0.1043\nEpoch 101/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1146 - val_loss: 0.0102 - val_mae: 0.1044\nEpoch 102/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0104 - mae: 0.1145 - val_loss: 0.0102 - val_mae: 0.1042\nEpoch 103/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0104 - mae: 0.1145 - val_loss: 0.0102 - val_mae: 0.1043\nEpoch 104/500\n33/33 [==============================] - 3s 100ms/step - loss: 0.0104 - mae: 0.1146 - val_loss: 0.0102 - val_mae: 0.1042\nEpoch 105/500\n33/33 [==============================] - 3s 96ms/step - loss: 0.0104 - mae: 0.1143 - val_loss: 0.0103 - val_mae: 0.1039\nEpoch 106/500\n33/33 [==============================] - 3s 73ms/step - loss: 0.0104 - mae: 0.1140 - val_loss: 0.0103 - val_mae: 0.1039\nEpoch 107/500\n33/33 [==============================] - 2s 66ms/step - loss: 0.0104 - mae: 0.1143 - val_loss: 0.0103 - val_mae: 0.1039\nEpoch 108/500\n33/33 [==============================] - 2s 64ms/step - loss: 0.0104 - mae: 0.1141 - val_loss: 0.0103 - val_mae: 0.1038\nEpoch 109/500\n33/33 [==============================] - 3s 78ms/step - loss: 0.0104 - mae: 0.1142 - val_loss: 0.0102 - val_mae: 0.1039\nEpoch 110/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1140 - val_loss: 0.0103 - val_mae: 0.1038\nEpoch 111/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0104 - mae: 0.1140 - val_loss: 0.0102 - val_mae: 0.1038\nEpoch 112/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0104 - mae: 0.1141 - val_loss: 0.0103 - val_mae: 0.1036\nEpoch 113/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1142 - val_loss: 0.0102 - val_mae: 0.1040\nEpoch 114/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1138 - val_loss: 0.0103 - val_mae: 0.1037\nEpoch 115/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1139 - val_loss: 0.0103 - val_mae: 0.1036\nEpoch 116/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1137 - val_loss: 0.0103 - val_mae: 0.1036\nEpoch 117/500\n33/33 [==============================] - 2s 56ms/step - loss: 0.0104 - mae: 0.1139 - val_loss: 0.0103 - val_mae: 0.1035\nEpoch 118/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0104 - mae: 0.1139 - val_loss: 0.0102 - val_mae: 0.1036\nEpoch 119/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1137 - val_loss: 0.0103 - val_mae: 0.1034\nEpoch 120/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1137 - val_loss: 0.0102 - val_mae: 0.1034\nEpoch 121/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1138 - val_loss: 0.0103 - val_mae: 0.1034\nEpoch 122/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1134 - val_loss: 0.0103 - val_mae: 0.1033\nEpoch 123/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1137 - val_loss: 0.0103 - val_mae: 0.1034\nEpoch 124/500\n33/33 [==============================] - 2s 44ms/step - loss: 0.0104 - mae: 0.1136 - val_loss: 0.0103 - val_mae: 0.1034\nEpoch 125/500\n33/33 [==============================] - 3s 99ms/step - loss: 0.0104 - mae: 0.1136 - val_loss: 0.0102 - val_mae: 0.1034\nEpoch 126/500\n33/33 [==============================] - 3s 95ms/step - loss: 0.0104 - mae: 0.1138 - val_loss: 0.0102 - val_mae: 0.1033\nEpoch 127/500\n33/33 [==============================] - 4s 124ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0103 - val_mae: 0.1033\nEpoch 128/500\n33/33 [==============================] - 3s 78ms/step - loss: 0.0104 - mae: 0.1138 - val_loss: 0.0102 - val_mae: 0.1033\nEpoch 129/500\n33/33 [==============================] - 4s 106ms/step - loss: 0.0104 - mae: 0.1136 - val_loss: 0.0102 - val_mae: 0.1034\nEpoch 130/500\n33/33 [==============================] - 3s 78ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0103 - val_mae: 0.1032\nEpoch 131/500\n33/33 [==============================] - 3s 78ms/step - loss: 0.0104 - mae: 0.1136 - val_loss: 0.0102 - val_mae: 0.1033\nEpoch 132/500\n33/33 [==============================] - 3s 85ms/step - loss: 0.0104 - mae: 0.1135 - val_loss: 0.0103 - val_mae: 0.1032\nEpoch 133/500\n33/33 [==============================] - 3s 85ms/step - loss: 0.0104 - mae: 0.1136 - val_loss: 0.0103 - val_mae: 0.1032\nEpoch 134/500\n33/33 [==============================] - 4s 114ms/step - loss: 0.0104 - mae: 0.1135 - val_loss: 0.0102 - val_mae: 0.1032\nEpoch 135/500\n33/33 [==============================] - 3s 77ms/step - loss: 0.0104 - mae: 0.1137 - val_loss: 0.0102 - val_mae: 0.1034\nEpoch 136/500\n33/33 [==============================] - 3s 78ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1032\nEpoch 137/500\n33/33 [==============================] - 3s 90ms/step - loss: 0.0103 - mae: 0.1133 - val_loss: 0.0103 - val_mae: 0.1032\nEpoch 138/500\n33/33 [==============================] - 4s 131ms/step - loss: 0.0103 - mae: 0.1135 - val_loss: 0.0103 - val_mae: 0.1031\nEpoch 139/500\n33/33 [==============================] - 3s 102ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0102 - val_mae: 0.1033\nEpoch 140/500\n33/33 [==============================] - 3s 79ms/step - loss: 0.0103 - mae: 0.1134 - val_loss: 0.0103 - val_mae: 0.1031\nEpoch 141/500\n33/33 [==============================] - 3s 86ms/step - loss: 0.0104 - mae: 0.1136 - val_loss: 0.0103 - val_mae: 0.1032\nEpoch 142/500\n33/33 [==============================] - 3s 80ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1031\nEpoch 143/500\n33/33 [==============================] - 2s 68ms/step - loss: 0.0104 - mae: 0.1135 - val_loss: 0.0102 - val_mae: 0.1031\nEpoch 144/500\n33/33 [==============================] - 2s 59ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1031\nEpoch 145/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1133 - val_loss: 0.0103 - val_mae: 0.1031\nEpoch 146/500\n33/33 [==============================] - 2s 54ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1031\nEpoch 147/500\n33/33 [==============================] - 2s 65ms/step - loss: 0.0104 - mae: 0.1136 - val_loss: 0.0102 - val_mae: 0.1032\nEpoch 148/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1030\nEpoch 149/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1031\nEpoch 150/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0103 - val_mae: 0.1031\nEpoch 151/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1031\nEpoch 152/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1031\nEpoch 153/500\n33/33 [==============================] - 2s 46ms/step - loss: 0.0103 - mae: 0.1133 - val_loss: 0.0103 - val_mae: 0.1030\nEpoch 154/500\n33/33 [==============================] - 2s 67ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 155/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1030\nEpoch 156/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1030\nEpoch 157/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0103 - val_mae: 0.1030\nEpoch 158/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 159/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1030\nEpoch 160/500\n33/33 [==============================] - 2s 60ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0102 - val_mae: 0.1031\nEpoch 161/500\n33/33 [==============================] - 2s 63ms/step - loss: 0.0103 - mae: 0.1133 - val_loss: 0.0103 - val_mae: 0.1030\nEpoch 162/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1030\nEpoch 163/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 164/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1030\nEpoch 165/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 166/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 167/500\n33/33 [==============================] - 1s 43ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 168/500\n33/33 [==============================] - 3s 98ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 169/500\n33/33 [==============================] - 3s 75ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1029\nEpoch 170/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 171/500\n33/33 [==============================] - 3s 77ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 172/500\n33/33 [==============================] - 2s 46ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 173/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 174/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 175/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 176/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 177/500\n33/33 [==============================] - 1s 43ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 178/500\n33/33 [==============================] - 3s 75ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1029\nEpoch 179/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 180/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 181/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0102 - val_mae: 0.1029\nEpoch 182/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 183/500\n33/33 [==============================] - 2s 53ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 184/500\n33/33 [==============================] - 2s 68ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 185/500\n33/33 [==============================] - 3s 76ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 186/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 187/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 188/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 189/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1031\nEpoch 190/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 191/500\n33/33 [==============================] - 2s 65ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 192/500\n33/33 [==============================] - 3s 75ms/step - loss: 0.0104 - mae: 0.1135 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 193/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 194/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 195/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 196/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1135 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 197/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 198/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 199/500\n33/33 [==============================] - 2s 74ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 200/500\n33/33 [==============================] - 2s 67ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 201/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 202/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 203/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 204/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 205/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 206/500\n33/33 [==============================] - 2s 73ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1031\nEpoch 207/500\n33/33 [==============================] - 2s 67ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 208/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 209/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 210/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1029\nEpoch 211/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 212/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 213/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 214/500\n33/33 [==============================] - 2s 73ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 215/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 216/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 217/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 218/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 219/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 220/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 221/500\n33/33 [==============================] - 2s 71ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 222/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 223/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 224/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 225/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0102 - val_mae: 0.1029\nEpoch 226/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1031\nEpoch 227/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 228/500\n33/33 [==============================] - 3s 75ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 229/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 230/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 231/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 232/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 233/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 234/500\n33/33 [==============================] - 3s 78ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 235/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1030\nEpoch 236/500\n33/33 [==============================] - 2s 55ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 237/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 238/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 239/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 240/500\n33/33 [==============================] - 2s 68ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1029\nEpoch 241/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 242/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 243/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1127 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 244/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 245/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 246/500\n33/33 [==============================] - 2s 68ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 247/500\n33/33 [==============================] - 2s 64ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1029\nEpoch 248/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 249/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 250/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 251/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0102 - val_mae: 0.1031\nEpoch 252/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 253/500\n33/33 [==============================] - 3s 94ms/step - loss: 0.0104 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 254/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 255/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 256/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 257/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 258/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 259/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 260/500\n33/33 [==============================] - 2s 67ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 261/500\n33/33 [==============================] - 3s 74ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 262/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 263/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 264/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 265/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 266/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 267/500\n33/33 [==============================] - 3s 77ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 268/500\n33/33 [==============================] - 2s 45ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 269/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 270/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 271/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 272/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 273/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 274/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 275/500\n33/33 [==============================] - 3s 77ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 276/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 277/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 278/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 279/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 280/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 281/500\n33/33 [==============================] - 2s 57ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 282/500\n33/33 [==============================] - 4s 105ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 283/500\n33/33 [==============================] - 3s 98ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 284/500\n33/33 [==============================] - 2s 70ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 285/500\n33/33 [==============================] - 3s 74ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 286/500\n33/33 [==============================] - 3s 89ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 287/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 288/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 289/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0102 - val_mae: 0.1029\nEpoch 290/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 291/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 292/500\n33/33 [==============================] - 2s 60ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 293/500\n33/33 [==============================] - 3s 78ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 294/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 295/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 296/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1126 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 297/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0102 - val_mae: 0.1029\nEpoch 298/500\n33/33 [==============================] - 2s 53ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 299/500\n33/33 [==============================] - 3s 76ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 300/500\n33/33 [==============================] - 2s 45ms/step - loss: 0.0103 - mae: 0.1127 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 301/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1029\nEpoch 302/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 303/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1028\nEpoch 304/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 305/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 306/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 307/500\n33/33 [==============================] - 3s 79ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029\nEpoch 308/500\n33/33 [==============================] - 1s 44ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 309/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030\nEpoch 310/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029\n3/3 [==============================] - 0s 12ms/step - loss: 0.0103 - mae: 0.1028\n\n\n102805.45055866241\n\n\nIts validation MAE is greater than 100,000! That was to be expected, for two reasons:\n\nThe model only has a single recurrent neuron, so the only data it can use to make a prediction at each time step is the input value at the current time step and the output value from the previous time step. That’s not much to go on! In other words, the RNN’s memory is extremely limited: it’s just a single number, its previous output. And let’s count how many parameters this model has: since there’s just one recurrent neuron with only two input values, the whole model only has three parameters (two weights plus a bias term). That’s far from enough for this time series. In contrast, our previous model could look at all 56 previous values at once, and it had a total of 57 parameters.\nThe time series actually contains values from 0 to about 1.4, but since the default activation function is tanh, the recurrent layer can only output values between –1 and +1. There’s no way it can predict values between 1.0 and 1.4.\n\nLet’s fix both of these issues: we will create a model with a larger recurrent layer, containing 32 recurrent neurons, and we will add a dense output layer on top of it with a single output neuron and no activation function. The recurrent layer will be able to carry much more information from one time step to the next, and the dense output layer will project the final output from 32 dimensions down to 1, without any value range constraints:\n\ntf.random.set_seed(42)  # extra code – ensures reproducibility\nunivar_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(32, input_shape=[None, 1]), # 32*(1+1) + 32*32\n    tf.keras.layers.Dense(1)  # no activation function by default, 1*(32+1)\n])\n\nunivar_model.summary()\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_1 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_1 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1,121\nTrainable params: 1,121\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfit_and_evaluate(univar_model, train_ds, valid_ds, learning_rate=0.05)\n\nEpoch 1/500\n33/33 [==============================] - 3s 44ms/step - loss: 0.0358 - mae: 0.2027 - val_loss: 0.0115 - val_mae: 0.1353\nEpoch 2/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0059 - mae: 0.0787 - val_loss: 0.0050 - val_mae: 0.0766\nEpoch 3/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0046 - mae: 0.0667 - val_loss: 0.0026 - val_mae: 0.0507\nEpoch 4/500\n33/33 [==============================] - 2s 71ms/step - loss: 0.0038 - mae: 0.0592 - val_loss: 0.0040 - val_mae: 0.0698\nEpoch 5/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0036 - mae: 0.0572 - val_loss: 0.0020 - val_mae: 0.0393\nEpoch 6/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0486 - val_loss: 0.0030 - val_mae: 0.0509\nEpoch 7/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0530 - val_loss: 0.0026 - val_mae: 0.0511\nEpoch 8/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0031 - mae: 0.0499 - val_loss: 0.0021 - val_mae: 0.0356\nEpoch 9/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0028 - mae: 0.0459 - val_loss: 0.0030 - val_mae: 0.0487\nEpoch 10/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0032 - mae: 0.0516 - val_loss: 0.0019 - val_mae: 0.0310\nEpoch 11/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0028 - mae: 0.0456 - val_loss: 0.0021 - val_mae: 0.0345\nEpoch 12/500\n33/33 [==============================] - 1s 44ms/step - loss: 0.0030 - mae: 0.0476 - val_loss: 0.0021 - val_mae: 0.0371\nEpoch 13/500\n33/33 [==============================] - 3s 79ms/step - loss: 0.0030 - mae: 0.0499 - val_loss: 0.0023 - val_mae: 0.0444\nEpoch 14/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0029 - mae: 0.0491 - val_loss: 0.0023 - val_mae: 0.0413\nEpoch 15/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0028 - mae: 0.0457 - val_loss: 0.0030 - val_mae: 0.0544\nEpoch 16/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0029 - mae: 0.0489 - val_loss: 0.0020 - val_mae: 0.0329\nEpoch 17/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0027 - mae: 0.0456 - val_loss: 0.0026 - val_mae: 0.0429\nEpoch 18/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0027 - val_mae: 0.0530\nEpoch 19/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0028 - mae: 0.0465 - val_loss: 0.0021 - val_mae: 0.0398\nEpoch 20/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0029 - mae: 0.0488 - val_loss: 0.0021 - val_mae: 0.0384\nEpoch 21/500\n33/33 [==============================] - 3s 76ms/step - loss: 0.0028 - mae: 0.0470 - val_loss: 0.0023 - val_mae: 0.0385\nEpoch 22/500\n33/33 [==============================] - 2s 45ms/step - loss: 0.0027 - mae: 0.0441 - val_loss: 0.0023 - val_mae: 0.0383\nEpoch 23/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0028 - mae: 0.0464 - val_loss: 0.0023 - val_mae: 0.0419\nEpoch 24/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0498 - val_loss: 0.0020 - val_mae: 0.0371\nEpoch 25/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0027 - mae: 0.0447 - val_loss: 0.0022 - val_mae: 0.0399\nEpoch 26/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0027 - mae: 0.0465 - val_loss: 0.0020 - val_mae: 0.0341\nEpoch 27/500\n33/33 [==============================] - 2s 44ms/step - loss: 0.0026 - mae: 0.0437 - val_loss: 0.0025 - val_mae: 0.0410\nEpoch 28/500\n33/33 [==============================] - 3s 77ms/step - loss: 0.0026 - mae: 0.0432 - val_loss: 0.0019 - val_mae: 0.0334\nEpoch 29/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0517 - val_loss: 0.0029 - val_mae: 0.0558\nEpoch 30/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0512 - val_loss: 0.0024 - val_mae: 0.0408\nEpoch 31/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0437 - val_loss: 0.0021 - val_mae: 0.0395\nEpoch 32/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0450 - val_loss: 0.0021 - val_mae: 0.0400\nEpoch 33/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0028 - mae: 0.0458 - val_loss: 0.0034 - val_mae: 0.0594\nEpoch 34/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0515 - val_loss: 0.0022 - val_mae: 0.0405\nEpoch 35/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0032 - mae: 0.0543 - val_loss: 0.0039 - val_mae: 0.0655\nEpoch 36/500\n33/33 [==============================] - 2s 60ms/step - loss: 0.0032 - mae: 0.0529 - val_loss: 0.0026 - val_mae: 0.0519\nEpoch 37/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0449 - val_loss: 0.0018 - val_mae: 0.0337\nEpoch 38/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0027 - mae: 0.0460 - val_loss: 0.0020 - val_mae: 0.0356\nEpoch 39/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0426 - val_loss: 0.0019 - val_mae: 0.0352\nEpoch 40/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0029 - mae: 0.0485 - val_loss: 0.0020 - val_mae: 0.0373\nEpoch 41/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0441 - val_loss: 0.0021 - val_mae: 0.0413\nEpoch 42/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0418 - val_loss: 0.0019 - val_mae: 0.0346\nEpoch 43/500\n33/33 [==============================] - 2s 70ms/step - loss: 0.0025 - mae: 0.0446 - val_loss: 0.0021 - val_mae: 0.0358\nEpoch 44/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0422 - val_loss: 0.0019 - val_mae: 0.0327\nEpoch 45/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0431 - val_loss: 0.0022 - val_mae: 0.0415\nEpoch 46/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0025 - mae: 0.0435 - val_loss: 0.0021 - val_mae: 0.0422\nEpoch 47/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0434 - val_loss: 0.0023 - val_mae: 0.0397\nEpoch 48/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0538 - val_loss: 0.0019 - val_mae: 0.0318\nEpoch 49/500\n33/33 [==============================] - 2s 60ms/step - loss: 0.0025 - mae: 0.0418 - val_loss: 0.0020 - val_mae: 0.0349\nEpoch 50/500\n33/33 [==============================] - 3s 74ms/step - loss: 0.0026 - mae: 0.0446 - val_loss: 0.0026 - val_mae: 0.0452\nEpoch 51/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0025 - mae: 0.0433 - val_loss: 0.0022 - val_mae: 0.0400\nEpoch 52/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0024 - mae: 0.0441 - val_loss: 0.0021 - val_mae: 0.0343\nEpoch 53/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0025 - mae: 0.0421 - val_loss: 0.0024 - val_mae: 0.0451\nEpoch 54/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0024 - mae: 0.0424 - val_loss: 0.0025 - val_mae: 0.0432\nEpoch 55/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0027 - mae: 0.0473 - val_loss: 0.0021 - val_mae: 0.0349\nEpoch 56/500\n33/33 [==============================] - 2s 54ms/step - loss: 0.0027 - mae: 0.0441 - val_loss: 0.0023 - val_mae: 0.0425\nEpoch 57/500\n33/33 [==============================] - 2s 63ms/step - loss: 0.0025 - mae: 0.0423 - val_loss: 0.0021 - val_mae: 0.0395\nEpoch 58/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0026 - mae: 0.0447 - val_loss: 0.0024 - val_mae: 0.0431\nEpoch 59/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0029 - mae: 0.0487 - val_loss: 0.0019 - val_mae: 0.0291\nEpoch 60/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0428 - val_loss: 0.0018 - val_mae: 0.0328\nEpoch 61/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0025 - mae: 0.0400 - val_loss: 0.0018 - val_mae: 0.0308\nEpoch 62/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0027 - mae: 0.0480 - val_loss: 0.0018 - val_mae: 0.0306\nEpoch 63/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0437 - val_loss: 0.0022 - val_mae: 0.0401\nEpoch 64/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0445 - val_loss: 0.0020 - val_mae: 0.0358\nEpoch 65/500\n33/33 [==============================] - 2s 71ms/step - loss: 0.0024 - mae: 0.0435 - val_loss: 0.0026 - val_mae: 0.0452\nEpoch 66/500\n33/33 [==============================] - 2s 60ms/step - loss: 0.0024 - mae: 0.0423 - val_loss: 0.0028 - val_mae: 0.0504\nEpoch 67/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0418 - val_loss: 0.0019 - val_mae: 0.0361\nEpoch 68/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0411 - val_loss: 0.0018 - val_mae: 0.0342\nEpoch 69/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0426 - val_loss: 0.0021 - val_mae: 0.0355\nEpoch 70/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0027 - mae: 0.0470 - val_loss: 0.0025 - val_mae: 0.0461\nEpoch 71/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0467 - val_loss: 0.0025 - val_mae: 0.0445\nEpoch 72/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0023 - mae: 0.0408 - val_loss: 0.0025 - val_mae: 0.0473\nEpoch 73/500\n33/33 [==============================] - 2s 58ms/step - loss: 0.0026 - mae: 0.0434 - val_loss: 0.0026 - val_mae: 0.0479\nEpoch 74/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0025 - mae: 0.0427 - val_loss: 0.0025 - val_mae: 0.0474\nEpoch 75/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0431 - val_loss: 0.0020 - val_mae: 0.0335\nEpoch 76/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0027 - mae: 0.0483 - val_loss: 0.0028 - val_mae: 0.0505\nEpoch 77/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0420 - val_loss: 0.0032 - val_mae: 0.0541\nEpoch 78/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0024 - mae: 0.0407 - val_loss: 0.0023 - val_mae: 0.0395\nEpoch 79/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0430 - val_loss: 0.0017 - val_mae: 0.0292\nEpoch 80/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0419 - val_loss: 0.0020 - val_mae: 0.0371\nEpoch 81/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0026 - mae: 0.0450 - val_loss: 0.0027 - val_mae: 0.0543\nEpoch 82/500\n33/33 [==============================] - 2s 64ms/step - loss: 0.0028 - mae: 0.0482 - val_loss: 0.0024 - val_mae: 0.0476\nEpoch 83/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0023 - mae: 0.0404 - val_loss: 0.0019 - val_mae: 0.0353\nEpoch 84/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0025 - mae: 0.0431 - val_loss: 0.0031 - val_mae: 0.0494\nEpoch 85/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0416 - val_loss: 0.0020 - val_mae: 0.0318\nEpoch 86/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0025 - mae: 0.0416 - val_loss: 0.0019 - val_mae: 0.0349\nEpoch 87/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0412 - val_loss: 0.0018 - val_mae: 0.0294\nEpoch 88/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0401 - val_loss: 0.0019 - val_mae: 0.0345\nEpoch 89/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0434 - val_loss: 0.0022 - val_mae: 0.0390\nEpoch 90/500\n33/33 [==============================] - 2s 71ms/step - loss: 0.0024 - mae: 0.0412 - val_loss: 0.0018 - val_mae: 0.0316\nEpoch 91/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0024 - mae: 0.0414 - val_loss: 0.0019 - val_mae: 0.0332\nEpoch 92/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0430 - val_loss: 0.0022 - val_mae: 0.0425\nEpoch 93/500\n33/33 [==============================] - 1s 37ms/step - loss: 0.0024 - mae: 0.0426 - val_loss: 0.0018 - val_mae: 0.0297\nEpoch 94/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0413 - val_loss: 0.0019 - val_mae: 0.0342\nEpoch 95/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0027 - mae: 0.0456 - val_loss: 0.0023 - val_mae: 0.0448\nEpoch 96/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0425 - val_loss: 0.0025 - val_mae: 0.0467\nEpoch 97/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0023 - mae: 0.0395 - val_loss: 0.0021 - val_mae: 0.0363\nEpoch 98/500\n33/33 [==============================] - 2s 63ms/step - loss: 0.0024 - mae: 0.0412 - val_loss: 0.0026 - val_mae: 0.0469\nEpoch 99/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0024 - mae: 0.0433 - val_loss: 0.0017 - val_mae: 0.0285\nEpoch 100/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0421 - val_loss: 0.0027 - val_mae: 0.0508\nEpoch 101/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0435 - val_loss: 0.0019 - val_mae: 0.0340\nEpoch 102/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0023 - mae: 0.0419 - val_loss: 0.0025 - val_mae: 0.0467\nEpoch 103/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0409 - val_loss: 0.0025 - val_mae: 0.0466\nEpoch 104/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0024 - mae: 0.0408 - val_loss: 0.0022 - val_mae: 0.0393\nEpoch 105/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0023 - mae: 0.0399 - val_loss: 0.0023 - val_mae: 0.0444\nEpoch 106/500\n33/33 [==============================] - 2s 64ms/step - loss: 0.0025 - mae: 0.0432 - val_loss: 0.0019 - val_mae: 0.0343\nEpoch 107/500\n33/33 [==============================] - 2s 60ms/step - loss: 0.0024 - mae: 0.0430 - val_loss: 0.0024 - val_mae: 0.0414\nEpoch 108/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0451 - val_loss: 0.0024 - val_mae: 0.0402\nEpoch 109/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0437 - val_loss: 0.0022 - val_mae: 0.0371\nEpoch 110/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0023 - mae: 0.0410 - val_loss: 0.0022 - val_mae: 0.0334\nEpoch 111/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0023 - mae: 0.0398 - val_loss: 0.0026 - val_mae: 0.0464\nEpoch 112/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0023 - mae: 0.0418 - val_loss: 0.0018 - val_mae: 0.0319\nEpoch 113/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0024 - mae: 0.0435 - val_loss: 0.0022 - val_mae: 0.0421\nEpoch 114/500\n33/33 [==============================] - 3s 73ms/step - loss: 0.0028 - mae: 0.0488 - val_loss: 0.0023 - val_mae: 0.0439\nEpoch 115/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0025 - mae: 0.0456 - val_loss: 0.0018 - val_mae: 0.0322\nEpoch 116/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0395 - val_loss: 0.0018 - val_mae: 0.0339\nEpoch 117/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0426 - val_loss: 0.0024 - val_mae: 0.0438\nEpoch 118/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0022 - mae: 0.0400 - val_loss: 0.0021 - val_mae: 0.0351\nEpoch 119/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0023 - mae: 0.0400 - val_loss: 0.0025 - val_mae: 0.0457\nEpoch 120/500\n33/33 [==============================] - 3s 75ms/step - loss: 0.0025 - mae: 0.0413 - val_loss: 0.0018 - val_mae: 0.0301\nEpoch 121/500\n33/33 [==============================] - 2s 43ms/step - loss: 0.0026 - mae: 0.0451 - val_loss: 0.0020 - val_mae: 0.0361\nEpoch 122/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0023 - mae: 0.0401 - val_loss: 0.0019 - val_mae: 0.0353\nEpoch 123/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0420 - val_loss: 0.0025 - val_mae: 0.0466\nEpoch 124/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0025 - mae: 0.0432 - val_loss: 0.0023 - val_mae: 0.0409\nEpoch 125/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0453 - val_loss: 0.0030 - val_mae: 0.0560\nEpoch 126/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0028 - mae: 0.0451 - val_loss: 0.0024 - val_mae: 0.0455\nEpoch 127/500\n33/33 [==============================] - 3s 76ms/step - loss: 0.0024 - mae: 0.0436 - val_loss: 0.0020 - val_mae: 0.0351\nEpoch 128/500\n33/33 [==============================] - 2s 52ms/step - loss: 0.0023 - mae: 0.0410 - val_loss: 0.0023 - val_mae: 0.0401\nEpoch 129/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0434 - val_loss: 0.0021 - val_mae: 0.0330\nEpoch 130/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0024 - mae: 0.0427 - val_loss: 0.0018 - val_mae: 0.0317\nEpoch 131/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0425 - val_loss: 0.0023 - val_mae: 0.0399\nEpoch 132/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0023 - mae: 0.0395 - val_loss: 0.0023 - val_mae: 0.0385\nEpoch 133/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0023 - mae: 0.0395 - val_loss: 0.0018 - val_mae: 0.0302\nEpoch 134/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0023 - mae: 0.0406 - val_loss: 0.0025 - val_mae: 0.0485\nEpoch 135/500\n33/33 [==============================] - 2s 66ms/step - loss: 0.0025 - mae: 0.0451 - val_loss: 0.0027 - val_mae: 0.0497\nEpoch 136/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0022 - mae: 0.0399 - val_loss: 0.0022 - val_mae: 0.0393\nEpoch 137/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0024 - mae: 0.0423 - val_loss: 0.0020 - val_mae: 0.0364\nEpoch 138/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0022 - mae: 0.0404 - val_loss: 0.0019 - val_mae: 0.0313\nEpoch 139/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0434 - val_loss: 0.0020 - val_mae: 0.0347\nEpoch 140/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0027 - mae: 0.0472 - val_loss: 0.0032 - val_mae: 0.0534\nEpoch 141/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0024 - mae: 0.0435 - val_loss: 0.0028 - val_mae: 0.0496\nEpoch 142/500\n33/33 [==============================] - 2s 70ms/step - loss: 0.0026 - mae: 0.0452 - val_loss: 0.0042 - val_mae: 0.0742\nEpoch 143/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0024 - mae: 0.0431 - val_loss: 0.0027 - val_mae: 0.0442\nEpoch 144/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0023 - mae: 0.0420 - val_loss: 0.0026 - val_mae: 0.0498\nEpoch 145/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0443 - val_loss: 0.0027 - val_mae: 0.0476\nEpoch 146/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0027 - mae: 0.0456 - val_loss: 0.0032 - val_mae: 0.0578\nEpoch 147/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0022 - mae: 0.0404 - val_loss: 0.0021 - val_mae: 0.0331\nEpoch 148/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0022 - mae: 0.0393 - val_loss: 0.0024 - val_mae: 0.0390\nEpoch 149/500\n33/33 [==============================] - 2s 68ms/step - loss: 0.0022 - mae: 0.0400 - val_loss: 0.0020 - val_mae: 0.0360\n3/3 [==============================] - 0s 17ms/step - loss: 0.0017 - mae: 0.0285\n\n\n28464.557603001595\n\n\nIts validation MAE reaches 28,464. That’s the best model we’ve trained so far, and it even beats the ARIMA model: we’re doing pretty well! We’ve only normalized the time series, without removing trend and seasonality, and yet the model still performs well. However, to get the best performance, you may want to try making the time series more stationary; for example, using differencing.\n\n\n10.2.7 Forecasting Using a Deep RNN\nImplementing a deep RNN with tf.keras is quite simple: just stack recurrent layers. In this example, we use three SimpleRNN layers. Make sure to set return_sequences=True for all recurrent layers (except the last one, if you only care about the last output). If you don’t, they will output a 2D array (containing only the output of the last time step) instead of a 3D array (containing outputs for all time steps), and the next recurrent layer will complain that you are not feeding it sequences in the expected 3D format.\nThe first two are sequence-to-sequence layers, and the last one is a sequence-to-vector layer. Finally, the Dense layer produces the model’s forecast (you can think of it as a vector-to-vector layer).\n\ntf.random.set_seed(42)\n\n# By default, recurrent layers in Keras only return the final output. \n# To make them return one output per time step, you must set return_sequences=True\n# number of parameters https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#recurrent-neural-networks-with-hidden-states\n\ndeep_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 1]), #1*32+32+32*32\n    tf.keras.layers.SimpleRNN(32, return_sequences=True), #32*32+32+32*32\n    tf.keras.layers.SimpleRNN(32), #32*1+1\n    tf.keras.layers.Dense(1)\n])\n\ndeep_model.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_2 (SimpleRNN)    (None, None, 32)          1088      \n                                                                 \n simple_rnn_3 (SimpleRNN)    (None, None, 32)          2080      \n                                                                 \n simple_rnn_4 (SimpleRNN)    (None, 32)                2080      \n                                                                 \n dense_2 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 5,281\nTrainable params: 5,281\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfit_and_evaluate(deep_model, train_ds, valid_ds, learning_rate=0.01)\n\nEpoch 1/500\n33/33 [==============================] - 7s 146ms/step - loss: 0.0300 - mae: 0.1895 - val_loss: 0.0143 - val_mae: 0.1314\nEpoch 2/500\n33/33 [==============================] - 6s 170ms/step - loss: 0.0144 - mae: 0.1474 - val_loss: 0.0178 - val_mae: 0.1244\nEpoch 3/500\n33/33 [==============================] - 5s 139ms/step - loss: 0.0102 - mae: 0.1181 - val_loss: 0.0065 - val_mae: 0.0875\nEpoch 4/500\n33/33 [==============================] - 7s 203ms/step - loss: 0.0066 - mae: 0.0839 - val_loss: 0.0029 - val_mae: 0.0553\nEpoch 5/500\n33/33 [==============================] - 5s 138ms/step - loss: 0.0057 - mae: 0.0747 - val_loss: 0.0025 - val_mae: 0.0507\nEpoch 6/500\n33/33 [==============================] - 7s 202ms/step - loss: 0.0050 - mae: 0.0672 - val_loss: 0.0022 - val_mae: 0.0442\nEpoch 7/500\n33/33 [==============================] - 5s 141ms/step - loss: 0.0044 - mae: 0.0620 - val_loss: 0.0020 - val_mae: 0.0402\nEpoch 8/500\n33/33 [==============================] - 7s 204ms/step - loss: 0.0041 - mae: 0.0587 - val_loss: 0.0020 - val_mae: 0.0409\nEpoch 9/500\n33/33 [==============================] - 5s 138ms/step - loss: 0.0040 - mae: 0.0572 - val_loss: 0.0025 - val_mae: 0.0491\nEpoch 10/500\n33/33 [==============================] - 5s 141ms/step - loss: 0.0042 - mae: 0.0609 - val_loss: 0.0020 - val_mae: 0.0427\nEpoch 11/500\n33/33 [==============================] - 7s 194ms/step - loss: 0.0039 - mae: 0.0570 - val_loss: 0.0020 - val_mae: 0.0381\nEpoch 12/500\n33/33 [==============================] - 5s 135ms/step - loss: 0.0042 - mae: 0.0613 - val_loss: 0.0031 - val_mae: 0.0602\nEpoch 13/500\n33/33 [==============================] - 7s 203ms/step - loss: 0.0037 - mae: 0.0566 - val_loss: 0.0018 - val_mae: 0.0368\nEpoch 14/500\n33/33 [==============================] - 4s 134ms/step - loss: 0.0035 - mae: 0.0520 - val_loss: 0.0022 - val_mae: 0.0444\nEpoch 15/500\n33/33 [==============================] - 5s 147ms/step - loss: 0.0035 - mae: 0.0521 - val_loss: 0.0020 - val_mae: 0.0397\nEpoch 16/500\n33/33 [==============================] - 6s 180ms/step - loss: 0.0034 - mae: 0.0511 - val_loss: 0.0018 - val_mae: 0.0315\nEpoch 17/500\n33/33 [==============================] - 5s 136ms/step - loss: 0.0035 - mae: 0.0529 - val_loss: 0.0035 - val_mae: 0.0649\nEpoch 18/500\n33/33 [==============================] - 7s 201ms/step - loss: 0.0041 - mae: 0.0619 - val_loss: 0.0017 - val_mae: 0.0318\nEpoch 19/500\n33/33 [==============================] - 5s 138ms/step - loss: 0.0034 - mae: 0.0506 - val_loss: 0.0017 - val_mae: 0.0303\nEpoch 20/500\n33/33 [==============================] - 5s 147ms/step - loss: 0.0031 - mae: 0.0476 - val_loss: 0.0017 - val_mae: 0.0301\nEpoch 21/500\n33/33 [==============================] - 6s 177ms/step - loss: 0.0030 - mae: 0.0466 - val_loss: 0.0031 - val_mae: 0.0597\nEpoch 22/500\n33/33 [==============================] - 5s 139ms/step - loss: 0.0032 - mae: 0.0506 - val_loss: 0.0018 - val_mae: 0.0301\nEpoch 23/500\n33/33 [==============================] - 7s 206ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0017 - val_mae: 0.0307\nEpoch 24/500\n33/33 [==============================] - 5s 139ms/step - loss: 0.0030 - mae: 0.0471 - val_loss: 0.0019 - val_mae: 0.0347\nEpoch 25/500\n33/33 [==============================] - 5s 159ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0018 - val_mae: 0.0311\nEpoch 26/500\n33/33 [==============================] - 5s 144ms/step - loss: 0.0030 - mae: 0.0472 - val_loss: 0.0023 - val_mae: 0.0427\nEpoch 27/500\n33/33 [==============================] - 7s 209ms/step - loss: 0.0031 - mae: 0.0496 - val_loss: 0.0017 - val_mae: 0.0330\nEpoch 28/500\n33/33 [==============================] - 5s 136ms/step - loss: 0.0031 - mae: 0.0500 - val_loss: 0.0018 - val_mae: 0.0321\nEpoch 29/500\n33/33 [==============================] - 7s 204ms/step - loss: 0.0037 - mae: 0.0573 - val_loss: 0.0020 - val_mae: 0.0351\nEpoch 30/500\n33/33 [==============================] - 5s 136ms/step - loss: 0.0031 - mae: 0.0504 - val_loss: 0.0018 - val_mae: 0.0348\nEpoch 31/500\n33/33 [==============================] - 7s 201ms/step - loss: 0.0032 - mae: 0.0524 - val_loss: 0.0018 - val_mae: 0.0346\nEpoch 32/500\n33/33 [==============================] - 5s 138ms/step - loss: 0.0030 - mae: 0.0475 - val_loss: 0.0027 - val_mae: 0.0504\nEpoch 33/500\n33/33 [==============================] - 7s 203ms/step - loss: 0.0030 - mae: 0.0465 - val_loss: 0.0017 - val_mae: 0.0287\nEpoch 34/500\n33/33 [==============================] - 5s 140ms/step - loss: 0.0029 - mae: 0.0479 - val_loss: 0.0020 - val_mae: 0.0361\nEpoch 35/500\n33/33 [==============================] - 5s 136ms/step - loss: 0.0028 - mae: 0.0444 - val_loss: 0.0018 - val_mae: 0.0307\nEpoch 36/500\n33/33 [==============================] - 7s 194ms/step - loss: 0.0038 - mae: 0.0613 - val_loss: 0.0030 - val_mae: 0.0608\nEpoch 37/500\n33/33 [==============================] - 5s 139ms/step - loss: 0.0038 - mae: 0.0607 - val_loss: 0.0018 - val_mae: 0.0345\nEpoch 38/500\n33/33 [==============================] - 7s 206ms/step - loss: 0.0029 - mae: 0.0478 - val_loss: 0.0020 - val_mae: 0.0387\nEpoch 39/500\n33/33 [==============================] - 5s 142ms/step - loss: 0.0028 - mae: 0.0461 - val_loss: 0.0017 - val_mae: 0.0330\nEpoch 40/500\n33/33 [==============================] - 13s 411ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0020 - val_mae: 0.0357\nEpoch 41/500\n33/33 [==============================] - 9s 255ms/step - loss: 0.0029 - mae: 0.0468 - val_loss: 0.0017 - val_mae: 0.0313\nEpoch 42/500\n33/33 [==============================] - 7s 201ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0020 - val_mae: 0.0358\nEpoch 43/500\n33/33 [==============================] - 5s 139ms/step - loss: 0.0027 - mae: 0.0425 - val_loss: 0.0018 - val_mae: 0.0336\nEpoch 44/500\n33/33 [==============================] - 5s 160ms/step - loss: 0.0029 - mae: 0.0478 - val_loss: 0.0017 - val_mae: 0.0321\nEpoch 45/500\n33/33 [==============================] - 6s 179ms/step - loss: 0.0030 - mae: 0.0484 - val_loss: 0.0019 - val_mae: 0.0352\nEpoch 46/500\n33/33 [==============================] - 14s 433ms/step - loss: 0.0030 - mae: 0.0489 - val_loss: 0.0022 - val_mae: 0.0399\nEpoch 47/500\n33/33 [==============================] - 12s 374ms/step - loss: 0.0029 - mae: 0.0475 - val_loss: 0.0017 - val_mae: 0.0310\nEpoch 48/500\n33/33 [==============================] - 12s 357ms/step - loss: 0.0027 - mae: 0.0438 - val_loss: 0.0018 - val_mae: 0.0305\nEpoch 49/500\n33/33 [==============================] - 5s 140ms/step - loss: 0.0028 - mae: 0.0454 - val_loss: 0.0018 - val_mae: 0.0317\nEpoch 50/500\n33/33 [==============================] - 8s 241ms/step - loss: 0.0026 - mae: 0.0430 - val_loss: 0.0018 - val_mae: 0.0334\nEpoch 51/500\n33/33 [==============================] - 11s 342ms/step - loss: 0.0027 - mae: 0.0459 - val_loss: 0.0017 - val_mae: 0.0312\nEpoch 52/500\n33/33 [==============================] - 8s 253ms/step - loss: 0.0027 - mae: 0.0444 - val_loss: 0.0017 - val_mae: 0.0316\nEpoch 53/500\n33/33 [==============================] - 7s 204ms/step - loss: 0.0029 - mae: 0.0469 - val_loss: 0.0022 - val_mae: 0.0394\nEpoch 54/500\n33/33 [==============================] - 5s 144ms/step - loss: 0.0027 - mae: 0.0434 - val_loss: 0.0019 - val_mae: 0.0343\nEpoch 55/500\n33/33 [==============================] - 6s 180ms/step - loss: 0.0027 - mae: 0.0452 - val_loss: 0.0017 - val_mae: 0.0301\nEpoch 56/500\n33/33 [==============================] - 7s 205ms/step - loss: 0.0026 - mae: 0.0438 - val_loss: 0.0024 - val_mae: 0.0450\nEpoch 57/500\n33/33 [==============================] - 5s 137ms/step - loss: 0.0027 - mae: 0.0441 - val_loss: 0.0017 - val_mae: 0.0311\nEpoch 58/500\n33/33 [==============================] - 5s 136ms/step - loss: 0.0027 - mae: 0.0447 - val_loss: 0.0020 - val_mae: 0.0375\nEpoch 59/500\n33/33 [==============================] - 8s 231ms/step - loss: 0.0025 - mae: 0.0420 - val_loss: 0.0023 - val_mae: 0.0450\nEpoch 60/500\n33/33 [==============================] - 13s 395ms/step - loss: 0.0026 - mae: 0.0433 - val_loss: 0.0019 - val_mae: 0.0349\nEpoch 61/500\n33/33 [==============================] - 10s 295ms/step - loss: 0.0026 - mae: 0.0430 - val_loss: 0.0017 - val_mae: 0.0301\nEpoch 62/500\n33/33 [==============================] - 5s 148ms/step - loss: 0.0027 - mae: 0.0444 - val_loss: 0.0019 - val_mae: 0.0363\nEpoch 63/500\n33/33 [==============================] - 5s 163ms/step - loss: 0.0026 - mae: 0.0426 - val_loss: 0.0017 - val_mae: 0.0316\nEpoch 64/500\n33/33 [==============================] - 5s 140ms/step - loss: 0.0025 - mae: 0.0417 - val_loss: 0.0018 - val_mae: 0.0324\nEpoch 65/500\n33/33 [==============================] - 7s 203ms/step - loss: 0.0025 - mae: 0.0420 - val_loss: 0.0022 - val_mae: 0.0422\nEpoch 66/500\n33/33 [==============================] - 5s 140ms/step - loss: 0.0027 - mae: 0.0452 - val_loss: 0.0032 - val_mae: 0.0607\nEpoch 67/500\n33/33 [==============================] - 12s 362ms/step - loss: 0.0030 - mae: 0.0509 - val_loss: 0.0024 - val_mae: 0.0451\nEpoch 68/500\n33/33 [==============================] - 12s 342ms/step - loss: 0.0029 - mae: 0.0476 - val_loss: 0.0028 - val_mae: 0.0538\nEpoch 69/500\n33/33 [==============================] - 8s 235ms/step - loss: 0.0031 - mae: 0.0514 - val_loss: 0.0024 - val_mae: 0.0441\nEpoch 70/500\n33/33 [==============================] - 5s 142ms/step - loss: 0.0030 - mae: 0.0497 - val_loss: 0.0019 - val_mae: 0.0379\nEpoch 71/500\n33/33 [==============================] - 7s 204ms/step - loss: 0.0033 - mae: 0.0571 - val_loss: 0.0023 - val_mae: 0.0436\nEpoch 72/500\n33/33 [==============================] - 5s 164ms/step - loss: 0.0029 - mae: 0.0480 - val_loss: 0.0028 - val_mae: 0.0523\nEpoch 73/500\n33/33 [==============================] - 6s 172ms/step - loss: 0.0027 - mae: 0.0447 - val_loss: 0.0023 - val_mae: 0.0430\nEpoch 74/500\n33/33 [==============================] - 7s 200ms/step - loss: 0.0028 - mae: 0.0458 - val_loss: 0.0019 - val_mae: 0.0343\nEpoch 75/500\n33/33 [==============================] - 5s 138ms/step - loss: 0.0026 - mae: 0.0433 - val_loss: 0.0017 - val_mae: 0.0324\nEpoch 76/500\n33/33 [==============================] - 12s 360ms/step - loss: 0.0026 - mae: 0.0422 - val_loss: 0.0018 - val_mae: 0.0329\nEpoch 77/500\n33/33 [==============================] - 12s 357ms/step - loss: 0.0028 - mae: 0.0468 - val_loss: 0.0019 - val_mae: 0.0344\nEpoch 78/500\n33/33 [==============================] - 5s 140ms/step - loss: 0.0029 - mae: 0.0497 - val_loss: 0.0018 - val_mae: 0.0344\nEpoch 79/500\n33/33 [==============================] - 5s 159ms/step - loss: 0.0027 - mae: 0.0445 - val_loss: 0.0026 - val_mae: 0.0510\nEpoch 80/500\n33/33 [==============================] - 8s 230ms/step - loss: 0.0031 - mae: 0.0531 - val_loss: 0.0017 - val_mae: 0.0317\nEpoch 81/500\n33/33 [==============================] - 5s 143ms/step - loss: 0.0025 - mae: 0.0429 - val_loss: 0.0017 - val_mae: 0.0315\nEpoch 82/500\n33/33 [==============================] - 7s 191ms/step - loss: 0.0024 - mae: 0.0409 - val_loss: 0.0017 - val_mae: 0.0313\nEpoch 83/500\n33/33 [==============================] - 6s 190ms/step - loss: 0.0025 - mae: 0.0424 - val_loss: 0.0026 - val_mae: 0.0502\n3/3 [==============================] - 0s 37ms/step - loss: 0.0017 - mae: 0.0287\n\n\n28707.98110961914\n\n\nYou will find that it reaches an MAE of about 28,707. That’s better than baseline, but it doesn’t better than our “shallower” RNN. It looks like this RNN is a bit too large for our task.\n\n\n10.2.8 Forecasting Multivariate Time Series\nA great quality of neural networks is their flexibility: in particular, they can deal with multivariate time series with almost no change to their architecture. For example, let’s try to forecast the rail time series using both the bus and rail data as input. In fact, let’s also throw in the day type! Since we can always know in advance whether tomorrow is going to be a weekday, a weekend, or a holiday, we can shift the day type series one day into the future, so that the model is given tomorrow’s day type as input. For simplicity, we’ll do this processing using Pandas:\n\ndf_mulvar = df[[\"bus\", \"rail\"]] / 1e6  # use both bus & rail series as input\ndf_mulvar[\"next_day_type\"] = df[\"day_type\"].shift(-1)  # we know tomorrow's type use it as another feature!\ndf_mulvar = pd.get_dummies(df_mulvar)  # one-hot encode the day type\n\n\ndf_mulvar.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nbus\nrail\nnext_day_type_A\nnext_day_type_U\nnext_day_type_W\n\n\ndate\n\n\n\n\n\n\n\n\n\n2001-01-01\n0.297192\n0.126455\n0\n0\n1\n\n\n2001-01-02\n0.780827\n0.501952\n0\n0\n1\n\n\n2001-01-03\n0.824923\n0.536432\n0\n0\n1\n\n\n2001-01-04\n0.870021\n0.550011\n0\n0\n1\n\n\n2001-01-05\n0.890426\n0.557917\n1\n0\n0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNow df_mulvar is a DataFrame with five columns: the bus and rail data, plus three columns containing the one-hot encoding of the next day’s type (recall that there are three possible day types, W, A, and U). Next we can proceed much like we did earlier. First we split the data into three periods, for training, validation, and testing:\n\nmulvar_train = df_mulvar[\"2016-01\":\"2018-12\"]\nmulvar_valid = df_mulvar[\"2019-01\":\"2019-05\"]\nmulvar_test = df_mulvar[\"2019-06\":]\n\n\ntf.random.set_seed(42)  # extra code – ensures reproducibility\n\ntrain_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(\n    mulvar_train.to_numpy(),  # use all 5 columns as input\n    targets=mulvar_train[\"rail\"][seq_length:],  # forecast only the rail series!\n    sequence_length=seq_length,\n    batch_size=32,\n    shuffle=True,\n    seed=42\n)\nvalid_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(\n    mulvar_valid.to_numpy(),\n    targets=mulvar_valid[\"rail\"][seq_length:],\n    sequence_length=seq_length,\n    batch_size=32\n)\n\n\ntf.random.set_seed(42)  # extra code – ensures reproducibility\nmulvar_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]), # Now input has five dimension\n    tf.keras.layers.Dense(1)\n])\n\n\nfit_and_evaluate(mulvar_model, train_mulvar_ds, valid_mulvar_ds, learning_rate=0.05)\n\nEpoch 1/500\n33/33 [==============================] - 3s 46ms/step - loss: 0.0528 - mae: 0.2433 - val_loss: 0.0025 - val_mae: 0.0499\nEpoch 2/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0027 - mae: 0.0535 - val_loss: 0.0011 - val_mae: 0.0369\nEpoch 3/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0019 - mae: 0.0423 - val_loss: 7.1735e-04 - val_mae: 0.0276\nEpoch 4/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0018 - mae: 0.0431 - val_loss: 0.0016 - val_mae: 0.0471\nEpoch 5/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0015 - mae: 0.0382 - val_loss: 7.3583e-04 - val_mae: 0.0283\nEpoch 6/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0014 - mae: 0.0368 - val_loss: 0.0012 - val_mae: 0.0387\nEpoch 7/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0014 - mae: 0.0377 - val_loss: 9.7083e-04 - val_mae: 0.0341\nEpoch 8/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0353 - val_loss: 6.9914e-04 - val_mae: 0.0273\nEpoch 9/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0013 - mae: 0.0349 - val_loss: 6.9447e-04 - val_mae: 0.0260\nEpoch 10/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0013 - mae: 0.0365 - val_loss: 7.4059e-04 - val_mae: 0.0284\nEpoch 11/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0013 - mae: 0.0352 - val_loss: 0.0018 - val_mae: 0.0513\nEpoch 12/500\n33/33 [==============================] - 2s 71ms/step - loss: 0.0014 - mae: 0.0379 - val_loss: 6.4515e-04 - val_mae: 0.0248\nEpoch 13/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0014 - mae: 0.0385 - val_loss: 0.0014 - val_mae: 0.0436\nEpoch 14/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0339 - val_loss: 6.5858e-04 - val_mae: 0.0254\nEpoch 15/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0345 - val_loss: 6.5689e-04 - val_mae: 0.0251\nEpoch 16/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0335 - val_loss: 0.0013 - val_mae: 0.0412\nEpoch 17/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0013 - mae: 0.0354 - val_loss: 8.6519e-04 - val_mae: 0.0328\nEpoch 18/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0013 - mae: 0.0362 - val_loss: 7.7581e-04 - val_mae: 0.0279\nEpoch 19/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0356 - val_loss: 7.0924e-04 - val_mae: 0.0262\nEpoch 20/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0013 - mae: 0.0365 - val_loss: 6.2600e-04 - val_mae: 0.0244\nEpoch 21/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 5.9935e-04 - val_mae: 0.0239\nEpoch 22/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0326 - val_loss: 8.8976e-04 - val_mae: 0.0331\nEpoch 23/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0330 - val_loss: 6.7831e-04 - val_mae: 0.0256\nEpoch 24/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0357 - val_loss: 8.2207e-04 - val_mae: 0.0294\nEpoch 25/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0350 - val_loss: 6.1505e-04 - val_mae: 0.0241\nEpoch 26/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0322 - val_loss: 6.4027e-04 - val_mae: 0.0254\nEpoch 27/500\n33/33 [==============================] - 3s 100ms/step - loss: 0.0012 - mae: 0.0337 - val_loss: 0.0016 - val_mae: 0.0484\nEpoch 28/500\n33/33 [==============================] - 2s 68ms/step - loss: 0.0011 - mae: 0.0331 - val_loss: 6.6656e-04 - val_mae: 0.0261\nEpoch 29/500\n33/33 [==============================] - 3s 79ms/step - loss: 0.0012 - mae: 0.0342 - val_loss: 7.0325e-04 - val_mae: 0.0273\nEpoch 30/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0011 - mae: 0.0318 - val_loss: 7.6200e-04 - val_mae: 0.0280\nEpoch 31/500\n33/33 [==============================] - 4s 102ms/step - loss: 0.0014 - mae: 0.0387 - val_loss: 6.5211e-04 - val_mae: 0.0257\nEpoch 32/500\n33/33 [==============================] - 3s 76ms/step - loss: 0.0011 - mae: 0.0320 - val_loss: 6.1206e-04 - val_mae: 0.0243\nEpoch 33/500\n33/33 [==============================] - 2s 64ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 0.0011 - val_mae: 0.0370\nEpoch 34/500\n33/33 [==============================] - 2s 60ms/step - loss: 0.0012 - mae: 0.0345 - val_loss: 8.6551e-04 - val_mae: 0.0315\nEpoch 35/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0349 - val_loss: 8.7583e-04 - val_mae: 0.0327\nEpoch 36/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0011 - mae: 0.0330 - val_loss: 6.2368e-04 - val_mae: 0.0247\nEpoch 37/500\n33/33 [==============================] - 2s 70ms/step - loss: 0.0014 - mae: 0.0385 - val_loss: 6.2868e-04 - val_mae: 0.0252\nEpoch 38/500\n33/33 [==============================] - 2s 58ms/step - loss: 0.0012 - mae: 0.0342 - val_loss: 7.7667e-04 - val_mae: 0.0288\nEpoch 39/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0317 - val_loss: 0.0011 - val_mae: 0.0382\nEpoch 40/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0342 - val_loss: 7.2795e-04 - val_mae: 0.0266\nEpoch 41/500\n33/33 [==============================] - 1s 37ms/step - loss: 0.0013 - mae: 0.0364 - val_loss: 6.5642e-04 - val_mae: 0.0255\nEpoch 42/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0011 - mae: 0.0319 - val_loss: 6.0994e-04 - val_mae: 0.0240\nEpoch 43/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0360 - val_loss: 8.2312e-04 - val_mae: 0.0301\nEpoch 44/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0013 - mae: 0.0375 - val_loss: 7.6323e-04 - val_mae: 0.0273\nEpoch 45/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0013 - mae: 0.0371 - val_loss: 7.8322e-04 - val_mae: 0.0292\nEpoch 46/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 9.6853e-04 - val_mae: 0.0356\nEpoch 47/500\n33/33 [==============================] - 1s 37ms/step - loss: 0.0011 - mae: 0.0323 - val_loss: 6.6022e-04 - val_mae: 0.0259\nEpoch 48/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 8.5946e-04 - val_mae: 0.0316\nEpoch 49/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0336 - val_loss: 0.0012 - val_mae: 0.0402\nEpoch 50/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 9.5002e-04 - val_mae: 0.0336\nEpoch 51/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 8.2247e-04 - val_mae: 0.0305\nEpoch 52/500\n33/33 [==============================] - 3s 76ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 8.8756e-04 - val_mae: 0.0322\nEpoch 53/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0010 - mae: 0.0318 - val_loss: 0.0013 - val_mae: 0.0429\nEpoch 54/500\n33/33 [==============================] - 3s 87ms/step - loss: 0.0013 - mae: 0.0367 - val_loss: 7.0816e-04 - val_mae: 0.0267\nEpoch 55/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0315 - val_loss: 7.1429e-04 - val_mae: 0.0270\nEpoch 56/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0333 - val_loss: 7.4288e-04 - val_mae: 0.0279\nEpoch 57/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0012 - mae: 0.0347 - val_loss: 6.5882e-04 - val_mae: 0.0265\nEpoch 58/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0013 - mae: 0.0376 - val_loss: 5.6581e-04 - val_mae: 0.0232\nEpoch 59/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0319 - val_loss: 9.7140e-04 - val_mae: 0.0356\nEpoch 60/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0010 - mae: 0.0306 - val_loss: 6.6180e-04 - val_mae: 0.0267\nEpoch 61/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0315 - val_loss: 9.5210e-04 - val_mae: 0.0339\nEpoch 62/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0013 - mae: 0.0373 - val_loss: 7.9024e-04 - val_mae: 0.0285\nEpoch 63/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0335 - val_loss: 6.9436e-04 - val_mae: 0.0266\nEpoch 64/500\n33/33 [==============================] - 2s 70ms/step - loss: 0.0010 - mae: 0.0310 - val_loss: 0.0011 - val_mae: 0.0387\nEpoch 65/500\n33/33 [==============================] - 2s 66ms/step - loss: 0.0011 - mae: 0.0329 - val_loss: 5.7811e-04 - val_mae: 0.0235\nEpoch 66/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0010 - mae: 0.0302 - val_loss: 7.4761e-04 - val_mae: 0.0293\nEpoch 67/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0014 - mae: 0.0376 - val_loss: 0.0019 - val_mae: 0.0549\nEpoch 68/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0342 - val_loss: 5.8371e-04 - val_mae: 0.0238\nEpoch 69/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0011 - mae: 0.0316 - val_loss: 0.0011 - val_mae: 0.0377\nEpoch 70/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0321 - val_loss: 5.9109e-04 - val_mae: 0.0244\nEpoch 71/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0344 - val_loss: 5.6596e-04 - val_mae: 0.0227\nEpoch 72/500\n33/33 [==============================] - 2s 65ms/step - loss: 0.0010 - mae: 0.0306 - val_loss: 8.0942e-04 - val_mae: 0.0285\nEpoch 73/500\n33/33 [==============================] - 2s 70ms/step - loss: 0.0010 - mae: 0.0311 - val_loss: 0.0015 - val_mae: 0.0466\nEpoch 74/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0375 - val_loss: 7.6460e-04 - val_mae: 0.0291\nEpoch 75/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0342 - val_loss: 0.0010 - val_mae: 0.0358\nEpoch 76/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 0.0010 - val_mae: 0.0354\nEpoch 77/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0339 - val_loss: 6.6938e-04 - val_mae: 0.0266\nEpoch 78/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0323 - val_loss: 0.0015 - val_mae: 0.0457\nEpoch 79/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0340 - val_loss: 0.0013 - val_mae: 0.0415\nEpoch 80/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0350 - val_loss: 9.1183e-04 - val_mae: 0.0330\nEpoch 81/500\n33/33 [==============================] - 3s 76ms/step - loss: 0.0012 - mae: 0.0356 - val_loss: 9.6658e-04 - val_mae: 0.0344\nEpoch 82/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0011 - mae: 0.0313 - val_loss: 8.6747e-04 - val_mae: 0.0328\nEpoch 83/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0349 - val_loss: 7.4910e-04 - val_mae: 0.0275\nEpoch 84/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 8.7549e-04 - val_mae: 0.0326\nEpoch 85/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0010 - mae: 0.0303 - val_loss: 6.5017e-04 - val_mae: 0.0257\nEpoch 86/500\n33/33 [==============================] - 1s 40ms/step - loss: 9.9429e-04 - mae: 0.0295 - val_loss: 7.1371e-04 - val_mae: 0.0264\nEpoch 87/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0010 - mae: 0.0314 - val_loss: 0.0012 - val_mae: 0.0398\nEpoch 88/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0318 - val_loss: 6.6436e-04 - val_mae: 0.0250\nEpoch 89/500\n33/33 [==============================] - 2s 44ms/step - loss: 0.0011 - mae: 0.0335 - val_loss: 5.7223e-04 - val_mae: 0.0234\nEpoch 90/500\n33/33 [==============================] - 3s 74ms/step - loss: 0.0012 - mae: 0.0346 - val_loss: 7.2035e-04 - val_mae: 0.0285\nEpoch 91/500\n33/33 [==============================] - 2s 53ms/step - loss: 0.0010 - mae: 0.0304 - val_loss: 6.9319e-04 - val_mae: 0.0261\nEpoch 92/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0010 - mae: 0.0301 - val_loss: 6.7648e-04 - val_mae: 0.0249\nEpoch 93/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0315 - val_loss: 7.1064e-04 - val_mae: 0.0269\nEpoch 94/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0010 - mae: 0.0314 - val_loss: 7.3646e-04 - val_mae: 0.0283\nEpoch 95/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0010 - mae: 0.0314 - val_loss: 6.3475e-04 - val_mae: 0.0250\nEpoch 96/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0013 - mae: 0.0372 - val_loss: 6.2060e-04 - val_mae: 0.0239\nEpoch 97/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0010 - mae: 0.0313 - val_loss: 9.6446e-04 - val_mae: 0.0351\nEpoch 98/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0011 - mae: 0.0320 - val_loss: 7.0585e-04 - val_mae: 0.0261\nEpoch 99/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0321 - val_loss: 7.6712e-04 - val_mae: 0.0299\nEpoch 100/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0364 - val_loss: 0.0025 - val_mae: 0.0637\nEpoch 101/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0330 - val_loss: 6.2223e-04 - val_mae: 0.0241\nEpoch 102/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0011 - mae: 0.0309 - val_loss: 6.4828e-04 - val_mae: 0.0254\nEpoch 103/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0010 - mae: 0.0302 - val_loss: 0.0012 - val_mae: 0.0399\nEpoch 104/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0335 - val_loss: 7.6277e-04 - val_mae: 0.0284\nEpoch 105/500\n33/33 [==============================] - 1s 40ms/step - loss: 9.6551e-04 - mae: 0.0298 - val_loss: 7.8750e-04 - val_mae: 0.0306\nEpoch 106/500\n33/33 [==============================] - 3s 81ms/step - loss: 9.3845e-04 - mae: 0.0287 - val_loss: 6.4166e-04 - val_mae: 0.0244\nEpoch 107/500\n33/33 [==============================] - 2s 70ms/step - loss: 9.5507e-04 - mae: 0.0292 - val_loss: 7.0416e-04 - val_mae: 0.0281\nEpoch 108/500\n33/33 [==============================] - 2s 72ms/step - loss: 9.6012e-04 - mae: 0.0294 - val_loss: 6.3996e-04 - val_mae: 0.0261\nEpoch 109/500\n33/33 [==============================] - 3s 78ms/step - loss: 0.0010 - mae: 0.0306 - val_loss: 6.6623e-04 - val_mae: 0.0251\nEpoch 110/500\n33/33 [==============================] - 3s 85ms/step - loss: 0.0010 - mae: 0.0310 - val_loss: 0.0012 - val_mae: 0.0414\nEpoch 111/500\n33/33 [==============================] - 3s 95ms/step - loss: 0.0011 - mae: 0.0337 - val_loss: 6.7468e-04 - val_mae: 0.0258\nEpoch 112/500\n33/33 [==============================] - 2s 68ms/step - loss: 0.0012 - mae: 0.0349 - val_loss: 5.8859e-04 - val_mae: 0.0232\nEpoch 113/500\n33/33 [==============================] - 3s 77ms/step - loss: 0.0011 - mae: 0.0324 - val_loss: 7.2965e-04 - val_mae: 0.0260\nEpoch 114/500\n33/33 [==============================] - 3s 83ms/step - loss: 0.0010 - mae: 0.0313 - val_loss: 6.9316e-04 - val_mae: 0.0273\nEpoch 115/500\n33/33 [==============================] - 4s 107ms/step - loss: 9.9251e-04 - mae: 0.0301 - val_loss: 6.8965e-04 - val_mae: 0.0250\nEpoch 116/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 6.6329e-04 - val_mae: 0.0253\nEpoch 117/500\n33/33 [==============================] - 1s 38ms/step - loss: 9.9937e-04 - mae: 0.0310 - val_loss: 7.2512e-04 - val_mae: 0.0284\nEpoch 118/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0014 - mae: 0.0393 - val_loss: 7.6992e-04 - val_mae: 0.0300\nEpoch 119/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0014 - mae: 0.0397 - val_loss: 6.2056e-04 - val_mae: 0.0235\nEpoch 120/500\n33/33 [==============================] - 1s 41ms/step - loss: 9.5764e-04 - mae: 0.0299 - val_loss: 0.0014 - val_mae: 0.0463\nEpoch 121/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0349 - val_loss: 7.6205e-04 - val_mae: 0.0291\n3/3 [==============================] - 0s 13ms/step - loss: 5.6596e-04 - mae: 0.0227\n\n\n22680.68492412567\n\n\nNotice that the only difference from the univar_model RNN we built earlier is the input shape: at each time step, the model now receives five inputs instead of one. This model actually reaches a validation MAE of 22,680. Now we’re making big progress!\nIn fact, it’s not too hard to make the RNN forecast both the bus and rail ridership. We can add an extra neuron in the output Dense layer, since it must now make two forecasts: one for tomorrow’s bus ridership, and the other for rail:\n\ntf.random.set_seed(42)\n\nseq_length = 56\ntrain_multask_ds = tf.keras.utils.timeseries_dataset_from_array(\n    mulvar_train.to_numpy(),\n    targets=mulvar_train[[\"bus\", \"rail\"]][seq_length:],  # 2 targets per day\n    sequence_length=seq_length,\n    batch_size=32,\n    shuffle=True,\n    seed=42\n)\nvalid_multask_ds = tf.keras.utils.timeseries_dataset_from_array(\n    mulvar_valid.to_numpy(),\n    targets=mulvar_valid[[\"bus\", \"rail\"]][seq_length:],\n    sequence_length=seq_length,\n    batch_size=32\n)\n\ntf.random.set_seed(42)\nmultask_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),\n    tf.keras.layers.Dense(2)\n])\n\nfit_and_evaluate(multask_model, train_multask_ds, valid_multask_ds,\n                 learning_rate=0.02)\n\nEpoch 1/500\n33/33 [==============================] - 4s 74ms/step - loss: 0.0255 - mae: 0.1584 - val_loss: 0.0031 - val_mae: 0.0633\nEpoch 2/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0036 - mae: 0.0635 - val_loss: 0.0021 - val_mae: 0.0503\nEpoch 3/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0485 - val_loss: 0.0011 - val_mae: 0.0357\nEpoch 4/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0022 - mae: 0.0465 - val_loss: 0.0021 - val_mae: 0.0530\nEpoch 5/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0021 - mae: 0.0459 - val_loss: 0.0010 - val_mae: 0.0335\nEpoch 6/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0020 - mae: 0.0444 - val_loss: 0.0011 - val_mae: 0.0363\nEpoch 7/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0019 - mae: 0.0434 - val_loss: 9.0769e-04 - val_mae: 0.0318\nEpoch 8/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0018 - mae: 0.0425 - val_loss: 0.0013 - val_mae: 0.0404\nEpoch 9/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0018 - mae: 0.0419 - val_loss: 0.0011 - val_mae: 0.0368\nEpoch 10/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0019 - mae: 0.0432 - val_loss: 0.0011 - val_mae: 0.0353\nEpoch 11/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0018 - mae: 0.0421 - val_loss: 0.0013 - val_mae: 0.0413\nEpoch 12/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0017 - mae: 0.0409 - val_loss: 0.0011 - val_mae: 0.0354\nEpoch 13/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0016 - mae: 0.0399 - val_loss: 8.3978e-04 - val_mae: 0.0307\nEpoch 14/500\n33/33 [==============================] - 2s 71ms/step - loss: 0.0015 - mae: 0.0387 - val_loss: 0.0012 - val_mae: 0.0382\nEpoch 15/500\n33/33 [==============================] - 2s 59ms/step - loss: 0.0015 - mae: 0.0385 - val_loss: 0.0010 - val_mae: 0.0347\nEpoch 16/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0015 - mae: 0.0376 - val_loss: 7.6741e-04 - val_mae: 0.0298\nEpoch 17/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0015 - mae: 0.0386 - val_loss: 0.0015 - val_mae: 0.0447\nEpoch 18/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0016 - mae: 0.0400 - val_loss: 0.0012 - val_mae: 0.0398\nEpoch 19/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0014 - mae: 0.0377 - val_loss: 7.9699e-04 - val_mae: 0.0300\nEpoch 20/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0014 - mae: 0.0374 - val_loss: 7.4050e-04 - val_mae: 0.0289\nEpoch 21/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0014 - mae: 0.0370 - val_loss: 9.6869e-04 - val_mae: 0.0342\nEpoch 22/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0014 - mae: 0.0366 - val_loss: 0.0012 - val_mae: 0.0389\nEpoch 23/500\n33/33 [==============================] - 2s 60ms/step - loss: 0.0014 - mae: 0.0370 - val_loss: 9.7866e-04 - val_mae: 0.0343\nEpoch 24/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0014 - mae: 0.0372 - val_loss: 8.6441e-04 - val_mae: 0.0316\nEpoch 25/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0013 - mae: 0.0363 - val_loss: 0.0010 - val_mae: 0.0359\nEpoch 26/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0014 - mae: 0.0366 - val_loss: 9.1490e-04 - val_mae: 0.0329\nEpoch 27/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0013 - mae: 0.0359 - val_loss: 7.5073e-04 - val_mae: 0.0299\nEpoch 28/500\n33/33 [==============================] - 1s 37ms/step - loss: 0.0013 - mae: 0.0363 - val_loss: 7.3197e-04 - val_mae: 0.0294\nEpoch 29/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0013 - mae: 0.0364 - val_loss: 8.3700e-04 - val_mae: 0.0310\nEpoch 30/500\n33/33 [==============================] - 2s 71ms/step - loss: 0.0013 - mae: 0.0356 - val_loss: 8.2648e-04 - val_mae: 0.0308\nEpoch 31/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0013 - mae: 0.0362 - val_loss: 9.7559e-04 - val_mae: 0.0346\nEpoch 32/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0362 - val_loss: 6.9691e-04 - val_mae: 0.0279\nEpoch 33/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0355 - val_loss: 8.4579e-04 - val_mae: 0.0317\nEpoch 34/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0013 - mae: 0.0353 - val_loss: 7.9581e-04 - val_mae: 0.0306\nEpoch 35/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0349 - val_loss: 7.7935e-04 - val_mae: 0.0301\nEpoch 36/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0359 - val_loss: 7.4623e-04 - val_mae: 0.0291\nEpoch 37/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0013 - mae: 0.0357 - val_loss: 7.1986e-04 - val_mae: 0.0287\nEpoch 38/500\n33/33 [==============================] - 2s 67ms/step - loss: 0.0013 - mae: 0.0353 - val_loss: 6.8000e-04 - val_mae: 0.0277\nEpoch 39/500\n33/33 [==============================] - 2s 68ms/step - loss: 0.0012 - mae: 0.0346 - val_loss: 8.2842e-04 - val_mae: 0.0314\nEpoch 40/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0347 - val_loss: 8.6199e-04 - val_mae: 0.0321\nEpoch 41/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0357 - val_loss: 8.5757e-04 - val_mae: 0.0320\nEpoch 42/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0013 - mae: 0.0353 - val_loss: 8.3496e-04 - val_mae: 0.0314\nEpoch 43/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0349 - val_loss: 0.0011 - val_mae: 0.0370\nEpoch 44/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0349 - val_loss: 6.8912e-04 - val_mae: 0.0280\nEpoch 45/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0346 - val_loss: 7.4413e-04 - val_mae: 0.0292\nEpoch 46/500\n33/33 [==============================] - 2s 68ms/step - loss: 0.0012 - mae: 0.0344 - val_loss: 7.0734e-04 - val_mae: 0.0289\nEpoch 47/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0012 - mae: 0.0350 - val_loss: 7.3556e-04 - val_mae: 0.0298\nEpoch 48/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 7.5389e-04 - val_mae: 0.0299\nEpoch 49/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 7.5621e-04 - val_mae: 0.0295\nEpoch 50/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0344 - val_loss: 0.0010 - val_mae: 0.0359\nEpoch 51/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0344 - val_loss: 7.1420e-04 - val_mae: 0.0293\nEpoch 52/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0340 - val_loss: 8.2671e-04 - val_mae: 0.0312\nEpoch 53/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0338 - val_loss: 7.3749e-04 - val_mae: 0.0292\nEpoch 54/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0345 - val_loss: 7.0496e-04 - val_mae: 0.0283\nEpoch 55/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 8.3124e-04 - val_mae: 0.0316\nEpoch 56/500\n33/33 [==============================] - 2s 57ms/step - loss: 0.0012 - mae: 0.0346 - val_loss: 6.7089e-04 - val_mae: 0.0273\nEpoch 57/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0349 - val_loss: 8.3932e-04 - val_mae: 0.0326\nEpoch 58/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 6.8043e-04 - val_mae: 0.0280\nEpoch 59/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0337 - val_loss: 7.0953e-04 - val_mae: 0.0284\nEpoch 60/500\n33/33 [==============================] - 1s 37ms/step - loss: 0.0012 - mae: 0.0336 - val_loss: 7.2480e-04 - val_mae: 0.0289\nEpoch 61/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0335 - val_loss: 7.3572e-04 - val_mae: 0.0289\nEpoch 62/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0334 - val_loss: 8.1480e-04 - val_mae: 0.0312\nEpoch 63/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0012 - mae: 0.0333 - val_loss: 7.8571e-04 - val_mae: 0.0301\nEpoch 64/500\n33/33 [==============================] - 2s 66ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 9.3828e-04 - val_mae: 0.0338\nEpoch 65/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0013 - mae: 0.0352 - val_loss: 7.4398e-04 - val_mae: 0.0294\nEpoch 66/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0335 - val_loss: 7.4975e-04 - val_mae: 0.0293\nEpoch 67/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0332 - val_loss: 6.8271e-04 - val_mae: 0.0279\nEpoch 68/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0331 - val_loss: 6.9375e-04 - val_mae: 0.0281\nEpoch 69/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0333 - val_loss: 7.2032e-04 - val_mae: 0.0289\nEpoch 70/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0350 - val_loss: 8.6118e-04 - val_mae: 0.0318\nEpoch 71/500\n33/33 [==============================] - 2s 67ms/step - loss: 0.0012 - mae: 0.0340 - val_loss: 7.9333e-04 - val_mae: 0.0302\nEpoch 72/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0344 - val_loss: 0.0010 - val_mae: 0.0353\nEpoch 73/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0344 - val_loss: 6.6572e-04 - val_mae: 0.0278\nEpoch 74/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0329 - val_loss: 6.5416e-04 - val_mae: 0.0270\nEpoch 75/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0011 - mae: 0.0326 - val_loss: 6.5704e-04 - val_mae: 0.0275\nEpoch 76/500\n33/33 [==============================] - 2s 63ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 6.6209e-04 - val_mae: 0.0273\nEpoch 77/500\n33/33 [==============================] - 2s 60ms/step - loss: 0.0011 - mae: 0.0332 - val_loss: 7.0564e-04 - val_mae: 0.0286\nEpoch 78/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0331 - val_loss: 7.5438e-04 - val_mae: 0.0296\nEpoch 79/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0334 - val_loss: 7.2475e-04 - val_mae: 0.0295\nEpoch 80/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 6.8965e-04 - val_mae: 0.0282\nEpoch 81/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0331 - val_loss: 7.2314e-04 - val_mae: 0.0293\nEpoch 82/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0330 - val_loss: 6.4668e-04 - val_mae: 0.0271\nEpoch 83/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 7.4061e-04 - val_mae: 0.0291\nEpoch 84/500\n33/33 [==============================] - 2s 45ms/step - loss: 0.0011 - mae: 0.0330 - val_loss: 6.9921e-04 - val_mae: 0.0278\nEpoch 85/500\n33/33 [==============================] - 3s 79ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 7.5894e-04 - val_mae: 0.0296\nEpoch 86/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0011 - mae: 0.0329 - val_loss: 6.4594e-04 - val_mae: 0.0266\nEpoch 87/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0328 - val_loss: 7.8189e-04 - val_mae: 0.0300\nEpoch 88/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0336 - val_loss: 6.3389e-04 - val_mae: 0.0262\nEpoch 89/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0337 - val_loss: 7.5339e-04 - val_mae: 0.0302\nEpoch 90/500\n33/33 [==============================] - 1s 43ms/step - loss: 0.0011 - mae: 0.0330 - val_loss: 8.4770e-04 - val_mae: 0.0316\nEpoch 91/500\n33/33 [==============================] - 4s 125ms/step - loss: 0.0012 - mae: 0.0340 - val_loss: 0.0011 - val_mae: 0.0359\nEpoch 92/500\n33/33 [==============================] - 3s 96ms/step - loss: 0.0012 - mae: 0.0344 - val_loss: 6.9146e-04 - val_mae: 0.0278\nEpoch 93/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0326 - val_loss: 6.4651e-04 - val_mae: 0.0270\nEpoch 94/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0328 - val_loss: 8.8887e-04 - val_mae: 0.0329\nEpoch 95/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0333 - val_loss: 7.8628e-04 - val_mae: 0.0306\nEpoch 96/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0012 - mae: 0.0340 - val_loss: 6.9680e-04 - val_mae: 0.0285\nEpoch 97/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0011 - mae: 0.0319 - val_loss: 6.9095e-04 - val_mae: 0.0282\nEpoch 98/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0331 - val_loss: 0.0010 - val_mae: 0.0359\nEpoch 99/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0329 - val_loss: 6.2891e-04 - val_mae: 0.0259\nEpoch 100/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0326 - val_loss: 7.0776e-04 - val_mae: 0.0286\nEpoch 101/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0324 - val_loss: 0.0010 - val_mae: 0.0358\nEpoch 102/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0012 - mae: 0.0336 - val_loss: 6.2758e-04 - val_mae: 0.0260\nEpoch 103/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0011 - mae: 0.0321 - val_loss: 6.9090e-04 - val_mae: 0.0277\nEpoch 104/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0012 - mae: 0.0340 - val_loss: 6.9598e-04 - val_mae: 0.0281\nEpoch 105/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0334 - val_loss: 6.9317e-04 - val_mae: 0.0282\nEpoch 106/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0012 - mae: 0.0334 - val_loss: 7.6347e-04 - val_mae: 0.0290\nEpoch 107/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 7.3428e-04 - val_mae: 0.0292\nEpoch 108/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0011 - mae: 0.0321 - val_loss: 6.4750e-04 - val_mae: 0.0268\nEpoch 109/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0318 - val_loss: 7.3960e-04 - val_mae: 0.0294\nEpoch 110/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0324 - val_loss: 6.7338e-04 - val_mae: 0.0275\nEpoch 111/500\n33/33 [==============================] - 2s 58ms/step - loss: 0.0011 - mae: 0.0326 - val_loss: 7.0008e-04 - val_mae: 0.0284\nEpoch 112/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0011 - mae: 0.0332 - val_loss: 9.0026e-04 - val_mae: 0.0326\nEpoch 113/500\n33/33 [==============================] - 1s 43ms/step - loss: 0.0011 - mae: 0.0331 - val_loss: 6.4597e-04 - val_mae: 0.0266\nEpoch 114/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0319 - val_loss: 9.2826e-04 - val_mae: 0.0337\nEpoch 115/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 6.2865e-04 - val_mae: 0.0260\nEpoch 116/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 6.3459e-04 - val_mae: 0.0265\nEpoch 117/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0320 - val_loss: 7.9237e-04 - val_mae: 0.0300\nEpoch 118/500\n33/33 [==============================] - 2s 73ms/step - loss: 0.0011 - mae: 0.0336 - val_loss: 7.2099e-04 - val_mae: 0.0289\nEpoch 119/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 6.5630e-04 - val_mae: 0.0270\nEpoch 120/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 0.0010 - val_mae: 0.0363\nEpoch 121/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0324 - val_loss: 6.3759e-04 - val_mae: 0.0265\nEpoch 122/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0313 - val_loss: 8.1906e-04 - val_mae: 0.0310\nEpoch 123/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 6.9466e-04 - val_mae: 0.0276\nEpoch 124/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0322 - val_loss: 6.7003e-04 - val_mae: 0.0275\nEpoch 125/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0323 - val_loss: 6.9022e-04 - val_mae: 0.0279\nEpoch 126/500\n33/33 [==============================] - 2s 75ms/step - loss: 0.0011 - mae: 0.0317 - val_loss: 6.8580e-04 - val_mae: 0.0278\nEpoch 127/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0011 - mae: 0.0322 - val_loss: 0.0010 - val_mae: 0.0351\nEpoch 128/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0331 - val_loss: 7.0300e-04 - val_mae: 0.0286\nEpoch 129/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 8.3916e-04 - val_mae: 0.0317\nEpoch 130/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0011 - mae: 0.0316 - val_loss: 6.9474e-04 - val_mae: 0.0285\nEpoch 131/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0324 - val_loss: 6.6539e-04 - val_mae: 0.0274\nEpoch 132/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0322 - val_loss: 7.1757e-04 - val_mae: 0.0284\nEpoch 133/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0319 - val_loss: 9.3395e-04 - val_mae: 0.0341\nEpoch 134/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0011 - mae: 0.0324 - val_loss: 6.8902e-04 - val_mae: 0.0282\nEpoch 135/500\n33/33 [==============================] - 2s 70ms/step - loss: 0.0011 - mae: 0.0316 - val_loss: 6.3758e-04 - val_mae: 0.0265\nEpoch 136/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0318 - val_loss: 7.0915e-04 - val_mae: 0.0280\nEpoch 137/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0011 - mae: 0.0322 - val_loss: 6.8697e-04 - val_mae: 0.0281\nEpoch 138/500\n33/33 [==============================] - 1s 43ms/step - loss: 0.0010 - mae: 0.0313 - val_loss: 6.8903e-04 - val_mae: 0.0280\nEpoch 139/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0318 - val_loss: 6.4388e-04 - val_mae: 0.0268\nEpoch 140/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 0.0011 - val_mae: 0.0379\nEpoch 141/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0011 - mae: 0.0326 - val_loss: 7.9957e-04 - val_mae: 0.0310\nEpoch 142/500\n33/33 [==============================] - 3s 76ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 6.6206e-04 - val_mae: 0.0274\nEpoch 143/500\n33/33 [==============================] - 2s 57ms/step - loss: 0.0011 - mae: 0.0315 - val_loss: 6.7114e-04 - val_mae: 0.0278\nEpoch 144/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0010 - mae: 0.0315 - val_loss: 6.3836e-04 - val_mae: 0.0262\nEpoch 145/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0010 - mae: 0.0312 - val_loss: 6.7101e-04 - val_mae: 0.0277\nEpoch 146/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0326 - val_loss: 8.0096e-04 - val_mae: 0.0308\nEpoch 147/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0011 - mae: 0.0332 - val_loss: 6.3757e-04 - val_mae: 0.0270\nEpoch 148/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0318 - val_loss: 6.3463e-04 - val_mae: 0.0263\nEpoch 149/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0010 - mae: 0.0312 - val_loss: 9.4175e-04 - val_mae: 0.0339\n3/3 [==============================] - 0s 16ms/step - loss: 6.2891e-04 - mae: 0.0259\n\n\n25919.86581683159\n\n\n\nbus_naive = mulvar_valid[\"bus\"].shift(7)[seq_length:]\nbus_target = mulvar_valid[\"bus\"][seq_length:]\n(bus_target - bus_naive).abs().mean() * 1e6\n\n43441.63157894738\n\n\n\nY_preds_valid = multask_model.predict(valid_multask_ds)\nfor idx, name in enumerate([\"bus\", \"rail\"]):\n    mae = 1e6 * tf.keras.metrics.mean_absolute_error(\n        mulvar_valid[name][seq_length:], Y_preds_valid[:, idx])\n    print(name, int(mae))\n\n3/3 [==============================] - 1s 14ms/step\nbus 27433\nrail 24406\n\n\nUsing a single model for multiple related tasks often results in better performance than using a separate model for each task, since features learned for one task may be useful for the other tasks, and also because having to perform well across multiple tasks prevents the model from overfitting (it’s a form of regularization). However, it depends on the task, and in this particular case the multitask RNN that forecasts both the bus and the rail ridership doesn’t perform quite as well as dedicated models that forecast one or the other (using all five columns as input). Still, it reaches a validation MAE of 27,433 for bus and 24,406 for rail, which is pretty good.\nYou might find https://www.tensorflow.org/api_docs/python/tf/keras/utils/timeseries_dataset_from_array or https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/TimeseriesGenerator useful\n\n\n10.2.9 Forecasting Several Time Steps Ahead\nSo far we have only predicted the value at the next time step, but we could just as easily have predicted the value several steps ahead by changing the targets appropriately (e.g., to predict the ridership 2 weeks from now, we could just change the targets to be the value 14 days ahead instead of 1 day ahead). But what if we want to predict the next 14 values?\nThe first option is to take the univar_model RNN we trained earlier for the rail time series, make it predict the next value, and add that value to the inputs, acting as if the predicted value had actually occurred; we would then use the model again to predict the following value, and so on, as in the following code:\n\nX = rail_valid.to_numpy()[np.newaxis, :seq_length, np.newaxis]\nfor step_ahead in range(14):\n    y_pred_one = univar_model.predict(X)\n    X = np.concatenate([X, y_pred_one.reshape(1, 1, 1)], axis=1) # Concatenate it as input (expanding window)\n\n1/1 [==============================] - 0s 216ms/step\n1/1 [==============================] - 0s 205ms/step\n1/1 [==============================] - 0s 51ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 29ms/step\n\n\nIn this code, we take the rail ridership of the first 56 days of the validation period, and we convert the data to a NumPy array of shape [1, 56, 1] (recall that recurrent layers expect 3D inputs). Then we repeatedly use the model to forecast the next value, and we append each forecast to the input series, along the time axis (axis=1).\n\n# The forecasts start on 2019-02-26, as it is the 57th day of 2019, and they end\n# on 2019-03-11. That's 14 days in total.\nY_pred = pd.Series(X[0, -14:, 0], index=pd.date_range(\"2019-02-26\", \"2019-03-11\"))\n\nfig, ax = plt.subplots(figsize=(8, 3.5))\n\n(rail_valid * 1e6)[\"2019-02-01\":\"2019-03-11\"].plot(\n    label=\"True\", marker=\".\", ax=ax)\n(Y_pred * 1e6).plot(\n    label=\"Predictions\", grid=True, marker=\"x\", color=\"r\", ax=ax)\n\nax.vlines(\"2019-02-25\", 0, 1e6, color=\"k\", linestyle=\"--\", label=\"Today\")\nax.set_ylim([200_000, 800_000])\nplt.legend(loc=\"center left\")\nplt.show()\n\n\n\n\nThe second option is to train an RNN to predict the next 14 values in one shot. We can still use a sequence-to-vector model, but it will output 14 values instead of 1. However, we first need to change the targets to be vectors containing the next 14 values. To do this, we can use timeseries_dataset_from_array() again, but this time asking it to create datasets without targets (targets=None) and with longer sequences, of length seq_length + 14. Then we can use the datasets’ map() method to apply a custom function to each batch of sequences, splitting them into inputs and targets. In this example, we use the multivariate time series as input (using all five columns), and we forecast the rail ridership for the next 14 days:\n\ntf.random.set_seed(42)  # extra code – ensures reproducibility\n\ndef split_inputs_and_targets(mulvar_series, ahead=14, target_col=1):\n    return mulvar_series[:, :-ahead], mulvar_series[:, -ahead:, target_col]\n\nahead_train_ds = tf.keras.utils.timeseries_dataset_from_array(\n    mulvar_train.to_numpy(),\n    targets=None,\n    sequence_length=seq_length + 14,\n    batch_size=32,\n    shuffle=True,\n    seed=42\n).map(split_inputs_and_targets)\n\nahead_valid_ds = tf.keras.utils.timeseries_dataset_from_array(\n    mulvar_valid.to_numpy(),\n    targets=None,\n    sequence_length=seq_length + 14,\n    batch_size=32\n).map(split_inputs_and_targets)\n\nNow we just need the output layer to have 14 units instead of 1:\n\ntf.random.set_seed(42)\n\nahead_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),\n    tf.keras.layers.Dense(14) # Output is 14 dimension!\n])\n\n\nfit_and_evaluate(ahead_model, ahead_train_ds, ahead_valid_ds, learning_rate=0.02)\n\nEpoch 1/500\n33/33 [==============================] - 5s 69ms/step - loss: 0.1590 - mae: 0.4254 - val_loss: 0.0288 - val_mae: 0.1948\nEpoch 2/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0202 - mae: 0.1616 - val_loss: 0.0138 - val_mae: 0.1287\nEpoch 3/500\n33/33 [==============================] - 4s 107ms/step - loss: 0.0133 - mae: 0.1292 - val_loss: 0.0106 - val_mae: 0.1128\nEpoch 4/500\n33/33 [==============================] - 3s 78ms/step - loss: 0.0111 - mae: 0.1170 - val_loss: 0.0088 - val_mae: 0.1036\nEpoch 5/500\n33/33 [==============================] - 2s 70ms/step - loss: 0.0099 - mae: 0.1096 - val_loss: 0.0079 - val_mae: 0.0965\nEpoch 6/500\n33/33 [==============================] - 2s 70ms/step - loss: 0.0090 - mae: 0.1033 - val_loss: 0.0070 - val_mae: 0.0915\nEpoch 7/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0084 - mae: 0.0986 - val_loss: 0.0063 - val_mae: 0.0864\nEpoch 8/500\n33/33 [==============================] - 2s 71ms/step - loss: 0.0080 - mae: 0.0951 - val_loss: 0.0060 - val_mae: 0.0832\nEpoch 9/500\n33/33 [==============================] - 2s 57ms/step - loss: 0.0076 - mae: 0.0921 - val_loss: 0.0057 - val_mae: 0.0802\nEpoch 10/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0072 - mae: 0.0889 - val_loss: 0.0052 - val_mae: 0.0774\nEpoch 11/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0069 - mae: 0.0864 - val_loss: 0.0049 - val_mae: 0.0746\nEpoch 12/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0067 - mae: 0.0845 - val_loss: 0.0046 - val_mae: 0.0724\nEpoch 13/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0066 - mae: 0.0832 - val_loss: 0.0047 - val_mae: 0.0726\nEpoch 14/500\n33/33 [==============================] - 2s 68ms/step - loss: 0.0064 - mae: 0.0810 - val_loss: 0.0043 - val_mae: 0.0698\nEpoch 15/500\n33/33 [==============================] - 4s 122ms/step - loss: 0.0062 - mae: 0.0797 - val_loss: 0.0040 - val_mae: 0.0677\nEpoch 16/500\n33/33 [==============================] - 3s 74ms/step - loss: 0.0060 - mae: 0.0783 - val_loss: 0.0040 - val_mae: 0.0679\nEpoch 17/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0059 - mae: 0.0771 - val_loss: 0.0040 - val_mae: 0.0674\nEpoch 18/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0058 - mae: 0.0761 - val_loss: 0.0037 - val_mae: 0.0649\nEpoch 19/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0057 - mae: 0.0750 - val_loss: 0.0035 - val_mae: 0.0634\nEpoch 20/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0055 - mae: 0.0735 - val_loss: 0.0035 - val_mae: 0.0635\nEpoch 21/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0054 - mae: 0.0727 - val_loss: 0.0034 - val_mae: 0.0617\nEpoch 22/500\n33/33 [==============================] - 2s 67ms/step - loss: 0.0053 - mae: 0.0718 - val_loss: 0.0032 - val_mae: 0.0607\nEpoch 23/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0052 - mae: 0.0710 - val_loss: 0.0031 - val_mae: 0.0597\nEpoch 24/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0051 - mae: 0.0701 - val_loss: 0.0033 - val_mae: 0.0612\nEpoch 25/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0051 - mae: 0.0696 - val_loss: 0.0031 - val_mae: 0.0592\nEpoch 26/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0050 - mae: 0.0684 - val_loss: 0.0029 - val_mae: 0.0575\nEpoch 27/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0049 - mae: 0.0676 - val_loss: 0.0029 - val_mae: 0.0573\nEpoch 28/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0048 - mae: 0.0670 - val_loss: 0.0027 - val_mae: 0.0552\nEpoch 29/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0047 - mae: 0.0659 - val_loss: 0.0026 - val_mae: 0.0535\nEpoch 30/500\n33/33 [==============================] - 2s 59ms/step - loss: 0.0046 - mae: 0.0652 - val_loss: 0.0025 - val_mae: 0.0527\nEpoch 31/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0046 - mae: 0.0652 - val_loss: 0.0025 - val_mae: 0.0525\nEpoch 32/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0045 - mae: 0.0637 - val_loss: 0.0025 - val_mae: 0.0523\nEpoch 33/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0045 - mae: 0.0631 - val_loss: 0.0024 - val_mae: 0.0511\nEpoch 34/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0044 - mae: 0.0626 - val_loss: 0.0023 - val_mae: 0.0501\nEpoch 35/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0043 - mae: 0.0618 - val_loss: 0.0024 - val_mae: 0.0513\nEpoch 36/500\n33/33 [==============================] - 2s 64ms/step - loss: 0.0043 - mae: 0.0614 - val_loss: 0.0023 - val_mae: 0.0498\nEpoch 37/500\n33/33 [==============================] - 2s 58ms/step - loss: 0.0043 - mae: 0.0610 - val_loss: 0.0021 - val_mae: 0.0479\nEpoch 38/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0042 - mae: 0.0605 - val_loss: 0.0021 - val_mae: 0.0474\nEpoch 39/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0041 - mae: 0.0597 - val_loss: 0.0021 - val_mae: 0.0475\nEpoch 40/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0041 - mae: 0.0592 - val_loss: 0.0021 - val_mae: 0.0479\nEpoch 41/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0041 - mae: 0.0585 - val_loss: 0.0022 - val_mae: 0.0483\nEpoch 42/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0040 - mae: 0.0582 - val_loss: 0.0020 - val_mae: 0.0467\nEpoch 43/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0040 - mae: 0.0576 - val_loss: 0.0019 - val_mae: 0.0445\nEpoch 44/500\n33/33 [==============================] - 2s 53ms/step - loss: 0.0040 - mae: 0.0577 - val_loss: 0.0019 - val_mae: 0.0446\nEpoch 45/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0040 - mae: 0.0577 - val_loss: 0.0018 - val_mae: 0.0434\nEpoch 46/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0039 - mae: 0.0568 - val_loss: 0.0019 - val_mae: 0.0446\nEpoch 47/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0039 - mae: 0.0561 - val_loss: 0.0018 - val_mae: 0.0434\nEpoch 48/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0039 - mae: 0.0559 - val_loss: 0.0018 - val_mae: 0.0424\nEpoch 49/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0038 - mae: 0.0555 - val_loss: 0.0018 - val_mae: 0.0435\nEpoch 50/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0038 - mae: 0.0550 - val_loss: 0.0017 - val_mae: 0.0416\nEpoch 51/500\n33/33 [==============================] - 2s 65ms/step - loss: 0.0038 - mae: 0.0551 - val_loss: 0.0017 - val_mae: 0.0406\nEpoch 52/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0037 - mae: 0.0544 - val_loss: 0.0017 - val_mae: 0.0411\nEpoch 53/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0037 - mae: 0.0540 - val_loss: 0.0016 - val_mae: 0.0402\nEpoch 54/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0038 - mae: 0.0558 - val_loss: 0.0017 - val_mae: 0.0409\nEpoch 55/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0037 - mae: 0.0536 - val_loss: 0.0017 - val_mae: 0.0416\nEpoch 56/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0037 - mae: 0.0533 - val_loss: 0.0016 - val_mae: 0.0397\nEpoch 57/500\n33/33 [==============================] - 2s 57ms/step - loss: 0.0037 - mae: 0.0537 - val_loss: 0.0017 - val_mae: 0.0410\nEpoch 58/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0036 - mae: 0.0530 - val_loss: 0.0016 - val_mae: 0.0400\nEpoch 59/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0036 - mae: 0.0528 - val_loss: 0.0016 - val_mae: 0.0395\nEpoch 60/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0036 - mae: 0.0528 - val_loss: 0.0017 - val_mae: 0.0408\nEpoch 61/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0036 - mae: 0.0527 - val_loss: 0.0016 - val_mae: 0.0388\nEpoch 62/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0036 - mae: 0.0527 - val_loss: 0.0017 - val_mae: 0.0412\nEpoch 63/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0036 - mae: 0.0520 - val_loss: 0.0015 - val_mae: 0.0382\nEpoch 64/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0035 - mae: 0.0519 - val_loss: 0.0015 - val_mae: 0.0381\nEpoch 65/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0035 - mae: 0.0514 - val_loss: 0.0016 - val_mae: 0.0387\nEpoch 66/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0035 - mae: 0.0515 - val_loss: 0.0016 - val_mae: 0.0387\nEpoch 67/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0035 - mae: 0.0526 - val_loss: 0.0016 - val_mae: 0.0404\nEpoch 68/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0035 - mae: 0.0514 - val_loss: 0.0016 - val_mae: 0.0399\nEpoch 69/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0035 - mae: 0.0520 - val_loss: 0.0018 - val_mae: 0.0431\nEpoch 70/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0035 - mae: 0.0509 - val_loss: 0.0015 - val_mae: 0.0367\nEpoch 71/500\n33/33 [==============================] - 2s 54ms/step - loss: 0.0034 - mae: 0.0508 - val_loss: 0.0016 - val_mae: 0.0386\nEpoch 72/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0035 - mae: 0.0505 - val_loss: 0.0015 - val_mae: 0.0370\nEpoch 73/500\n33/33 [==============================] - 1s 43ms/step - loss: 0.0034 - mae: 0.0506 - val_loss: 0.0015 - val_mae: 0.0370\nEpoch 74/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0034 - mae: 0.0503 - val_loss: 0.0015 - val_mae: 0.0383\nEpoch 75/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0034 - mae: 0.0503 - val_loss: 0.0015 - val_mae: 0.0380\nEpoch 76/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0034 - mae: 0.0498 - val_loss: 0.0016 - val_mae: 0.0389\nEpoch 77/500\n33/33 [==============================] - 1s 37ms/step - loss: 0.0034 - mae: 0.0499 - val_loss: 0.0015 - val_mae: 0.0368\nEpoch 78/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0034 - mae: 0.0497 - val_loss: 0.0014 - val_mae: 0.0359\nEpoch 79/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0034 - mae: 0.0497 - val_loss: 0.0015 - val_mae: 0.0371\nEpoch 80/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0034 - mae: 0.0496 - val_loss: 0.0016 - val_mae: 0.0405\nEpoch 81/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0034 - mae: 0.0499 - val_loss: 0.0014 - val_mae: 0.0353\nEpoch 82/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0034 - mae: 0.0499 - val_loss: 0.0014 - val_mae: 0.0366\nEpoch 83/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0033 - mae: 0.0492 - val_loss: 0.0014 - val_mae: 0.0359\nEpoch 84/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0495 - val_loss: 0.0014 - val_mae: 0.0354\nEpoch 85/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0491 - val_loss: 0.0016 - val_mae: 0.0395\nEpoch 86/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0034 - mae: 0.0502 - val_loss: 0.0015 - val_mae: 0.0370\nEpoch 87/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0492 - val_loss: 0.0014 - val_mae: 0.0359\nEpoch 88/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0033 - mae: 0.0487 - val_loss: 0.0014 - val_mae: 0.0358\nEpoch 89/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0033 - mae: 0.0488 - val_loss: 0.0014 - val_mae: 0.0355\nEpoch 90/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0487 - val_loss: 0.0014 - val_mae: 0.0359\nEpoch 91/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0486 - val_loss: 0.0015 - val_mae: 0.0372\nEpoch 92/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0033 - mae: 0.0493 - val_loss: 0.0014 - val_mae: 0.0356\nEpoch 93/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0033 - mae: 0.0483 - val_loss: 0.0014 - val_mae: 0.0347\nEpoch 94/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0033 - mae: 0.0486 - val_loss: 0.0014 - val_mae: 0.0350\nEpoch 95/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0488 - val_loss: 0.0014 - val_mae: 0.0354\nEpoch 96/500\n33/33 [==============================] - 2s 73ms/step - loss: 0.0033 - mae: 0.0486 - val_loss: 0.0014 - val_mae: 0.0346\nEpoch 97/500\n33/33 [==============================] - 2s 58ms/step - loss: 0.0032 - mae: 0.0480 - val_loss: 0.0014 - val_mae: 0.0349\nEpoch 98/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0014 - val_mae: 0.0345\nEpoch 99/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0032 - mae: 0.0485 - val_loss: 0.0014 - val_mae: 0.0356\nEpoch 100/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0014 - val_mae: 0.0348\nEpoch 101/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0014 - val_mae: 0.0363\nEpoch 102/500\n33/33 [==============================] - 2s 64ms/step - loss: 0.0032 - mae: 0.0480 - val_loss: 0.0015 - val_mae: 0.0375\nEpoch 103/500\n33/33 [==============================] - 2s 73ms/step - loss: 0.0032 - mae: 0.0486 - val_loss: 0.0014 - val_mae: 0.0355\nEpoch 104/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0033 - mae: 0.0490 - val_loss: 0.0013 - val_mae: 0.0344\nEpoch 105/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0014 - val_mae: 0.0344\nEpoch 106/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0014 - val_mae: 0.0351\nEpoch 107/500\n33/33 [==============================] - 3s 104ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0014 - val_mae: 0.0361\nEpoch 108/500\n33/33 [==============================] - 4s 117ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0014 - val_mae: 0.0348\nEpoch 109/500\n33/33 [==============================] - 3s 98ms/step - loss: 0.0032 - mae: 0.0474 - val_loss: 0.0013 - val_mae: 0.0344\nEpoch 110/500\n33/33 [==============================] - 2s 59ms/step - loss: 0.0032 - mae: 0.0475 - val_loss: 0.0014 - val_mae: 0.0343\nEpoch 111/500\n33/33 [==============================] - 3s 101ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0014 - val_mae: 0.0347\nEpoch 112/500\n33/33 [==============================] - 4s 103ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0014 - val_mae: 0.0348\nEpoch 113/500\n33/33 [==============================] - 2s 73ms/step - loss: 0.0032 - mae: 0.0477 - val_loss: 0.0014 - val_mae: 0.0345\nEpoch 114/500\n33/33 [==============================] - 2s 73ms/step - loss: 0.0032 - mae: 0.0475 - val_loss: 0.0015 - val_mae: 0.0369\nEpoch 115/500\n33/33 [==============================] - 2s 71ms/step - loss: 0.0032 - mae: 0.0487 - val_loss: 0.0014 - val_mae: 0.0361\nEpoch 116/500\n33/33 [==============================] - 2s 64ms/step - loss: 0.0032 - mae: 0.0472 - val_loss: 0.0014 - val_mae: 0.0354\nEpoch 117/500\n33/33 [==============================] - 2s 73ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0014 - val_mae: 0.0344\nEpoch 118/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0032 - mae: 0.0480 - val_loss: 0.0014 - val_mae: 0.0347\nEpoch 119/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0032 - mae: 0.0474 - val_loss: 0.0013 - val_mae: 0.0340\nEpoch 120/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0014 - val_mae: 0.0344\nEpoch 121/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0031 - mae: 0.0475 - val_loss: 0.0014 - val_mae: 0.0356\nEpoch 122/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0015 - val_mae: 0.0374\nEpoch 123/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0014 - val_mae: 0.0346\nEpoch 124/500\n33/33 [==============================] - 4s 103ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0014 - val_mae: 0.0347\nEpoch 125/500\n33/33 [==============================] - 3s 84ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0014 - val_mae: 0.0352\nEpoch 126/500\n33/33 [==============================] - 3s 83ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0014 - val_mae: 0.0346\nEpoch 127/500\n33/33 [==============================] - 3s 86ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0014 - val_mae: 0.0345\nEpoch 128/500\n33/33 [==============================] - 4s 103ms/step - loss: 0.0031 - mae: 0.0475 - val_loss: 0.0015 - val_mae: 0.0367\nEpoch 129/500\n33/33 [==============================] - 2s 56ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0014 - val_mae: 0.0348\nEpoch 130/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0474 - val_loss: 0.0014 - val_mae: 0.0343\nEpoch 131/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0014 - val_mae: 0.0350\nEpoch 132/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0014 - val_mae: 0.0348\nEpoch 133/500\n33/33 [==============================] - 2s 53ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0013 - val_mae: 0.0338\nEpoch 134/500\n33/33 [==============================] - 2s 74ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0014 - val_mae: 0.0350\nEpoch 135/500\n33/33 [==============================] - 2s 43ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0015 - val_mae: 0.0372\nEpoch 136/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0014 - val_mae: 0.0353\nEpoch 137/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0464 - val_loss: 0.0014 - val_mae: 0.0349\nEpoch 138/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0478 - val_loss: 0.0015 - val_mae: 0.0366\nEpoch 139/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0013 - val_mae: 0.0337\nEpoch 140/500\n33/33 [==============================] - 1s 44ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0014 - val_mae: 0.0351\nEpoch 141/500\n33/33 [==============================] - 2s 65ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0014 - val_mae: 0.0356\nEpoch 142/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0475 - val_loss: 0.0014 - val_mae: 0.0342\nEpoch 143/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0013 - val_mae: 0.0342\nEpoch 144/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0463 - val_loss: 0.0014 - val_mae: 0.0345\nEpoch 145/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0032 - mae: 0.0485 - val_loss: 0.0013 - val_mae: 0.0341\nEpoch 146/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0014 - val_mae: 0.0352\nEpoch 147/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0013 - val_mae: 0.0339\nEpoch 148/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0015 - val_mae: 0.0365\nEpoch 149/500\n33/33 [==============================] - 3s 75ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0014 - val_mae: 0.0348\nEpoch 150/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0477 - val_loss: 0.0014 - val_mae: 0.0349\nEpoch 151/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0014 - val_mae: 0.0345\nEpoch 152/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0014 - val_mae: 0.0349\nEpoch 153/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0464 - val_loss: 0.0014 - val_mae: 0.0348\nEpoch 154/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0014 - val_mae: 0.0353\nEpoch 155/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0013 - val_mae: 0.0336\nEpoch 156/500\n33/33 [==============================] - 3s 76ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0014 - val_mae: 0.0344\nEpoch 157/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0031 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0345\nEpoch 158/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0014 - val_mae: 0.0347\nEpoch 159/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0015 - val_mae: 0.0363\nEpoch 160/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0014 - val_mae: 0.0350\nEpoch 161/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0013 - val_mae: 0.0339\nEpoch 162/500\n33/33 [==============================] - 2s 66ms/step - loss: 0.0030 - mae: 0.0466 - val_loss: 0.0014 - val_mae: 0.0346\nEpoch 163/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0014 - val_mae: 0.0347\nEpoch 164/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0014 - val_mae: 0.0341\nEpoch 165/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0347\nEpoch 166/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0014 - val_mae: 0.0352\nEpoch 167/500\n33/33 [==============================] - 2s 74ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0014 - val_mae: 0.0357\nEpoch 168/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0013 - val_mae: 0.0338\nEpoch 169/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0344\nEpoch 170/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0014 - val_mae: 0.0344\nEpoch 171/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0354\nEpoch 172/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0013 - val_mae: 0.0340\nEpoch 173/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0347\nEpoch 174/500\n33/33 [==============================] - 2s 46ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0342\nEpoch 175/500\n33/33 [==============================] - 2s 68ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0344\nEpoch 176/500\n33/33 [==============================] - 2s 65ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0014 - val_mae: 0.0354\nEpoch 177/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0014 - val_mae: 0.0341\nEpoch 178/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0013 - val_mae: 0.0338\nEpoch 179/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0475 - val_loss: 0.0014 - val_mae: 0.0343\nEpoch 180/500\n33/33 [==============================] - 2s 60ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0014 - val_mae: 0.0344\nEpoch 181/500\n33/33 [==============================] - 2s 65ms/step - loss: 0.0030 - mae: 0.0457 - val_loss: 0.0014 - val_mae: 0.0356\nEpoch 182/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0014 - val_mae: 0.0354\nEpoch 183/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0015 - val_mae: 0.0377\nEpoch 184/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0465 - val_loss: 0.0014 - val_mae: 0.0351\nEpoch 185/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0341\nEpoch 186/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0013 - val_mae: 0.0338\nEpoch 187/500\n33/33 [==============================] - 1s 43ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0014 - val_mae: 0.0344\nEpoch 188/500\n33/33 [==============================] - 3s 93ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0014 - val_mae: 0.0348\nEpoch 189/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0348\nEpoch 190/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0014 - val_mae: 0.0345\nEpoch 191/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0014 - val_mae: 0.0348\nEpoch 192/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0014 - val_mae: 0.0349\nEpoch 193/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0014 - val_mae: 0.0355\nEpoch 194/500\n33/33 [==============================] - 2s 70ms/step - loss: 0.0030 - mae: 0.0455 - val_loss: 0.0015 - val_mae: 0.0367\nEpoch 195/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0474 - val_loss: 0.0013 - val_mae: 0.0336\nEpoch 196/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0014 - val_mae: 0.0346\nEpoch 197/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0014 - val_mae: 0.0341\nEpoch 198/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0348\nEpoch 199/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0013 - val_mae: 0.0340\nEpoch 200/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0454 - val_loss: 0.0014 - val_mae: 0.0352\nEpoch 201/500\n33/33 [==============================] - 2s 71ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0356\nEpoch 202/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0013 - val_mae: 0.0339\nEpoch 203/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0013 - val_mae: 0.0339\nEpoch 204/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0454 - val_loss: 0.0015 - val_mae: 0.0364\nEpoch 205/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0469 - val_loss: 0.0014 - val_mae: 0.0344\nEpoch 206/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0341\nEpoch 207/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0453 - val_loss: 0.0014 - val_mae: 0.0360\nEpoch 208/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0453 - val_loss: 0.0014 - val_mae: 0.0350\nEpoch 209/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0015 - val_mae: 0.0371\nEpoch 210/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0020 - val_mae: 0.0458\nEpoch 211/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0032 - mae: 0.0489 - val_loss: 0.0013 - val_mae: 0.0338\nEpoch 212/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0341\nEpoch 213/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0455 - val_loss: 0.0014 - val_mae: 0.0346\nEpoch 214/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0457 - val_loss: 0.0014 - val_mae: 0.0356\nEpoch 215/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0030 - mae: 0.0465 - val_loss: 0.0014 - val_mae: 0.0350\nEpoch 216/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0030 - mae: 0.0457 - val_loss: 0.0014 - val_mae: 0.0346\nEpoch 217/500\n33/33 [==============================] - 2s 73ms/step - loss: 0.0029 - mae: 0.0451 - val_loss: 0.0014 - val_mae: 0.0349\nEpoch 218/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0458 - val_loss: 0.0014 - val_mae: 0.0344\nEpoch 219/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0348\nEpoch 220/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0014 - val_mae: 0.0360\nEpoch 221/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0455 - val_loss: 0.0014 - val_mae: 0.0350\nEpoch 222/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0029 - mae: 0.0453 - val_loss: 0.0014 - val_mae: 0.0355\nEpoch 223/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0029 - mae: 0.0452 - val_loss: 0.0014 - val_mae: 0.0341\nEpoch 224/500\n33/33 [==============================] - 2s 54ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0351\nEpoch 225/500\n33/33 [==============================] - 2s 63ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0014 - val_mae: 0.0348\nEpoch 226/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0015 - val_mae: 0.0362\nEpoch 227/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0361\nEpoch 228/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0455 - val_loss: 0.0014 - val_mae: 0.0353\nEpoch 229/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0029 - mae: 0.0459 - val_loss: 0.0014 - val_mae: 0.0351\nEpoch 230/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0029 - mae: 0.0457 - val_loss: 0.0014 - val_mae: 0.0340\nEpoch 231/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0466 - val_loss: 0.0014 - val_mae: 0.0346\nEpoch 232/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0343\nEpoch 233/500\n33/33 [==============================] - 2s 67ms/step - loss: 0.0029 - mae: 0.0454 - val_loss: 0.0015 - val_mae: 0.0364\nEpoch 234/500\n33/33 [==============================] - 1s 39ms/step - loss: 0.0029 - mae: 0.0455 - val_loss: 0.0014 - val_mae: 0.0350\nEpoch 235/500\n33/33 [==============================] - 1s 41ms/step - loss: 0.0029 - mae: 0.0451 - val_loss: 0.0014 - val_mae: 0.0360\nEpoch 236/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0343\nEpoch 237/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0014 - val_mae: 0.0345\nEpoch 238/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0029 - mae: 0.0450 - val_loss: 0.0019 - val_mae: 0.0438\nEpoch 239/500\n33/33 [==============================] - 2s 68ms/step - loss: 0.0031 - mae: 0.0490 - val_loss: 0.0014 - val_mae: 0.0344\nEpoch 240/500\n33/33 [==============================] - 2s 61ms/step - loss: 0.0029 - mae: 0.0456 - val_loss: 0.0013 - val_mae: 0.0338\nEpoch 241/500\n33/33 [==============================] - 1s 38ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0353\nEpoch 242/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0029 - mae: 0.0453 - val_loss: 0.0013 - val_mae: 0.0337\nEpoch 243/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0029 - mae: 0.0451 - val_loss: 0.0014 - val_mae: 0.0347\nEpoch 244/500\n33/33 [==============================] - 1s 42ms/step - loss: 0.0029 - mae: 0.0450 - val_loss: 0.0013 - val_mae: 0.0344\nEpoch 245/500\n33/33 [==============================] - 1s 40ms/step - loss: 0.0029 - mae: 0.0452 - val_loss: 0.0015 - val_mae: 0.0376\n3/3 [==============================] - 0s 12ms/step - loss: 0.0013 - mae: 0.0336\n\n\n33568.80694627762\n\n\nAfter training this model, you can predict the next 14 values at once like this:\n\nX = mulvar_valid.to_numpy()[np.newaxis, :seq_length]  # shape [1, 56, 5]\nY_pred = ahead_model.predict(X)  # shape [1, 14]\n\n1/1 [==============================] - 0s 197ms/step\n\n\nThis approach works quite well. Its forecasts for the next day are obviously better than its forecasts for 14 days into the future, but it doesn’t accumulate errors like the previous approach did. However, we can still do better, using a sequence-to-sequence (or seq2seq) model.\n\n\n10.2.10 Forecasting Using a Sequence-to-Sequence Model\nInstead of training the model to forecast the next 14 values only at the very last time step, we can train it to forecast the next 14 values at each and every time step. In other words, we can turn this sequence-to-vector RNN into a sequence-to-sequence RNN. The advantage of this technique is that the loss will contain a term for the output of the RNN at each and every time step, not just for the output at the last time step.\nThis means there will be many more error gradients flowing through the model, and they won’t have to flow through time as much since they will come from the output of each time step, not just the last one. This will both stabilize and speed up training. To be clear, at time step 0 the model will output a vector containing the forecasts for time steps 1 to 14, then at time step 1 the model will forecast time steps 2 to 15, and so on. In other words, the targets are sequences of consecutive windows, shifted by one time step at each time step. The target is not a vector anymore, but a sequence of the same length as the inputs, containing a 14-dimensional vector at each step.\nPreparing the datasets is not trivial, since each instance has a window as input and a sequence of windows as output. One way to do this is to use the to_windows() utility function we created earlier, twice in a row, to get windows of consecutive windows. For example, let’s turn the series of numbers 0 to 6 into a dataset containing sequences of 4 consecutive windows, each of length 3:\n\nmy_series = tf.data.Dataset.range(7)\ndataset = to_windows(to_windows(my_series, 3), 4)\nlist(dataset)\n\n[&lt;tf.Tensor: shape=(4, 3), dtype=int64, numpy=\n array([[0, 1, 2],\n        [1, 2, 3],\n        [2, 3, 4],\n        [3, 4, 5]])&gt;,\n &lt;tf.Tensor: shape=(4, 3), dtype=int64, numpy=\n array([[1, 2, 3],\n        [2, 3, 4],\n        [3, 4, 5],\n        [4, 5, 6]])&gt;]\n\n\nNow we can use the map() method to split these windows of windows into inputs and targets:\n\ndataset = dataset.map(lambda S: (S[:, 0], S[:, 1:]))\nlist(dataset)\n\n[(&lt;tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 1, 2, 3])&gt;,\n  &lt;tf.Tensor: shape=(4, 2), dtype=int64, numpy=\n  array([[1, 2],\n         [2, 3],\n         [3, 4],\n         [4, 5]])&gt;),\n (&lt;tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 2, 3, 4])&gt;,\n  &lt;tf.Tensor: shape=(4, 2), dtype=int64, numpy=\n  array([[2, 3],\n         [3, 4],\n         [4, 5],\n         [5, 6]])&gt;)]\n\n\nNow the dataset contains sequences of length 4 as inputs, and the targets are sequences containing the next two steps, for each time step. For example, the first input sequence is [0, 1, 2, 3], and its corresponding targets are [[1, 2], [2, 3], [3, 4], [4, 5]], which are the next two values for each time step.\nLet’s create another little utility function to prepare the datasets for our sequence-to-sequence model. It will also take care of shuffling (optional) and batching:\n\ndef to_seq2seq_dataset(series, seq_length=56, ahead=14, target_col=1,\n                       batch_size=32, shuffle=False, seed=None):\n    ds = to_windows(tf.data.Dataset.from_tensor_slices(series), ahead + 1)\n    ds = to_windows(ds, seq_length).map(lambda S: (S[:, 0], S[:, 1:, 1]))\n    if shuffle:\n        ds = ds.shuffle(8 * batch_size, seed=seed)\n    return ds.batch(batch_size)\n\nNow we can use this function to create the datasets:\n\nseq2seq_train = to_seq2seq_dataset(mulvar_train, shuffle=True, seed=42)\nseq2seq_valid = to_seq2seq_dataset(mulvar_valid)\n\nAnd lastly, we can build the sequence-to-sequence model:\n\ntf.random.set_seed(42)  # extra code – ensures reproducibility\nseq2seq_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 5]),\n    tf.keras.layers.Dense(14) # Output is 14 dimension\n    # equivalent: tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(14))\n    # also equivalent: tf.keras.layers.Conv1D(14, kernel_size=1)\n])\n\n\nseq2seq_model.summary()\n\nModel: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_8 (SimpleRNN)    (None, None, 32)          1216      \n                                                                 \n dense_6 (Dense)             (None, None, 14)          462       \n                                                                 \n=================================================================\nTotal params: 1,678\nTrainable params: 1,678\nNon-trainable params: 0\n_________________________________________________________________\n\n\nIt is almost identical to our previous model: the only difference is that we set return_sequences=True in the SimpleRNN layer. This way, it will output a sequence of vectors (each of size 32), instead of outputting a single vector at the last time step. The Dense layer is smart enough to handle sequences as input: it will be applied at each time step, taking a 32-dimensional vector as input and outputting a 14-dimensional vector. In fact, another way to get the exact same result is to use a Conv1D layer with a kernel size of 1: Conv1D(14, kernel_size=1).\n\ntf.Keras offers a TimeDistributed layer that lets you apply any vector-to-vector layer to every vector in the input sequences, at every time step. It does this efficiently, by reshaping the inputs so that each time step is treated as a separate instance, then it reshapes the layer’s outputs to recover the time dimension. In our case, we don’t need it since the Dense layer already supports sequences as inputs.\n\n\nfit_and_evaluate(seq2seq_model, seq2seq_train, seq2seq_valid, learning_rate=0.1)\n\nEpoch 1/500\n33/33 [==============================] - 3s 68ms/step - loss: 0.0542 - mae: 0.2405 - val_loss: 0.0141 - val_mae: 0.1308\nEpoch 2/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0093 - mae: 0.1001 - val_loss: 0.0067 - val_mae: 0.0815\nEpoch 3/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0065 - mae: 0.0788 - val_loss: 0.0057 - val_mae: 0.0723\nEpoch 4/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0056 - mae: 0.0709 - val_loss: 0.0051 - val_mae: 0.0672\nEpoch 5/500\n33/33 [==============================] - 2s 70ms/step - loss: 0.0051 - mae: 0.0665 - val_loss: 0.0046 - val_mae: 0.0621\nEpoch 6/500\n33/33 [==============================] - 3s 80ms/step - loss: 0.0048 - mae: 0.0645 - val_loss: 0.0047 - val_mae: 0.0637\nEpoch 7/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0046 - mae: 0.0625 - val_loss: 0.0045 - val_mae: 0.0604\nEpoch 8/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0044 - mae: 0.0609 - val_loss: 0.0043 - val_mae: 0.0582\nEpoch 9/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0043 - mae: 0.0602 - val_loss: 0.0041 - val_mae: 0.0564\nEpoch 10/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0042 - mae: 0.0593 - val_loss: 0.0043 - val_mae: 0.0584\nEpoch 11/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0041 - mae: 0.0585 - val_loss: 0.0042 - val_mae: 0.0574\nEpoch 12/500\n33/33 [==============================] - 2s 65ms/step - loss: 0.0041 - mae: 0.0580 - val_loss: 0.0041 - val_mae: 0.0556\nEpoch 13/500\n33/33 [==============================] - 3s 89ms/step - loss: 0.0040 - mae: 0.0568 - val_loss: 0.0042 - val_mae: 0.0574\nEpoch 14/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0039 - mae: 0.0560 - val_loss: 0.0042 - val_mae: 0.0567\nEpoch 15/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0039 - mae: 0.0557 - val_loss: 0.0040 - val_mae: 0.0544\nEpoch 16/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0039 - mae: 0.0555 - val_loss: 0.0040 - val_mae: 0.0544\nEpoch 17/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0038 - mae: 0.0550 - val_loss: 0.0040 - val_mae: 0.0539\nEpoch 18/500\n33/33 [==============================] - 3s 100ms/step - loss: 0.0038 - mae: 0.0545 - val_loss: 0.0041 - val_mae: 0.0553\nEpoch 19/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0037 - mae: 0.0540 - val_loss: 0.0040 - val_mae: 0.0539\nEpoch 20/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0037 - mae: 0.0537 - val_loss: 0.0039 - val_mae: 0.0533\nEpoch 21/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0037 - mae: 0.0537 - val_loss: 0.0040 - val_mae: 0.0535\nEpoch 22/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0037 - mae: 0.0535 - val_loss: 0.0040 - val_mae: 0.0533\nEpoch 23/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0037 - mae: 0.0536 - val_loss: 0.0039 - val_mae: 0.0527\nEpoch 24/500\n33/33 [==============================] - 3s 98ms/step - loss: 0.0036 - mae: 0.0526 - val_loss: 0.0039 - val_mae: 0.0529\nEpoch 25/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0036 - mae: 0.0527 - val_loss: 0.0039 - val_mae: 0.0522\nEpoch 26/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0036 - mae: 0.0523 - val_loss: 0.0039 - val_mae: 0.0518\nEpoch 27/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0036 - mae: 0.0524 - val_loss: 0.0039 - val_mae: 0.0526\nEpoch 28/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0035 - mae: 0.0520 - val_loss: 0.0038 - val_mae: 0.0513\nEpoch 29/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0035 - mae: 0.0521 - val_loss: 0.0039 - val_mae: 0.0526\nEpoch 30/500\n33/33 [==============================] - 3s 83ms/step - loss: 0.0036 - mae: 0.0524 - val_loss: 0.0040 - val_mae: 0.0535\nEpoch 31/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0035 - mae: 0.0518 - val_loss: 0.0038 - val_mae: 0.0515\nEpoch 32/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0035 - mae: 0.0509 - val_loss: 0.0038 - val_mae: 0.0513\nEpoch 33/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0035 - mae: 0.0514 - val_loss: 0.0038 - val_mae: 0.0508\nEpoch 34/500\n33/33 [==============================] - 3s 100ms/step - loss: 0.0035 - mae: 0.0511 - val_loss: 0.0039 - val_mae: 0.0517\nEpoch 35/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0034 - mae: 0.0506 - val_loss: 0.0038 - val_mae: 0.0510\nEpoch 36/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0034 - mae: 0.0507 - val_loss: 0.0038 - val_mae: 0.0507\nEpoch 37/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0034 - mae: 0.0507 - val_loss: 0.0038 - val_mae: 0.0509\nEpoch 38/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0034 - mae: 0.0503 - val_loss: 0.0039 - val_mae: 0.0519\nEpoch 39/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0034 - mae: 0.0506 - val_loss: 0.0038 - val_mae: 0.0510\nEpoch 40/500\n33/33 [==============================] - 3s 104ms/step - loss: 0.0034 - mae: 0.0501 - val_loss: 0.0038 - val_mae: 0.0507\nEpoch 41/500\n33/33 [==============================] - 2s 53ms/step - loss: 0.0034 - mae: 0.0500 - val_loss: 0.0039 - val_mae: 0.0515\nEpoch 42/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0034 - mae: 0.0499 - val_loss: 0.0038 - val_mae: 0.0506\nEpoch 43/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0034 - mae: 0.0497 - val_loss: 0.0038 - val_mae: 0.0509\nEpoch 44/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0034 - mae: 0.0496 - val_loss: 0.0038 - val_mae: 0.0508\nEpoch 45/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0034 - mae: 0.0501 - val_loss: 0.0038 - val_mae: 0.0506\nEpoch 46/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0034 - mae: 0.0499 - val_loss: 0.0039 - val_mae: 0.0519\nEpoch 47/500\n33/33 [==============================] - 3s 77ms/step - loss: 0.0033 - mae: 0.0494 - val_loss: 0.0038 - val_mae: 0.0509\nEpoch 48/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0033 - mae: 0.0495 - val_loss: 0.0039 - val_mae: 0.0534\nEpoch 49/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0033 - mae: 0.0498 - val_loss: 0.0038 - val_mae: 0.0518\nEpoch 50/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0034 - mae: 0.0501 - val_loss: 0.0038 - val_mae: 0.0504\nEpoch 51/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0033 - mae: 0.0491 - val_loss: 0.0038 - val_mae: 0.0504\nEpoch 52/500\n33/33 [==============================] - 3s 77ms/step - loss: 0.0033 - mae: 0.0489 - val_loss: 0.0038 - val_mae: 0.0504\nEpoch 53/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0033 - mae: 0.0493 - val_loss: 0.0038 - val_mae: 0.0504\nEpoch 54/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0033 - mae: 0.0490 - val_loss: 0.0040 - val_mae: 0.0534\nEpoch 55/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0033 - mae: 0.0492 - val_loss: 0.0038 - val_mae: 0.0505\nEpoch 56/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0033 - mae: 0.0493 - val_loss: 0.0038 - val_mae: 0.0512\nEpoch 57/500\n33/33 [==============================] - 3s 96ms/step - loss: 0.0033 - mae: 0.0489 - val_loss: 0.0039 - val_mae: 0.0530\nEpoch 58/500\n33/33 [==============================] - 2s 60ms/step - loss: 0.0033 - mae: 0.0490 - val_loss: 0.0038 - val_mae: 0.0507\nEpoch 59/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0033 - mae: 0.0493 - val_loss: 0.0037 - val_mae: 0.0498\nEpoch 60/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0033 - mae: 0.0488 - val_loss: 0.0041 - val_mae: 0.0567\nEpoch 61/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0033 - mae: 0.0496 - val_loss: 0.0038 - val_mae: 0.0521\nEpoch 62/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0033 - mae: 0.0491 - val_loss: 0.0039 - val_mae: 0.0523\nEpoch 63/500\n33/33 [==============================] - 2s 68ms/step - loss: 0.0033 - mae: 0.0494 - val_loss: 0.0039 - val_mae: 0.0526\nEpoch 64/500\n33/33 [==============================] - 3s 90ms/step - loss: 0.0033 - mae: 0.0494 - val_loss: 0.0037 - val_mae: 0.0497\nEpoch 65/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0486 - val_loss: 0.0037 - val_mae: 0.0497\nEpoch 66/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0483 - val_loss: 0.0037 - val_mae: 0.0496\nEpoch 67/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0486 - val_loss: 0.0038 - val_mae: 0.0502\nEpoch 68/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0032 - mae: 0.0483 - val_loss: 0.0038 - val_mae: 0.0502\nEpoch 69/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0032 - mae: 0.0485 - val_loss: 0.0037 - val_mae: 0.0497\nEpoch 70/500\n33/33 [==============================] - 3s 86ms/step - loss: 0.0032 - mae: 0.0484 - val_loss: 0.0037 - val_mae: 0.0498\nEpoch 71/500\n33/33 [==============================] - 2s 66ms/step - loss: 0.0032 - mae: 0.0485 - val_loss: 0.0037 - val_mae: 0.0499\nEpoch 72/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0032 - mae: 0.0486 - val_loss: 0.0037 - val_mae: 0.0498\nEpoch 73/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0485 - val_loss: 0.0039 - val_mae: 0.0538\nEpoch 74/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0033 - mae: 0.0500 - val_loss: 0.0037 - val_mae: 0.0494\nEpoch 75/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0037 - val_mae: 0.0500\nEpoch 76/500\n33/33 [==============================] - 2s 52ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0037 - val_mae: 0.0496\nEpoch 77/500\n33/33 [==============================] - 3s 85ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0037 - val_mae: 0.0494\nEpoch 78/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0037 - val_mae: 0.0495\nEpoch 79/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0484 - val_loss: 0.0039 - val_mae: 0.0529\nEpoch 80/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0488 - val_loss: 0.0037 - val_mae: 0.0498\nEpoch 81/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0032 - mae: 0.0484 - val_loss: 0.0041 - val_mae: 0.0555\nEpoch 82/500\n33/33 [==============================] - 3s 75ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0037 - val_mae: 0.0492\nEpoch 83/500\n33/33 [==============================] - 3s 83ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0038 - val_mae: 0.0502\nEpoch 84/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0483 - val_loss: 0.0037 - val_mae: 0.0496\nEpoch 85/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0484 - val_loss: 0.0037 - val_mae: 0.0494\nEpoch 86/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0037 - val_mae: 0.0493\nEpoch 87/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0037 - val_mae: 0.0494\nEpoch 88/500\n33/33 [==============================] - 3s 94ms/step - loss: 0.0032 - mae: 0.0485 - val_loss: 0.0037 - val_mae: 0.0493\nEpoch 89/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0037 - val_mae: 0.0493\nEpoch 90/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0475 - val_loss: 0.0038 - val_mae: 0.0506\nEpoch 91/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0037 - val_mae: 0.0495\nEpoch 92/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0479 - val_loss: 0.0038 - val_mae: 0.0510\nEpoch 93/500\n33/33 [==============================] - 2s 70ms/step - loss: 0.0032 - mae: 0.0475 - val_loss: 0.0038 - val_mae: 0.0503\nEpoch 94/500\n33/33 [==============================] - 3s 82ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0037 - val_mae: 0.0492\nEpoch 95/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0032 - mae: 0.0474 - val_loss: 0.0037 - val_mae: 0.0494\nEpoch 96/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0475 - val_loss: 0.0038 - val_mae: 0.0524\nEpoch 97/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0484 - val_loss: 0.0037 - val_mae: 0.0493\nEpoch 98/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0037 - val_mae: 0.0499\nEpoch 99/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0032 - mae: 0.0480 - val_loss: 0.0037 - val_mae: 0.0495\nEpoch 100/500\n33/33 [==============================] - 4s 103ms/step - loss: 0.0032 - mae: 0.0484 - val_loss: 0.0037 - val_mae: 0.0494\nEpoch 101/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0038 - val_mae: 0.0527\nEpoch 102/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0037 - val_mae: 0.0492\nEpoch 103/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0032 - mae: 0.0477 - val_loss: 0.0037 - val_mae: 0.0496\nEpoch 104/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0037 - val_mae: 0.0492\nEpoch 105/500\n33/33 [==============================] - 3s 94ms/step - loss: 0.0032 - mae: 0.0486 - val_loss: 0.0037 - val_mae: 0.0490\nEpoch 106/500\n33/33 [==============================] - 2s 59ms/step - loss: 0.0032 - mae: 0.0475 - val_loss: 0.0037 - val_mae: 0.0491\nEpoch 107/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0037 - val_mae: 0.0494\nEpoch 108/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0039 - val_mae: 0.0527\nEpoch 109/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0483 - val_loss: 0.0037 - val_mae: 0.0491\nEpoch 110/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0038 - val_mae: 0.0501\nEpoch 111/500\n33/33 [==============================] - 3s 100ms/step - loss: 0.0032 - mae: 0.0485 - val_loss: 0.0039 - val_mae: 0.0526\nEpoch 112/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0032 - mae: 0.0488 - val_loss: 0.0038 - val_mae: 0.0512\nEpoch 113/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 114/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0038 - val_mae: 0.0504\nEpoch 115/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0037 - val_mae: 0.0494\nEpoch 116/500\n33/33 [==============================] - 3s 95ms/step - loss: 0.0031 - mae: 0.0476 - val_loss: 0.0037 - val_mae: 0.0501\nEpoch 117/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0037 - val_mae: 0.0493\nEpoch 118/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0037 - val_mae: 0.0490\nEpoch 119/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0032 - mae: 0.0480 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 120/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0037 - val_mae: 0.0498\nEpoch 121/500\n33/33 [==============================] - 2s 58ms/step - loss: 0.0031 - mae: 0.0474 - val_loss: 0.0037 - val_mae: 0.0496\nEpoch 122/500\n33/33 [==============================] - 3s 81ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0491\nEpoch 123/500\n33/33 [==============================] - 2s 52ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0038 - val_mae: 0.0517\nEpoch 124/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0493\nEpoch 125/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0477 - val_loss: 0.0037 - val_mae: 0.0490\nEpoch 126/500\n33/33 [==============================] - 2s 46ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0504\nEpoch 127/500\n33/33 [==============================] - 2s 67ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0490\nEpoch 128/500\n33/33 [==============================] - 3s 81ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0039 - val_mae: 0.0525\nEpoch 129/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0039 - val_mae: 0.0536\nEpoch 130/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0038 - val_mae: 0.0517\nEpoch 131/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0037 - val_mae: 0.0498\nEpoch 132/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0475 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 133/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 134/500\n33/33 [==============================] - 4s 103ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0492\nEpoch 135/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0493\nEpoch 136/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0490\nEpoch 137/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0494\nEpoch 138/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0038 - val_mae: 0.0518\nEpoch 139/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0037 - val_mae: 0.0491\nEpoch 140/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0490\nEpoch 141/500\n33/33 [==============================] - 3s 85ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0037 - val_mae: 0.0492\nEpoch 142/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0037 - val_mae: 0.0494\nEpoch 143/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0497\nEpoch 144/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0038 - val_mae: 0.0507\nEpoch 145/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0491\nEpoch 146/500\n33/33 [==============================] - 3s 91ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0039 - val_mae: 0.0533\nEpoch 147/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0480 - val_loss: 0.0037 - val_mae: 0.0496\nEpoch 148/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0491\nEpoch 149/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0476 - val_loss: 0.0037 - val_mae: 0.0500\nEpoch 150/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0037 - val_mae: 0.0491\nEpoch 151/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 152/500\n33/33 [==============================] - 3s 97ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0490\nEpoch 153/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0037 - val_mae: 0.0488\nEpoch 154/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0493\nEpoch 155/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0495\nEpoch 156/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0037 - val_mae: 0.0497\nEpoch 157/500\n33/33 [==============================] - 2s 67ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0491\nEpoch 158/500\n33/33 [==============================] - 3s 86ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 159/500\n33/33 [==============================] - 2s 53ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0037 - val_mae: 0.0487\nEpoch 160/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 161/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 162/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0488\nEpoch 163/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0038 - val_mae: 0.0503\nEpoch 164/500\n33/33 [==============================] - 3s 90ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0488\nEpoch 165/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0485\nEpoch 166/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0038 - val_mae: 0.0510\nEpoch 167/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0037 - val_mae: 0.0497\nEpoch 168/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0038 - val_mae: 0.0509\nEpoch 169/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0037 - val_mae: 0.0496\nEpoch 170/500\n33/33 [==============================] - 3s 84ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0492\nEpoch 171/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0037 - val_mae: 0.0496\nEpoch 172/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0496\nEpoch 173/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0036 - val_mae: 0.0486\nEpoch 174/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0491\nEpoch 175/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 176/500\n33/33 [==============================] - 2s 72ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0490\nEpoch 177/500\n33/33 [==============================] - 3s 84ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0488\nEpoch 178/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0037 - val_mae: 0.0493\nEpoch 179/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0486\nEpoch 180/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0037 - val_mae: 0.0495\nEpoch 181/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0037 - val_mae: 0.0487\nEpoch 182/500\n33/33 [==============================] - 3s 88ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0490\nEpoch 183/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0488\nEpoch 184/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0490\nEpoch 185/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0508\nEpoch 186/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 187/500\n33/33 [==============================] - 2s 64ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 188/500\n33/33 [==============================] - 3s 91ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0037 - val_mae: 0.0493\nEpoch 189/500\n33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0037 - val_mae: 0.0488\nEpoch 190/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 191/500\n33/33 [==============================] - 2s 47ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0492\nEpoch 192/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0488\nEpoch 193/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0031 - mae: 0.0476 - val_loss: 0.0036 - val_mae: 0.0486\nEpoch 194/500\n33/33 [==============================] - 3s 97ms/step - loss: 0.0030 - mae: 0.0465 - val_loss: 0.0038 - val_mae: 0.0509\nEpoch 195/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0036 - val_mae: 0.0485\nEpoch 196/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0493\nEpoch 197/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0037 - val_mae: 0.0488\nEpoch 198/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0486\nEpoch 199/500\n33/33 [==============================] - 3s 94ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0037 - val_mae: 0.0485\nEpoch 200/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0501\nEpoch 201/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0488\nEpoch 202/500\n33/33 [==============================] - 2s 55ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0037 - val_mae: 0.0488\nEpoch 203/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 204/500\n33/33 [==============================] - 3s 95ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0490\nEpoch 205/500\n33/33 [==============================] - 2s 62ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0488\nEpoch 206/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0037 - val_mae: 0.0486\nEpoch 207/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0037 - val_mae: 0.0502\nEpoch 208/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0487\nEpoch 209/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0498\nEpoch 210/500\n33/33 [==============================] - 2s 69ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0492\nEpoch 211/500\n33/33 [==============================] - 3s 92ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0487\nEpoch 212/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0038 - val_mae: 0.0513\nEpoch 213/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0030 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0485\nEpoch 214/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0037 - val_mae: 0.0487\nEpoch 215/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0488\nEpoch 216/500\n33/33 [==============================] - 2s 53ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0486\nEpoch 217/500\n33/33 [==============================] - 3s 89ms/step - loss: 0.0030 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0497\nEpoch 218/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0486\nEpoch 219/500\n33/33 [==============================] - 2s 52ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0037 - val_mae: 0.0486\nEpoch 220/500\n33/33 [==============================] - 2s 53ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 221/500\n33/33 [==============================] - 2s 58ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0037 - val_mae: 0.0496\nEpoch 222/500\n33/33 [==============================] - 3s 99ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0036 - val_mae: 0.0485\nEpoch 223/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0037 - val_mae: 0.0485\nEpoch 224/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0037 - val_mae: 0.0504\nEpoch 225/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0497\nEpoch 226/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0037 - val_mae: 0.0496\nEpoch 227/500\n33/33 [==============================] - 4s 109ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0491\nEpoch 228/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0037 - val_mae: 0.0496\nEpoch 229/500\n33/33 [==============================] - 2s 53ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0039 - val_mae: 0.0545\nEpoch 230/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0486\nEpoch 231/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0037 - val_mae: 0.0491\nEpoch 232/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0487\nEpoch 233/500\n33/33 [==============================] - 3s 96ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0488\nEpoch 234/500\n33/33 [==============================] - 2s 66ms/step - loss: 0.0030 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0487\nEpoch 235/500\n33/33 [==============================] - 2s 52ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0036 - val_mae: 0.0485\nEpoch 236/500\n33/33 [==============================] - 2s 52ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0037 - val_mae: 0.0487\nEpoch 237/500\n33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0037 - val_mae: 0.0490\nEpoch 238/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0037 - val_mae: 0.0489\nEpoch 239/500\n33/33 [==============================] - 3s 101ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0500\nEpoch 240/500\n33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0498\nEpoch 241/500\n33/33 [==============================] - 2s 52ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0037 - val_mae: 0.0491\nEpoch 242/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0038 - val_mae: 0.0516\nEpoch 243/500\n33/33 [==============================] - 2s 51ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0490\nEpoch 244/500\n33/33 [==============================] - 2s 53ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0037 - val_mae: 0.0497\nEpoch 245/500\n33/33 [==============================] - 4s 104ms/step - loss: 0.0030 - mae: 0.0467 - val_loss: 0.0037 - val_mae: 0.0497\n3/3 [==============================] - 0s 52ms/step - loss: 0.0036 - mae: 0.0485\n\n\n48454.705625772476\n\n\nThe training code is the same as usual. During training, all the model’s outputs are used, but after training only the output of the very last time step matters, and the rest can be ignored. For example, we can forecast the rail ridership for the next 14 days like this:\n\nX = mulvar_valid.to_numpy()[np.newaxis, :seq_length]\ny_pred_14 = seq2seq_model.predict(X)[0, -1]  # only the last time step's output\n\n1/1 [==============================] - 0s 146ms/step\n\n\nIf you evaluate this model’s forecasts for \\(t+1\\), you will find a validation MAE of 24,655. For \\(t+2\\) it’s 29,310, and the performance continues to drop gradually as the model tries to forecast further into the future. At \\(t+14\\), the MAE is 34,311.\n\nY_pred_valid = seq2seq_model.predict(seq2seq_valid)\nfor ahead in range(14):\n    preds = pd.Series(Y_pred_valid[:-1, -1, ahead],\n                      index=mulvar_valid.index[56 + ahead : -14 + ahead])\n    mae = (preds - mulvar_valid[\"rail\"]).abs().mean() * 1e6\n    print(f\"MAE for +{ahead + 1}: {mae:,.0f}\")\n\n3/3 [==============================] - 0s 23ms/step\nMAE for +1: 24,655\nMAE for +2: 29,310\nMAE for +3: 32,148\nMAE for +4: 34,271\nMAE for +5: 34,646\nMAE for +6: 34,537\nMAE for +7: 36,120\nMAE for +8: 38,538\nMAE for +9: 34,308\nMAE for +10: 31,896\nMAE for +11: 37,567\nMAE for +12: 36,741\nMAE for +13: 36,003\nMAE for +14: 34,311\n\n\nSimple RNNs can be quite good at forecasting time series or handling other kinds of sequences, but they do not perform as well on long time series or sequences.\n\n\n10.2.11 Deep RNNs with Layer Norm\nLet’s use tf.keras to implement Layer Normalization within a simple memory cell. We need to define a custom memory cell. It is just like a regular layer, except its call() method takes two arguments: the inputs at the current time step and the hidden states from the previous time step. Note that the states argument is a list containing one or more tensors. In the case of a simple RNN cell it contains a single tensor equal to the outputs of the previous time step, but other cells may have multiple state tensors (e.g., an LSTMCell has a long-term state and a short-term state). A cell must also have a state_size attribute and an output_size attribute. In a simple RNN, both are simply equal to the number of units. The following code implements a custom memory cell which will behave like a SimpleRNNCell, except it will also apply Layer Normalization at each time step:\n\nclass LNSimpleRNNCell(tf.keras.layers.Layer):\n    def __init__(self, units, activation=\"tanh\", **kwargs):\n        super().__init__(**kwargs)\n        self.state_size = units\n        self.output_size = units\n        self.simple_rnn_cell = tf.keras.layers.SimpleRNNCell(units,\n                                                             activation=None)\n        self.layer_norm = tf.keras.layers.LayerNormalization()\n        self.activation = tf.keras.activations.get(activation)\n\n    def call(self, inputs, states):\n        outputs, new_states = self.simple_rnn_cell(inputs, states)\n        norm_outputs = self.activation(self.layer_norm(outputs))\n        return norm_outputs, [norm_outputs]\n\n\ntf.random.set_seed(42)  # extra code – ensures reproducibility\ncustom_ln_model = tf.keras.Sequential([\n    tf.keras.layers.RNN(LNSimpleRNNCell(32), return_sequences=True,\n                        input_shape=[None, 5]),\n    tf.keras.layers.Dense(14)\n])\n\n\nfit_and_evaluate(custom_ln_model, seq2seq_train, seq2seq_valid, learning_rate=0.1, epochs=5)\n\nEpoch 1/5\n33/33 [==============================] - 6s 108ms/step - loss: 0.0660 - mae: 0.2576 - val_loss: 0.0178 - val_mae: 0.1455\nEpoch 2/5\n33/33 [==============================] - 5s 165ms/step - loss: 0.0150 - mae: 0.1458 - val_loss: 0.0169 - val_mae: 0.1272\nEpoch 3/5\n33/33 [==============================] - 4s 119ms/step - loss: 0.0130 - mae: 0.1351 - val_loss: 0.0147 - val_mae: 0.1236\nEpoch 4/5\n33/33 [==============================] - 9s 265ms/step - loss: 0.0121 - mae: 0.1291 - val_loss: 0.0137 - val_mae: 0.1190\nEpoch 5/5\n33/33 [==============================] - 9s 259ms/step - loss: 0.0115 - mae: 0.1232 - val_loss: 0.0134 - val_mae: 0.1156\n3/3 [==============================] - 0s 33ms/step - loss: 0.0134 - mae: 0.1156\n\n\n115600.42947530746\n\n\nSimilarly, you could create a custom cell to apply dropout between each time step. But there’s a simpler way: all recurrent layers and all cells provided by tf.Keras have a dropout hyperparameter and a recurrent_dropout hyperparameter: the former defines the dropout rate to apply to the inputs (at each time step), and the latter defines the dropout rate for the hidden states (also at each time step).\nWith these techniques, you can alleviate the unstable gradients problem and train an RNN much more efficiently. Now let’s look at how to deal with the short-term memory problem.\n\n\n10.2.12 LSTMs\nIn tf.Keras, you can simply use the LSTM layer instead of the SimpleRNN layer:\n\ntf.random.set_seed(42)  # extra code – ensures reproducibility\nlstm_model = tf.keras.models.Sequential([\n    tf.keras.layers.LSTM(32, return_sequences=True, input_shape=[None, 5]),\n    tf.keras.layers.Dense(14)\n])\n\nlstm_model.summary()\n\nModel: \"sequential_9\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, None, 32)          4864      \n                                                                 \n dense_8 (Dense)             (None, None, 14)          462       \n                                                                 \n=================================================================\nTotal params: 5,326\nTrainable params: 5,326\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nAlternatively, you could use the general-purpose tf.keras.layers.RNN layer, giving it an LSTMCell as an argument. However, the LSTM layer uses an optimized implementation when running on a GPU.\n\n\n# keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True, input_shape=[None, 1]) also works\n# However, the LSTM layer uses an optimized implementation when running on a GPU\n# RNN layer is mostly useful when you define custom cells, as we did earl\n\n\nfit_and_evaluate(lstm_model, seq2seq_train, seq2seq_valid,\n                 learning_rate=0.1, epochs=5)\n\nEpoch 1/5\n33/33 [==============================] - 5s 47ms/step - loss: 0.0665 - mae: 0.2765 - val_loss: 0.0186 - val_mae: 0.1574\nEpoch 2/5\n33/33 [==============================] - 0s 13ms/step - loss: 0.0166 - mae: 0.1573 - val_loss: 0.0175 - val_mae: 0.1448\nEpoch 3/5\n33/33 [==============================] - 1s 14ms/step - loss: 0.0155 - mae: 0.1510 - val_loss: 0.0166 - val_mae: 0.1425\nEpoch 4/5\n33/33 [==============================] - 0s 13ms/step - loss: 0.0149 - mae: 0.1473 - val_loss: 0.0160 - val_mae: 0.1396\nEpoch 5/5\n33/33 [==============================] - 1s 13ms/step - loss: 0.0143 - mae: 0.1444 - val_loss: 0.0155 - val_mae: 0.1362\n3/3 [==============================] - 0s 27ms/step - loss: 0.0155 - mae: 0.1362\n\n\n136158.73456001282\n\n\n\n\n10.2.13 GRUs\ntf.Keras provides a tf.keras.layers.GRU layer: using it is just a matter of replacing SimpleRNN or LSTM with GRU. It also provides a tf.keras.layers.GRUCell, in case you want to create a custom cell based on a GRU cell.\n\ntf.random.set_seed(42)  # extra code – ensures reproducibility\ngru_model = tf.keras.Sequential([\n    tf.keras.layers.GRU(32, return_sequences=True, input_shape=[None, 5]),\n    tf.keras.layers.Dense(14)\n])\n\n\nfit_and_evaluate(gru_model, seq2seq_train, seq2seq_valid, learning_rate=0.1, epochs=5)\n\nEpoch 1/5\n33/33 [==============================] - 4s 28ms/step - loss: 0.0568 - mae: 0.2545 - val_loss: 0.0187 - val_mae: 0.1694\nEpoch 2/5\n33/33 [==============================] - 0s 13ms/step - loss: 0.0162 - mae: 0.1500 - val_loss: 0.0156 - val_mae: 0.1373\nEpoch 3/5\n33/33 [==============================] - 0s 12ms/step - loss: 0.0134 - mae: 0.1369 - val_loss: 0.0140 - val_mae: 0.1283\nEpoch 4/5\n33/33 [==============================] - 0s 12ms/step - loss: 0.0121 - mae: 0.1286 - val_loss: 0.0128 - val_mae: 0.1229\nEpoch 5/5\n33/33 [==============================] - 1s 13ms/step - loss: 0.0112 - mae: 0.1225 - val_loss: 0.0121 - val_mae: 0.1179\n3/3 [==============================] - 0s 27ms/step - loss: 0.0121 - mae: 0.1179\n\n\n117899.73080158234\n\n\n\n\n10.2.14 Using One-Dimensional Convolutional Layers to Process Sequences\nThe following model is the same as earlier, except it starts with a 1D convolutional layer that downsamples the input sequence by a factor of 2, using a stride of 2. The kernel size is larger than the stride, so all inputs will be used to compute the layer’s output, and therefore the model can learn to preserve the useful information, dropping only the unimportant details. By shortening the sequences the convolutional layer may help the GRU layers detect longer patterns, so we can afford to double the input sequence length to 112 days. Note that we must also crop off the first three time steps in the targets: indeed, the kernel’s size is 4, so the first output of the convolutional layer will be based on the input time steps 0 to 3, and the first forecasts will be for time steps 4 to 17 (instead of time steps 1 to 14). Moreover, we must downsample the targets by a factor of 2, because of the stride:\n\ntf.random.set_seed(42)  # extra code – ensures reproducibility\nconv_rnn_model = tf.keras.Sequential([\n    tf.keras.layers.Conv1D(filters=32, kernel_size=4, strides=2,\n                           activation=\"relu\", input_shape=[None, 5]),\n    tf.keras.layers.GRU(32, return_sequences=True),\n    tf.keras.layers.Dense(14)\n])\n\nconv_rnn_model.summary()\n\nModel: \"sequential_11\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv1d (Conv1D)             (None, None, 32)          672       \n                                                                 \n gru_1 (GRU)                 (None, None, 32)          6336      \n                                                                 \n dense_10 (Dense)            (None, None, 14)          462       \n                                                                 \n=================================================================\nTotal params: 7,470\nTrainable params: 7,470\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nlonger_train = to_seq2seq_dataset(mulvar_train, seq_length=112, shuffle=True, seed=42)\nlonger_valid = to_seq2seq_dataset(mulvar_valid, seq_length=112)\ndownsampled_train = longer_train.map(lambda X, Y: (X, Y[:, 3::2]))\ndownsampled_valid = longer_valid.map(lambda X, Y: (X, Y[:, 3::2]))\n\n\nfit_and_evaluate(conv_rnn_model, downsampled_train, downsampled_valid, learning_rate=0.1, epochs=5)\n\nEpoch 1/5\n31/31 [==============================] - 7s 33ms/step - loss: 0.0555 - mae: 0.2556 - val_loss: 0.0209 - val_mae: 0.1595\nEpoch 2/5\n31/31 [==============================] - 1s 21ms/step - loss: 0.0160 - mae: 0.1503 - val_loss: 0.0166 - val_mae: 0.1425\nEpoch 3/5\n31/31 [==============================] - 1s 36ms/step - loss: 0.0140 - mae: 0.1417 - val_loss: 0.0155 - val_mae: 0.1335\nEpoch 4/5\n31/31 [==============================] - 1s 28ms/step - loss: 0.0128 - mae: 0.1338 - val_loss: 0.0143 - val_mae: 0.1274\nEpoch 5/5\n31/31 [==============================] - 1s 21ms/step - loss: 0.0117 - mae: 0.1260 - val_loss: 0.0130 - val_mae: 0.1225\n1/1 [==============================] - 0s 154ms/step - loss: 0.0130 - mae: 0.1225\n\n\n122495.1446056366"
  },
  {
    "objectID": "10_Recurrent_Neural_Networks.html#natural-language-processing",
    "href": "10_Recurrent_Neural_Networks.html#natural-language-processing",
    "title": "10  Sequence Processing with RNNs and Attention - Tensforflow",
    "section": "10.3 Natural-language processing",
    "text": "10.3 Natural-language processing\n\n10.3.1 Preparing text data\nVectorizing process using Python may be done as follows:\n\nclass Vectorizer:\n    def standardize(self, text):\n        text = text.lower()\n        return \"\".join(char for char in text if char not in string.punctuation)\n\n    def tokenize(self, text):\n        text = self.standardize(text)\n        return text.split()\n\n    def make_vocabulary(self, dataset):\n        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n        for text in dataset:\n            text = self.standardize(text)\n            tokens = self.tokenize(text)\n            for token in tokens:\n                if token not in self.vocabulary:\n                    self.vocabulary[token] = len(self.vocabulary)\n        self.inverse_vocabulary = dict(\n            (v, k) for k, v in self.vocabulary.items())\n\n    def encode(self, text):\n        text = self.standardize(text)\n        tokens = self.tokenize(text)\n        return [self.vocabulary.get(token, 1) for token in tokens]\n\n    def decode(self, int_sequence):\n        return \" \".join(\n            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n\nvectorizer = Vectorizer()\ndataset = [\n    \"I write, erase, rewrite\",\n    \"Erase again, and then\",\n    \"A poppy blooms.\",\n]\nvectorizer.make_vocabulary(dataset)\n\n\ntest_sentence = \"I write, rewrite, and still rewrite again\"\nencoded_sentence = vectorizer.encode(test_sentence)\nprint(encoded_sentence)\n\n[2, 3, 5, 7, 1, 5, 6]\n\n\n\ndecoded_sentence = vectorizer.decode(encoded_sentence)\nprint(decoded_sentence)\n\ni write rewrite and [UNK] rewrite again\n\n\nHowever, using something like this wouldn’t be very performant. In practice, you’ll work with the tf.Keras TextVectorization layer, which is fast and efficient and can be dropped directly into a tf.data pipeline or a tf.Keras model.\n\n# Configures the layer to return sequences of words encoded\n# as integer indices.\ntext_vectorization = tf.keras.layers.TextVectorization(\n    output_mode=\"int\",\n)\n\nBy default, the TextVectorization layer will use the setting “convert to lowercase and remove punctuation” for text standardization, and “split on whitespace” for tokenization.\nBut importantly, you can provide custom functions for standardization and tokenization, which means the layer is flexible enough to handle any use case. To index the vocabulary of a text corpus, just call the adapt() method of the layer with a Dataset object that yields strings, or just with a list of Python strings:\n\ndataset = [\n    \"I write, erase, rewrite\",\n    \"Erase again, and then\",\n    \"A poppy blooms.\",\n]\ntext_vectorization.adapt(dataset)\n\nNote that you can retrieve the computed vocabulary via get_vocabulary()—this can be useful if you need to convert text encoded as integer sequences back into words. The first two entries in the vocabulary are the mask token (index 0) and the OOV token (index 1). Entries in the vocabulary list are sorted by frequency, so with a realworld dataset, very common words like “the” or “a” would come first.\n\ntext_vectorization.get_vocabulary()\n\n['',\n '[UNK]',\n 'erase',\n 'write',\n 'then',\n 'rewrite',\n 'poppy',\n 'i',\n 'blooms',\n 'and',\n 'again',\n 'a']\n\n\nFor a demonstration, let’s try to encode and then decode an example sentence:\n\nvocabulary = text_vectorization.get_vocabulary()\ntest_sentence = \"I write, rewrite, and still rewrite again\"\nencoded_sentence = text_vectorization(test_sentence)\nprint(encoded_sentence)\n\ntf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n\n\n\ninverse_vocab = dict(enumerate(vocabulary))\ndecoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\nprint(decoded_sentence)\n\ni write rewrite and [UNK] rewrite again\n\n\n\n\n10.3.2 Two approaches for representing groups of words: Sets and sequences\nWe’ll demonstrate bag-of-words and sequence approach on a well-known text classification benchmark: the IMDB movie review sentiment-classification dataset. Let’s process the raw IMDB text data, just like you would do when approaching a new text-classification problem in the real world. You can check out the data here.\nLet’s start by downloading the dataset from the Stanford page of Andrew Maas and uncompressing it\n\n!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n!tar -xf aclImdb_v1.tar.gz\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  8848k      0  0:00:09  0:00:09 --:--:-- 16.4M\n\n\nThe train/pos/ directory contains a set of 12,500 text files, each of which contains the text body of a positive-sentiment movie review to be used as training data. The negative-sentiment reviews live in the train/neg directories. In total, there are 25,000 text files for training and another 25,000 for testing. There’s also a train/unsup subdirectory in there, which we don’t need. Let’s delete it:\n\n!rm -r aclImdb/train/unsup\n\nTake a look at the content of a few of these text files. Whether you’re working with text data or image data, remember to always inspect what your data looks like before you dive into modeling it:\n\n!cat aclImdb/train/pos/4077_10.txt\n\nI first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.&lt;br /&gt;&lt;br /&gt;Enjoy\n\n\nNext, let’s prepare a validation set by setting apart 20% of the training text files in a new directory, aclImdb/val:\n\nbase_dir = Path(\"aclImdb\")\nval_dir = base_dir / \"val\"\ntrain_dir = base_dir / \"train\"\n\nfor category in (\"neg\", \"pos\"):\n    os.makedirs(val_dir / category)\n    files = os.listdir(train_dir / category)\n    random.Random(1337).shuffle(files)\n    num_val_samples = int(0.2 * len(files))\n    val_files = files[-num_val_samples:]\n    for fname in val_files:\n        shutil.move(train_dir / category / fname,\n                    val_dir / category / fname)\n\nLet’s create three Dataset objects for training, validation, and testing:\n\nbatch_size = 32\n\ntrain_ds = tf.keras.utils.text_dataset_from_directory(\n    \"aclImdb/train\", batch_size=batch_size\n)\nval_ds = tf.keras.utils.text_dataset_from_directory(\n    \"aclImdb/val\", batch_size=batch_size\n)\ntest_ds = tf.keras.utils.text_dataset_from_directory(\n    \"aclImdb/test\", batch_size=batch_size\n)\n\nFound 20000 files belonging to 2 classes.\nFound 5000 files belonging to 2 classes.\nFound 25000 files belonging to 2 classes.\n\n\nThese datasets yield inputs that are TensorFlow tf.string tensors and targets that are int32 tensors encoding the value “0” or “1.”\n\nfor inputs, targets in train_ds:\n    print(\"inputs.shape:\", inputs.shape)\n    print(\"inputs.dtype:\", inputs.dtype)\n    print(\"targets.shape:\", targets.shape)\n    print(\"targets.dtype:\", targets.dtype)\n    print(\"inputs[0]:\", inputs[0])\n    print(\"targets[0]:\", targets[0])\n    break\n\ninputs.shape: (32,)\ninputs.dtype: &lt;dtype: 'string'&gt;\ntargets.shape: (32,)\ntargets.dtype: &lt;dtype: 'int32'&gt;\ninputs[0]: tf.Tensor(b'I think that this is possibly the funniest movie I have ever seen. Robert Harling\\'s script is near perfect, just check out the \"quotes\" section; on second thought, just rent the DVD, since it\\'s the delivery that really makes the lines sing.&lt;br /&gt;&lt;br /&gt;Sally Field gives a comic, over-the-top performance like you\\'ve never seen from her anywhere else, and Kevin Kline is effortlessly hilarious. Robert Downey, Jr. is typically brilliant, and in a very small role, Kathy Najimy is a riot as the beleaguered costumer. I was never much of a fan of Elisabeth Shue, but she\\'s great here as the one *real* person surrounded by a bevy of cartoon characters on the set of \"The Sun Also Sets\" -- that rumbling you feel beneath you is Hemingway rolling over in his grave. Either that, or he\\'s laughing really hard.&lt;br /&gt;&lt;br /&gt;Five stars. Funny, funny, funny.', shape=(), dtype=string)\ntargets[0]: tf.Tensor(1, shape=(), dtype=int32)\n\n\n\n10.3.2.1 Single words (unigrams) with binary encoding\nFirst, let’s process our raw text datasets with a TextVectorization layer so that they yield multi-hot encoded binary word vectors. Our layer will only look at single words (that is to say, unigrams). We will limit the vocabulary to the 20,000 most frequent words. Otherwise we’d be indexing every word in the training data— potentially tens of thousands of terms that only occur once or twice and thus aren’t informative. In general, 20,000 is the right vocabulary size for text classification.\n\n# Encode the output tokens as multi-hot binary vectors.\ntext_vectorization = tf.keras.layers.TextVectorization(\n    max_tokens=20000,\n    output_mode=\"multi_hot\",\n)\n# Prepare a dataset that only yields raw text inputs (no labels).\ntext_only_train_ds = train_ds.map(lambda x, y: x)\ntext_vectorization.adapt(text_only_train_ds)\n\nbinary_1gram_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nbinary_1gram_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nbinary_1gram_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\n\nYou can try to inspect the output of one of these datasets:\n\nfor inputs, targets in binary_1gram_train_ds:\n    print(\"inputs.shape:\", inputs.shape)\n    print(\"inputs.dtype:\", inputs.dtype)\n    print(\"targets.shape:\", targets.shape)\n    print(\"targets.dtype:\", targets.dtype)\n    print(\"inputs[0]:\", inputs[0])\n    print(\"targets[0]:\", targets[0])\n    break\n\ninputs.shape: (32, 20000)\ninputs.dtype: &lt;dtype: 'float32'&gt;\ntargets.shape: (32,)\ntargets.dtype: &lt;dtype: 'int32'&gt;\ninputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\ntargets[0]: tf.Tensor(0, shape=(), dtype=int32)\n\n\nNext, let’s write a reusable model-building function that we’ll use in all of our experiments in this section.\n\n# A densely connected NN\ndef get_model(max_tokens=20000, hidden_dim=16):\n    inputs = tf.keras.Input(shape=(max_tokens,))\n    x = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n    model = tf.keras.Model(inputs, outputs)\n    \n    model.compile(optimizer=\"nadam\",\n                  loss=\"binary_crossentropy\",\n                  metrics=[\"accuracy\"])\n    return model\n\nFinally, let’s train and test our model.\n\nmodel = get_model()\nmodel.summary()\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", save_best_only=True)\n]\nmodel.fit(binary_1gram_train_ds.cache(),\n          validation_data=binary_1gram_val_ds.cache(),\n          epochs=10,\n          callbacks=callbacks)\n\nmodel = tf.keras.models.load_model(\"binary_1gram.keras\")\nprint(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 20000)]           0         \n                                                                 \n dense_11 (Dense)            (None, 16)                320016    \n                                                                 \n dropout (Dropout)           (None, 16)                0         \n                                                                 \n dense_12 (Dense)            (None, 1)                 17        \n                                                                 \n=================================================================\nTotal params: 320,033\nTrainable params: 320,033\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 9s 12ms/step - loss: 0.3861 - accuracy: 0.8356 - val_loss: 0.2701 - val_accuracy: 0.8942\nEpoch 2/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2161 - accuracy: 0.9189 - val_loss: 0.2624 - val_accuracy: 0.8912\nEpoch 3/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.1517 - accuracy: 0.9446 - val_loss: 0.2799 - val_accuracy: 0.8886\nEpoch 4/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.1134 - accuracy: 0.9606 - val_loss: 0.3063 - val_accuracy: 0.8870\nEpoch 5/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.0913 - accuracy: 0.9689 - val_loss: 0.3290 - val_accuracy: 0.8878\nEpoch 6/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.0752 - accuracy: 0.9729 - val_loss: 0.3540 - val_accuracy: 0.8848\nEpoch 7/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.0626 - accuracy: 0.9774 - val_loss: 0.4038 - val_accuracy: 0.8842\nEpoch 8/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.0517 - accuracy: 0.9818 - val_loss: 0.4530 - val_accuracy: 0.8866\nEpoch 9/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.0478 - accuracy: 0.9819 - val_loss: 0.4665 - val_accuracy: 0.8848\nEpoch 10/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.0420 - accuracy: 0.9842 - val_loss: 0.5062 - val_accuracy: 0.8844\n782/782 [==============================] - 5s 6ms/step - loss: 0.2860 - accuracy: 0.8842\nTest acc: 0.884\n\n\nThis gets us to a test accuracy of 88.4%: not bad!\n\n\n10.3.2.2 Bigrams with binary encoding\nOf course, discarding word order is very reductive, because even atomic concepts can be expressed via multiple words. The TextVectorization layer can be configured to return arbitrary N-grams: bigrams, trigrams, etc. Just pass an ngrams=N argument as in the following listing.\n\ntext_vectorization = tf.keras.layers.TextVectorization(\n    ngrams=2,\n    max_tokens=20000,\n    output_mode=\"multi_hot\",\n)\n\nLet’s test how our model performs when trained on such binary-encoded bags of bigrams.\n\ntext_vectorization.adapt(text_only_train_ds)\nbinary_2gram_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nbinary_2gram_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nbinary_2gram_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\n\nmodel = get_model()\nmodel.summary()\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n                                    save_best_only=True)\n]\nmodel.fit(binary_2gram_train_ds.cache(),\n          validation_data=binary_2gram_val_ds.cache(),\n          epochs=10,\n          callbacks=callbacks)\n\nmodel = tf.keras.models.load_model(\"binary_2gram.keras\")\nprint(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 20000)]           0         \n                                                                 \n dense_13 (Dense)            (None, 16)                320016    \n                                                                 \n dropout_1 (Dropout)         (None, 16)                0         \n                                                                 \n dense_14 (Dense)            (None, 1)                 17        \n                                                                 \n=================================================================\nTotal params: 320,033\nTrainable params: 320,033\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 8s 9ms/step - loss: 0.3697 - accuracy: 0.8462 - val_loss: 0.2565 - val_accuracy: 0.8986\nEpoch 2/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.1926 - accuracy: 0.9287 - val_loss: 0.2587 - val_accuracy: 0.8912\nEpoch 3/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.1319 - accuracy: 0.9527 - val_loss: 0.2706 - val_accuracy: 0.8930\nEpoch 4/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.0986 - accuracy: 0.9661 - val_loss: 0.3235 - val_accuracy: 0.8914\nEpoch 5/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.0806 - accuracy: 0.9699 - val_loss: 0.3211 - val_accuracy: 0.8926\nEpoch 6/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.0633 - accuracy: 0.9775 - val_loss: 0.3796 - val_accuracy: 0.8886\nEpoch 7/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.0573 - accuracy: 0.9780 - val_loss: 0.4114 - val_accuracy: 0.8874\nEpoch 8/10\n625/625 [==============================] - 5s 8ms/step - loss: 0.0490 - accuracy: 0.9809 - val_loss: 0.4756 - val_accuracy: 0.8894\nEpoch 9/10\n625/625 [==============================] - 5s 8ms/step - loss: 0.0473 - accuracy: 0.9814 - val_loss: 0.4639 - val_accuracy: 0.8888\nEpoch 10/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.0439 - accuracy: 0.9826 - val_loss: 0.4937 - val_accuracy: 0.8876\n782/782 [==============================] - 8s 10ms/step - loss: 0.2622 - accuracy: 0.8970\nTest acc: 0.897\n\n\nWe’re now getting 89.7% test accuracy, a marked improvement! Turns out local order is pretty important.\n\n\n10.3.2.3 Bigrams with TF-IDF encoding\nYou can also add a bit more information to this representation by counting how many times each word or N-gram occurs. TF-IDF is so common that it’s built into the TextVectorization layer. All you need to do to start using it is to switch the output_mode argument to tf_idf.\n\ntext_vectorization = tf.keras.layers.TextVectorization(\n    ngrams=2,\n    max_tokens=20000,\n    output_mode=\"tf_idf\",\n)\n\ntext_vectorization.adapt(text_only_train_ds)\n\nLet’s train a new model with this scheme.\n\ntfidf_2gram_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\ntfidf_2gram_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\ntfidf_2gram_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\n\nmodel = get_model()\nmodel.summary()\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\", save_best_only=True)\n]\nmodel.fit(tfidf_2gram_train_ds.cache(),\n          validation_data=tfidf_2gram_val_ds.cache(),\n          epochs=10,\n          callbacks=callbacks)\n\nmodel = tf.keras.models.load_model(\"tfidf_2gram.keras\")\nprint(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 20000)]           0         \n                                                                 \n dense_15 (Dense)            (None, 16)                320016    \n                                                                 \n dropout_2 (Dropout)         (None, 16)                0         \n                                                                 \n dense_16 (Dense)            (None, 1)                 17        \n                                                                 \n=================================================================\nTotal params: 320,033\nTrainable params: 320,033\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 8s 10ms/step - loss: 0.5177 - accuracy: 0.7586 - val_loss: 0.3129 - val_accuracy: 0.8848\nEpoch 2/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.3008 - accuracy: 0.8597 - val_loss: 0.2661 - val_accuracy: 0.8934\nEpoch 3/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2498 - accuracy: 0.8859 - val_loss: 0.2716 - val_accuracy: 0.8946\nEpoch 4/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.2016 - accuracy: 0.9042 - val_loss: 0.2982 - val_accuracy: 0.8912\nEpoch 5/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.1795 - accuracy: 0.9121 - val_loss: 0.3066 - val_accuracy: 0.8898\nEpoch 6/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.1584 - accuracy: 0.9194 - val_loss: 0.3194 - val_accuracy: 0.8934\nEpoch 7/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.1473 - accuracy: 0.9231 - val_loss: 0.3821 - val_accuracy: 0.8812\nEpoch 8/10\n625/625 [==============================] - 3s 6ms/step - loss: 0.1372 - accuracy: 0.9280 - val_loss: 0.4076 - val_accuracy: 0.8850\nEpoch 9/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.1239 - accuracy: 0.9352 - val_loss: 0.3869 - val_accuracy: 0.8926\nEpoch 10/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.1174 - accuracy: 0.9379 - val_loss: 0.4329 - val_accuracy: 0.8912\n782/782 [==============================] - 6s 8ms/step - loss: 0.2749 - accuracy: 0.8910\nTest acc: 0.891\n\n\nThis gets us an 89.1% test accuracy on the IMDB classification task: it doesn’t seem to be particularly helpful in this case. However, for many text-classification datasets, it would be typical to see a one-percentage-point increase when using TF-IDF compared to plain binary encoding.\n\n\n\n10.3.3 Processing words as a sequence: The sequence model approach\nWhat if, instead of manually crafting order-based features, we exposed the model to raw word sequences and let it figure out such features on its own? This is what sequence models are about. Let’s try out a sequence model in practice. First, let’s prepare datasets that return integer sequences. In order to keep a manageable input size, we’ll truncate the inputs after the first 600 words.\n\nThis is a reasonable choice, since the average review length is 233 words, and only 5% of reviews are longer than 600 words.\n\n\nmax_length = 600\nmax_tokens = 20000\ntext_vectorization = tf.keras.layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=max_length,\n)\ntext_vectorization.adapt(text_only_train_ds)\n\nint_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\n\nNext, let’s make a model. The simplest way to convert our integer sequences to vector sequences is to one-hot encode the integers (each dimension would represent one possible term in the vocabulary). On top of these one-hot vectors, we’ll add a simple bidirectional LSTM.\n\ninputs = tf.keras.Input(shape=(None,), dtype=\"int64\") # One input is a sequence of integers\nembedded = tf.one_hot(inputs, depth=max_tokens) # A 3D tensor of shape [batch size, time steps, embedding size]\nx = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(embedded)\nx = tf.keras.layers.Dropout(0.5)(x)\noutputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x) # Classification layer\nmodel = tf.keras.Model(inputs, outputs)\n\nmodel.compile(optimizer=\"nadam\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\nModel: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, None)]            0         \n                                                                 \n tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n                                                                 \n bidirectional (Bidirectiona  (None, 64)               5128448   \n l)                                                              \n                                                                 \n dropout_3 (Dropout)         (None, 64)                0         \n                                                                 \n dense_17 (Dense)            (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 5,128,513\nTrainable params: 5,128,513\nNon-trainable params: 0\n_________________________________________________________________\n\n\nNow, let’s train our model:\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\", save_best_only=True)\n]\nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\nmodel = tf.keras.models.load_model(\"one_hot_bidir_lstm.keras\")\nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n\nEpoch 1/10\n625/625 [==============================] - 169s 259ms/step - loss: 0.6148 - accuracy: 0.6773 - val_loss: 0.6323 - val_accuracy: 0.6720\nEpoch 2/10\n625/625 [==============================] - 163s 261ms/step - loss: 0.3993 - accuracy: 0.8426 - val_loss: 0.3126 - val_accuracy: 0.8762\nEpoch 3/10\n625/625 [==============================] - 162s 260ms/step - loss: 0.2752 - accuracy: 0.8935 - val_loss: 0.6013 - val_accuracy: 0.7044\nEpoch 4/10\n625/625 [==============================] - 163s 261ms/step - loss: 0.7385 - accuracy: 0.6064 - val_loss: 0.8323 - val_accuracy: 0.5076\nEpoch 5/10\n625/625 [==============================] - 164s 262ms/step - loss: 0.3485 - accuracy: 0.8608 - val_loss: 0.3655 - val_accuracy: 0.8506\nEpoch 6/10\n625/625 [==============================] - 163s 261ms/step - loss: 0.4231 - accuracy: 0.8137 - val_loss: 0.3651 - val_accuracy: 0.8582\nEpoch 7/10\n625/625 [==============================] - 163s 261ms/step - loss: 0.2117 - accuracy: 0.9301 - val_loss: 0.3304 - val_accuracy: 0.8662\nEpoch 8/10\n625/625 [==============================] - 163s 261ms/step - loss: 0.1301 - accuracy: 0.9624 - val_loss: 0.3815 - val_accuracy: 0.8686\nEpoch 9/10\n625/625 [==============================] - 162s 258ms/step - loss: 0.1255 - accuracy: 0.9597 - val_loss: 0.3904 - val_accuracy: 0.8774\nEpoch 10/10\n625/625 [==============================] - 163s 261ms/step - loss: 0.3040 - accuracy: 0.8884 - val_loss: 0.3462 - val_accuracy: 0.8748\n782/782 [==============================] - 97s 123ms/step - loss: 0.3429 - accuracy: 0.8618\nTest acc: 0.862\n\n\nA first observation: this model will train very slowly, especially compared to the lightweight model of the previous section. This is because our inputs are quite large: each input sample is encoded as a matrix of size (600, 20000) (600 words per sample, 20,000 possible words). That’s 12,000,000 floats for a single movie review. Our bidirectional LSTM has a lot of work to do.\n\n10.3.3.1 Understanding word embeddings\nLet’s try word embedding. What makes a good word-embedding space depends heavily on your task: the perfect word-embedding space for an English-language movie-review sentiment-analysis model may look different from the perfect embedding space for an English-language legal-document classification model, because the importance of certain semantic relationships varies from task to task. It’s thus reasonable to learn a new embedding space with every new task. Fortunately, backpropagation makes this easy, and tf.Keras makes it even easier. It’s about learning the weights of a layer: the Embedding layer.\n\n# The Embedding layer takes at least two arguments: the number of\n# possible tokens and the dimensionality of the embeddings (here, 256).\nembedding_layer = tf.keras.layers.Embedding(input_dim=max_tokens, output_dim=256)\n\nThe Embedding layer is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. The Embedding layer takes as input a rank-2 tensor of integers, of shape (batch_size, sequence_length), where each entry is a sequence of integers. The layer then returns a 3D floating-point tensor of shape(batch_size, sequence_length, embedding_dimensionality).\nWhen you instantiate an Embedding layer, its weights (its internal dictionary of token vectors) are initially random, just as with any other layer. During training, these word vectors are gradually adjusted via backpropagation, structuring the space into something the downstream model can exploit. Once fully trained, the embedding space will show a lot of structure—a kind of structure specialized for the specific problem for which you’re training your model.\nLet’s build a model that includes an Embedding layer and benchmark it on our task:\n\ninputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\nembedded = tf.keras.layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\nx = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(embedded)\nx = tf.keras.layers.Dropout(0.5)(x)\noutputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = tf.keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n                                    save_best_only=True)\n]\nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\nmodel = tf.keras.models.load_model(\"embeddings_bidir_gru.keras\")\nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n\nModel: \"model_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_6 (InputLayer)        [(None, None)]            0         \n                                                                 \n embedding_1 (Embedding)     (None, None, 256)         5120000   \n                                                                 \n bidirectional_1 (Bidirectio  (None, 64)               73984     \n nal)                                                            \n                                                                 \n dropout_5 (Dropout)         (None, 64)                0         \n                                                                 \n dense_18 (Dense)            (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 5,194,049\nTrainable params: 5,194,049\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 106s 163ms/step - loss: 0.5462 - accuracy: 0.7190 - val_loss: 0.4001 - val_accuracy: 0.8340\nEpoch 2/10\n625/625 [==============================] - 50s 80ms/step - loss: 0.3678 - accuracy: 0.8590 - val_loss: 0.4064 - val_accuracy: 0.8292\nEpoch 3/10\n625/625 [==============================] - 37s 60ms/step - loss: 0.2948 - accuracy: 0.8942 - val_loss: 0.3913 - val_accuracy: 0.8474\nEpoch 4/10\n625/625 [==============================] - 34s 55ms/step - loss: 0.2403 - accuracy: 0.9163 - val_loss: 0.3350 - val_accuracy: 0.8678\nEpoch 5/10\n625/625 [==============================] - 29s 47ms/step - loss: 0.2076 - accuracy: 0.9298 - val_loss: 0.4022 - val_accuracy: 0.8720\nEpoch 6/10\n625/625 [==============================] - 34s 54ms/step - loss: 0.1757 - accuracy: 0.9408 - val_loss: 0.3736 - val_accuracy: 0.8590\nEpoch 7/10\n625/625 [==============================] - 29s 46ms/step - loss: 0.1630 - accuracy: 0.9442 - val_loss: 0.4029 - val_accuracy: 0.8652\nEpoch 8/10\n625/625 [==============================] - 32s 52ms/step - loss: 0.1361 - accuracy: 0.9583 - val_loss: 0.4101 - val_accuracy: 0.8772\nEpoch 9/10\n625/625 [==============================] - 29s 46ms/step - loss: 0.1196 - accuracy: 0.9638 - val_loss: 0.5701 - val_accuracy: 0.8320\nEpoch 10/10\n625/625 [==============================] - 28s 44ms/step - loss: 0.1012 - accuracy: 0.9686 - val_loss: 0.4684 - val_accuracy: 0.8718\n782/782 [==============================] - 15s 18ms/step - loss: 0.3471 - accuracy: 0.8624\nTest acc: 0.862\n\n\nIt trains much faster than the one-hot model (since the LSTM only has to process 256-dimensional vectors instead of 20,000-dimensional), and its test accuracy is comparable (86%). We’re still some way off from the results of our basic bigram model. Part of the reason why is simply that the model is looking at slightly less data: the bigram model processed full reviews, while our sequence model truncates sequences after 600 words.\n\n\n10.3.3.2 Understanding padding and masking\nOne thing that’s slightly hurting model performance here is that our input sequences are full of zeros. This comes from our use of the output_sequence_length=max_length option in TextVectorization (with max_length equal to 600): sentences longer than 600 tokens are truncated to a length of 600 tokens, and sentences shorter than 600 tokens are padded with zeros at the end so that they can be concatenated together with other sequences to form contiguous batches.\nWe’re using a bidirectional RNN: two RNN layers running in parallel, with one processing the tokens in their natural order, and the other processing the same tokens in reverse. The RNN that looks at the tokens in their natural order will spend its last iterations seeing only vectors that encode padding—possibly for several hundreds of iterations if the original sentence was short. The information stored in the internal state of the RNN will gradually fade out as it gets exposed to these meaningless inputs.\nWe need some way to tell the RNN that it should skip these iterations. There’s an API for that: masking. The Embedding layer is capable of generating a “mask” that corresponds to its input data. This mask is a tensor of ones and zeros (or True/False booleans), of shape (batch_size, sequence_length), where the entry mask[i, t] indicates where timestep t of sample i should be skipped or not (the timestep will be skipped if mask[i, t] is 0 or False, and processed otherwise).\nBy default, this option isn’t active — you can turn it on by passing mask_zero=True to your Embedding layer. You can retrieve the mask with the compute_mask() method:\n\nembedding_layer = tf.keras.layers.Embedding(input_dim=10, output_dim=256, mask_zero=True)\n\n\nsome_input = [\n    [4, 3, 2, 1, 0, 0, 0],\n    [5, 4, 3, 2, 1, 0, 0],\n    [2, 1, 0, 0, 0, 0, 0]\n]\n\nmask = embedding_layer.compute_mask(some_input)\nmask\n\n&lt;tf.Tensor: shape=(3, 7), dtype=bool, numpy=\narray([[ True,  True,  True,  True, False, False, False],\n       [ True,  True,  True,  True,  True, False, False],\n       [ True,  True, False, False, False, False, False]])&gt;\n\n\nIn practice, you will almost never have to manage masks by hand. Instead, tf.Keras will automatically pass on the mask to every layer that is able to process it. This mask will be used by RNN layers to skip masked steps. If your model returns an entire sequence, the mask will also be used by the loss function to skip masked steps in the output sequence. Let’s try retraining our model with masking enabled:\n\ninputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\nembedded = tf.keras.layers.Embedding(\n    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs) # You can turn mask on by passing mask_zero=True\nx = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(embedded)\nx = tf.keras.layers.Dropout(0.5)(x)\noutputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = tf.keras.Model(inputs, outputs)\n\n\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\", save_best_only=True)\n]\nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\nmodel = tf.keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n\nModel: \"model_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_7 (InputLayer)        [(None, None)]            0         \n                                                                 \n embedding_3 (Embedding)     (None, None, 256)         5120000   \n                                                                 \n bidirectional_2 (Bidirectio  (None, 64)               73984     \n nal)                                                            \n                                                                 \n dropout_6 (Dropout)         (None, 64)                0         \n                                                                 \n dense_19 (Dense)            (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 5,194,049\nTrainable params: 5,194,049\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 95s 133ms/step - loss: 0.4575 - accuracy: 0.7808 - val_loss: 0.3454 - val_accuracy: 0.8558\nEpoch 2/10\n625/625 [==============================] - 49s 78ms/step - loss: 0.2750 - accuracy: 0.8925 - val_loss: 0.3157 - val_accuracy: 0.8844\nEpoch 3/10\n625/625 [==============================] - 40s 65ms/step - loss: 0.2060 - accuracy: 0.9245 - val_loss: 0.2950 - val_accuracy: 0.8808\nEpoch 4/10\n625/625 [==============================] - 36s 58ms/step - loss: 0.1623 - accuracy: 0.9421 - val_loss: 0.3303 - val_accuracy: 0.8762\nEpoch 5/10\n625/625 [==============================] - 34s 54ms/step - loss: 0.1211 - accuracy: 0.9577 - val_loss: 0.3438 - val_accuracy: 0.8780\nEpoch 6/10\n625/625 [==============================] - 35s 56ms/step - loss: 0.0948 - accuracy: 0.9672 - val_loss: 0.4910 - val_accuracy: 0.8684\nEpoch 7/10\n625/625 [==============================] - 33s 52ms/step - loss: 0.0770 - accuracy: 0.9733 - val_loss: 0.4370 - val_accuracy: 0.8718\nEpoch 8/10\n625/625 [==============================] - 35s 57ms/step - loss: 0.0569 - accuracy: 0.9807 - val_loss: 0.4648 - val_accuracy: 0.8610\nEpoch 9/10\n625/625 [==============================] - 35s 56ms/step - loss: 0.0464 - accuracy: 0.9838 - val_loss: 0.6365 - val_accuracy: 0.8464\nEpoch 10/10\n625/625 [==============================] - 34s 55ms/step - loss: 0.0354 - accuracy: 0.9880 - val_loss: 0.5367 - val_accuracy: 0.8640\n782/782 [==============================] - 20s 20ms/step - loss: 0.3014 - accuracy: 0.8774\nTest acc: 0.877\n\n\nThis time we get to 88% test accuracy — a small but noticeable improvement.\n\n\n10.3.3.3 Using pretrained word embeddings\nSometimes you have so little training data available that you can’t use your data alone to learn an appropriate task - specific embedding of your vocabulary. In such cases, instead of learning word embeddings jointly with the problem you want to solve, you can load embedding vectors from a precomputed embedding space that you know is highly structured and exhibits useful properties. There are various precomputed databases of word embeddings that you can download and use in a tf.Keras Embedding layer. Word2vec is one of them. Another popular one is called GloVe, which was developed by Stanford researchers in 2014.\nFirst, let’s download the GloVe word embeddings precomputed on the 2014 English Wikipedia dataset. It’s an 822 MB zip file containing 100-dimensional embedding vectors for 400,000 words (or non-word tokens).\n\n!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip -q glove.6B.zip\n\n--2023-04-24 10:36:45--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2023-04-24 10:36:45--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2023-04-24 10:36:46--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: ‘glove.6B.zip’\n\nglove.6B.zip        100%[===================&gt;] 822.24M  5.24MB/s    in 2m 39s  \n\n2023-04-24 10:39:26 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n\n\n\nLet’s parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation.\n\npath_to_glove_file = \"glove.6B.100d.txt\"\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(f\"Found {len(embeddings_index)} word vectors.\")\n\nFound 400000 word vectors.\n\n\nNext, let’s build an embedding matrix that you can load into an Embedding layer. It must be a matrix of shape (max_words, embedding_dim), where each entry i contains the embedding_dim - dimensional vector for the word of index i in the reference word index (built during tokenization).\n\nembedding_dim = 100\n\nvocabulary = text_vectorization.get_vocabulary()\nword_index = dict(zip(vocabulary, range(len(vocabulary))))\n\nembedding_matrix = np.zeros((max_tokens, embedding_dim))\nfor word, i in word_index.items():\n    if i &lt; max_tokens:\n        embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nFinally, we use a Constant initializer to load the pretrained embeddings in an Embedding layer. So as not to disrupt the pretrained representations during training, we freeze the layer via trainable=False:\n\nembedding_layer = tf.keras.layers.Embedding(\n    max_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=False,\n    mask_zero=True,\n)\n\nWe’re now ready to train a new model—identical to our previous model, but leveraging the 100-dimensional pretrained GloVe embeddings instead of 128-dimensional learned embeddings.\n\ninputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\nembedded = embedding_layer(inputs)\nx = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(embedded)\nx = tf.keras.layers.Dropout(0.5)(x)\noutputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = tf.keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\", save_best_only=True)\n]\nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n\nmodel = tf.keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n\nModel: \"model_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_8 (InputLayer)        [(None, None)]            0         \n                                                                 \n embedding_4 (Embedding)     (None, None, 100)         2000000   \n                                                                 \n bidirectional_3 (Bidirectio  (None, 64)               34048     \n nal)                                                            \n                                                                 \n dropout_7 (Dropout)         (None, 64)                0         \n                                                                 \n dense_20 (Dense)            (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 2,034,113\nTrainable params: 34,113\nNon-trainable params: 2,000,000\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 49s 65ms/step - loss: 0.5726 - accuracy: 0.6988 - val_loss: 0.5082 - val_accuracy: 0.7418\nEpoch 2/10\n625/625 [==============================] - 33s 53ms/step - loss: 0.4545 - accuracy: 0.7942 - val_loss: 0.4163 - val_accuracy: 0.8126\nEpoch 3/10\n625/625 [==============================] - 37s 60ms/step - loss: 0.4056 - accuracy: 0.8196 - val_loss: 0.4130 - val_accuracy: 0.8088\nEpoch 4/10\n625/625 [==============================] - 40s 63ms/step - loss: 0.3720 - accuracy: 0.8360 - val_loss: 0.3538 - val_accuracy: 0.8462\nEpoch 5/10\n625/625 [==============================] - 42s 67ms/step - loss: 0.3479 - accuracy: 0.8535 - val_loss: 0.3453 - val_accuracy: 0.8530\nEpoch 6/10\n625/625 [==============================] - 46s 74ms/step - loss: 0.3288 - accuracy: 0.8623 - val_loss: 0.3401 - val_accuracy: 0.8544\nEpoch 7/10\n625/625 [==============================] - 38s 61ms/step - loss: 0.3116 - accuracy: 0.8705 - val_loss: 0.3243 - val_accuracy: 0.8576\nEpoch 8/10\n625/625 [==============================] - 51s 81ms/step - loss: 0.2929 - accuracy: 0.8773 - val_loss: 0.3139 - val_accuracy: 0.8626\nEpoch 9/10\n625/625 [==============================] - 34s 54ms/step - loss: 0.2817 - accuracy: 0.8844 - val_loss: 0.3128 - val_accuracy: 0.8676\nEpoch 10/10\n625/625 [==============================] - 34s 55ms/step - loss: 0.2654 - accuracy: 0.8918 - val_loss: 0.3159 - val_accuracy: 0.8628\n782/782 [==============================] - 22s 26ms/step - loss: 0.3020 - accuracy: 0.8690\nTest acc: 0.869\n\n\nLeveraging pretrained embeddings can be very helpful when you’re working with a smaller dataset.\n\n\n\n10.3.4 The Transformer encoder (Optional)\nThe encoder part of transformer can be used for text classification—it’s a very generic module that ingests a sequence and learns to turn it into a more useful representation. Let’s implement a Transformer encoder using tf.Keras subclassing API.\n\nclass TransformerEncoder(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim  # Size of the input token vectors\n        self.dense_dim = dense_dim  # Size of the inner dense layer\n        self.num_heads = num_heads  # Number of attention heads\n        self.attention = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = tf.keras.Sequential(\n            [tf.keras.layers.Dense(dense_dim, activation=\"relu\"),\n             tf.keras.layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n    # Computation goes in call().\n    def call(self, inputs, mask=None): \n        # The mask that will be generated by the Embedding layer will be 2D, but\n        # the attention layer expects to be 3D or 4D, so we expand its rank. \n        if mask is not None:\n            mask = mask[:, tf.newaxis, :]\n        attention_output = self.attention(\n            inputs, inputs, attention_mask=mask)\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n    # Implement serialization so we can save the model.\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"dense_dim\": self.dense_dim,\n        })\n        return config\n\nWhen you write custom layers, make sure to implement the get_config() method: this enables the layer to be reinstantiated from its config dict, which is useful during model saving and loading.\nTo add positional encoding, we’ll do something simpler and more effective: we’ll learn position embedding vectors the same way we learn to embed word indices. We’ll then proceed to add our position embeddings to the corresponding word embeddings, to obtain a position-aware word embedding. It is noted that neural networks don’t like very large input values, or discrete input distributions therefore simply adding a position information as interger is not a good idea.\n\nclass PositionalEmbedding(tf.keras.layers.Layer):\n    # A downside of position embeddings is that the sequence length needs to be known in advance.\n    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        # Prepare an Embedding layer for the token indices.\n        self.token_embeddings = tf.keras.layers.Embedding(\n            input_dim=input_dim, output_dim=output_dim)\n        # And another one for the token positions\n        self.position_embeddings = tf.keras.layers.Embedding(\n            input_dim=sequence_length, output_dim=output_dim)\n        self.sequence_length = sequence_length\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        # Add both embedding vectors together\n        return embedded_tokens + embedded_positions\n    \n    # Like the Embedding layer, this layer should be able to generate a\n    # mask so we can ignore padding 0s in the inputs. The compute_mask\n    # method will called automatically by the framework, and the\n    # mask will get propagated to the next layer.\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"output_dim\": self.output_dim,\n            \"sequence_length\": self.sequence_length,\n            \"input_dim\": self.input_dim,\n        })\n        return config\n\nAll you have to do to start taking word order into account is swap the old Embedding layer with our position-aware version.\n\nvocab_size = 20000\nsequence_length = 600\nembed_dim = 256\nnum_heads = 2\ndense_dim = 32\n\ninputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\nx = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\nx = tf.keras.layers.GlobalMaxPooling1D()(x)\nx = tf.keras.layers.Dropout(0.5)(x)\noutputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = tf.keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\", save_best_only=True)\n]\nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\nmodel = tf.keras.models.load_model(\n    \"full_transformer_encoder.keras\",\n    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n                    \"PositionalEmbedding\": PositionalEmbedding})\nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n\nModel: \"model_8\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_9 (InputLayer)        [(None, None)]            0         \n                                                                 \n positional_embedding (Posit  (None, None, 256)        5273600   \n ionalEmbedding)                                                 \n                                                                 \n transformer_encoder (Transf  (None, None, 256)        543776    \n ormerEncoder)                                                   \n                                                                 \n global_max_pooling1d (Globa  (None, 256)              0         \n lMaxPooling1D)                                                  \n                                                                 \n dropout_8 (Dropout)         (None, 256)               0         \n                                                                 \n dense_23 (Dense)            (None, 1)                 257       \n                                                                 \n=================================================================\nTotal params: 5,817,633\nTrainable params: 5,817,633\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/20\n625/625 [==============================] - 96s 148ms/step - loss: 0.5369 - accuracy: 0.7337 - val_loss: 0.3355 - val_accuracy: 0.8620\nEpoch 2/20\n625/625 [==============================] - 64s 102ms/step - loss: 0.3016 - accuracy: 0.8734 - val_loss: 0.3112 - val_accuracy: 0.8690\nEpoch 3/20\n625/625 [==============================] - 65s 104ms/step - loss: 0.2341 - accuracy: 0.9053 - val_loss: 0.3086 - val_accuracy: 0.8766\nEpoch 4/20\n625/625 [==============================] - 56s 89ms/step - loss: 0.1916 - accuracy: 0.9258 - val_loss: 0.2963 - val_accuracy: 0.8832\nEpoch 5/20\n625/625 [==============================] - 55s 88ms/step - loss: 0.1659 - accuracy: 0.9363 - val_loss: 0.3897 - val_accuracy: 0.8822\nEpoch 6/20\n625/625 [==============================] - 49s 78ms/step - loss: 0.1396 - accuracy: 0.9473 - val_loss: 0.3166 - val_accuracy: 0.8756\nEpoch 7/20\n625/625 [==============================] - 47s 75ms/step - loss: 0.1139 - accuracy: 0.9569 - val_loss: 0.3463 - val_accuracy: 0.8760\nEpoch 8/20\n625/625 [==============================] - 47s 75ms/step - loss: 0.0933 - accuracy: 0.9669 - val_loss: 0.5166 - val_accuracy: 0.8336\nEpoch 9/20\n625/625 [==============================] - 47s 75ms/step - loss: 0.0742 - accuracy: 0.9740 - val_loss: 0.5596 - val_accuracy: 0.8640\nEpoch 10/20\n625/625 [==============================] - 47s 75ms/step - loss: 0.0546 - accuracy: 0.9808 - val_loss: 0.5631 - val_accuracy: 0.8704\nEpoch 11/20\n625/625 [==============================] - 46s 73ms/step - loss: 0.0435 - accuracy: 0.9845 - val_loss: 0.6425 - val_accuracy: 0.8680\nEpoch 12/20\n625/625 [==============================] - 46s 74ms/step - loss: 0.0348 - accuracy: 0.9887 - val_loss: 0.7147 - val_accuracy: 0.8726\nEpoch 13/20\n625/625 [==============================] - 45s 72ms/step - loss: 0.0272 - accuracy: 0.9907 - val_loss: 0.7546 - val_accuracy: 0.8640\nEpoch 14/20\n625/625 [==============================] - 46s 74ms/step - loss: 0.0214 - accuracy: 0.9930 - val_loss: 1.0937 - val_accuracy: 0.8680\nEpoch 15/20\n625/625 [==============================] - 46s 73ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 1.3181 - val_accuracy: 0.8612\nEpoch 16/20\n625/625 [==============================] - 46s 74ms/step - loss: 0.0172 - accuracy: 0.9950 - val_loss: 1.1156 - val_accuracy: 0.8640\nEpoch 17/20\n625/625 [==============================] - 46s 73ms/step - loss: 0.0141 - accuracy: 0.9955 - val_loss: 1.0343 - val_accuracy: 0.8670\nEpoch 18/20\n625/625 [==============================] - 46s 73ms/step - loss: 0.0150 - accuracy: 0.9959 - val_loss: 1.2374 - val_accuracy: 0.8644\nEpoch 19/20\n625/625 [==============================] - 46s 73ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 1.3844 - val_accuracy: 0.8640\nEpoch 20/20\n625/625 [==============================] - 45s 72ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 1.3549 - val_accuracy: 0.8634\n782/782 [==============================] - 22s 28ms/step - loss: 0.2870 - accuracy: 0.8830\nTest acc: 0.883\n\n\nWe get to 88.3% test accuracy, a solid improvement that clearly demonstrates the value of word order information for text classification. This is our best sequence model so far!"
  },
  {
    "objectID": "10_Recurrent_Neural_Networks.html#hugging-faces-transformers-library",
    "href": "10_Recurrent_Neural_Networks.html#hugging-faces-transformers-library",
    "title": "10  Sequence Processing with RNNs and Attention - Tensforflow",
    "section": "10.4 Hugging Face’s Transformers Library",
    "text": "10.4 Hugging Face’s Transformers Library\nIt’s impossible to talk about transformers today without mentioning Hugging Face, an AI company that has built a whole ecosystem of easy-to-use open source tools for NLP, vision, and beyond. The central component of their ecosystem is the Transformers library, which allows you to easily download a pretrained model, including its corresponding tokenizer, and then fine-tune it on your own dataset, if needed. Plus, the library supports TensorFlow, PyTorch, and JAX (with the high-level Flax library).\nThe simplest way to use the Transformers library is to use the transformers. pipeline() function: you just specify which task you want, such as sentiment analysis, and it downloads a default pretrained model, ready to be used:\n\nclassifier = pipeline(\"sentiment-analysis\")  # many other tasks are available\nresult = classifier(\"The actors were very convincing.\")\n\nNo model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\nThe result is a Python list containing one dictionary per input text:\n\nresult\n\n[{'label': 'POSITIVE', 'score': 0.9998071789741516}]\n\n\nIn this example, the model correctly found that the sentence is positive, with around 99.98% confidence. Of course, you can also pass a batch of sentences to the model (Models can be very biased. For example, it may like or dislike some countries depending on the data it was trained on, and how it is used, so use it with care):\n\nclassifier([\"I am from India.\", \"I am from Iraq.\"]) # Note that is has bias\n\n[{'label': 'POSITIVE', 'score': 0.9896161556243896},\n {'label': 'NEGATIVE', 'score': 0.9811071157455444}]\n\n\nThe pipeline() function uses the default model for the given task. For example, for text classification tasks such as sentiment analysis, it defaults to distilbert-base-uncased-finetuned-sst-2-english — a DistilBERT model with an uncased tokenizer, trained on English Wikipedia and a corpus of English books, and fine-tuned on the Stanford Sentiment Treebank v2 (SST 2) task.\n\nclassifier.model, classifier.tokenizer \n\n(DistilBertForSequenceClassification(\n   (distilbert): DistilBertModel(\n     (embeddings): Embeddings(\n       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n       (position_embeddings): Embedding(512, 768)\n       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n       (dropout): Dropout(p=0.1, inplace=False)\n     )\n     (transformer): Transformer(\n       (layer): ModuleList(\n         (0-5): 6 x TransformerBlock(\n           (attention): MultiHeadSelfAttention(\n             (dropout): Dropout(p=0.1, inplace=False)\n             (q_lin): Linear(in_features=768, out_features=768, bias=True)\n             (k_lin): Linear(in_features=768, out_features=768, bias=True)\n             (v_lin): Linear(in_features=768, out_features=768, bias=True)\n             (out_lin): Linear(in_features=768, out_features=768, bias=True)\n           )\n           (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n           (ffn): FFN(\n             (dropout): Dropout(p=0.1, inplace=False)\n             (lin1): Linear(in_features=768, out_features=3072, bias=True)\n             (lin2): Linear(in_features=3072, out_features=768, bias=True)\n             (activation): GELUActivation()\n           )\n           (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n         )\n       )\n     )\n   )\n   (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n   (classifier): Linear(in_features=768, out_features=2, bias=True)\n   (dropout): Dropout(p=0.2, inplace=False)\n ),\n DistilBertTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True))\n\n\nIt’s also possible to manually specify a different model. For example, you could use a DistilBERT model fine-tuned on the Multi-Genre Natural Language Inference (MultiNLI) task, which classifies two sentences into three classes: contradiction, neutral, or entailment. Here is how:\n\nmodel_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\nclassifier_mnli = pipeline(\"text-classification\", model=model_name)\nclassifier_mnli(\"She loves me. [SEP] She loves me not.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[{'label': 'contradiction', 'score': 0.9790191650390625}]\n\n\nThe pipeline() API is very simple and convenient, but sometimes you will need more control. For such cases, the Transformers library provides many classes, including all sorts of tokenizers, models, configurations, callbacks, and much more. For example, let’s load the same DistilBERT model, along with its corresponding tokenizer, using the TFAutoModelForSequenceClassification and AutoTokenizer classes:\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n\n\n\n\nSome layers from the model checkpoint at huggingface/distilbert-base-uncased-finetuned-mnli were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at huggingface/distilbert-base-uncased-finetuned-mnli and are newly initialized: ['dropout_23']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nNext, let’s tokenize a couple of pairs of sentences. In this code, we activate padding and specify that we want TensorFlow tensors instead of Python lists:\n\ntoken_ids = tokenizer([\"I like soccer. [SEP] We all love soccer!\",\n                       \"Joe lived for a very long time. [SEP] Joe is old.\"],\n                      padding=True, return_tensors=\"tf\")\n\n\nInstead of passing \"Sentence 1 [SEP] Sentence 2\" to the tokenizer, you can equivalently pass it a tuple: (\"Sentence 1\", \"Sentence 2\").\n\nThe output is a dictionary-like instance of the BatchEncoding class, which contains the sequences of token IDs, as well as a mask containing 0s for the padding tokens:\n\ntoken_ids\n\n{'input_ids': &lt;tf.Tensor: shape=(2, 15), dtype=int32, numpy=\narray([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,\n         102,    0,    0,    0],\n       [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,\n        2003, 2214, 1012,  102]], dtype=int32)&gt;, 'attention_mask': &lt;tf.Tensor: shape=(2, 15), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)&gt;}\n\n\nIf you set return_token_type_ids=True when calling the tokenizer, you will also get an extra tensor that indicates which sentence each token belongs to. This is needed by some models, but not DistilBERT. Next, we can directly pass this BatchEncoding object to the model; it returns a TFSequenceClassifierOutput object containing its predicted class logits:\n\noutputs = model(token_ids)\noutputs\n\nTFSequenceClassifierOutput(loss=None, logits=&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[-2.1123812 ,  1.178679  ,  1.4101001 ],\n       [-0.01478288,  1.0962466 , -0.9919953 ]], dtype=float32)&gt;, hidden_states=None, attentions=None)\n\n\nLastly, we can apply the softmax activation function to convert these logits to class probabilities, and use the argmax() function to predict the class with the highest probability for each input sentence pair:\n\nY_probas = tf.keras.activations.softmax(outputs.logits)\nY_probas\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[0.01619703, 0.43523598, 0.54856706],\n       [0.22655995, 0.6881722 , 0.08526783]], dtype=float32)&gt;\n\n\n\nY_pred = tf.argmax(Y_probas, axis=1)\nY_pred  # 0 = contradiction, 1 = entailment, 2 = neutral\n\n&lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 1])&gt;\n\n\nIn this example, the model correctly classifies the first sentence pair as neutral (the fact that I like soccer does not imply that everyone else does) and the second pair as an entailment (Joe must indeed be quite old).\nIf you wish to fine-tune this model on your own dataset, you can train the model as usual with tf.Keras since it’s just a regular tf.Keras model with a few extra methods. However, because the model outputs logits instead of probabilities, you must use the tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) loss instead of the usual sparse_categorical_crossentropy loss. Moreover, the model does not support BatchEncoding inputs during training, so you must use its data attribute to get a regular dictionary instead:\n\nsentences = [(\"Sky is blue\", \"Sky is red\"), (\"I love her\", \"She loves me\")]\nX_train = tokenizer(sentences, padding=True, return_tensors=\"tf\").data\ny_train = tf.constant([0, 2])  # contradiction, neutral\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(loss=loss, optimizer=\"nadam\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train, epochs=2)\n\nEpoch 1/2\n1/1 [==============================] - 47s 47s/step - loss: 0.6666 - accuracy: 0.5000\nEpoch 2/2\n1/1 [==============================] - 0s 71ms/step - loss: 0.3430 - accuracy: 1.0000\n\n\nHugging Face has also built a Datasets library that you can use to easily download a standard dataset (such as IMDb) or a custom one, and use it to fine-tune your model. It’s similar to TensorFlow Datasets, but it also provides tools to perform common preprocessing tasks on the fly, such as masking. The list of datasets is available at https://huggingface.co/datasets.\nThis should get you started with Hugging Face’s ecosystem. To learn more, you can head over to https://huggingface.co/docs for the documentation, which includes many tutorial notebooks, videos, the full API, and more.\n\n10.4.1 Deal with IMDB (Optional)\n\nimdb = load_dataset(\"imdb\")\n\nWARNING:datasets.builder:Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n\n\n\n\n\n\nimdb['train'][100]\n\n{'text': \"Terrible movie. Nuff Said.&lt;br /&gt;&lt;br /&gt;These Lines are Just Filler. The movie was bad. Why I have to expand on that I don't know. This is already a waste of my time. I just wanted to warn others. Avoid this movie. The acting sucks and the writing is just moronic. Bad in every way. The only nice thing about the movie are Deniz Akkaya's breasts. Even that was ruined though by a terrible and unneeded rape scene. The movie is a poorly contrived and totally unbelievable piece of garbage.&lt;br /&gt;&lt;br /&gt;OK now I am just going to rag on IMDb for this stupid rule of 10 lines of text minimum. First I waste my time watching this offal. Then feeling compelled to warn others I create an account with IMDb only to discover that I have to write a friggen essay on the film just to express how bad I think it is. Totally unnecessary.\",\n 'label': 0}\n\n\n\nid2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\nlabel2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n\n# If we use AutoModelForSequenceClassification, it will use pytorch model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n)\n\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)\n \ntokenized_imdb = imdb.map(preprocess_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-7a7a5d00cfff76c8.arrow\n\n\n\n\n\nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-435fdd3faf14640e.arrow\n\n\n\naccuracy = evaluate.load(\"accuracy\")\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)\n\ntraining_args = TrainingArguments(\n    output_dir=\"my_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_imdb[\"train\"],\n    eval_dataset=tokenized_imdb[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n\n    \n      \n      \n      [1563/1563 26:16, Epoch 1/1]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.227100\n0.193795\n0.926880\n\n\n\n\n\n\nTrainOutput(global_step=1563, training_loss=0.2581237346334329, metrics={'train_runtime': 1578.5525, 'train_samples_per_second': 15.837, 'train_steps_per_second': 0.99, 'total_flos': 3281068438885632.0, 'train_loss': 0.2581237346334329, 'epoch': 1.0})"
  },
  {
    "objectID": "10_Recurrent_Neural_Networks.html#references",
    "href": "10_Recurrent_Neural_Networks.html#references",
    "title": "10  Sequence Processing with RNNs and Attention - Tensforflow",
    "section": "10.5 References",
    "text": "10.5 References\n\nhttps://github.com/ageron/handson-ml3/\nhttps://github.com/fchollet/deep-learning-with-python-notebooks\nhttps://androidkt.com/save-and-load-fine-tuned-huggingface-transformers-model-from-local-disk/\nhttps://github.com/nlp-with-transformers/notebooks/tree/main"
  },
  {
    "objectID": "11_Transfer_learning.html#setup",
    "href": "11_Transfer_learning.html#setup",
    "title": "11  Transfer learning and self-supervised learning",
    "section": "11.1 Setup",
    "text": "11.1 Setup\nThe following installation may take some time (~5min). If you do not want to run self-supervised learning you could comment out the first two packages.\n\n%pip install git+https://github.com/tensorflow/similarity.git -qq\n%pip install tensorflow-addons -qq\n%pip install transformers datasets evaluate -qq\n\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.7/188.7 kB 6.8 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.2/88.2 kB 10.6 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 16.0 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n  Building wheel for tensorflow-similarity (pyproject.toml) ... done\n  Building wheel for nmslib (setup.py) ... done\n  Building wheel for umap-learn (setup.py) ... done\n  Building wheel for pynndescent (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 591.0/591.0 kB 26.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.1/7.1 MB 91.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 474.6/474.6 kB 43.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 9.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 23.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 115.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 14.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 26.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.3/134.3 kB 17.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 71.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 15.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 17.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 17.7 MB/s eta 0:00:00\n\n\n\n# Python ≥3.7 is recommended\nimport sys\nassert sys.version_info &gt;= (3, 7)\n\n# Scikit-Learn ≥1.01 is recommended\nfrom packaging import version\nimport sklearn\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\n# Tensorflow ≥2.8.0 is recommended\nimport tensorflow as tf\nassert version.parse(tf.__version__) &gt;= version.parse(\"2.8.0\")\nfrom tensorflow.keras.utils import image_dataset_from_directory\nimport tensorflow_datasets as tfds\nimport tensorflow_hub as hub\n\n# Self-supervised learning (Comment below if you do not want to train the SSL model)\nimport tensorflow_similarity as tfsim\nimport tensorflow_addons as tfa\n\n# Fintune CLIP, BERT, GPT\nfrom transformers import pipeline\nfrom transformers import AutoProcessor, TFAutoModelForZeroShotImageClassification\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification, TFAutoModelForCausalLM\nfrom transformers import BertTokenizerFast, AutoModelForCausalLM\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom transformers import create_optimizer\nfrom transformers.keras_callbacks import KerasMetricCallback\nfrom transformers.keras_callbacks import PushToHubCallback\nfrom datasets import load_dataset\nimport evaluate\n\n# Common imports\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport random\nimport time\nimport os\nimport shutil, pathlib\nimport PIL.Image as Image\nimport requests\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n\n\n\nif not tf.config.list_physical_devices('GPU'):\n    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n    if \"google.colab\" in sys.modules:\n        print(\"Go to Runtime &gt; Change runtime and select a GPU hardware \"\n              \"accelerator.\")\n    if \"kaggle_secrets\" in sys.modules:\n        print(\"Go to Settings &gt; Accelerator and select GPU.\")"
  },
  {
    "objectID": "11_Transfer_learning.html#transfer-learning-on-a-small-dataset",
    "href": "11_Transfer_learning.html#transfer-learning-on-a-small-dataset",
    "title": "11  Transfer learning and self-supervised learning",
    "section": "11.2 Transfer learning on a small dataset",
    "text": "11.2 Transfer learning on a small dataset\nHere we will use the dataset that we have introduced in Lab 9.\n\n11.2.1 Downloading the data\nThe Dogs vs. Cats dataset that we will use isn’t packaged with Keras. It was made available by Kaggle as part of a computer vision competition in late 2013, back when convnets weren’t mainstream. You can download the original dataset from www.kaggle.com/c/dogs-vs-cats/data.\nBut you can also use Kaggle API. First, you need to create a Kaggle API key and download it to your local machine. Just navigate to the Kaggle website in a web browser, log in, and go to the My Account page. In your account settings, you’ll find an API section. Clicking the Create New API Token button will generate a kaggle.json key file and will download it to your machine.\n\n\n\n\n# Upload the API’s key JSON file to your Colab\n# session by running the following code in a notebook cell:\nfrom google.colab import files\nfiles.upload()\n\nFinally, create a ~/.kaggle folder, and copy the key file to it. As a security best practice, you should also make sure that the file is only readable by the current user, yourself:\n\n!mkdir ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n\n\n# You can now download the data we’re about to use:\n!kaggle competitions download -c dogs-vs-cats\n\nDownloading dogs-vs-cats.zip to /content\n100% 811M/812M [00:22&lt;00:00, 31.7MB/s]\n100% 812M/812M [00:22&lt;00:00, 37.3MB/s]\n\n\nThe first time you try to download the data, you may get a “403 Forbidden” error. That’s because you need to accept the terms associated with the dataset before you download it—you’ll have to go to www.kaggle.com/c/dogs-vs-cats/rules (while logged into your Kaggle account) and click the I Understand and Accept button. You only need to do this once.\n\n!unzip -qq dogs-vs-cats.zip\n!unzip -qq train.zip\n\nreplace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n\n\n\noriginal_dir = pathlib.Path(\"train\")\nnew_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n\ndef make_subset(subset_name, start_index, end_index):\n    for category in (\"cat\", \"dog\"):\n        dir = new_base_dir / subset_name / category\n        os.makedirs(dir)\n        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n        for fname in fnames:\n            shutil.copyfile(src=original_dir / fname,\n                            dst=dir / fname)\n\nmake_subset(\"train\", start_index=0, end_index=1000)\nmake_subset(\"validation\", start_index=1000, end_index=1500)\nmake_subset(\"test\", start_index=1500, end_index=2500)\n\nWe now have 2,000 training images, 1,000 validation images, and 2,000 test images. Each split contains the same number of samples from each class: this is a balanced binary-classification problem, which means classification accuracy will be an appropriate measure of success.\n\n\n11.2.2 Data preprocessing\n\ntrain_dataset = image_dataset_from_directory(\n    new_base_dir / \"train\",\n    image_size=(180, 180),\n    batch_size=32)\nvalidation_dataset = image_dataset_from_directory(\n    new_base_dir / \"validation\",\n    image_size=(180, 180),\n    batch_size=32)\ntest_dataset = image_dataset_from_directory(\n    new_base_dir / \"test\",\n    image_size=(180, 180),\n    batch_size=32)\n\nFound 2000 files belonging to 2 classes.\nFound 1000 files belonging to 2 classes.\nFound 2000 files belonging to 2 classes.\n\n\nLet’s look at the output of one of these Dataset objects: it yields batches of 180 × 180 RGB images (shape (32, 180, 180, 3)) and integer labels (shape (32,)). There are 32 samples in each batch (the batch size).\n\nfor data_batch, labels_batch in train_dataset:\n    print(\"data batch shape:\", data_batch.shape)\n    print(\"labels batch shape:\", labels_batch.shape)\n    break\n\ndata batch shape: (32, 180, 180, 3)\nlabels batch shape: (32,)\n\n\n\n\n11.2.3 Leveraging a pretrained model\nA common and highly effective approach to deep learning on small image datasets is to use a pretrained model. A pretrained model is a model that was previously trained on a large dataset, typically on a large-scale image-classification task. If this original dataset is large enough and general enough, the spatial hierarchy of features learned by the pretrained model can effectively act as a generic model of the visual world, and hence, its features can prove useful for many different computer vision problems, even though these new problems may involve completely different classes than those of the original task.\nIn this case, let’s consider a large convnet trained on the ImageNet dataset (1.2 million labeled images and 1,000 different classes). ImageNet contains many animal classes, including different species of cats and dogs, and you can thus expect it to perform well on the dogs-versus-cats classification problem. We’ll use the VGG16 architecturetrained on ImageNet, to extract interesting features from cat and dog images, and then train a dogs-versus-cats classifier on top of these features. The VGG16 model, among others, comes prepackaged with tf.Keras. You can import it from the tf.keras.applications module.\n\nconv_base = tf.keras.applications.vgg16.VGG16(\n    weights=\"imagenet\",\n    include_top=False,\n    input_shape=(180, 180, 3))\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58889256/58889256 [==============================] - 2s 0us/step\n\n\nWe pass three arguments to the constructor:\n\nweights specifies the weight checkpoint from which to initialize the model.\ninclude_top refers to including (or not) the densely connected classifier on top of the network. By default, this densely connected classifier corresponds to the 1,000 classes from ImageNet. Because we intend to use our own densely connected classifier (with only two classes: cat and dog), we don’t need to include it.\ninput_shape is the shape of the image tensors that we’ll feed to the network. This argument is purely optional: if we don’t pass it, the network will be able to process inputs of any size. Here we pass it so that we can visualize (in the following summary) how the size of the feature maps shrinks with each new convolution and pooling layer.\n\n\nconv_base.summary()\n\nModel: \"vgg16\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 180, 180, 3)]     0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 180, 180, 64)      1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 180, 180, 64)      36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 90, 90, 64)        0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 90, 90, 128)       73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 90, 90, 128)       147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 45, 45, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 45, 45, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 45, 45, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 45, 45, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 22, 22, 256)       0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 22, 22, 512)       1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 22, 22, 512)       2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 22, 22, 512)       2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 11, 11, 512)       0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 11, 11, 512)       2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 11, 11, 512)       2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 11, 11, 512)       2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 5, 5, 512)         0         \n                                                                 \n=================================================================\nTotal params: 14,714,688\nTrainable params: 14,714,688\nNon-trainable params: 0\n_________________________________________________________________\n\n\nThe final feature map has shape (5, 5, 512). That’s the feature map on top of which we’ll stick a densely connected classifier. At this point, there are two ways we could proceed:\n\nRun the convolutional base over our dataset, record its output to a NumPy array on disk, and then use this data as input to a standalone, densely connected classifier. This solution is fast and cheap to run, because it only requires running the convolutional base once for every input image, and the convolutional base is by far the most expensive part of the pipeline. But for the same reason, this technique won’t allow us to use data augmentation.\nExtend the model we have (conv_base) by adding Dense layers on top, and run the whole thing from end to end on the input data. This will allow us to use data augmentation, because every input image goes through the convolutional base every time it’s seen by the model. But for the same reason, this technique is far more expensive than the first.\n\n\n11.2.3.1 Fast feature extration\nWe’ll start by extracting features as NumPy arrays by calling the predict() method of the conv_base model on our training, validation, and testing datasets. Let’s iterate over our datasets to extract the VGG16 features.\n\ndef get_features_and_labels(dataset):\n    all_features = []\n    all_labels = []\n    for images, labels in dataset:\n        preprocessed_images = tf.keras.applications.vgg16.preprocess_input(images)\n        features = conv_base.predict(preprocessed_images, verbose=0)\n        all_features.append(features)\n        all_labels.append(labels)\n    return np.concatenate(all_features), np.concatenate(all_labels)\n\ntrain_features, train_labels =  get_features_and_labels(train_dataset)\nval_features, val_labels =  get_features_and_labels(validation_dataset)\ntest_features, test_labels =  get_features_and_labels(test_dataset)\n\nImportantly, predict() only expects images, not labels, but our current dataset yields batches that contain both images and their labels. Moreover, the VGG16 model expects inputs that are preprocessed with the function tf.keras.applications.vgg16.preprocess_input, which scales pixel values to an appropriate range.\n\ntrain_features.shape\n\n(2000, 5, 5, 512)\n\n\n\n\n11.2.3.2 Defining and training the densely connected classifier\nAt this point, we can define our densely connected classifier (note the use of dropout for regularization) and train it on the data and labels that we just recorded.\n\ntf.keras.backend.clear_session()\n# 1. Define the model\n# Note the use of the Flatten layer before passing the\n# features to a Dense layer.\ninputs = tf.keras.Input(shape=(5, 5, 512))\nx = tf.keras.layers.Flatten()(inputs)\nx = tf.keras.layers.Dense(256)(x)\nx = tf.keras.layers.Dropout(0.5)(x)\noutputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n\n# 2. Compile the model\nmodel = tf.keras.Model(inputs, outputs)\nmodel.compile(loss=\"binary_crossentropy\",\n              optimizer=\"rmsprop\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\n\n# 3. Train the model\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\n      filepath=\"feature_extraction.keras\",\n      save_best_only=True,\n      monitor=\"val_loss\")\n]\nhistory = model.fit(\n    train_features, train_labels,\n    epochs=20,\n    validation_data=(val_features, val_labels),\n    callbacks=callbacks)\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 5, 5, 512)]       0         \n                                                                 \n flatten (Flatten)           (None, 12800)             0         \n                                                                 \n dense (Dense)               (None, 256)               3277056   \n                                                                 \n dropout (Dropout)           (None, 256)               0         \n                                                                 \n dense_1 (Dense)             (None, 1)                 257       \n                                                                 \n=================================================================\nTotal params: 3,277,313\nTrainable params: 3,277,313\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/20\n63/63 [==============================] - 3s 15ms/step - loss: 19.6994 - accuracy: 0.9300 - val_loss: 3.5770 - val_accuracy: 0.9720\nEpoch 2/20\n63/63 [==============================] - 1s 9ms/step - loss: 4.5717 - accuracy: 0.9730 - val_loss: 4.5395 - val_accuracy: 0.9750\nEpoch 3/20\n63/63 [==============================] - 1s 9ms/step - loss: 2.1191 - accuracy: 0.9825 - val_loss: 4.8215 - val_accuracy: 0.9800\nEpoch 4/20\n63/63 [==============================] - 0s 8ms/step - loss: 1.0535 - accuracy: 0.9910 - val_loss: 5.5058 - val_accuracy: 0.9750\nEpoch 5/20\n63/63 [==============================] - 0s 6ms/step - loss: 1.1637 - accuracy: 0.9930 - val_loss: 6.8575 - val_accuracy: 0.9730\nEpoch 6/20\n63/63 [==============================] - 0s 6ms/step - loss: 1.1385 - accuracy: 0.9925 - val_loss: 5.3760 - val_accuracy: 0.9780\nEpoch 7/20\n63/63 [==============================] - 0s 7ms/step - loss: 0.2850 - accuracy: 0.9970 - val_loss: 9.1840 - val_accuracy: 0.9690\nEpoch 8/20\n63/63 [==============================] - 0s 7ms/step - loss: 0.1600 - accuracy: 0.9975 - val_loss: 4.3178 - val_accuracy: 0.9800\nEpoch 9/20\n63/63 [==============================] - 0s 6ms/step - loss: 0.3182 - accuracy: 0.9960 - val_loss: 7.5394 - val_accuracy: 0.9710\nEpoch 10/20\n63/63 [==============================] - 0s 6ms/step - loss: 0.4183 - accuracy: 0.9950 - val_loss: 5.9668 - val_accuracy: 0.9750\nEpoch 11/20\n63/63 [==============================] - 0s 7ms/step - loss: 0.1682 - accuracy: 0.9990 - val_loss: 6.4060 - val_accuracy: 0.9810\nEpoch 12/20\n63/63 [==============================] - 0s 7ms/step - loss: 0.1512 - accuracy: 0.9980 - val_loss: 4.7570 - val_accuracy: 0.9820\nEpoch 13/20\n63/63 [==============================] - 0s 6ms/step - loss: 0.4809 - accuracy: 0.9955 - val_loss: 8.5195 - val_accuracy: 0.9650\nEpoch 14/20\n63/63 [==============================] - 0s 6ms/step - loss: 0.3540 - accuracy: 0.9975 - val_loss: 5.7928 - val_accuracy: 0.9750\nEpoch 15/20\n63/63 [==============================] - 0s 7ms/step - loss: 0.0225 - accuracy: 0.9990 - val_loss: 4.8152 - val_accuracy: 0.9750\nEpoch 16/20\n63/63 [==============================] - 0s 6ms/step - loss: 0.3120 - accuracy: 0.9975 - val_loss: 4.3951 - val_accuracy: 0.9780\nEpoch 17/20\n63/63 [==============================] - 0s 7ms/step - loss: 0.0374 - accuracy: 0.9990 - val_loss: 4.6171 - val_accuracy: 0.9790\nEpoch 18/20\n63/63 [==============================] - 0s 7ms/step - loss: 0.0702 - accuracy: 0.9990 - val_loss: 4.8373 - val_accuracy: 0.9810\nEpoch 19/20\n63/63 [==============================] - 0s 7ms/step - loss: 2.8716e-28 - accuracy: 1.0000 - val_loss: 4.8373 - val_accuracy: 0.9810\nEpoch 20/20\n63/63 [==============================] - 0s 6ms/step - loss: 0.1421 - accuracy: 0.9985 - val_loss: 5.7440 - val_accuracy: 0.9770\n\n\nTraining is very fast because we only have to deal with two Dense layers. Let’s look at the loss and accuracy curves during training\n\nacc = history.history[\"accuracy\"]\nval_acc = history.history[\"val_accuracy\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nWe reach a validation accuracy between 97%~98% — much better than we achieved in Lab 9 with the small model trained from scratch. This is a bit of an unfair comparison, however, because ImageNet contains many dog and cat instances, which means that our pretrained model already has the exact knowledge required for the task at hand. This won’t always be the case when you use pretrained features.\nHowever, the plots also indicate that we’re overfitting almost from the start— despite using dropout with a fairly large rate. That’s because this technique doesn’t use data augmentation, which is essential for preventing overfitting with small image datasets.\n\n\n\n11.2.4 Feature extraction together with data augmentation\nNow let’s review the second technique for doing feature extraction, which is much slower and more expensive, but which allows us to use data augmentation during training: creating a model that chains the conv_base with a new dense classifier, and training it end to end on the inputs.\nIn order to do this, we will first freeze the convolutional base. Freezing a layer or set of layers means preventing their weights from being updated during training. If we don’t do this, the representations that were previously learned by the convolutional base will be modified during training. Because the Dense layers on top are randomly initialized, very large weight updates would be propagated through the network, effectively destroying the representations previously learned. In tf.Keras, we freeze a layer or model by setting its trainable attribute to False.\n\nconv_base  = tf.keras.applications.vgg16.VGG16(\n    weights=\"imagenet\",\n    include_top=False)\n\n\nconv_base.trainable_weights\n\n[&lt;tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32, numpy=\n array([[[[ 4.29470569e-01,  1.17273867e-01,  3.40129584e-02, ...,\n           -1.32241577e-01, -5.33475243e-02,  7.57738389e-03],\n          [ 5.50379455e-01,  2.08774377e-02,  9.88311544e-02, ...,\n           -8.48205537e-02, -5.11389151e-02,  3.74943428e-02],\n          [ 4.80015397e-01, -1.72696680e-01,  3.75577137e-02, ...,\n           -1.27135560e-01, -5.02991639e-02,  3.48965675e-02]],\n \n         [[ 3.73466998e-01,  1.62062630e-01,  1.70863140e-03, ...,\n           -1.48207128e-01, -2.35300660e-01, -6.30356818e-02],\n          [ 4.40074533e-01,  4.73412387e-02,  5.13819456e-02, ...,\n           -9.88498852e-02, -2.96195745e-01, -7.04357103e-02],\n          [ 4.08547401e-01, -1.70375049e-01, -4.96297423e-03, ...,\n           -1.22360572e-01, -2.76450396e-01, -3.90796512e-02]],\n \n         [[-6.13601133e-02,  1.35693997e-01, -1.15694344e-01, ...,\n           -1.40158370e-01, -3.77666801e-01, -3.00509870e-01],\n          [-8.13870355e-02,  4.18543853e-02, -1.01763301e-01, ...,\n           -9.43124294e-02, -5.05662560e-01, -3.83694321e-01],\n          [-6.51455522e-02, -1.54351532e-01, -1.38038069e-01, ...,\n           -1.29404560e-01, -4.62243795e-01, -3.23985279e-01]]],\n \n \n        [[[ 2.74769872e-01,  1.48350164e-01,  1.61559835e-01, ...,\n           -1.14316158e-01,  3.65494519e-01,  3.39938998e-01],\n          [ 3.45739067e-01,  3.10493708e-02,  2.40750551e-01, ...,\n           -6.93419054e-02,  4.37116861e-01,  4.13171440e-01],\n          [ 3.10477257e-01, -1.87601492e-01,  1.66595340e-01, ...,\n           -9.88388434e-02,  4.04058546e-01,  3.92561197e-01]],\n \n         [[ 3.86807770e-02,  2.02298447e-01,  1.56414255e-01, ...,\n           -5.20089604e-02,  2.57149011e-01,  3.71682674e-01],\n          [ 4.06322069e-02,  6.58102185e-02,  2.20311403e-01, ...,\n           -3.78979952e-03,  2.69412428e-01,  4.09505904e-01],\n          [ 5.02023660e-02, -1.77571565e-01,  1.51188180e-01, ...,\n           -1.40649760e-02,  2.59300828e-01,  4.23764467e-01]],\n \n         [[-3.67223352e-01,  1.61688417e-01, -8.99365395e-02, ...,\n           -1.45945460e-01, -2.71823555e-01, -2.39718184e-01],\n          [-4.53501314e-01,  4.62574959e-02, -6.67438358e-02, ...,\n           -1.03502415e-01, -3.45792353e-01, -2.92486250e-01],\n          [-4.03383434e-01, -1.74399972e-01, -1.09849639e-01, ...,\n           -1.25688612e-01, -3.14026326e-01, -2.32839763e-01]]],\n \n \n        [[[-5.74681684e-02,  1.29344285e-01,  1.29030216e-02, ...,\n           -1.41449392e-01,  2.41099641e-01,  4.55602147e-02],\n          [-5.86349145e-02,  3.16787697e-02,  7.59588331e-02, ...,\n           -1.05017252e-01,  3.39550197e-01,  9.86374393e-02],\n          [-5.08716851e-02, -1.66002661e-01,  1.56279504e-02, ...,\n           -1.49742723e-01,  3.06801915e-01,  8.82701725e-02]],\n \n         [[-2.62249678e-01,  1.71572417e-01,  5.44555223e-05, ...,\n           -1.22728683e-01,  2.44687453e-01,  5.32913655e-02],\n          [-3.30669671e-01,  5.47101051e-02,  4.86797579e-02, ...,\n           -8.29023942e-02,  2.95466095e-01,  7.44469985e-02],\n          [-2.85227507e-01, -1.66666731e-01, -7.96697661e-03, ...,\n           -1.09780088e-01,  2.79203743e-01,  9.46525261e-02]],\n \n         [[-3.50096762e-01,  1.38710454e-01, -1.25339806e-01, ...,\n           -1.53092295e-01, -1.39917329e-01, -2.65075237e-01],\n          [-4.85030204e-01,  4.23195846e-02, -1.12076312e-01, ...,\n           -1.18306056e-01, -1.67058021e-01, -3.22241962e-01],\n          [-4.18516338e-01, -1.57048807e-01, -1.49133086e-01, ...,\n           -1.56839803e-01, -1.42874300e-01, -2.69694626e-01]]]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32, numpy=\n array([ 0.73429835,  0.09340367,  0.06775674,  0.8862966 ,  0.25994542,\n         0.66426694, -0.01582893,  0.3249065 ,  0.68600726,  0.06247932,\n         0.58156496,  0.2361475 ,  0.69694996,  0.19451167,  0.4858922 ,\n         0.44571847,  0.5113422 ,  0.208576  ,  0.57557714,  0.33199573,\n         0.4997983 ,  0.7117759 ,  0.30284074,  0.7082712 ,  0.04548979,\n         0.7446502 ,  0.29845494,  0.48211655,  0.81658626,  0.62603897,\n         0.3768093 ,  2.064037  ,  0.77311045,  0.3459577 ,  0.6130958 ,\n         0.65459156,  0.39045632,  0.50869167,  0.2625384 ,  0.23669638,\n         0.07971057,  1.1179353 ,  0.26129362,  0.8697589 ,  0.21543622,\n         0.78007823,  0.37015367,  0.47993386,  0.4313978 ,  0.5084194 ,\n         0.23049663,  0.7636527 ,  0.35419866,  0.45794216,  0.4662595 ,\n         0.09850298,  0.3803252 ,  0.66880196,  0.4015123 ,  0.90510356,\n         0.43166816,  1.302014  ,  0.5306885 ,  0.48993504], dtype=float32)&gt;,\n &lt;tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32, numpy=\n array([[[[ 1.66219279e-01,  1.42701820e-01, -4.02113283e-03, ...,\n            6.18828237e-02, -1.74057148e-02, -3.00644431e-02],\n          [ 9.46945231e-03,  3.87477316e-03,  5.08365929e-02, ...,\n           -2.77981739e-02,  1.71373668e-03,  6.82722731e-03],\n          [ 6.32681847e-02,  2.12877709e-02, -1.63465310e-02, ...,\n            8.80054955e-04,  6.68104272e-03, -1.41139806e-03],\n          ...,\n          [ 3.47490981e-03,  8.47019628e-02, -4.07223180e-02, ...,\n           -1.13523193e-02, -7.48998486e-03,  3.19077494e-03],\n          [ 5.97234145e-02,  4.97663505e-02, -3.23118735e-03, ...,\n            1.43114366e-02,  3.03175431e-02, -4.23925705e-02],\n          [ 1.33459672e-01,  4.95484173e-02, -1.78808011e-02, ...,\n            2.25385167e-02,  3.02020740e-02, -2.17075031e-02]],\n \n         [[ 2.12007999e-01,  2.10127644e-02, -1.47626130e-02, ...,\n            2.29580477e-02,  1.23102348e-02, -3.08422819e-02],\n          [-2.62175221e-03,  7.42094172e-03,  6.74030930e-02, ...,\n           -3.06594316e-02,  1.80578313e-03,  4.27369215e-03],\n          [ 2.27197763e-02, -1.07841045e-02, -1.31095545e-02, ...,\n           -1.15751950e-02,  4.18359675e-02, -1.92268589e-03],\n          ...,\n          [-2.70304317e-03,  7.41161704e-02, -3.32262330e-02, ...,\n           -1.10277236e-02,  1.39831286e-02,  5.34419343e-03],\n          [-3.20506282e-02, -2.40584910e-02, -4.52397857e-03, ...,\n           -6.04042644e-03,  2.01962605e-01, -5.04491515e-02],\n          [ 1.68114193e-02, -2.33167298e-02, -1.40886130e-02, ...,\n           -7.79278344e-03,  1.28428593e-01, -2.58184522e-02]],\n \n         [[-5.91698708e-03, -2.26223674e-02,  4.88128467e-03, ...,\n            4.13784146e-04, -4.84175496e-02,  1.63675251e-03],\n          [-3.93767562e-03,  9.07397643e-03,  5.36517277e-02, ...,\n           -2.56106984e-02, -4.17886395e-03,  2.47476017e-03],\n          [-3.07008922e-02, -1.09781921e-02, -3.69096454e-03, ...,\n           -1.19221993e-02, -1.39777903e-02,  8.52933805e-03],\n          ...,\n          [ 5.83332591e-03,  2.96198055e-02, -3.56571227e-02, ...,\n           -1.01153394e-02,  5.06090466e-03, -2.86859111e-03],\n          [-7.80934095e-02, -1.95693132e-02,  2.14814320e-02, ...,\n            7.42452731e-03,  5.41617945e-02, -2.35790759e-02],\n          [-7.82641992e-02, -2.47034207e-02,  1.36250593e-02, ...,\n           -7.53259566e-03, -2.59861015e-02,  4.52143652e-03]]],\n \n \n        [[[ 1.99202523e-01,  5.03257252e-02, -1.93236247e-02, ...,\n            1.62308700e-02, -1.15369558e-02,  1.55777042e-03],\n          [-1.38656620e-03,  4.52297740e-03,  6.65066689e-02, ...,\n           -3.00004911e-02,  1.71904117e-04, -1.11898372e-03],\n          [ 2.14257818e-02,  1.79782789e-03, -1.76792312e-02, ...,\n           -6.09106943e-03, -3.21686491e-02, -1.07312342e-03],\n          ...,\n          [-1.47800148e-02,  7.77616128e-02, -2.67065037e-02, ...,\n           -8.10234528e-03, -1.15585206e-02, -5.30505227e-03],\n          [-1.39283072e-02,  1.69547591e-02, -1.20282415e-02, ...,\n            1.25513040e-02, -8.44068080e-02, -4.48253192e-02],\n          [ 2.55671684e-02,  7.02229328e-03, -2.82986648e-02, ...,\n            1.17048556e-02, -7.34802186e-02, -2.06979234e-02]],\n \n         [[-2.59410664e-02, -4.97468375e-02, -2.42083799e-02, ...,\n           -2.39597876e-02,  2.09030695e-02,  1.69499479e-02],\n          [-4.78290301e-03,  1.04173329e-02,  8.31823424e-02, ...,\n           -3.18786576e-02,  3.48328054e-03, -3.56311840e-03],\n          [-3.81173752e-02, -1.85197107e-02, -1.43900532e-02, ...,\n           -1.41960718e-02,  3.14613059e-02,  4.11967095e-03],\n          ...,\n          [-1.79932006e-02,  7.17700496e-02, -1.99375115e-02, ...,\n           -6.12784317e-03,  1.66521706e-02, -7.48346280e-03],\n          [-8.91536921e-02, -4.69055325e-02, -1.37380632e-02, ...,\n           -8.06835108e-03,  6.04359731e-02, -4.47445139e-02],\n          [-1.00021079e-01, -3.55393849e-02, -2.46136412e-02, ...,\n           -7.50665693e-03,  9.11064222e-02, -1.06519591e-02]],\n \n         [[-9.84916389e-02, -5.77464141e-02, -4.49889433e-03, ...,\n           -2.21592579e-02, -9.32130869e-03,  5.57853021e-02],\n          [ 4.70533501e-03,  1.18583711e-02,  6.85855076e-02, ...,\n           -2.62385849e-02, -2.82203127e-03, -4.67023626e-03],\n          [-2.28894763e-02, -7.75412470e-03, -9.86886956e-03, ...,\n           -1.22523597e-02,  4.30414118e-02,  1.28994938e-02],\n          ...,\n          [ 2.93275771e-05,  1.62800644e-02, -2.53228396e-02, ...,\n           -8.11440870e-03,  9.42004658e-03, -1.69352964e-02],\n          [-4.56118677e-03, -3.88334878e-02, -8.15101550e-04, ...,\n           -7.37672672e-05,  2.05828294e-01, -1.84731018e-02],\n          [-3.79626676e-02, -1.82646457e-02, -7.20302528e-03, ...,\n           -1.28044421e-03,  1.40015617e-01,  1.13147246e-02]]],\n \n \n        [[[-1.11585744e-02, -4.15539136e-03, -6.83113467e-03, ...,\n            3.17358202e-03, -6.52846992e-02,  8.59456360e-02],\n          [-6.14583818e-03,  4.85422462e-03,  5.14627509e-02, ...,\n           -2.23777443e-02, -1.03412592e-03, -8.19014851e-03],\n          [-1.00106997e-02, -1.59974187e-03, -1.14092128e-02, ...,\n           -3.87838250e-03, -3.99293639e-02,  1.13589503e-02],\n          ...,\n          [-1.68860350e-02,  5.18255346e-02, -2.66014934e-02, ...,\n           -1.99763244e-03,  4.35560057e-03, -4.49330434e-02],\n          [-1.34146223e-02,  6.32185815e-03, -7.33473897e-03, ...,\n            4.59014578e-03, -2.98611149e-02,  3.69221941e-02],\n          [-2.88858525e-02, -4.22423892e-03, -1.42897591e-02, ...,\n            1.11823296e-02, -7.52132908e-02,  1.67242344e-02]],\n \n         [[-1.02114283e-01, -5.26558608e-02, -1.00238100e-02, ...,\n           -1.37801170e-02, -4.94753271e-02,  1.25953421e-01],\n          [ 2.65349774e-03,  1.04075735e-02,  6.73149154e-02, ...,\n           -2.37589777e-02,  6.13968645e-04, -9.96601582e-03],\n          [-1.50703192e-02, -1.28871305e-02, -8.77845287e-03, ...,\n           -1.01640215e-02, -4.76290248e-02,  1.36717530e-02],\n          ...,\n          [-5.43789240e-03,  3.35888155e-02, -2.11151708e-02, ...,\n           -3.83130647e-03, -1.02301398e-02, -5.32924160e-02],\n          [-2.21891310e-02, -4.92077954e-02, -1.38867581e-02, ...,\n           -1.28568839e-02, -9.37007815e-02,  5.59309907e-02],\n          [-2.61166841e-02, -3.50148268e-02, -1.47816772e-02, ...,\n           -2.86548026e-03, -1.01101905e-01,  1.94419883e-02]],\n \n         [[ 2.73401570e-02, -3.54428887e-02,  1.18577422e-03, ...,\n            5.02943760e-04,  4.51298095e-02,  1.23520821e-01],\n          [ 9.48381796e-03,  1.12858564e-02,  5.32916747e-02, ...,\n           -1.81195941e-02, -1.31687860e-03, -1.01092532e-02],\n          [ 3.73495184e-02, -7.93821830e-03, -5.94039401e-03, ...,\n           -7.96878152e-03,  1.53274648e-02,  9.63651482e-03],\n          ...,\n          [-2.60413792e-02, -2.53635142e-02, -2.81382054e-02, ...,\n           -6.63971901e-03, -7.26283342e-03, -5.57855107e-02],\n          [ 2.67488584e-02, -4.73727398e-02, -3.06198676e-03, ...,\n           -4.82000969e-03,  4.08617556e-02,  6.99444860e-02],\n          [ 7.98859298e-02, -2.87372041e-02,  6.38093508e-04, ...,\n            9.73386108e-04,  4.21557203e-02,  1.60824433e-02]]]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32, numpy=\n array([-0.30912212,  0.36397225,  0.13737613,  0.07717966,  0.9052184 ,\n         0.08885256,  0.10789118, -0.23106739, -0.6318097 ,  0.18161367,\n        -0.33391494,  0.1961724 ,  0.43838617,  0.1938708 ,  0.10894354,\n         0.10315038, -1.0271513 ,  0.05252688,  0.13118458,  0.22851577,\n        -0.71377224,  0.2154155 , -0.6981962 ,  0.04061132,  0.13955347,\n         0.28767544,  0.35358745,  0.3937295 ,  0.43452853, -0.48259264,\n         0.02631121,  0.09220165,  0.05196398,  0.550705  ,  0.32368094,\n         0.07263482, -0.17745508,  0.3761972 ,  0.39344102,  0.17673127,\n        -0.15784228,  0.26051855,  0.08342359,  0.08603705, -0.09131282,\n         0.22932515,  0.2998315 ,  0.13511261, -0.3835829 ,  0.10912544,\n        -0.53432876,  0.4741787 , -0.0740848 ,  0.30046257,  0.12590808,\n         0.16480374,  0.2403943 ,  0.23401979, -0.19334187,  0.01663565,\n         0.27144948,  0.03385786,  0.06076292, -0.5825159 ], dtype=float32)&gt;,\n &lt;tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32, numpy=\n array([[[[-8.90296325e-03,  3.78169715e-02,  1.39756575e-02, ...,\n           -6.35457635e-02,  2.83470433e-02, -5.54975234e-02],\n          [-5.66543266e-03, -1.05560187e-03, -2.29203496e-02, ...,\n           -1.30956469e-03,  2.52478309e-02, -2.95403805e-02],\n          [-7.85139855e-03, -4.38382570e-03, -1.29586197e-02, ...,\n            1.22073088e-02,  4.46309976e-04, -2.49174051e-02],\n          ...,\n          [-5.82487276e-03,  3.78986611e-03, -1.48562957e-02, ...,\n            1.27910008e-03,  1.27594778e-02, -1.33825615e-02],\n          [-1.54342148e-02,  5.62145375e-03,  3.05225104e-02, ...,\n            1.61413271e-02, -4.20058556e-02, -3.35679092e-02],\n          [-4.21814434e-03,  2.95577031e-02,  2.57514068e-03, ...,\n           -2.59456299e-02, -1.35917868e-03, -5.85189313e-02]],\n \n         [[-1.03394873e-02,  3.75955850e-02,  6.87777717e-03, ...,\n           -5.84545359e-02,  3.90824042e-02,  3.70086096e-02],\n          [ 4.86768410e-03,  4.54719039e-03, -1.62224416e-02, ...,\n           -2.55017504e-02,  1.88396666e-02, -6.24489179e-03],\n          [-5.31206699e-03, -1.19874987e-03, -1.11892410e-02, ...,\n            1.98805016e-02, -1.90202110e-02, -3.79090838e-04],\n          ...,\n          [-6.00387994e-03,  7.28632743e-03, -1.53944585e-02, ...,\n           -5.26854116e-03,  1.68532860e-02,  3.76290455e-03],\n          [-1.78998355e-02,  1.46924695e-02,  9.51688085e-03, ...,\n            1.07260697e-01,  2.80233864e-02, -1.89538375e-02],\n          [-3.70884268e-03,  3.08528487e-02, -5.70769282e-03, ...,\n           -3.34496573e-02, -4.65932442e-03,  1.17018344e-02]],\n \n         [[-1.01951612e-02,  1.21747078e-02,  1.11724688e-02, ...,\n            2.69737188e-02, -2.22921986e-02,  6.45900667e-02],\n          [ 1.22869862e-02,  6.45142654e-03, -3.07416776e-03, ...,\n           -1.61525756e-02, -2.23605409e-02,  1.09485257e-02],\n          [-7.09734391e-03, -5.40411752e-03, -9.43870191e-03, ...,\n           -1.52021297e-03, -2.17991360e-02, -3.77143058e-03],\n          ...,\n          [-5.21149486e-03,  1.03359867e-03, -7.52882753e-03, ...,\n           -1.92244481e-02, -6.39424520e-03,  3.69722745e-03],\n          [-1.53886806e-02, -1.34543087e-02,  5.72274020e-03, ...,\n            5.47401682e-02, -6.08180426e-02, -2.23444626e-02],\n          [-3.45917395e-03,  2.13373415e-02, -2.14421172e-02, ...,\n           -6.91150427e-02, -1.54741071e-02,  4.44306619e-02]]],\n \n \n        [[[-1.23298699e-02, -1.91225596e-02,  1.29218930e-02, ...,\n            4.33554836e-02,  1.26128104e-02, -5.96396960e-02],\n          [-1.31860480e-03, -9.51261446e-03, -1.85527243e-02, ...,\n            2.88356710e-02,  2.38556787e-02, -4.06758748e-02],\n          [-4.25214088e-03, -6.78345608e-03, -1.38743157e-02, ...,\n            2.22070217e-02,  1.34524666e-02, -2.03017425e-02],\n          ...,\n          [-7.83586409e-03, -2.57805572e-03, -1.08931847e-02, ...,\n            1.92594435e-02,  1.59353893e-02, -1.89792961e-02],\n          [-1.94495711e-02, -3.60343307e-02,  6.04022592e-02, ...,\n           -2.22710613e-02, -5.43530397e-02, -4.17403802e-02],\n          [-4.36492451e-03, -1.08945146e-02, -3.35957273e-03, ...,\n            1.72520187e-02, -1.28518986e-02, -4.72729132e-02]],\n \n         [[-1.45433825e-02, -3.68030928e-02,  3.51226889e-02, ...,\n            5.09550609e-03, -8.08145478e-02,  1.02682762e-01],\n          [ 7.81208277e-03, -1.63471382e-02, -8.99212994e-03, ...,\n           -3.65188569e-02, -8.37247167e-03, -7.34759448e-03],\n          [-1.76124368e-03, -4.82094986e-03, -1.33856600e-02, ...,\n            1.97411682e-02, -6.24130992e-03,  2.11908761e-03],\n          ...,\n          [-8.09418783e-03, -5.44183236e-03, -9.95591376e-03, ...,\n            1.03512814e-03, -1.01502263e-03,  9.89024434e-03],\n          [-2.14152560e-02, -1.22487647e-02,  4.47364189e-02, ...,\n            1.18361183e-01, -1.86819024e-02,  5.11289574e-02],\n          [-3.61343310e-03, -1.55030079e-02, -1.55327516e-02, ...,\n            2.49069761e-02, -6.95429044e-03,  1.30494153e-02]],\n \n         [[-1.17718522e-02, -2.43640933e-02,  2.16566566e-02, ...,\n            1.52593330e-01, -1.00578181e-02,  8.74510705e-02],\n          [ 1.56845041e-02, -1.70876738e-02, -1.32727087e-03, ...,\n            3.55747603e-02, -4.06013057e-02,  2.82184500e-02],\n          [-3.66246584e-03, -9.08438675e-03, -8.76258500e-03, ...,\n           -7.32058019e-04, -1.63256414e-02,  8.90959927e-04],\n          ...,\n          [-6.92374213e-03, -1.25479233e-02, -7.42880348e-03, ...,\n            3.50585720e-03, -2.07631718e-02,  1.39397075e-02],\n          [-1.86536163e-02, -9.74296406e-03, -1.49605959e-03, ...,\n           -1.41743785e-02, -1.41348932e-02,  5.55095226e-02],\n          [-1.87194371e-03, -7.81994499e-03, -1.14830295e-02, ...,\n           -5.49505465e-02, -8.39654077e-03,  3.37504521e-02]]],\n \n \n        [[[-7.51168234e-03, -1.57704763e-02,  4.40593623e-03, ...,\n            8.08628425e-02,  1.54139521e-02, -3.71525325e-02],\n          [ 3.67319165e-03, -4.84256167e-03, -9.42679588e-03, ...,\n            1.78013276e-02,  1.10012209e-02, -3.05487644e-02],\n          [-5.12040500e-03, -2.92832567e-03, -1.70742609e-02, ...,\n           -1.19602494e-03,  2.55921856e-02, -2.38123816e-02],\n          ...,\n          [-6.98121730e-03, -5.92697598e-03, -4.53395769e-03, ...,\n            1.43473689e-02,  1.32299559e-02, -1.86933577e-02],\n          [-1.64366160e-02, -2.88906470e-02,  5.16189029e-03, ...,\n            6.27482450e-03,  3.09459344e-02, -3.86791453e-02],\n          [ 7.87568372e-03,  3.53289805e-02, -8.80536530e-03, ...,\n            1.03661101e-02,  1.50152808e-02, -2.25142129e-02]],\n \n         [[-8.03535990e-03, -6.21744758e-03,  3.11630405e-02, ...,\n            1.99134022e-01, -6.17154641e-03,  1.39599340e-02],\n          [ 1.37082441e-02, -4.04076884e-03, -9.80419107e-03, ...,\n            5.20565063e-02,  6.79946598e-03, -1.67599937e-04],\n          [-2.68182647e-03,  9.37074132e-04, -1.77672971e-02, ...,\n            7.02630170e-03,  1.19604953e-02, -2.38061114e-03],\n          ...,\n          [-6.67789765e-03, -8.06673337e-03, -4.78939526e-03, ...,\n            2.99814232e-02,  3.63594829e-03,  6.42562099e-03],\n          [-2.00417619e-02, -5.05231880e-02,  2.04375144e-02, ...,\n           -9.16022658e-02, -2.39189416e-02,  5.67581393e-02],\n          [ 9.37450770e-03,  4.14647050e-02, -1.89982001e-02, ...,\n            9.74066462e-03,  2.91863047e-02, -2.41922438e-02]],\n \n         [[-7.34003959e-03,  7.00018462e-03,  1.78023316e-02, ...,\n            4.10795175e-02,  1.60309300e-02, -6.50887610e-03],\n          [ 1.98623370e-02, -4.18649800e-03, -1.23496391e-02, ...,\n           -1.08256517e-03, -6.54376950e-03,  1.12769182e-03],\n          [-4.14572377e-03, -3.37855937e-03, -1.22076506e-02, ...,\n           -1.23778330e-02,  1.39588572e-03, -7.97146652e-03],\n          ...,\n          [-5.31869614e-03, -1.09051373e-02, -5.76505670e-03, ...,\n           -1.08993258e-02, -4.60993918e-03, -3.81152914e-03],\n          [-1.69911478e-02, -3.15449387e-02,  8.09636060e-03, ...,\n           -1.06819831e-02, -2.92898342e-02,  7.60448575e-02],\n          [ 9.20531712e-03,  3.90281789e-02, -1.15968632e-02, ...,\n           -2.66397055e-02,  2.04669069e-02, -8.92718323e-03]]]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32, numpy=\n array([-4.5777660e-02,  6.4407811e-02, -3.1368658e-02,  8.6917542e-02,\n         1.2785070e-01,  2.4616894e-01,  1.6408961e-01,  2.3979376e-01,\n         2.2164847e-01,  1.4246197e-01,  3.3131771e-02,  1.4955571e-01,\n         1.1550413e-02, -1.8516029e-04,  2.7014315e-01,  2.4461022e-01,\n         2.7350754e-01,  2.7142024e-01,  2.7011135e-01,  2.7027962e-01,\n         2.2164130e-01,  1.8953906e-01,  3.9793090e-03,  6.1518628e-02,\n         3.6547425e-01,  2.1779412e-01,  1.4007972e-01,  2.2691796e-02,\n        -1.4714201e-02,  6.5621443e-02,  3.1406230e-01, -7.7050015e-02,\n         1.0462435e-01,  2.0993298e-01, -7.4100301e-02, -8.5880190e-02,\n         1.3652268e-01,  2.3663981e-01,  3.0105148e-02, -3.6816043e-03,\n         7.6591093e-03,  3.0664349e-01,  1.3023020e-01,  4.1607008e-03,\n         2.7055809e-01,  2.5207508e-01,  2.8904697e-01,  1.2730265e-01,\n         2.8076398e-01, -4.4764560e-02, -4.2357225e-02,  6.2559925e-02,\n         1.0018661e-01, -9.9623539e-02,  1.0176195e-01,  2.3699294e-01,\n         7.8422479e-02,  1.2831993e-01,  1.1392979e-01, -1.3174531e-02,\n        -1.0383477e-01,  2.2861110e-01,  9.1953836e-02,  6.5147914e-02,\n         1.9676815e-01,  7.1536556e-02,  2.6198325e-01, -6.4189523e-02,\n         4.9607549e-02, -6.1662562e-02,  1.3248019e-01,  9.0231813e-02,\n         1.0324936e-01,  2.2680637e-01, -3.3503436e-02, -1.7922063e-01,\n         2.0180932e-01,  5.2303892e-02,  1.7740984e-01,  2.1679863e-01,\n         1.2819616e-01,  1.6143866e-01,  1.6376132e-01,  3.5884404e-01,\n        -5.2223109e-02,  2.1117201e-01,  1.4626507e-01,  1.0392818e-01,\n         2.3689806e-01, -1.7589157e-02,  1.4187863e-01, -1.5676373e-01,\n         5.5077035e-02,  1.7919958e-01,  1.9530067e-01,  4.9757544e-02,\n         1.4647865e-01,  1.5057972e-01,  4.9321059e-02,  2.2425601e-01,\n         2.8004406e-02,  4.6344060e-02,  5.4642409e-02,  3.2176018e-01,\n        -2.3094574e-02,  5.1442902e-03,  2.2401743e-02,  1.9448744e-01,\n        -9.5472252e-03,  2.5650701e-01,  1.3772449e-01,  2.8766793e-01,\n        -9.4692776e-04, -8.9352727e-03,  7.2267972e-02,  1.9602686e-02,\n         2.5652054e-01, -3.6785334e-02,  3.2890160e-02,  2.6736060e-01,\n        -1.0560317e-01,  3.7151142e-03,  2.8068131e-01, -8.9383557e-02,\n         6.6791281e-02,  3.1667921e-01,  9.5985740e-02,  1.3152032e-01],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32, numpy=\n array([[[[-2.30123661e-03, -5.89181297e-03,  3.96523578e-03, ...,\n            8.69934738e-04,  4.31257114e-03,  2.39815353e-03],\n          [ 6.93455897e-03,  1.77669781e-03,  1.87194608e-02, ...,\n           -3.17397565e-02, -4.21442185e-03, -1.76308881e-02],\n          [-1.27900802e-02, -6.50780788e-03, -4.10286430e-03, ...,\n            2.31482647e-03, -4.67822663e-02, -3.75668402e-03],\n          ...,\n          [-4.75286070e-04, -2.96283532e-02,  3.03538516e-02, ...,\n            1.51648500e-03, -2.71695852e-02, -1.84430566e-03],\n          [ 1.41743217e-02, -3.64480889e-04, -2.33754283e-03, ...,\n           -2.28237733e-02, -1.60014853e-02,  2.46631019e-02],\n          [ 1.18608214e-02, -2.77200472e-02, -2.48606838e-02, ...,\n            4.10957001e-02,  5.57063818e-02, -2.19316985e-02]],\n \n         [[-1.38294604e-03, -5.54690696e-03, -1.06138433e-03, ...,\n            1.82250992e-03, -3.59015947e-04,  9.99924168e-03],\n          [ 6.75601466e-03,  1.10390119e-03,  2.18637586e-02, ...,\n           -3.56409214e-02, -2.45026941e-03, -6.04407070e-03],\n          [-1.35804852e-02, -8.60609580e-03,  1.28111104e-02, ...,\n           -1.01041503e-03, -3.83789763e-02,  1.18479142e-02],\n          ...,\n          [-3.61973490e-03,  4.19556797e-02,  5.89246452e-02, ...,\n            3.10248807e-02, -1.03577944e-02, -2.23138258e-02],\n          [ 1.48718422e-02,  1.86822154e-02, -3.46057713e-02, ...,\n           -2.32062247e-02, -1.91165209e-02,  8.65985733e-03],\n          [-5.93924429e-03,  1.08361673e-02,  2.81376876e-02, ...,\n            1.17414417e-02,  8.40279087e-02, -6.45612031e-02]],\n \n         [[-9.91582056e-04,  4.55033936e-04, -9.36682150e-03, ...,\n            3.41384741e-03, -5.51316887e-03,  1.24911871e-02],\n          [ 1.07221371e-02, -3.19505017e-03,  2.47716717e-02, ...,\n           -1.63276494e-02,  8.99969228e-03, -3.90315428e-03],\n          [ 4.23295191e-03,  1.27883134e-02,  1.17508639e-02, ...,\n           -1.22649251e-02,  2.91934842e-03,  1.42700337e-02],\n          ...,\n          [ 2.98261386e-03,  4.09221351e-02,  2.68427730e-02, ...,\n            9.12894122e-03,  2.94868927e-02, -3.27900308e-03],\n          [ 8.43982119e-03, -1.85558088e-02,  1.26812793e-02, ...,\n           -9.75666288e-03,  6.52654376e-03, -5.94963925e-03],\n          [-3.94721312e-04,  6.81950822e-02,  8.70230980e-03, ...,\n            1.19850151e-02,  9.72716659e-02, -2.87150545e-03]]],\n \n \n        [[[-6.72967779e-03, -6.32101856e-03,  1.24791730e-03, ...,\n           -5.51869674e-03,  5.54611022e-03,  2.74169119e-03],\n          [-1.90283672e-03,  2.39737774e-03,  7.45097920e-03, ...,\n           -6.27250373e-02, -1.16790943e-02,  1.36177419e-02],\n          [-2.43349690e-02, -9.58706811e-03,  5.12539688e-03, ...,\n            2.03479634e-04, -5.18149063e-02, -1.06236124e-02],\n          ...,\n          [-8.23690323e-04, -4.52437773e-02,  4.49064933e-02, ...,\n           -1.48973670e-02, -3.74213159e-02, -2.50751078e-02],\n          [ 1.20533947e-02, -4.14178381e-03, -2.23253462e-02, ...,\n           -8.56044441e-02, -2.15003770e-02,  5.37014790e-02],\n          [ 1.72066651e-02, -2.44826339e-02, -1.10037383e-02, ...,\n            4.80076745e-02,  3.21536772e-02, -2.96083037e-02]],\n \n         [[-4.56147827e-03, -4.65280237e-03, -8.31684005e-03, ...,\n           -5.54831931e-03,  1.18862418e-03,  9.64665227e-03],\n          [ 2.62077316e-03,  5.77449473e-03,  1.19720455e-02, ...,\n           -6.83966503e-02,  1.35026115e-03,  5.91752259e-03],\n          [-2.97386330e-02, -3.90352937e-03,  1.28964186e-02, ...,\n            6.27378060e-04, -4.46732230e-02,  1.30419582e-02],\n          ...,\n          [-2.97931256e-03,  7.04262778e-02,  4.67705168e-02, ...,\n           -2.63439678e-02, -2.75591165e-02, -3.54478620e-02],\n          [ 9.99665074e-03,  1.62281040e-02,  8.01674929e-03, ...,\n           -7.52160326e-02, -1.95330698e-02,  7.38804974e-03],\n          [ 9.53063485e-04,  5.06784841e-02,  2.04059742e-02, ...,\n            4.51842928e-03,  4.47761454e-02, -7.42928088e-02]],\n \n         [[-2.81608710e-03, -3.68988403e-04, -9.52884182e-03, ...,\n           -1.62106077e-03, -4.23267297e-03,  1.35960653e-02],\n          [ 7.26349046e-03,  3.17442766e-03,  2.06175316e-02, ...,\n           -3.99732366e-02,  2.22933367e-02, -7.41374539e-03],\n          [-1.37994336e-02,  2.29467135e-02, -6.29038131e-03, ...,\n           -5.79428207e-03, -1.50836250e-02,  1.31318215e-02],\n          ...,\n          [ 2.00410513e-03,  3.91520634e-02, -3.18683162e-02, ...,\n           -3.54397879e-03, -8.54784995e-03,  1.36236998e-03],\n          [ 5.49011305e-03, -1.51369004e-02,  2.77213678e-02, ...,\n           -2.81145964e-02,  9.72870039e-04, -6.28185738e-03],\n          [ 7.10674515e-03,  8.02865252e-02,  6.19208254e-03, ...,\n            1.03570186e-02,  7.63423666e-02, -7.15952739e-03]]],\n \n \n        [[[-1.08065400e-02, -7.45501043e-03, -5.10151871e-03, ...,\n           -4.49332688e-03,  7.89736584e-03, -3.35464210e-05],\n          [-6.12053089e-03, -1.79325249e-02, -1.11700445e-02, ...,\n           -4.62095961e-02, -4.28734301e-03,  2.02734675e-03],\n          [-2.64673289e-02, -1.31910611e-02,  1.76633168e-02, ...,\n           -6.08402630e-03, -3.14402916e-02, -1.81844179e-02],\n          ...,\n          [ 1.37444083e-02, -2.84396205e-02,  1.88047513e-02, ...,\n           -1.93887874e-02, -3.75849679e-02, -2.45505963e-02],\n          [ 7.21093081e-03, -1.23673957e-02, -5.60066104e-03, ...,\n           -4.09601741e-02, -1.16450675e-02,  3.63268666e-02],\n          [ 1.34990485e-02, -1.55054999e-03,  5.54364361e-03, ...,\n            3.92058305e-02,  2.16900297e-02, -1.13665536e-02]],\n \n         [[-8.32169224e-03, -6.18967554e-03, -1.13438964e-02, ...,\n           -3.40542640e-03,  3.85991205e-03,  7.26305228e-03],\n          [-2.55200220e-03, -1.41635584e-02, -3.80055211e-03, ...,\n           -5.45916855e-02,  2.17752811e-03, -5.23553835e-03],\n          [-3.21345851e-02, -2.83276942e-03, -2.20403913e-03, ...,\n           -1.67740230e-02, -3.22663859e-02,  2.23135133e-03],\n          ...,\n          [ 3.27714277e-03,  8.55310634e-02, -2.84323972e-02, ...,\n            2.72681676e-02, -3.39798741e-02, -1.60314962e-02],\n          [ 3.99958156e-03,  1.23148263e-02,  3.01949140e-02, ...,\n           -5.21975756e-02, -1.14355180e-02, -7.70385157e-07],\n          [-1.03606293e-02,  7.14352727e-02,  1.61257926e-02, ...,\n           -1.47721104e-04,  9.00403503e-03, -3.94313596e-02]],\n \n         [[-5.75947622e-03,  4.07667161e-04, -5.18753566e-03, ...,\n            8.14088504e-04, -4.34151705e-04,  1.14657767e-02],\n          [ 4.41048900e-03, -3.57159856e-03,  3.01857502e-03, ...,\n           -3.98184508e-02,  1.49278138e-02, -8.20163079e-03],\n          [-2.44483184e-02,  2.32945569e-02, -2.34904625e-02, ...,\n           -1.79554094e-02, -3.21781076e-02,  1.07798241e-02],\n          ...,\n          [ 5.12963720e-03,  2.35215593e-02, -3.53378877e-02, ...,\n            3.48278917e-02, -2.01826524e-02,  2.35197158e-03],\n          [ 3.45730968e-03, -1.26461498e-02,  1.03930170e-02, ...,\n           -3.20292413e-02, -6.37839548e-04, -7.48459855e-03],\n          [ 3.67492763e-03,  6.75461590e-02,  9.46969260e-03, ...,\n            4.72220499e-03,  4.39464301e-02,  6.21765293e-03]]]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32, numpy=\n array([-0.06418542, -0.11568324, -0.13206749,  0.0608536 ,  0.2689488 ,\n        -0.10177708,  0.05753241, -0.0253701 ,  0.02453307, -0.16847736,\n         0.1316739 , -0.01181007, -0.27968737, -0.01710657,  0.14509922,\n        -0.2100034 ,  0.09921633, -0.01273028, -0.10428324,  0.11836739,\n        -0.21125603, -0.22762309,  0.44482312,  0.04079875, -0.30996218,\n        -0.03813913, -0.2617242 ,  0.04896495,  0.14684829, -0.09759409,\n        -0.16100882, -0.08115347,  0.2567695 ,  0.09040406,  0.03538936,\n         0.23496653, -0.59533477,  0.280156  ,  0.02269722,  0.15407647,\n         0.10761564,  0.15549968, -0.27726585,  0.14555272, -0.07341251,\n         0.0086478 , -0.08294716,  0.16158143, -0.17374775, -0.00751741,\n        -0.2599541 ,  0.0932769 ,  0.35911047, -0.09218857,  0.08189061,\n        -0.24053718, -0.12922512, -0.01862046, -0.11662732, -0.11673097,\n        -0.1816601 ,  0.16486758, -0.04786587, -0.04397993,  0.14281501,\n         0.26927033,  0.3568714 ,  0.03537106, -0.17966093,  0.04918984,\n         0.63375777, -0.20748404, -0.06041681,  0.12094544, -0.08843092,\n        -0.30141872,  0.28632042,  0.10882995,  0.14509913, -0.04585594,\n        -0.08846492,  0.13556598,  0.16782367,  0.07018866, -0.10737082,\n         0.44746682, -0.08115204, -0.04683498,  0.18236732, -0.01913908,\n        -0.0207867 ,  0.37967235,  0.07472062, -0.09300313,  0.3070309 ,\n         0.01550341,  0.15720516, -0.18576111,  0.07595339, -0.28695267,\n         0.20711343, -0.22942008,  0.17479539,  0.2029741 , -0.44217134,\n         0.04695964, -0.14719886, -0.02473967,  0.14871362,  0.00526441,\n         0.00490166, -0.17585608, -0.00920897, -0.25866592, -0.01254782,\n         0.04957367,  0.173761  , -0.01913427, -0.1067858 ,  0.01878852,\n        -0.08317517,  0.06669547,  0.01067064,  0.37834755,  0.11717828,\n         0.20528455,  0.2403423 ,  0.27507326], dtype=float32)&gt;,\n &lt;tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32, numpy=\n array([[[[ 2.48631509e-03,  3.29096033e-03, -3.13071609e-02, ...,\n            4.71851649e-03,  6.10046380e-04,  1.08059337e-02],\n          [ 1.36040282e-02,  3.80906137e-03, -3.50438058e-02, ...,\n            8.50365497e-03,  4.99894470e-03,  2.45373725e-04],\n          [-1.02960337e-02,  1.11045251e-02, -1.05560301e-02, ...,\n            1.18119251e-02,  4.14630165e-03,  1.04994141e-02],\n          ...,\n          [ 2.14050314e-03,  2.17348989e-03, -4.09283377e-02, ...,\n           -8.34189355e-03,  2.35738587e-02, -1.85471829e-02],\n          [ 1.61261633e-02,  8.04742594e-05, -3.95269990e-02, ...,\n           -3.94453527e-03,  8.42030905e-03, -6.98040065e-04],\n          [-3.51465750e-03,  1.08316056e-02, -2.23830044e-02, ...,\n            1.92155335e-02,  4.21972945e-03, -1.55707300e-02]],\n \n         [[ 6.11093594e-03,  5.44822821e-03,  1.57541838e-02, ...,\n            5.60781872e-03,  1.93978904e-03,  1.49202365e-02],\n          [-3.13426033e-02,  1.45718846e-02,  1.18744513e-02, ...,\n            1.21863568e-02,  1.49443159e-02,  2.60730181e-03],\n          [-2.32275911e-02,  2.08911151e-02,  1.11537259e-02, ...,\n            2.36150771e-02,  1.06090466e-02,  4.31246823e-03],\n          ...,\n          [ 2.83093913e-03,  5.63543057e-03,  1.72611587e-02, ...,\n           -2.76210159e-03,  3.27554196e-02, -1.73731633e-02],\n          [ 4.87203449e-02,  1.24137765e-02,  3.64911221e-02, ...,\n            9.30428132e-03,  5.34809241e-03,  6.80550281e-03],\n          [-2.03473382e-02,  1.38210657e-03, -2.35674269e-02, ...,\n            3.24176252e-02,  2.19412390e-02, -3.02630980e-02]],\n \n         [[ 1.05408970e-02,  7.44795799e-03, -3.36561389e-02, ...,\n            2.62853602e-04, -6.13683043e-03,  1.33312661e-02],\n          [-3.32584940e-02,  1.55691411e-02, -2.89147953e-04, ...,\n           -3.60562908e-03,  1.49507467e-02,  7.65243312e-03],\n          [ 3.94205609e-03,  1.48937441e-02, -1.59246754e-02, ...,\n            1.39236227e-02,  3.22588254e-03,  6.02472946e-03],\n          ...,\n          [ 9.61219659e-04, -2.27980153e-03, -4.54332307e-02, ...,\n           -8.10870901e-03,  3.84118455e-03, -1.65238492e-02],\n          [ 2.77294349e-02,  1.70650017e-02,  2.01114286e-02, ...,\n            8.16452038e-03,  4.37149359e-03,  8.13044887e-03],\n          [-1.77335721e-02,  1.02985008e-02, -3.90648767e-02, ...,\n            2.65211053e-02,  1.88891459e-02, -1.68677885e-02]]],\n \n \n        [[[ 4.49297950e-03, -3.98075627e-03,  1.72210410e-02, ...,\n            8.42886325e-03, -1.04969469e-04,  5.74578904e-03],\n          [-4.00591874e-03, -2.41769738e-02, -1.10928714e-02, ...,\n           -1.52679645e-02,  6.81134174e-03, -3.05488333e-03],\n          [-1.56376939e-02, -5.00010047e-03,  4.81709233e-03, ...,\n            5.51846065e-03,  4.62341262e-03,  5.28823584e-03],\n          ...,\n          [ 3.78267164e-03, -7.20065041e-03,  2.24571675e-02, ...,\n            1.21734152e-02,  1.66180804e-02, -2.85227131e-02],\n          [ 3.54970875e-03, -7.91174546e-03,  6.50567329e-03, ...,\n            5.75853884e-03, -5.25432348e-04, -3.29109584e-03],\n          [ 2.42216233e-03, -1.42111657e-02,  3.40532698e-02, ...,\n           -8.09886493e-03,  6.40509045e-03, -2.32355390e-02]],\n \n         [[ 6.58771209e-03, -3.84311262e-03,  6.62338808e-02, ...,\n            5.46263764e-03,  2.69173388e-03,  9.38945077e-03],\n          [-5.54355457e-02, -2.00809669e-02,  3.39177996e-02, ...,\n           -2.65794396e-02,  7.77757354e-03,  1.45330857e-02],\n          [-5.56085445e-03, -1.38856005e-02,  3.95981222e-02, ...,\n           -7.25830847e-04,  4.43886919e-03,  2.62149167e-03],\n          ...,\n          [ 2.70528113e-03, -5.45389997e-03,  8.94473717e-02, ...,\n            2.45114118e-02,  5.10585234e-02, -3.43545228e-02],\n          [ 5.07756323e-02,  1.51661120e-03,  8.18592831e-02, ...,\n            2.16385573e-02,  1.26126707e-02,  1.81507389e-03],\n          [-2.27336828e-02, -3.00953630e-02,  3.85253243e-02, ...,\n           -9.51036997e-03,  8.18072818e-03, -3.70927304e-02]],\n \n         [[ 5.95341111e-03,  8.94323515e-04,  1.05835460e-02, ...,\n            4.89834580e-04,  5.24436589e-04,  9.38476902e-03],\n          [-4.07811590e-02,  4.78578452e-03,  1.44789219e-02, ...,\n           -5.62724238e-03, -4.85808152e-04,  1.29066659e-02],\n          [ 1.34129170e-02, -1.18021872e-02,  4.19147732e-03, ...,\n           -8.13282002e-03, -4.46207495e-03,  8.42996407e-03],\n          ...,\n          [ 1.52341533e-03, -6.47965074e-03,  1.00679537e-02, ...,\n            1.56302936e-02,  3.72295864e-02, -3.21733207e-02],\n          [ 3.96381617e-02,  1.58769619e-02,  3.92291285e-02, ...,\n            1.72687024e-02,  2.41979603e-02,  5.91078307e-03],\n          [-1.79783162e-02, -2.94427648e-02,  1.28269298e-02, ...,\n           -1.60066597e-02, -5.16920770e-03, -2.00248957e-02]]],\n \n \n        [[[ 4.85232845e-03, -2.46043317e-03, -9.62745398e-03, ...,\n            1.07441526e-02, -2.84585054e-03, -1.90984423e-03],\n          [-1.19528649e-02, -1.80358142e-02, -3.36395353e-02, ...,\n           -1.47682996e-02,  1.02861486e-02, -3.36993136e-03],\n          [ 4.98365518e-03, -1.66885350e-02, -1.20581677e-02, ...,\n           -1.59126818e-02,  1.31858326e-02,  2.94562749e-04],\n          ...,\n          [-5.69525547e-03, -1.74089391e-02, -3.64525653e-02, ...,\n            7.06359418e-03, -1.58607736e-02, -2.61105206e-02],\n          [-3.87686165e-03, -2.40983404e-02, -5.60978092e-02, ...,\n           -1.22877983e-02, -3.04629817e-03, -1.03539054e-03],\n          [ 1.35685736e-02, -2.09947508e-02, -1.52395582e-02, ...,\n           -2.87993886e-02,  1.74087044e-02, -1.86969731e-02]],\n \n         [[ 1.45290594e-03, -6.42601680e-03,  1.14819771e-02, ...,\n            1.17859095e-02,  8.51634599e-04, -2.82667065e-03],\n          [-2.43061744e-02, -1.82640739e-02,  1.50347399e-02, ...,\n           -2.63812207e-02,  2.30616182e-02,  4.71212249e-03],\n          [ 1.60123650e-02, -1.25727728e-02,  1.22705530e-02, ...,\n           -1.90321188e-02,  1.43274162e-02, -9.16460936e-04],\n          ...,\n          [ 2.31739320e-03, -1.62090212e-02,  1.71782896e-02, ...,\n            1.23864021e-02,  7.53226131e-03, -2.98976954e-02],\n          [ 1.98312830e-02, -1.12192128e-02,  1.70756541e-02, ...,\n           -1.62761565e-02, -2.72694766e-03, -1.83170743e-03],\n          [-3.89542966e-03, -3.53942066e-02, -1.64013300e-02, ...,\n           -4.73240092e-02,  1.97671168e-02, -2.87492648e-02]],\n \n         [[ 1.00636412e-03, -5.19909943e-03, -3.75667326e-02, ...,\n            7.44984252e-03,  4.31809276e-05, -3.22302600e-04],\n          [-1.04536200e-02, -9.70086199e-04,  1.94423497e-02, ...,\n           -1.29293650e-04,  9.82195791e-03, -1.07756234e-03],\n          [ 5.58500132e-03, -1.86028529e-03, -8.09577946e-03, ...,\n           -6.18272461e-03,  1.71643522e-04,  7.06695300e-03],\n          ...,\n          [-3.72283580e-03, -1.86357144e-02, -4.54351157e-02, ...,\n            1.30783571e-02,  3.22722102e-04, -3.35289426e-02],\n          [ 1.59978885e-02,  8.11644178e-03, -1.23626338e-02, ...,\n            2.87816068e-03,  2.04773736e-03, -6.40190148e-04],\n          [-4.09534993e-03, -3.75131555e-02, -3.33721489e-02, ...,\n           -4.82492335e-02,  3.41874664e-03, -1.31510263e-02]]]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32, numpy=\n array([-7.43588656e-02, -7.82142300e-03,  1.99858069e-01,  5.86250983e-02,\n         2.52379458e-02,  1.12540629e-02,  1.31672308e-01,  9.02242810e-02,\n         1.49592683e-01, -3.50398421e-02,  6.14030147e-03, -8.11117887e-03,\n        -5.05330227e-03, -1.01226144e-01, -5.19916452e-02,  1.39830606e-02,\n         1.07287608e-01,  1.85816567e-02, -8.91413689e-02,  9.85322371e-02,\n         1.40273839e-01, -2.21021902e-02,  2.16904543e-02, -8.71320534e-03,\n         4.80595827e-02,  4.82086278e-02,  1.46308407e-01,  1.21595077e-02,\n        -3.28533053e-02,  8.22634995e-02, -7.40040168e-02,  7.76286274e-02,\n         3.04200482e-02, -2.72025205e-02,  1.55593306e-02,  1.18354801e-03,\n        -9.05257389e-02, -1.03295766e-01, -3.65283228e-02, -5.99550568e-02,\n         1.17646568e-01,  2.00315118e-02, -1.71818864e-02,  3.41015868e-02,\n        -6.83562905e-02,  3.90349738e-02,  1.33812602e-03, -5.84527552e-02,\n        -1.86945014e-02, -3.10241263e-02, -5.08952476e-02,  1.03504502e-03,\n         1.68015808e-02,  6.09565713e-02, -5.96898086e-02,  7.87034333e-02,\n         1.27807269e-02,  8.19958001e-02, -3.29079060e-03,  1.05128504e-01,\n        -1.42537421e-02,  5.01402766e-02,  3.35593562e-04,  7.92880729e-02,\n        -4.18967754e-02,  8.43806267e-02, -2.04840805e-02, -5.14604710e-03,\n         5.63466437e-02,  4.63298373e-02,  4.95597087e-02, -6.12193942e-02,\n         1.22491747e-01,  6.75263703e-02,  4.25784215e-02,  1.58944473e-01,\n         3.33125927e-02, -3.51674929e-02,  2.65883021e-02,  1.72785539e-02,\n        -1.55145142e-04, -2.77894381e-02, -5.92041761e-02,  7.75184184e-02,\n         1.10869654e-01,  5.67513518e-02,  2.83265803e-02, -1.53156728e-01,\n        -4.65256162e-02, -2.83089001e-02, -2.22908221e-02,  1.96909793e-02,\n         5.58021385e-03,  4.27054390e-02, -1.02149777e-01,  1.14883122e-03,\n        -9.66805816e-02, -8.41323435e-02,  6.23559691e-02,  1.09561108e-01,\n         7.00691808e-03, -5.50029874e-02, -4.91849408e-02, -1.76147781e-02,\n        -2.40004361e-02,  8.58763605e-02,  5.70180900e-02,  1.39770836e-01,\n        -9.41964015e-02,  1.72558725e-01,  1.54925585e-02, -2.44386829e-02,\n         5.99474199e-02,  8.23580176e-02,  1.26813375e-03, -6.25416916e-03,\n         1.58674151e-01,  5.76364733e-02,  1.74609572e-02,  1.49641156e-01,\n        -8.72500893e-03, -2.41600052e-02,  4.93557611e-03,  7.58234710e-02,\n        -1.68748066e-01, -4.25629057e-02, -5.20474203e-02, -1.54869869e-01,\n        -6.91577718e-02, -7.06267878e-02,  4.56875004e-02, -3.20760012e-02,\n        -2.00978965e-01, -1.41668320e-01,  7.83928782e-02,  3.81931625e-02,\n        -8.14075861e-03, -3.03441286e-02, -2.73723528e-03,  1.04369465e-02,\n         3.72486524e-02, -7.18454318e-03,  7.87754655e-02,  2.32973583e-02,\n         4.12695780e-02,  1.18990578e-01,  7.35382959e-02,  5.69657655e-03,\n         9.87366810e-02,  1.87426712e-02,  1.03674002e-01, -6.63928641e-03,\n        -2.62002982e-02,  3.02988961e-02,  1.46475844e-02,  9.10094008e-02,\n        -7.52718672e-02,  2.63437424e-02,  1.66083038e-01, -4.93965335e-02,\n        -9.30522976e-04,  3.64395380e-02, -4.38934527e-02,  6.48857951e-02,\n        -2.94738077e-02,  9.18932706e-02, -3.09058055e-02,  2.83562243e-02,\n         2.82770721e-03, -3.29918638e-02,  5.85343279e-02, -5.04371524e-02,\n        -3.62766236e-02, -4.32307646e-02, -6.57807514e-02, -3.66672277e-02,\n        -2.41919458e-02, -6.04458451e-02,  5.60694151e-02,  3.48671922e-03,\n         1.31377563e-01, -5.67615330e-02, -3.71594913e-02,  3.77775840e-02,\n         1.12982266e-01,  4.22195755e-02,  6.63153529e-02, -4.67135347e-02,\n        -9.42966118e-02, -9.68727842e-03, -1.33719174e-02,  1.70798182e-01,\n        -2.22101770e-02, -1.39698302e-02,  9.83864516e-02,  4.94368449e-02,\n         7.70076737e-02,  1.01149976e-02, -9.57828313e-02,  4.99044210e-02,\n        -1.55091844e-02,  8.29517245e-02,  1.27148554e-02, -2.46906392e-02,\n         6.82564229e-02,  9.56533104e-02,  2.05127731e-01, -1.88522916e-02,\n        -6.21608971e-03,  3.39489020e-02, -1.58346687e-02,  6.97900355e-02,\n         9.69509557e-02,  1.38234586e-01,  4.95215319e-03,  5.39944358e-02,\n        -9.61965974e-03,  4.53798585e-02,  1.25275165e-01,  6.28290139e-03,\n         1.96305774e-02, -1.46517288e-02, -4.26939204e-02, -5.98763116e-02,\n        -4.93083969e-02, -9.13676899e-03,  7.50048086e-02,  1.19404271e-01,\n         4.21564467e-02, -3.56978104e-02,  6.22963049e-02, -5.56988753e-02,\n        -8.98351986e-03,  4.67670858e-02,  1.15553811e-01,  1.54885754e-03,\n        -6.53147623e-02, -2.76188087e-03, -2.88737379e-02,  3.49496126e-01,\n         1.32759318e-01,  7.41247907e-02,  9.06468648e-03,  8.58006533e-04,\n         2.35754419e-02, -3.43079567e-02,  5.42532057e-02,  3.14495265e-02,\n         3.93943256e-03,  6.62599280e-02,  8.03745165e-02, -2.91163549e-02,\n        -1.98691040e-02,  5.25035225e-02, -4.46449406e-02, -2.78639924e-02],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32, numpy=\n array([[[[-1.03266295e-02, -2.57659331e-03,  2.39167653e-04, ...,\n           -1.18998659e-03, -9.75547917e-03, -2.47324049e-03],\n          [-9.87034664e-03, -1.88852882e-03,  2.75667437e-04, ...,\n           -9.98693518e-03, -5.98258805e-03,  4.40906314e-03],\n          [ 1.61835626e-02, -9.38057806e-03, -6.73405966e-03, ...,\n           -2.42401171e-03,  1.09589035e-02,  1.39167206e-02],\n          ...,\n          [-1.40176648e-02, -3.08811688e-03,  3.41502228e-03, ...,\n            5.33248903e-03, -1.70355793e-02, -1.36359241e-02],\n          [ 1.74193655e-03, -9.95234400e-03, -4.35903465e-04, ...,\n           -2.41840957e-03, -1.68555733e-02,  1.05463974e-02],\n          [-4.77309758e-03, -6.66776532e-03,  5.07126655e-03, ...,\n           -1.20526762e-03,  3.55557725e-03,  1.17330160e-03]],\n \n         [[-6.64285664e-03, -3.47881275e-03,  2.39190087e-03, ...,\n           -7.53906369e-03, -5.72045427e-03, -9.24838521e-03],\n          [-2.97287968e-03,  3.34521150e-03, -3.18630366e-03, ...,\n           -1.15623679e-02, -7.90791214e-03,  5.92658529e-03],\n          [ 2.53777411e-02,  2.53286422e-03,  2.17707790e-02, ...,\n            2.15302818e-02,  1.67279467e-02,  2.45583057e-02],\n          ...,\n          [-7.73527799e-03,  6.76160492e-03, -1.16094667e-03, ...,\n            4.41490719e-03, -1.44865643e-02, -1.29020950e-02],\n          [ 1.03217484e-02, -7.36614782e-03, -5.86766237e-03, ...,\n           -4.04950976e-03, -1.98591519e-02,  1.01560345e-02],\n          [-1.55079812e-02,  1.91596281e-02,  2.70862889e-04, ...,\n           -6.27748622e-03,  4.34006471e-03,  1.73485430e-04]],\n \n         [[ 4.12674667e-03, -4.57745697e-03,  6.64306013e-03, ...,\n           -2.34278967e-03, -9.18853935e-03, -1.25898141e-03],\n          [-3.07818851e-03,  7.09499512e-03, -1.60038937e-03, ...,\n           -6.78885821e-03, -1.09325945e-02,  7.65817193e-03],\n          [ 2.55893916e-02, -1.47064438e-03,  9.20196623e-03, ...,\n           -5.94317727e-03,  2.50070821e-03,  6.69544796e-03],\n          ...,\n          [-9.56582557e-03,  7.91247934e-03, -5.33100963e-03, ...,\n            4.76566935e-03, -1.49843339e-02, -6.69805566e-03],\n          [ 1.19609423e-02, -3.48291663e-03,  1.67369528e-03, ...,\n            8.51548160e-04, -1.82969198e-02,  8.18597153e-03],\n          [-1.66008994e-02,  1.42412046e-02, -3.94398719e-03, ...,\n           -3.43332323e-03,  2.92040431e-03,  3.60240089e-03]]],\n \n \n        [[[-1.50715457e-02,  3.58128431e-03,  1.96502986e-03, ...,\n            1.93002622e-03,  1.64739266e-02, -1.30437613e-02],\n          [-1.05031207e-02, -1.34232629e-03,  8.39629397e-03, ...,\n           -6.13174262e-03, -7.77114695e-03, -7.34500634e-03],\n          [ 1.59280524e-02, -4.93563304e-04,  2.06456911e-02, ...,\n            3.44217420e-02,  2.81744432e-02,  4.31641713e-02],\n          ...,\n          [-2.63509643e-03, -1.55394357e-02,  1.79553684e-02, ...,\n            5.62341791e-03,  7.54640112e-03, -1.32122431e-02],\n          [-8.40738416e-03, -1.08226598e-03, -8.15254636e-03, ...,\n            7.50581920e-03, -2.10973900e-02,  4.46510874e-03],\n          [ 3.56538873e-03, -1.49670860e-03,  3.90027976e-03, ...,\n            2.24968046e-03, -1.23405561e-03, -5.28686540e-03]],\n \n         [[-1.69400852e-02,  2.77157854e-02, -1.96665525e-03, ...,\n           -1.36413379e-02,  1.78107675e-02, -1.66987181e-02],\n          [-7.62252975e-03,  2.68657506e-03,  5.14760846e-03, ...,\n           -1.13447458e-02, -9.81413107e-03, -6.15228107e-03],\n          [ 2.45921463e-02,  1.22981239e-02,  4.96367253e-02, ...,\n            5.45314848e-02,  3.91974635e-02,  5.24803512e-02],\n          ...,\n          [ 4.61020367e-03, -1.76493190e-02,  1.43087450e-02, ...,\n            5.83570963e-03,  8.79333634e-03, -1.14443125e-02],\n          [ 6.86999364e-03, -9.89954080e-03, -1.38055198e-02, ...,\n            3.88898421e-03, -2.33089775e-02,  5.92778437e-03],\n          [-1.12817856e-02,  2.83943769e-02,  8.38675944e-04, ...,\n           -4.76626121e-03, -4.87527344e-03, -6.83464762e-03]],\n \n         [[-6.74894638e-03,  4.08497546e-03,  1.99278723e-03, ...,\n           -1.32070435e-02,  6.80110930e-03, -1.38106188e-02],\n          [-5.74127119e-03,  3.86035186e-03, -2.07123021e-03, ...,\n           -1.55830057e-02, -9.33692232e-03, -3.36189335e-03],\n          [ 2.10623369e-02,  6.44711405e-03,  3.90940160e-02, ...,\n            2.81947572e-02,  2.81029381e-02,  2.58975681e-02],\n          ...,\n          [ 2.77161086e-03, -9.33946855e-03, -3.96435941e-03, ...,\n            3.31284921e-03,  4.68998821e-03, -8.24241433e-03],\n          [ 9.93933622e-03, -1.26227587e-02, -5.30784763e-03, ...,\n            3.16704577e-03, -2.50813738e-02,  6.99498504e-03],\n          [-1.09259533e-02,  2.50802208e-02, -4.98637464e-03, ...,\n           -5.42974379e-03, -2.79875263e-03, -1.12388714e-03]]],\n \n \n        [[[-1.05695715e-02, -6.65004551e-03,  1.14270793e-02, ...,\n            1.13982856e-02,  2.48910710e-02, -2.01158524e-02],\n          [ 3.46275163e-03,  5.79921063e-03, -1.63540186e-03, ...,\n           -6.75595738e-03, -5.29716769e-03, -1.25067402e-02],\n          [-4.70241671e-03, -3.87823209e-03,  2.08221911e-03, ...,\n            3.55734415e-02,  1.33386767e-02,  4.47410196e-02],\n          ...,\n          [ 1.49224093e-02, -1.12200771e-02,  1.46477595e-02, ...,\n            7.56622152e-03,  3.02859787e-02, -3.93884303e-03],\n          [-5.07909758e-03,  1.31783877e-02, -1.02952961e-02, ...,\n            7.72063248e-03, -2.13399660e-02,  2.11469270e-03],\n          [ 1.11438399e-02, -1.19275544e-02,  3.20557575e-03, ...,\n            3.00401030e-03, -1.19526843e-02, -8.13317019e-03]],\n \n         [[-1.70508623e-02,  8.58010631e-03, -1.14486006e-03, ...,\n           -5.15660597e-03,  1.45966867e-02, -2.28679758e-02],\n          [-3.17657227e-03,  5.48885623e-03, -6.66785199e-05, ...,\n           -9.54495370e-03, -5.38741006e-03, -1.21885082e-02],\n          [ 1.06314057e-02,  3.72430380e-03,  2.05148254e-02, ...,\n            4.78186421e-02,  2.96977889e-02,  5.15434667e-02],\n          ...,\n          [ 3.04241665e-02, -1.32622952e-02,  8.83379206e-03, ...,\n            1.04780542e-03,  2.52286401e-02, -4.45278827e-03],\n          [-1.25949143e-03,  3.22387321e-03, -2.05024239e-02, ...,\n            5.14487643e-03, -2.70085242e-02,  2.27415795e-03],\n          [ 3.74635536e-04,  1.97586371e-03, -2.43942440e-03, ...,\n           -5.21882903e-03, -1.60979982e-02, -1.23087317e-02]],\n \n         [[-1.17229745e-02, -4.97850310e-03, -1.49952564e-02, ...,\n           -2.01662164e-02, -6.41265698e-03, -1.63828563e-02],\n          [-4.89720202e-04,  3.05005093e-03, -2.13698624e-03, ...,\n           -1.25116957e-02, -7.37623428e-04, -9.91498586e-03],\n          [ 3.77920503e-03,  4.25475789e-03,  4.48718248e-03, ...,\n            1.36323832e-02,  2.40158159e-02,  2.37192456e-02],\n          ...,\n          [ 2.55739931e-02, -8.89623910e-03, -4.37693624e-03, ...,\n           -1.07840088e-03,  1.47468811e-02, -7.86467455e-03],\n          [-2.09070556e-03, -6.24532858e-03, -2.55214944e-02, ...,\n           -1.60773040e-03, -2.46536992e-02,  2.47029914e-03],\n          [-9.88541869e-04,  3.25043732e-03, -7.95861613e-03, ...,\n           -6.07489794e-03, -1.01532796e-02, -7.52132060e-03]]]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32, numpy=\n array([ 1.96750183e-02, -1.37529126e-03,  9.24313217e-02,  1.27414614e-01,\n         1.92912251e-01,  9.04280916e-02,  6.80206418e-02, -1.09257409e-02,\n        -5.11011407e-02,  3.73470113e-02, -1.46845924e-02,  3.61276455e-02,\n         4.47974689e-02,  1.14233188e-01,  3.96334603e-02,  3.30968201e-02,\n        -4.14270945e-02,  1.26172557e-01,  9.16754156e-02,  1.18464626e-01,\n        -1.31671187e-02,  7.50441104e-02,  1.28335312e-01,  8.00440181e-03,\n        -1.07573196e-01,  1.32133102e-03, -3.40635329e-02,  1.16940521e-01,\n         2.87493374e-02,  3.08547784e-02,  3.87796462e-02, -6.80996701e-02,\n        -7.13176187e-03,  1.05054155e-02,  2.06267424e-02,  3.34174782e-02,\n         4.57665790e-03, -5.27579226e-02,  4.83131185e-02, -5.47899045e-02,\n         1.68360040e-01,  1.15788290e-02,  2.01778896e-02,  1.68149248e-02,\n         7.08065927e-02,  2.17223540e-01,  1.53843046e-03,  1.64112911e-01,\n         3.06196604e-03,  1.16954986e-02, -5.86124463e-03,  5.19056506e-02,\n        -5.61557151e-02,  8.73392746e-02,  1.58808101e-02,  8.77240673e-02,\n         1.32318228e-01,  3.81806754e-02, -1.87280662e-02, -6.92762062e-02,\n         5.70531711e-02,  9.26729850e-03,  4.84045595e-02,  2.37994827e-03,\n         3.50837666e-03,  2.03800365e-01,  1.20056927e-01,  1.70883879e-01,\n         2.25789636e-01,  8.34129192e-03,  4.15678285e-02,  8.81546885e-02,\n        -2.30569169e-02,  5.50335534e-02,  4.86409804e-03, -4.03119810e-03,\n         2.39200722e-02,  7.44936839e-02, -1.64328683e-02, -1.47285322e-02,\n         2.35676393e-01,  1.22408867e-01,  1.46704437e-02, -1.77873492e-01,\n        -1.29345879e-01, -3.14137377e-02,  2.27468424e-02,  9.65398178e-03,\n        -3.20120193e-02, -2.52930503e-02, -8.79025236e-02,  1.34076819e-01,\n         1.59370661e-01,  8.77960846e-02, -3.43334638e-02,  5.45805022e-02,\n        -4.89804074e-02,  3.54287066e-02, -1.97445303e-02,  4.43372093e-02,\n         1.45251527e-01,  7.77085498e-02,  1.02961650e-02, -2.59530023e-02,\n        -4.75013483e-04, -8.74524266e-02, -7.51014277e-02,  5.91358207e-02,\n         6.15935996e-02,  9.37355682e-02, -1.49535025e-02,  1.78817362e-02,\n         2.74845004e-01,  1.53017521e-01,  7.27292001e-02, -1.04169786e-01,\n         4.42982130e-02,  1.73191041e-01, -6.08791672e-02,  1.77203849e-01,\n         8.40698332e-02,  6.98892819e-03, -3.18103135e-02,  2.10375153e-02,\n         4.22864854e-02,  6.35605901e-02,  4.04434502e-02,  3.76647301e-02,\n         3.59471031e-02,  1.02083944e-01,  8.73470417e-05,  9.38471332e-02,\n         4.68499288e-02,  6.32981807e-02, -3.78860421e-02,  7.93554038e-02,\n         1.13128021e-01, -3.96006666e-02, -5.83097935e-02, -8.02249536e-02,\n        -4.65227999e-02, -2.61594597e-02, -3.32310535e-02,  3.17427353e-03,\n        -7.52109811e-02,  2.36260928e-02, -7.63228387e-02,  7.11988732e-02,\n         6.62565529e-02,  1.73571974e-01,  2.45762110e-01, -8.00079331e-02,\n         1.99536737e-02,  1.06031880e-01,  5.98321632e-02,  3.33041959e-02,\n         9.50865969e-02,  4.98257298e-03,  8.89858678e-02,  2.47590989e-02,\n         1.48124650e-01,  1.98556110e-01,  7.35398382e-02,  1.08685516e-01,\n         2.04420015e-01,  2.04963032e-02,  4.57405373e-02,  4.20015566e-02,\n         1.85964089e-02, -2.58497410e-02, -6.26892224e-02,  1.17059596e-01,\n        -2.29490604e-02,  3.40300016e-02,  5.02955355e-02,  1.62097052e-01,\n        -2.05496908e-04,  1.18654802e-01,  9.64896977e-02,  6.90344572e-02,\n        -1.41159356e-01,  3.72017443e-04, -7.38080963e-02, -2.61590891e-02,\n         7.57541806e-02, -7.59534352e-03,  7.93360546e-02,  1.25323590e-02,\n         1.25249624e-01,  4.01754351e-03,  2.97375340e-02,  2.78345328e-02,\n        -2.80193053e-02, -2.40374007e-03,  1.14893205e-02,  3.83159034e-02,\n         3.55407484e-02, -6.24656267e-02,  1.68975070e-01,  6.55031577e-02,\n        -7.11620823e-02,  1.10839553e-01,  4.87981783e-03,  3.38754095e-02,\n         1.01495132e-01,  3.48226465e-02,  5.01782522e-02,  4.26199585e-02,\n         6.07496174e-03, -1.44697577e-02,  6.88180104e-02, -2.36204937e-02,\n        -5.09823486e-03, -3.54031064e-02, -1.53827097e-03, -1.58810578e-02,\n         4.25821505e-02, -5.80020947e-03,  1.18461207e-01, -1.95797547e-04,\n         2.66481228e-02,  6.76563978e-02, -1.81248754e-01, -5.80569245e-02,\n         2.94329431e-02,  5.82340583e-02,  3.74395847e-02,  1.62397213e-02,\n         2.10489873e-02,  8.41620490e-02, -3.63396388e-03,  1.31090730e-02,\n        -2.61265058e-02,  2.01664791e-02,  3.85067798e-02, -1.50281250e-01,\n         6.60450682e-02, -7.15182498e-02, -4.00999747e-02, -2.08548680e-02,\n         1.32243648e-01, -1.70564819e-02,  1.42355695e-01,  1.68773502e-01,\n        -1.45762125e-02,  1.76661983e-02,  1.47060126e-01,  3.11568659e-02,\n        -8.82530883e-02,  2.04308227e-01,  4.66664173e-02,  2.81380210e-02,\n        -8.83346796e-03,  7.88244158e-02,  9.81942266e-02,  1.63116872e-01],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32, numpy=\n array([[[[ 2.49182852e-03, -8.25067237e-03, -1.34461222e-03, ...,\n            8.61167070e-03, -3.73780821e-03,  6.45499909e-04],\n          [-4.42567188e-03, -1.88350787e-05, -1.74789131e-02, ...,\n           -1.74075942e-02, -2.41387892e-03, -2.45446875e-03],\n          [-1.45422723e-02,  6.53489307e-03,  4.29056808e-02, ...,\n            2.32872367e-03, -8.22560862e-03, -1.34951407e-02],\n          ...,\n          [ 8.14430788e-03,  4.64800792e-03,  1.78648401e-02, ...,\n            1.30022259e-03, -1.02303829e-02, -8.59441701e-03],\n          [-1.25717523e-03, -4.16684226e-04, -5.34598716e-03, ...,\n           -1.43156825e-02, -5.26516093e-03, -1.92927010e-03],\n          [ 7.58334296e-03,  3.03687761e-04,  1.76798610e-03, ...,\n           -6.30416349e-03,  5.41360758e-04, -2.08757468e-03]],\n \n         [[ 5.62601024e-04, -1.57458056e-02, -1.89978105e-03, ...,\n            3.74664553e-03, -7.80931325e-04,  7.86909834e-03],\n          [-9.07994434e-03, -1.88095204e-03, -1.08374842e-02, ...,\n            3.25201056e-03, -8.60407297e-03, -1.06352521e-02],\n          [-1.02429595e-02,  8.97591282e-03,  2.03796718e-02, ...,\n           -1.58790015e-02, -3.18478863e-03, -1.46591766e-02],\n          ...,\n          [ 2.76015643e-02,  3.51342745e-02,  1.60577446e-02, ...,\n           -3.92214302e-03, -3.94038716e-03, -1.28798457e-02],\n          [-9.14710574e-03,  7.83217512e-03, -1.45204505e-03, ...,\n           -2.50025140e-03,  4.37620515e-03, -2.92862463e-03],\n          [ 1.53262382e-02,  3.19609744e-03, -3.10626184e-03, ...,\n           -1.26120343e-03,  1.93217583e-03, -4.34497744e-03]],\n \n         [[ 1.19078634e-02,  1.22632412e-02,  1.10837901e-02, ...,\n           -7.12652551e-03, -4.51076403e-03, -1.80176343e-04],\n          [ 1.09123939e-04, -2.57430226e-03,  5.92016324e-04, ...,\n            1.00612864e-02, -6.97242282e-03, -1.89536007e-03],\n          [-3.65449372e-03, -5.48841013e-03,  2.82002846e-03, ...,\n           -9.52219777e-03,  1.63100380e-03, -9.41131264e-03],\n          ...,\n          [ 2.27532778e-02,  2.56275944e-02,  1.38883954e-02, ...,\n           -3.09439981e-03,  8.96250270e-03, -1.08324187e-02],\n          [-1.14783114e-02, -4.79689304e-04, -5.75645873e-03, ...,\n            5.71634853e-03,  1.44765461e-02, -3.67933162e-03],\n          [ 1.40898814e-02, -3.01762018e-03, -1.16279358e-02, ...,\n           -3.89153371e-03,  4.75310662e-04, -1.12566911e-02]]],\n \n \n        [[[-9.41542629e-03, -9.80154704e-03, -3.69167840e-03, ...,\n            8.45058262e-03, -1.11409568e-03,  1.57259796e-02],\n          [-2.05228683e-02,  3.40567291e-04, -1.74381770e-02, ...,\n           -1.73003264e-02, -4.24027769e-03, -1.03721013e-02],\n          [-8.17882922e-03,  4.71192319e-03,  5.10138553e-03, ...,\n            2.30808929e-03, -3.67084774e-03, -6.74672890e-03],\n          ...,\n          [ 8.09922162e-03, -6.36649085e-03,  7.09572807e-03, ...,\n            1.55610824e-02, -4.00183955e-03,  8.52918660e-04],\n          [ 9.46240834e-06, -4.80348710e-03,  6.38567237e-03, ...,\n           -1.66918933e-02,  7.42614502e-03, -5.22509357e-03],\n          [ 6.23346865e-03, -8.45635124e-03,  1.28625380e-02, ...,\n            7.55864847e-03,  3.60822654e-03,  1.02037117e-02]],\n \n         [[-9.66143794e-03, -1.66031346e-02, -3.75456340e-03, ...,\n            3.77679267e-03,  8.85611749e-04,  3.90423387e-02],\n          [-2.29902640e-02, -5.48137818e-04,  6.26104232e-03, ...,\n            3.09622195e-03, -1.12797739e-02, -2.21406780e-02],\n          [-7.34363217e-03,  9.97547805e-03, -1.59962792e-02, ...,\n           -2.01132614e-02,  4.04101564e-03, -1.30811296e-02],\n          ...,\n          [ 2.91116592e-02,  2.56295223e-02, -5.18021779e-03, ...,\n            1.64482743e-02,  9.89971100e-04, -5.71119506e-03],\n          [-3.59656615e-03,  3.92737426e-03,  1.15985656e-02, ...,\n           -4.35973983e-03,  1.28727686e-02, -3.06857913e-03],\n          [ 1.48936510e-02, -2.79081590e-03,  1.20031787e-02, ...,\n            1.37832770e-02,  1.05134724e-02,  2.34108530e-02]],\n \n         [[ 4.74231754e-04,  3.63741280e-03,  2.15365533e-02, ...,\n           -1.89999817e-03, -5.15478104e-03,  1.31619368e-02],\n          [-9.18400660e-03,  1.93454546e-03,  1.09110810e-02, ...,\n            9.14575998e-03, -8.21557175e-03, -8.30788538e-03],\n          [-1.69444911e-03, -4.36456734e-03, -2.26653703e-02, ...,\n           -1.38805434e-02,  1.06112342e-02, -1.10037765e-02],\n          ...,\n          [ 2.16357969e-02,  1.74818505e-02, -4.67658462e-03, ...,\n           -9.58664925e-04,  1.37514127e-02, -7.61160767e-03],\n          [-1.90485851e-03, -4.57544671e-03,  5.04404213e-03, ...,\n            7.49490084e-03,  2.18671728e-02, -5.64512424e-03],\n          [ 1.06539074e-02, -6.43269531e-03, -5.04566648e-04, ...,\n            8.10153224e-03,  1.10825021e-02,  1.70141142e-02]]],\n \n \n        [[[-4.50136606e-03, -6.02411199e-03,  3.11873481e-03, ...,\n           -5.44655835e-04,  2.41484423e-03,  2.56585758e-02],\n          [-1.60623565e-02,  2.61958572e-03, -4.11093468e-03, ...,\n           -1.52113223e-02, -2.44396157e-03, -9.40611679e-03],\n          [ 1.03867343e-02,  9.93811246e-03, -6.63214130e-03, ...,\n            4.57423186e-04, -1.41171238e-03,  1.74400141e-03],\n          ...,\n          [-3.64836096e-03, -3.09328665e-03, -1.06807053e-02, ...,\n            1.12805534e-02, -2.41819987e-04,  9.97136254e-03],\n          [ 2.19470632e-04, -1.80030242e-03,  4.31476161e-03, ...,\n           -1.55662363e-02,  1.01363128e-02, -5.82250673e-03],\n          [-1.94603333e-03, -1.33034596e-02,  1.00488653e-02, ...,\n            1.57581102e-02,  6.38718856e-03,  9.57524404e-03]],\n \n         [[-1.47843594e-03, -1.02676339e-02,  1.72790524e-03, ...,\n           -7.44713098e-03,  7.77076697e-03,  6.32125139e-02],\n          [-1.55830020e-02, -9.06818022e-04,  1.38317700e-02, ...,\n            1.86586718e-03, -1.31476419e-02, -2.01811269e-02],\n          [ 4.06060950e-04,  1.30023547e-02, -2.33655237e-02, ...,\n           -2.13477034e-02,  1.25812916e-02, -6.86774962e-03],\n          ...,\n          [ 8.52249004e-03,  2.30366867e-02, -2.14423127e-02, ...,\n            2.31715962e-02,  7.81933870e-03,  9.37404856e-03],\n          [ 3.03807063e-03,  3.03328532e-04,  1.46860117e-02, ...,\n           -4.26525855e-03,  1.57574061e-02, -4.55228426e-03],\n          [-1.30841765e-03, -5.18516358e-03,  4.14379733e-03, ...,\n            2.45135967e-02,  1.46029377e-02,  2.09346768e-02]],\n \n         [[ 1.05792219e-02,  7.71222962e-03,  1.60458777e-02, ...,\n           -1.14335474e-02,  1.80619667e-04,  3.85264903e-02],\n          [-3.40054696e-03,  3.24828108e-03, -2.46915198e-03, ...,\n            1.20599633e-02, -8.42132140e-03, -9.39693395e-03],\n          [-6.27992442e-03,  5.48810531e-05, -3.39485556e-02, ...,\n           -1.67110953e-02,  2.01697182e-02, -9.91834793e-03],\n          ...,\n          [ 6.44019945e-03,  1.73932556e-02, -1.23419259e-02, ...,\n            4.06535063e-03,  1.56269483e-02,  4.64981329e-03],\n          [ 4.31994861e-03, -6.28653821e-03,  1.34719890e-02, ...,\n            1.59082916e-02,  2.07160246e-02, -4.53479309e-03],\n          [-8.85906070e-03, -5.81115717e-03, -5.50442899e-04, ...,\n            1.17796622e-02,  1.62397400e-02,  2.18943544e-02]]]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32, numpy=\n array([ 3.54152434e-02,  1.07918017e-01, -1.80004574e-02,  4.33173068e-02,\n        -5.59721142e-02,  3.76750678e-02,  7.40262270e-02,  1.16711902e-02,\n        -4.17180508e-02, -1.17407888e-01,  6.78363219e-02,  1.22183353e-01,\n         2.05125120e-02,  3.72891687e-02, -1.76102147e-02, -5.29137291e-02,\n         1.31046280e-01, -2.64913682e-02, -3.43576743e-04, -1.73489619e-02,\n        -4.91021085e-04, -2.93992013e-02, -1.80223621e-02,  3.43877301e-02,\n         1.05296724e-01, -3.09039615e-02, -5.63405044e-02, -2.91478150e-02,\n        -5.64637184e-02,  1.73687801e-01,  2.17615440e-02,  2.04272568e-01,\n         1.78167019e-02, -2.08299085e-02,  3.57796438e-02, -3.39595117e-02,\n         4.13637934e-03,  5.32299802e-02,  6.11784309e-02, -2.24591643e-02,\n         2.53849160e-02,  7.26907048e-03,  1.39820218e-01,  1.36458054e-01,\n        -4.59598787e-02,  1.15342690e-02,  3.22952569e-02, -1.42887920e-01,\n        -1.28008483e-03, -2.50399150e-02,  9.68392193e-02, -8.88353959e-02,\n         4.10740227e-02, -3.00087966e-02,  2.81882793e-01,  5.05600125e-02,\n        -1.34811839e-02,  5.92962280e-02, -2.92030107e-02,  9.09703039e-03,\n         2.90436903e-03,  7.35089853e-02, -6.01027114e-03, -4.77841496e-02,\n         7.36456318e-03, -6.64260387e-02, -6.09310083e-02,  3.67748976e-01,\n        -1.16953542e-02,  4.41312604e-02,  1.32252788e-02,  2.26632342e-01,\n         9.52477679e-02, -2.39434820e-02, -5.79178929e-02, -5.94564974e-02,\n        -8.09515342e-02,  4.82652113e-02,  7.17421249e-03,  2.41490360e-02,\n        -1.98139753e-02,  2.84963101e-02, -8.00397545e-02,  2.38018021e-01,\n        -3.05617340e-02,  1.00589961e-01,  1.44958310e-02,  3.77400517e-02,\n         7.74130449e-02,  1.30725756e-01,  7.96947032e-02, -1.45637768e-03,\n        -3.19442749e-02, -1.51081849e-02,  3.52219008e-02, -4.26961035e-02,\n         1.68760002e-01,  2.80141309e-02,  1.78513229e-01, -4.34451364e-02,\n        -2.37757768e-02, -3.47408876e-02, -1.49866249e-02,  5.66772446e-02,\n         1.29111171e-01,  6.48782328e-02, -1.44754872e-02, -2.24494301e-02,\n        -2.54037064e-02,  3.96308079e-02, -2.56226268e-02, -5.03154919e-02,\n        -4.97624055e-02,  2.23401305e-03, -2.17827000e-02, -2.89310124e-02,\n        -2.45345235e-02, -1.02057420e-02, -5.17127523e-03,  1.35025755e-01,\n         9.88981575e-02, -1.56548005e-02, -9.30538997e-02, -7.71050751e-02,\n        -1.69187840e-02, -4.97867577e-02, -2.05100849e-02, -2.40226332e-02,\n         1.10174127e-01,  6.45432994e-03, -6.35399818e-02, -1.00426357e-02,\n         6.12853579e-02,  6.90499842e-02,  5.01222908e-03,  3.43363807e-02,\n         7.83027411e-02, -5.07733002e-02,  1.40164588e-02, -6.67239502e-02,\n         4.06932011e-02,  2.68647261e-02, -6.07886091e-02, -9.31503717e-03,\n        -2.62611974e-02,  3.05419695e-03, -1.34765180e-02,  9.73728299e-02,\n         1.78622589e-01,  4.50597964e-02,  5.42690158e-02,  7.46591017e-02,\n        -8.95660184e-03,  7.22454041e-02, -3.68183441e-02, -2.30303593e-02,\n         7.66965896e-02,  1.46697029e-01,  2.13573244e-03,  7.85259679e-02,\n         3.42821283e-03,  1.11104727e-01,  4.24453318e-02,  1.09375820e-01,\n        -6.04121909e-02, -8.60929023e-03,  4.85750735e-02,  2.85593793e-02,\n         1.59954399e-01, -3.05411941e-03,  3.45985368e-02, -4.95158508e-02,\n         1.74595189e-05, -9.63191688e-02, -4.78641056e-02, -3.91172655e-02,\n        -4.34033386e-02,  4.52988893e-02,  6.55773655e-02, -2.72106268e-02,\n        -2.12168340e-02,  1.46709710e-01,  3.16328891e-02,  8.86552259e-02,\n         2.89379451e-02,  2.73449346e-02, -2.53844894e-02, -1.90591477e-02,\n        -3.78769357e-03, -5.86589575e-02,  6.94475025e-02,  9.80708972e-02,\n        -5.76754995e-02,  5.84980808e-02,  2.11332172e-01,  3.06425267e-03,\n        -2.66177114e-02,  8.75359029e-02,  1.38149649e-01,  2.33814374e-01,\n         2.02122167e-01, -5.10597089e-03, -2.80219093e-02,  5.77546321e-02,\n        -5.45554142e-03,  1.32973596e-01, -6.44019470e-02,  1.02306986e-02,\n         2.11395919e-02, -8.47095437e-03, -1.46873491e-02, -8.90242215e-03,\n         1.57834932e-01,  1.07433885e-01,  7.52661005e-02, -6.60077631e-02,\n        -2.63087489e-02, -2.50031222e-02, -5.58990352e-02,  1.49529558e-02,\n         1.06838427e-01,  2.11164802e-01,  1.23072572e-01, -2.98848525e-02,\n        -1.59130655e-02, -4.41072173e-02, -3.21661904e-02, -1.79614685e-02,\n         2.25747688e-04, -7.31781572e-02,  9.49809700e-02, -4.98221256e-02,\n         5.14554046e-03, -5.61895669e-02,  7.96963274e-02,  1.39233992e-01,\n         5.66332527e-02,  3.33853289e-02, -5.52441776e-02, -1.10003902e-02,\n         1.32760391e-01,  1.08468086e-01, -9.70453620e-02,  4.95204031e-02,\n        -2.47506332e-02, -1.51634403e-02,  5.94771743e-01,  6.86122477e-02,\n         1.20992266e-01,  4.66077290e-02, -3.62980738e-02, -1.40720494e-02,\n         1.71003804e-01, -6.05119439e-03,  5.99404657e-03, -2.67574973e-02],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32, numpy=\n array([[[[-1.24538150e-02,  8.40655994e-03, -5.28845051e-03, ...,\n           -2.04581907e-03, -3.92359449e-03,  1.30683812e-03],\n          [ 1.15566896e-02,  2.46055960e-03, -4.09082584e-02, ...,\n           -4.92062652e-03, -1.63423386e-03,  8.94230697e-03],\n          [-1.43469922e-04, -5.65873180e-03,  1.84066985e-02, ...,\n           -2.08840682e-03,  2.63321817e-06, -2.50728261e-02],\n          ...,\n          [ 1.37888230e-02, -6.58180192e-03,  9.65370331e-03, ...,\n            7.17610447e-03,  3.45725450e-03,  5.33173978e-03],\n          [ 2.65833316e-03,  7.77704641e-04,  2.86239345e-04, ...,\n           -5.40583069e-03,  8.93119723e-03,  1.40043022e-02],\n          [-8.53933115e-03,  1.43394794e-03, -2.91879033e-03, ...,\n            3.77660524e-03,  2.05454584e-02, -1.35868033e-02]],\n \n         [[-1.20176468e-02,  1.51250837e-03,  3.43130133e-03, ...,\n           -1.57830697e-02, -2.39501009e-03,  2.93573504e-03],\n          [ 1.07167801e-02, -1.72885903e-03, -1.83217339e-02, ...,\n           -3.87932011e-03, -1.74722821e-02,  3.09405895e-03],\n          [-2.78812763e-03,  1.09002152e-02,  7.31469830e-03, ...,\n            1.32350891e-03,  5.71234105e-03, -2.06499193e-02],\n          ...,\n          [ 5.34640905e-03, -7.61074666e-03, -2.95266091e-05, ...,\n            1.06462219e-03,  7.16414442e-03,  5.50952321e-03],\n          [-1.97489071e-03, -5.18740201e-03, -3.50479130e-03, ...,\n           -7.65207084e-03,  6.48583705e-03,  7.29372769e-05],\n          [-8.13952927e-03, -1.72344397e-03, -1.71952751e-02, ...,\n            6.51383121e-03,  2.62788888e-02, -7.40201725e-03]],\n \n         [[-1.13097280e-02, -2.71398202e-03,  5.21831634e-03, ...,\n            3.75532242e-03, -4.37350944e-03, -1.71778537e-03],\n          [ 9.51003470e-03,  2.10385933e-03, -6.23627566e-03, ...,\n            9.84872226e-04, -1.22357579e-02, -5.77230006e-03],\n          [ 1.09487341e-03,  2.16768365e-02, -1.11458432e-02, ...,\n           -1.15952350e-03,  7.16441032e-03, -6.74266228e-03],\n          ...,\n          [ 3.84493452e-03,  6.50770171e-03, -3.08423012e-04, ...,\n            7.76557205e-03,  4.50123427e-03,  2.94768345e-03],\n          [-5.20469574e-03,  2.53336993e-03, -2.00857781e-03, ...,\n            6.74798503e-04,  1.27291884e-02, -5.95079921e-03],\n          [-4.31006867e-03, -4.26639896e-03, -1.50389588e-02, ...,\n           -6.62087603e-03,  1.03521571e-02,  2.33515655e-03]]],\n \n \n        [[[-1.66037567e-02,  2.41671409e-02, -1.76821016e-02, ...,\n           -8.38128850e-03, -3.02293082e-03,  3.58365453e-03],\n          [ 9.80825536e-03,  1.05249183e-02, -4.66881320e-02, ...,\n           -7.78164901e-03, -3.83233977e-03,  2.94483802e-03],\n          [ 1.34947952e-02, -1.25661753e-02,  1.29038952e-02, ...,\n            1.43833098e-03, -5.72983874e-03, -1.78362131e-02],\n          ...,\n          [ 9.57040023e-03,  1.17314365e-02,  1.55258663e-02, ...,\n            2.69203974e-05,  2.30762083e-03,  7.98452704e-04],\n          [-2.26363889e-03, -2.44599045e-03,  5.42335818e-03, ...,\n           -7.61786243e-04,  7.27130566e-03,  2.00658664e-02],\n          [-6.25453144e-03,  8.70768726e-03,  5.96766314e-03, ...,\n            2.25311015e-02,  1.17731886e-02, -8.07471387e-03]],\n \n         [[-2.05247719e-02,  9.02097579e-03, -1.39437476e-03, ...,\n           -1.84835698e-02,  1.36483200e-02,  1.48000638e-03],\n          [ 8.35212879e-03, -3.76963033e-03, -2.36531124e-02, ...,\n           -4.67878534e-03, -1.02635678e-02, -9.82333149e-05],\n          [ 1.35796964e-02, -1.72847509e-02,  9.88537073e-03, ...,\n            7.85652269e-03,  1.02855563e-02, -2.21692659e-02],\n          ...,\n          [ 4.50371159e-03,  7.92721228e-04,  8.28000717e-03, ...,\n           -1.25710340e-03,  2.60498724e-03,  6.89739687e-03],\n          [-4.07085335e-03, -4.51655919e-03, -2.78022187e-03, ...,\n            1.75143685e-03,  7.57344998e-03,  1.27716700e-03],\n          [-4.38698288e-03,  1.11662695e-04, -1.52793936e-02, ...,\n            3.82050462e-02,  3.65496846e-03, -6.30834652e-03]],\n \n         [[-1.67291705e-02, -7.51205778e-04,  8.62288661e-03, ...,\n           -4.92068566e-03,  2.60210619e-03, -8.31924565e-03],\n          [ 7.60383205e-03, -2.44896696e-03, -9.95151792e-03, ...,\n            2.84005492e-03,  1.40986012e-04, -7.82902353e-03],\n          [ 2.45916261e-03, -6.49120752e-03, -8.32979660e-03, ...,\n            7.72433681e-03,  1.30170006e-02, -1.23488028e-02],\n          ...,\n          [ 1.33765105e-03,  7.21688429e-03,  1.26626331e-03, ...,\n            3.94847291e-03,  5.32880379e-03, -2.07663863e-03],\n          [-2.70236470e-03,  1.28381781e-03, -6.93748286e-03, ...,\n            8.98882467e-03,  1.46908136e-02, -4.21756599e-03],\n          [-2.33115349e-03, -6.12587482e-03, -2.12312099e-02, ...,\n            9.15342360e-04,  3.17362952e-04,  1.23123394e-03]]],\n \n \n        [[[-1.45651354e-02,  9.64931585e-03, -1.16087059e-02, ...,\n            2.59297341e-03, -8.05760548e-03,  2.21103709e-03],\n          [ 6.19267486e-03,  8.93716048e-03, -4.51339036e-02, ...,\n           -6.12378679e-03, -4.00708383e-03,  6.47111796e-03],\n          [ 1.43068740e-02, -2.66728899e-03,  3.41038289e-03, ...,\n            3.44638783e-03, -8.62873974e-04, -1.33675877e-02],\n          ...,\n          [ 1.29059236e-02,  9.14617069e-03,  8.28087237e-03, ...,\n           -4.39318595e-03, -8.14242568e-03, -2.26692040e-03],\n          [-1.31062279e-03, -5.64920157e-03,  6.37932634e-03, ...,\n            6.66187890e-03,  4.44209296e-03,  4.41963132e-03],\n          [-6.42832462e-03,  4.18221578e-03,  7.65334209e-03, ...,\n            1.20814089e-02, -6.66856999e-03,  3.25094647e-04]],\n \n         [[-1.66320223e-02,  4.54504183e-03, -6.67143054e-03, ...,\n            2.28197756e-03, -6.31665680e-05, -4.71796095e-03],\n          [ 2.97540380e-03, -6.40762178e-03, -2.85963546e-02, ...,\n           -2.35724798e-03,  4.74322308e-03,  4.67349339e-04],\n          [ 2.36185007e-02, -1.89911220e-02,  4.60000196e-03, ...,\n            1.08400211e-02, -9.18388832e-03, -2.74709109e-02],\n          ...,\n          [ 8.74977279e-03,  1.61244010e-03,  3.06530995e-03, ...,\n            4.53018304e-03, -7.39837857e-03,  4.47823061e-03],\n          [-1.20457367e-03, -2.87331175e-03, -5.18758828e-03, ...,\n            1.17142722e-02,  7.62202404e-03, -5.77956159e-03],\n          [-4.07704106e-03,  5.63840521e-03, -1.10617159e-02, ...,\n            2.38051061e-02, -1.34268589e-02, -2.97562080e-03]],\n \n         [[-8.99161492e-03,  5.84716024e-03,  5.29400352e-03, ...,\n            3.04830913e-03, -1.18276821e-02, -8.41523055e-03],\n          [ 3.12718400e-03, -5.05524641e-03, -1.94435567e-02, ...,\n            3.33701703e-03,  1.45587865e-02, -4.61388379e-03],\n          [ 1.11383200e-02, -2.28672028e-02,  2.00467315e-04, ...,\n           -3.49199137e-04, -8.52558948e-03, -1.46135921e-02],\n          ...,\n          [ 4.12542978e-03,  1.88087288e-03, -2.15299334e-03, ...,\n            3.59076308e-03, -1.91574858e-03, -3.53228627e-03],\n          [-2.84922371e-05,  9.47301451e-04, -8.46835505e-03, ...,\n            1.27441790e-02,  1.32457484e-02, -2.80284905e-03],\n          [-6.17068168e-03, -5.06139966e-03, -1.87711921e-02, ...,\n           -2.79223267e-03, -1.10712685e-02,  4.57540294e-03]]]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32, numpy=\n array([ 0.01348567,  0.00756114,  0.06701895,  0.04177905,  0.00131124,\n         0.19431135, -0.04237143,  0.02754786,  0.07195634,  0.0540045 ,\n         0.05481127,  0.00045339,  0.01170474,  0.07333044,  0.06471756,\n        -0.01727704,  0.0117172 , -0.00914959,  0.05935393,  0.00778987,\n         0.19473521, -0.03160755,  0.08230364,  0.00812414, -0.01093625,\n         0.11831848, -0.00162986, -0.03104967,  0.10524632,  0.08911344,\n        -0.01753394,  0.01114011,  0.0273301 ,  0.03628114,  0.11654162,\n         0.0065666 ,  0.05314495, -0.07114487,  0.02403423,  0.05467438,\n         0.01867165, -0.01507242,  0.06879217,  0.01969217, -0.01445228,\n         0.10442033,  0.0295262 ,  0.01075144,  0.12457283,  0.00351205,\n        -0.0739503 , -0.01016993,  0.01367457,  0.05722002, -0.01053831,\n         0.06229207,  0.04473517,  0.05864166,  0.04295328,  0.03258709,\n        -0.04706465,  0.01346881, -0.00432716,  0.01531392, -0.01141861,\n         0.07937651,  0.07459345,  0.04317596,  0.0418528 , -0.00830521,\n         0.12413809,  0.02634931,  0.01499857, -0.01562519,  0.13549338,\n        -0.02361966,  0.10109243,  0.02215822,  0.04679871, -0.02305423,\n        -0.00035752,  0.00886868,  0.01744638,  0.00717962, -0.00932838,\n         0.01467725,  0.03773836, -0.01804588, -0.01379897,  0.03704951,\n        -0.05143043,  0.02446899,  0.02469411,  0.04909822,  0.05199512,\n         0.1023118 ,  0.05641348, -0.01583332, -0.03320146, -0.05584373,\n         0.00470411, -0.14548235, -0.03380134,  0.03405067,  0.05419153,\n        -0.00342409,  0.02665846, -0.01147734,  0.00723815,  0.02606396,\n         0.05343403,  0.06094407,  0.10145702,  0.02285037,  0.0050243 ,\n         0.00643407,  0.07472897, -0.02839745, -0.02860967, -0.05288227,\n        -0.04451503, -0.02369499, -0.04215489,  0.09184299, -0.03788895,\n        -0.01263917,  0.01668115,  0.07473072, -0.03339927,  0.09063061,\n         0.08986019,  0.01870072,  0.03427459,  0.01882564,  0.0362142 ,\n         0.02372144, -0.01470978,  0.02087447,  0.03057157,  0.14249216,\n         0.03813924, -0.06163649, -0.0136106 ,  0.01268591,  0.0237574 ,\n         0.00188927, -0.03021016, -0.02962662, -0.01139644,  0.07387836,\n        -0.02347502,  0.04443461, -0.0061994 ,  0.02155864, -0.06407253,\n        -0.02514911, -0.06183469,  0.09569915, -0.00980667, -0.01131836,\n         0.08887472,  0.06592884,  0.01114294, -0.03150102,  0.02841251,\n         0.01129664,  0.02996553, -0.07221939,  0.00725339,  0.00594673,\n        -0.01459754,  0.05204234, -0.02142872,  0.1504253 , -0.01999968,\n         0.12134195,  0.04612613, -0.01188883, -0.00529259, -0.02203224,\n         0.04083564,  0.00939796, -0.03749346,  0.13649143,  0.0641688 ,\n        -0.00701836,  0.00902929, -0.00858151,  0.11478466,  0.01996962,\n         0.01550458, -0.01010027,  0.00470255, -0.00364825,  0.08448643,\n         0.0163641 ,  0.06599426, -0.01890062, -0.00969238, -0.05405046,\n         0.04458565,  0.06576306, -0.02386773, -0.03610412,  0.00980019,\n        -0.09455095,  0.00291193,  0.0237151 ,  0.01857767, -0.02965006,\n        -0.01155117,  0.11206248, -0.01298213,  0.00983512, -0.0407909 ,\n         0.03063179,  0.14880319,  0.03121548, -0.03412959,  0.00541294,\n         0.03908843, -0.04607837,  0.05219414,  0.05060191, -0.05693981,\n         0.02740915, -0.01628359,  0.18281287,  0.03640307,  0.06375421,\n        -0.00875941, -0.0144028 ,  0.01180603, -0.00780482,  0.12496937,\n         0.03204761, -0.04198339,  0.00742837,  0.00188249, -0.0122803 ,\n        -0.05174524,  0.03891598,  0.09401204,  0.02095588,  0.05364544,\n         0.02157122, -0.0223824 , -0.05735386, -0.03900814,  0.0450025 ,\n        -0.0018383 ,  0.01242615, -0.00938961,  0.00032437,  0.06582566,\n         0.02123269, -0.01413163, -0.03544443, -0.00126603,  0.00080917,\n         0.09520332,  0.01864311, -0.01185386, -0.04681906,  0.04993291,\n        -0.01657778, -0.04607387,  0.03666889,  0.08055599,  0.1407983 ,\n        -0.05214395, -0.00461419,  0.01410781, -0.02427171,  0.00154092,\n         0.126334  ,  0.01971441,  0.01302725,  0.03932913,  0.03358722,\n         0.00090988, -0.02318616,  0.0212261 , -0.0188354 ,  0.03896225,\n         0.04033942,  0.00431076, -0.05325833,  0.03454122, -0.00824694,\n        -0.02183646, -0.04885524,  0.0934401 ,  0.01431899, -0.01373951,\n        -0.01864055,  0.01714154,  0.02857943, -0.00934389,  0.11435111,\n         0.16847515,  0.14865492,  0.06036819,  0.10966414, -0.00876428,\n        -0.00182351, -0.0097525 ,  0.10607314,  0.05854449, -0.05090364,\n         0.03826505,  0.01531792,  0.1413483 ,  0.1375193 ,  0.13819131,\n        -0.03390937,  0.03122845,  0.07244698,  0.01329123,  0.0786965 ,\n        -0.01342774, -0.01067122,  0.04975825,  0.060603  , -0.04138377,\n         0.2900992 ,  0.09210091,  0.04867851, -0.0245617 ,  0.04394877,\n        -0.01570157, -0.01720707,  0.01999883, -0.06262601,  0.04849933,\n         0.17290471, -0.01396008,  0.01316997, -0.01597587,  0.2017592 ,\n        -0.01904629,  0.03416235,  0.0256703 , -0.02117538,  0.08179075,\n         0.02009195,  0.01043306,  0.03629194, -0.05558422, -0.01445823,\n         0.01679071,  0.01544859,  0.3148451 , -0.01249089, -0.03043203,\n        -0.02117118,  0.00314888,  0.01473002,  0.05949752, -0.0103932 ,\n         0.05455083, -0.03988231,  0.01832076, -0.07607763, -0.06348411,\n         0.04249362,  0.01351619,  0.10402187, -0.00215642,  0.01042758,\n        -0.06371963,  0.13125049, -0.00413846, -0.01627147, -0.00981681,\n        -0.04110023,  0.00102312, -0.02674357, -0.01093246, -0.02083088,\n         0.02330049,  0.06305598, -0.02870597, -0.06720389,  0.05937791,\n        -0.02225667,  0.07693806,  0.01540439,  0.03038635,  0.00505839,\n        -0.02343038, -0.03993971,  0.06336603,  0.00374609, -0.00875121,\n         0.04515095, -0.01071331,  0.02696336,  0.0318309 , -0.04767392,\n         0.01778279, -0.0170456 ,  0.12305465, -0.06846349, -0.03814644,\n         0.07269525,  0.06233151,  0.14082748,  0.03557994,  0.04744978,\n         0.05929145, -0.00076887,  0.08290517,  0.01969881, -0.00040537,\n        -0.03909905,  0.01344012, -0.0103886 , -0.01689904, -0.05682559,\n        -0.01224083,  0.03786673, -0.0409046 ,  0.01133572,  0.02511554,\n        -0.01610163, -0.01083341,  0.08275371, -0.02322658, -0.02001276,\n        -0.02932116,  0.11604706, -0.01643755, -0.05238251,  0.02983823,\n         0.00451954,  0.04716583,  0.01829614,  0.03937242,  0.0603151 ,\n         0.02920038, -0.02627408, -0.01419285,  0.14424483, -0.02115806,\n        -0.03981708,  0.08781821,  0.00357609,  0.01189661, -0.00536245,\n         0.00421988,  0.01712015, -0.03556567,  0.00546586,  0.0413718 ,\n         0.02160333,  0.00648857,  0.08955134, -0.01836678, -0.05678857,\n         0.06376465,  0.07066101,  0.00479759,  0.04906932,  0.00496414,\n         0.00224556,  0.01189737, -0.00762232,  0.03490169,  0.06040287,\n         0.04139943, -0.00251852,  0.00871861, -0.04251447, -0.00289279,\n         0.00944588, -0.01643149,  0.01481972,  0.08260825,  0.06986772,\n         0.0169169 ,  0.08311743,  0.00420612,  0.13570775, -0.01252192,\n        -0.0481341 , -0.00103277,  0.05884005,  0.01856414,  0.0359413 ,\n         0.05466994,  0.06611989, -0.03170012, -0.00673592, -0.07229806,\n         0.07541752, -0.06429004,  0.08279631, -0.03808228,  0.03377485,\n        -0.02604658, -0.06577716,  0.07294629,  0.04258815,  0.01734693,\n         0.03642308,  0.05075784, -0.00875437,  0.06135759,  0.03979362,\n        -0.00413027,  0.0101658 ], dtype=float32)&gt;,\n &lt;tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32, numpy=\n array([[[[ 7.11654220e-03,  7.00819055e-06,  9.24700499e-03, ...,\n            4.03929735e-03,  8.05561524e-03, -3.74245713e-03],\n          [ 1.12781115e-03,  1.61532906e-03, -9.08267568e-04, ...,\n           -7.93790072e-03, -4.59095975e-03, -8.13464867e-04],\n          [-4.63789236e-03, -1.18727237e-02,  6.58529717e-03, ...,\n            7.42091658e-03, -6.53812988e-03, -2.42086244e-03],\n          ...,\n          [ 1.28063676e-03,  1.49819313e-03,  7.28668272e-03, ...,\n           -5.99841867e-03, -2.01151473e-03, -1.18386478e-03],\n          [-4.89709014e-03, -6.26946846e-03, -5.09714335e-03, ...,\n            1.26905064e-03,  1.07506765e-02, -2.44210777e-03],\n          [ 2.12570513e-03, -3.61318327e-03, -7.26481713e-03, ...,\n            3.64817417e-04,  1.50787262e-02,  4.10703570e-03]],\n \n         [[ 6.64269971e-03, -6.01840531e-03,  9.42177977e-03, ...,\n            4.53453884e-03,  4.71206789e-04,  3.50896595e-03],\n          [-1.24128824e-02,  1.62265194e-03,  2.18410187e-05, ...,\n           -9.26828291e-03, -1.32720219e-02,  7.84974638e-03],\n          [-2.27750093e-03, -1.37851508e-02,  6.04773080e-03, ...,\n            5.82123734e-03, -4.03309939e-03, -3.08526703e-03],\n          ...,\n          [ 9.42836609e-03, -7.80543603e-04,  1.71435010e-02, ...,\n           -7.94082507e-03, -7.73798907e-04,  3.02146119e-03],\n          [-3.22291674e-03, -2.85354297e-04, -2.08436535e-03, ...,\n           -4.60813055e-03, -5.56954136e-03,  6.86109858e-03],\n          [ 9.59297060e-04,  7.38721387e-03, -8.91729910e-03, ...,\n           -1.65850704e-03,  7.24582188e-03,  6.19835919e-03]],\n \n         [[ 2.34439736e-03, -9.28215683e-03, -3.19302635e-04, ...,\n            8.40835925e-03,  4.12485213e-04,  3.09582963e-03],\n          [-1.55776711e-02, -1.08954648e-03, -4.28956561e-03, ...,\n           -1.46012632e-02, -1.17095094e-03, -1.09702547e-03],\n          [-7.46384822e-03, -6.07428560e-03,  1.01981256e-02, ...,\n           -3.00147524e-03,  6.63558196e-04, -6.11750875e-03],\n          ...,\n          [ 4.24216688e-03,  4.56949836e-03,  8.23827088e-03, ...,\n           -5.40188514e-03, -8.07413179e-03,  1.09999124e-02],\n          [-7.86238699e-04,  1.90356933e-03,  1.71771063e-03, ...,\n           -2.16562231e-03, -1.10405972e-02,  9.30893887e-03],\n          [ 6.02629362e-03,  1.86428446e-02, -1.34665128e-02, ...,\n           -8.40879139e-03,  7.48337386e-03, -4.10805363e-03]]],\n \n \n        [[[ 5.44611085e-03, -9.18421778e-04,  9.77920461e-03, ...,\n           -1.19165263e-04, -3.40916682e-03, -6.98734634e-03],\n          [ 2.51410692e-03,  4.60237265e-03, -2.46173400e-03, ...,\n           -5.48852608e-03, -7.09898863e-03,  5.42021822e-03],\n          [-9.05828271e-03, -1.00242114e-02,  2.94465595e-03, ...,\n            9.01548378e-03,  6.37277029e-04, -2.18915544e-03],\n          ...,\n          [-3.00789997e-03, -9.66227599e-05,  1.85708259e-03, ...,\n           -2.42409506e-03,  2.34206222e-04, -3.27465497e-03],\n          [-5.01462910e-03, -3.50114633e-03, -6.08791783e-03, ...,\n           -7.96810200e-04,  2.84676291e-02, -1.13302022e-02],\n          [-8.52283556e-03, -1.07422760e-02, -2.84677697e-03, ...,\n            6.36414066e-03,  1.55793177e-02,  2.62972061e-03]],\n \n         [[ 6.34042593e-03, -5.55177638e-03,  1.12122018e-02, ...,\n            3.84862011e-04, -7.39399204e-03, -2.59233406e-03],\n          [ 1.83797081e-03,  4.37067216e-03, -2.16640974e-03, ...,\n           -9.72099602e-03, -4.33123438e-03,  6.84785657e-03],\n          [-2.52128625e-03, -9.25008021e-03,  5.38919820e-03, ...,\n            6.01652497e-03,  7.39997532e-03, -3.47864232e-03],\n          ...,\n          [ 6.54197996e-03, -1.80534972e-03,  6.87478622e-03, ...,\n           -2.96625658e-03,  7.19870348e-03, -3.49612092e-03],\n          [-1.20014576e-02, -9.55184049e-04,  3.21550784e-03, ...,\n           -6.86675077e-03,  3.81029584e-03, -1.35953142e-03],\n          [-1.33076878e-02, -5.95784932e-03,  2.00847466e-03, ...,\n            3.71311558e-03, -4.85057989e-03,  6.59804884e-03]],\n \n         [[ 2.90424004e-03, -9.89661831e-03,  1.91741413e-03, ...,\n            5.65523747e-03, -6.10089628e-03, -2.42118305e-03],\n          [-3.49682709e-03,  1.72886357e-04, -5.87909436e-03, ...,\n           -1.28070693e-02,  4.43585636e-03,  6.80614915e-03],\n          [-1.09087434e-02, -7.73155596e-03,  1.13571603e-02, ...,\n           -1.85778399e-03,  4.53320518e-03, -6.28825603e-03],\n          ...,\n          [ 4.70118131e-03,  1.73194357e-03,  2.63497210e-03, ...,\n            2.44574575e-03, -1.33330061e-03, -3.43957436e-06],\n          [-8.93086009e-03,  3.52435117e-03,  3.11156409e-03, ...,\n           -6.06179424e-03, -1.80058610e-02,  1.25560276e-02],\n          [-1.18324636e-02,  8.83054268e-03,  3.15376674e-03, ...,\n           -1.96965900e-03, -5.68017829e-03,  7.08003854e-03]]],\n \n \n        [[[ 8.75416305e-03, -4.36640298e-03,  4.88167582e-03, ...,\n            8.39018787e-04, -5.99739561e-03, -1.24212792e-02],\n          [ 4.01801849e-03,  3.51571711e-04,  3.97934671e-03, ...,\n            1.03333630e-02, -1.19119482e-02,  1.45259704e-02],\n          [-1.23062739e-02, -7.49821402e-03, -1.16881379e-03, ...,\n            1.00372350e-02,  3.78591637e-03, -3.27527802e-03],\n          ...,\n          [ 3.92382033e-04, -5.11869090e-03, -5.72134461e-03, ...,\n           -5.19906683e-03,  2.44495110e-03, -2.64277123e-03],\n          [ 3.15165427e-03,  4.59999545e-03, -4.49594622e-03, ...,\n           -7.07745692e-03,  2.20317803e-02, -9.98178683e-03],\n          [ 2.48351321e-03, -4.45801299e-03, -1.41727342e-03, ...,\n            1.14606246e-02,  2.31800862e-02,  1.90563314e-03]],\n \n         [[ 9.47191007e-03, -3.94198066e-03,  3.83984810e-03, ...,\n            2.04855483e-03, -3.98352416e-03, -9.07041319e-03],\n          [ 4.09901841e-03, -5.52556419e-04,  9.95470490e-03, ...,\n            6.61444664e-03,  1.68336404e-03,  5.98631799e-03],\n          [-5.33673773e-03, -3.10067786e-03, -2.68109678e-03, ...,\n            6.30002609e-03,  9.23030172e-03, -1.12451822e-03],\n          ...,\n          [ 3.42691923e-03, -3.57319019e-03, -3.67652508e-03, ...,\n           -2.96659744e-03,  2.97217467e-03, -5.51824691e-03],\n          [-6.08776370e-03, -3.16021242e-03, -3.34903644e-03, ...,\n           -1.02244793e-02,  5.09558059e-03, -7.77694955e-03],\n          [-6.22821553e-03, -5.55740483e-03,  1.33099854e-02, ...,\n            1.53019335e-02, -1.71303935e-03,  3.84857692e-03]],\n \n         [[ 6.73177093e-03, -7.76051125e-03, -1.60716416e-03, ...,\n            3.37521662e-03, -3.43736820e-03, -7.08474452e-03],\n          [-1.92464353e-03, -1.43057271e-03,  1.68074947e-03, ...,\n            1.18604672e-04,  4.35965555e-03,  5.03915455e-03],\n          [-4.46902961e-03, -7.52293598e-03, -3.74621828e-03, ...,\n           -2.28520279e-04,  2.16215150e-04, -1.52036909e-03],\n          ...,\n          [ 4.53559542e-03, -1.34184927e-04, -9.41309205e-04, ...,\n            1.11481512e-03, -4.47113672e-03, -4.62406827e-03],\n          [ 9.73428891e-04, -9.99093894e-03, -3.64008895e-03, ...,\n           -7.60031957e-03, -1.03551112e-02,  3.14689800e-03],\n          [-8.02905019e-03,  7.79318798e-04,  2.27084830e-02, ...,\n            1.30782668e-02,  1.38504582e-03,  7.73181953e-03]]]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32, numpy=\n array([ 2.36804616e-02,  4.58608419e-02,  9.30080377e-03, -1.76745132e-02,\n         5.61465882e-02, -3.86770591e-02, -2.51645874e-02, -4.41752784e-02,\n         1.51768718e-02, -3.16961072e-02,  8.50665495e-02,  7.12005869e-02,\n         7.06173340e-03,  2.40633171e-03, -4.92551364e-04,  4.08053398e-02,\n        -9.15167946e-03,  2.95944735e-02,  5.72967753e-02,  7.18083829e-02,\n         4.05868217e-02,  1.59624778e-02, -4.80147712e-02,  8.60013366e-02,\n         1.49936572e-01,  5.72768301e-02,  2.15365668e-03,  9.44054965e-03,\n         4.17007990e-02,  5.99614047e-02,  3.43391895e-02,  6.58695325e-02,\n         2.11896710e-02,  8.31157863e-02,  5.43225817e-02, -5.79251582e-03,\n         3.20369564e-02, -8.30057659e-04, -4.15707454e-02,  9.22825262e-02,\n         6.68698847e-02, -6.84389696e-02,  9.47043598e-02, -1.12843735e-03,\n        -2.23761816e-02,  2.50922889e-02, -2.84489598e-02,  5.39898127e-02,\n         7.43930340e-02,  3.20168249e-02,  8.59978795e-03, -5.32773882e-02,\n         4.53467518e-02,  3.28817107e-02,  5.67115955e-02,  7.34080523e-02,\n         3.73719223e-02,  3.60966846e-02,  3.43550667e-02,  5.64249307e-02,\n         2.54807081e-02,  6.70317188e-02,  1.28744515e-02,  9.89226550e-02,\n         6.96597947e-03, -3.15162614e-02,  4.48129922e-02,  8.75441507e-02,\n         6.66466355e-02,  4.87044863e-02,  6.54821023e-02,  1.99410841e-02,\n         5.62615953e-02,  3.89720760e-02,  1.39327748e-02,  1.10009555e-02,\n         6.95480630e-02,  3.56875435e-02, -1.68426204e-02,  2.98396703e-02,\n         2.90730037e-02,  6.01015538e-02,  2.66220719e-02, -3.43452133e-02,\n         7.85440952e-02,  3.76850516e-02,  4.77634035e-02, -1.97641533e-02,\n        -4.85963747e-02,  4.12238836e-02, -5.63965291e-02,  7.81174526e-02,\n        -7.17336386e-02, -4.26510349e-02,  1.49284765e-01,  2.79012583e-02,\n         7.29890093e-02,  8.44665095e-02,  1.24374516e-02,  3.11371125e-02,\n         3.95305417e-02,  8.57612863e-03,  3.95121984e-02,  5.59164956e-02,\n         1.30104106e-02,  8.03536028e-02, -2.42298953e-02, -8.33528489e-02,\n         3.14368680e-02,  7.11350963e-02,  4.30872925e-02,  7.73343956e-04,\n         8.30505490e-02,  7.94098452e-02, -7.20322924e-03, -3.55564989e-02,\n         5.22524863e-02,  5.55702783e-02,  8.56031999e-02,  5.28425612e-02,\n        -3.70398574e-02,  8.51874650e-02,  6.22633323e-02,  3.93357947e-02,\n        -1.28750065e-02,  4.44916598e-02, -2.21009292e-02,  5.60100749e-02,\n        -2.78310552e-02,  1.34631293e-02, -9.32684354e-03,  6.18774630e-02,\n         4.95636985e-02, -1.75742130e-03, -6.44769073e-02, -1.18143186e-02,\n         1.88705772e-02, -1.64837334e-02, -2.21495563e-03,  5.64453267e-02,\n         9.39463228e-02, -5.49071236e-03,  1.04334205e-01, -1.40368156e-02,\n         3.51246423e-03,  2.20790058e-02, -9.24790557e-03,  3.40608843e-02,\n         8.49933252e-02,  1.35613550e-02,  7.09332302e-02,  7.53967017e-02,\n         2.47984054e-03,  2.75810179e-03,  2.61413254e-04, -3.28828371e-03,\n         6.55157939e-02,  6.88281208e-02, -1.43462196e-02, -4.51357253e-02,\n         9.97304320e-02,  1.88522600e-02,  5.81260659e-02, -4.68679285e-03,\n         5.57830883e-03, -1.78299807e-02, -3.54016311e-02,  6.53371587e-02,\n         1.66017213e-03,  1.20751202e-01,  1.06749490e-01,  6.68219179e-02,\n        -1.23153459e-02, -1.70852672e-02,  6.34886473e-02, -1.96746569e-02,\n         3.80039997e-02,  5.81620410e-02,  7.67944083e-02, -3.60054709e-02,\n         1.79855078e-01,  1.34394066e-02,  4.39099148e-02,  1.57692712e-02,\n         1.69655830e-02,  2.86468342e-02,  1.62922195e-03,  3.54323685e-02,\n        -6.61849082e-02,  6.75174445e-02,  2.82581467e-02,  6.62062168e-02,\n        -1.12029212e-02, -1.03414077e-02,  9.13238619e-03,  1.23161366e-02,\n         2.86449417e-02,  3.96534093e-02,  4.21681348e-03,  4.30835858e-02,\n         2.70667262e-02,  6.36299849e-02, -3.28771397e-02, -3.93768884e-02,\n        -3.95091139e-02,  3.15880030e-02,  3.39579359e-02,  1.45476088e-01,\n         1.81212425e-02, -1.29203789e-03,  4.00362015e-02,  6.00088015e-02,\n        -3.58503722e-02,  3.18499692e-02,  2.06580404e-02,  7.16133118e-02,\n         5.03218733e-02, -1.84153430e-02, -5.64385876e-02,  1.77689474e-02,\n         4.27779555e-02,  5.53759374e-03,  8.75296220e-02,  7.26943687e-02,\n        -1.46048391e-04, -3.86572555e-02,  8.42765160e-03, -2.44882749e-03,\n         9.77501273e-02,  5.97970672e-02,  2.38099266e-02,  6.74851984e-02,\n         6.92723393e-02, -2.47257901e-03, -1.88391749e-02,  7.13638812e-02,\n        -2.04261877e-02,  1.72214035e-03, -1.22058857e-02,  5.31939827e-02,\n         7.06707910e-02,  3.33899781e-02,  2.76175775e-02, -3.17915380e-02,\n        -3.35121602e-02,  6.86854348e-02,  6.27762377e-02,  1.82375818e-01,\n         2.08962727e-02,  2.71611679e-02,  7.87345693e-02,  1.27381524e-02,\n         4.10160795e-02,  8.34491253e-02,  3.96295004e-02, -3.02669052e-02,\n         2.91156285e-02,  5.06780371e-02,  7.99330138e-03,  4.92900796e-02,\n         6.86231107e-02, -4.09106277e-02,  4.41469885e-02,  3.64646725e-02,\n         5.65120801e-02,  3.60980742e-02, -6.79135090e-04, -1.23290103e-02,\n         5.94152361e-02,  6.77347705e-02,  3.31165828e-02,  8.35942701e-02,\n         8.64599496e-02,  1.75043959e-02,  5.81907071e-02,  4.46802489e-02,\n         3.51500278e-03,  5.20365909e-02,  4.39204052e-02, -2.08746772e-02,\n        -2.65764166e-03, -2.51777451e-02, -1.43754883e-02,  1.98809154e-04,\n        -3.50387505e-04,  7.73703633e-03,  1.29311699e-02,  1.20962057e-02,\n         9.34748948e-02,  3.94030213e-02,  7.13626072e-02, -1.41413277e-02,\n         1.30935848e-01,  5.73753752e-02,  4.46959585e-02, -2.83598644e-03,\n         7.10531026e-02,  5.70916161e-02,  3.74317057e-02,  1.44724911e-02,\n        -3.37322205e-02,  8.81157368e-02,  1.48834094e-01,  3.28580663e-02,\n         7.96187371e-02,  1.93439052e-02, -2.61149053e-02,  3.17374878e-02,\n        -5.79251535e-03,  4.70720194e-02,  9.29550678e-02, -1.25735486e-03,\n         6.31417558e-02,  2.10241545e-02,  7.46275783e-02,  3.79877761e-02,\n         8.13457370e-03,  1.80716086e-02, -8.13563913e-02,  8.72360542e-02,\n         3.88438925e-02,  3.85152847e-02, -5.04073105e-04,  6.49840385e-02,\n        -3.43470611e-02,  3.77040654e-02,  1.06859244e-02,  4.20840010e-02,\n        -1.40292663e-02,  1.56042529e-02, -5.81686664e-03,  9.35967714e-02,\n        -3.35334591e-03,  9.38937068e-03,  5.93062770e-03,  6.13242537e-02,\n         3.53276427e-03,  2.86255982e-02,  1.81747284e-02,  2.83325668e-02,\n         3.06540430e-02,  3.95519510e-02, -5.09557687e-02,  1.71096679e-02,\n         3.50877829e-02,  2.47029122e-02,  2.00173520e-02,  1.35433283e-02,\n        -2.70954855e-02,  3.94329019e-02,  5.53422607e-02,  4.07129750e-02,\n        -3.68579365e-02,  3.36302258e-02,  1.04768693e-01,  9.88726839e-02,\n        -4.80895909e-03,  4.32820879e-02, -2.89156623e-02, -2.33092438e-02,\n         1.11722033e-02,  8.99728853e-03,  4.12003621e-02,  4.24312092e-02,\n         3.04449759e-02,  1.79922655e-02,  3.36466879e-02,  3.25002447e-02,\n         2.52752118e-02,  3.15993875e-02,  8.63030627e-02,  8.40602890e-02,\n         1.13799475e-01,  1.04616433e-01,  9.19467285e-02,  5.76423779e-02,\n         1.50477784e-02,  6.21597804e-02, -8.19574576e-03,  1.45130316e-02,\n         6.93013240e-03, -1.41948294e-02,  9.23714042e-02,  2.28072926e-02,\n         4.99698967e-02,  2.91759204e-02, -3.53789842e-03,  6.67891949e-02,\n         8.36458728e-02,  6.09480292e-02, -1.20814377e-02,  5.52044734e-02,\n         7.13785812e-02,  2.94296518e-02, -1.88638773e-02,  7.29193389e-02,\n         6.74001798e-02,  3.02355215e-02, -7.86741672e-04,  1.45119922e-02,\n         8.32995921e-02, -1.73066929e-02, -1.24155683e-02,  1.52539127e-02,\n         1.04089398e-02,  5.72987050e-02, -4.07144874e-02,  3.56261283e-02,\n         3.88583280e-02, -5.36897331e-02, -9.50060226e-03,  6.02441728e-02,\n         4.38612467e-03,  8.26571807e-02,  3.73540111e-02,  4.33641709e-02,\n         1.30440863e-02,  6.59309933e-03, -1.42299663e-02,  7.20351040e-02,\n         2.15775855e-02, -4.12956886e-02,  1.52458809e-02,  7.83393085e-02,\n         1.87031198e-02,  4.43611369e-02,  1.41580310e-02,  6.79276958e-02,\n         7.30631500e-02,  2.71355044e-02,  4.56770025e-02,  1.13588129e-03,\n        -3.38738561e-02,  6.33674264e-02,  2.99458709e-02,  3.39248106e-02,\n         1.06991984e-01, -8.42845291e-02,  4.86224405e-02,  1.18488865e-03,\n        -3.36305089e-02,  4.24424782e-02,  5.77495024e-02,  3.30150053e-02,\n         8.73824283e-02, -4.48965207e-02, -5.31662896e-04,  5.03749959e-02,\n        -4.53572124e-02, -6.34666234e-02,  2.32011992e-02,  5.11378534e-02,\n        -6.63259625e-03, -7.29774730e-03,  1.05731055e-01,  4.01319265e-02,\n         4.83435765e-02,  2.68024467e-02,  1.22054461e-02,  3.18117142e-02,\n         1.63015239e-02,  9.50399265e-02,  4.35097888e-02,  7.05098733e-02,\n         8.54057595e-02,  5.15690167e-03, -1.57226622e-02,  1.24573760e-01,\n         9.91350785e-03,  1.01143122e-01,  7.39932582e-02, -8.41629207e-02,\n         6.01060316e-02,  4.33958322e-02,  1.55565543e-02, -1.67539064e-02,\n         4.74077426e-02,  1.56774074e-01,  2.07913518e-02, -2.10556053e-02,\n        -3.02008906e-04,  6.85021579e-02, -6.47229934e-03, -2.29938451e-06,\n         4.77778434e-04,  2.19754819e-02,  5.20597920e-02,  7.71911144e-02,\n        -7.61257559e-02,  5.04526794e-02,  1.11279115e-02,  4.20127772e-02,\n         9.41807628e-02,  5.82801849e-02,  7.23777413e-02,  2.26395763e-02,\n        -6.38820156e-02,  1.67707615e-02, -7.39061274e-03,  1.33731205e-03,\n         9.29854214e-02,  1.62864681e-02,  1.48589224e-01,  1.30760461e-01,\n         5.62454313e-02,  1.43612195e-02,  6.42168671e-02,  4.83797006e-02,\n         1.25911102e-01,  1.25651538e-01,  7.45308250e-02, -1.71924084e-02],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32, numpy=\n array([[[[-4.13556537e-03,  7.47195398e-03, -3.50115891e-03, ...,\n            4.89854301e-06, -2.53839465e-03, -5.25326468e-03],\n          [-9.53834411e-03,  1.41056092e-03, -1.65442061e-02, ...,\n           -1.52774970e-03,  1.11036829e-03, -1.79786165e-03],\n          [-3.51967965e-03,  8.38233996e-03,  1.37014373e-03, ...,\n           -2.04362301e-03, -3.33098392e-03, -3.59164621e-03],\n          ...,\n          [-4.51420387e-03, -9.90713015e-03, -7.90119357e-03, ...,\n           -2.74181948e-04,  8.04592157e-04, -9.21490043e-03],\n          [ 4.13362030e-03,  1.64503278e-03, -1.24853069e-03, ...,\n           -4.80085961e-04,  1.43603736e-03, -2.37931171e-03],\n          [-9.46063921e-03, -3.02881887e-03, -3.93793161e-04, ...,\n           -8.37404653e-03, -8.49721371e-04,  2.19010212e-03]],\n \n         [[-4.10446478e-03, -1.54824147e-03, -5.41875651e-03, ...,\n           -2.24129972e-03, -2.28596875e-03, -3.34858289e-03],\n          [-4.84787673e-03,  2.47201161e-03, -1.29879676e-02, ...,\n            8.18996597e-03, -4.54588607e-03,  9.62644164e-03],\n          [-3.10205529e-03,  3.47901974e-03,  2.52036308e-03, ...,\n           -5.83003787e-03,  1.09997432e-04, -3.84454383e-03],\n          ...,\n          [-7.76591944e-03, -1.44146327e-02, -7.61551922e-03, ...,\n           -3.92687973e-03, -6.16982626e-03, -5.35330735e-03],\n          [ 1.01468374e-03,  1.67173836e-02, -6.25725277e-03, ...,\n           -2.52300198e-03, -4.94909706e-03, -9.14588978e-04],\n          [-7.97870383e-03, -4.20195796e-03,  1.45270012e-03, ...,\n           -1.22739337e-02, -2.10561347e-03,  5.49114076e-03]],\n \n         [[-2.88630649e-03,  2.41750688e-03,  1.50172273e-03, ...,\n           -1.28930504e-03, -4.96551802e-04,  2.69375363e-04],\n          [ 3.95747193e-05, -1.94887386e-03, -7.08143180e-03, ...,\n            1.00128017e-02, -4.74765385e-03,  1.33702094e-02],\n          [ 4.38523851e-03,  2.79099098e-03, -4.09206329e-03, ...,\n           -2.73540127e-03, -3.12048779e-03, -2.88168900e-03],\n          ...,\n          [-4.95913485e-03, -1.23296445e-02, -5.42156352e-03, ...,\n           -2.32423958e-03, -8.56377464e-03, -7.15223281e-03],\n          [-6.16921112e-03,  1.57617237e-02, -4.47978219e-03, ...,\n           -2.25248467e-03, -9.93228331e-03, -6.10898901e-03],\n          [-4.50641429e-03, -5.51745854e-03, -2.20075878e-03, ...,\n           -1.30875418e-02, -3.93497292e-03,  6.03448506e-03]]],\n \n \n        [[[-4.41893004e-03,  1.37486542e-02, -8.45907908e-03, ...,\n           -2.21222895e-03, -2.15952285e-03,  1.23409089e-02],\n          [-6.33585500e-03, -4.42368211e-03, -3.91021417e-03, ...,\n           -9.76585504e-03,  1.04509713e-02, -1.36236623e-02],\n          [-4.40114317e-03,  7.94552453e-03,  2.42980407e-03, ...,\n           -5.41694043e-03,  8.71099706e-04, -9.86627582e-03],\n          ...,\n          [-1.34519069e-02, -1.00430250e-02,  1.51449756e-04, ...,\n            1.60089054e-04,  3.75668914e-03, -9.09452140e-03],\n          [ 2.45954143e-03,  5.74873528e-03, -4.06519882e-03, ...,\n            4.02215123e-03,  7.62883713e-03,  5.94564248e-03],\n          [-9.32596903e-03, -1.13636081e-03, -6.24895329e-03, ...,\n           -1.08463904e-02,  1.40982226e-03, -7.70578452e-04]],\n \n         [[-5.51303476e-03,  5.81868412e-03, -1.07901720e-02, ...,\n           -3.43920151e-03, -7.63218326e-04,  1.36730876e-02],\n          [-1.67119992e-03, -3.32675921e-03,  7.43533950e-03, ...,\n           -3.50478198e-03,  6.62456639e-03, -4.99307271e-03],\n          [-5.86137315e-03,  6.13010547e-04,  8.30108393e-03, ...,\n           -9.87416040e-03,  5.06723206e-03, -7.71902688e-03],\n          ...,\n          [-1.29670175e-02, -1.55268898e-02, -8.49602278e-04, ...,\n            6.30461425e-03,  5.32882579e-04, -6.07468095e-03],\n          [-1.12099759e-03,  2.91328244e-02, -4.23871959e-03, ...,\n            2.93749222e-03,  3.47164372e-04,  8.26061238e-03],\n          [-5.98738715e-03, -9.14713892e-04, -3.92263103e-03, ...,\n           -1.36881256e-02, -8.12742568e-04,  7.36264093e-03]],\n \n         [[-3.41959228e-03,  4.66367882e-03,  3.37764691e-03, ...,\n           -2.56140856e-03, -1.21631820e-04,  1.15059484e-02],\n          [ 2.40420690e-04, -3.77301918e-03,  6.58437563e-03, ...,\n            9.60138626e-03,  2.05660984e-03,  9.79973841e-03],\n          [ 1.32909336e-04,  1.90253675e-04,  6.17130077e-04, ...,\n           -8.72793701e-03, -1.45053165e-03, -3.49612208e-03],\n          ...,\n          [-9.77272261e-03, -1.07788146e-02, -9.54640738e-04, ...,\n            6.30754652e-03, -2.78572715e-03, -5.98356128e-03],\n          [-1.03954598e-02,  2.61091348e-02, -5.07673249e-03, ...,\n           -2.80671171e-03, -5.53699769e-03, -2.10620486e-03],\n          [-3.11570894e-03, -3.08538810e-03, -3.10157356e-03, ...,\n           -1.59878880e-02, -3.33151151e-03,  1.18652089e-02]]],\n \n \n        [[[-4.26715612e-03,  8.63417238e-03,  1.76529714e-03, ...,\n           -2.84227985e-03, -1.75439665e-04,  2.90537663e-02],\n          [-6.29396923e-03, -1.76275475e-03, -6.42163330e-04, ...,\n           -1.01052411e-02,  3.43423686e-03, -1.41305421e-02],\n          [-3.68094444e-03,  6.39184844e-03,  7.07066664e-03, ...,\n           -2.24087667e-03,  8.55041016e-03, -8.31523445e-03],\n          ...,\n          [-1.18850870e-02, -7.78572122e-03, -4.24755132e-03, ...,\n           -1.43992031e-04,  2.01374627e-04, -2.33163615e-03],\n          [ 2.30761128e-03, -9.47567285e-04, -1.79581740e-03, ...,\n           -4.05802298e-03,  5.83436852e-03,  1.24853859e-02],\n          [-7.39294710e-03,  6.41175592e-03, -1.01529667e-02, ...,\n           -1.17332852e-02,  1.69780722e-03,  1.15358143e-03]],\n \n         [[-4.61219344e-03, -3.59420269e-03, -8.21814407e-04, ...,\n           -4.81084595e-03, -4.44865436e-04,  3.18669118e-02],\n          [-1.15395896e-03, -5.12891915e-03,  9.74675361e-03, ...,\n           -7.82980397e-03,  7.25134509e-03, -6.98588043e-03],\n          [-5.98424906e-03,  1.12204099e-04,  9.43994615e-03, ...,\n           -4.51271143e-03,  8.94110464e-03, -8.36270489e-03],\n          ...,\n          [-1.31882168e-02, -9.57226939e-03, -6.37137098e-03, ...,\n            1.04999328e-02, -1.76120212e-03, -3.13745732e-05],\n          [ 1.22509198e-03,  1.17349727e-02, -2.26895651e-03, ...,\n           -6.69820094e-03,  4.28338209e-03,  1.29151903e-02],\n          [-2.21199635e-03,  1.19306184e-02, -3.41698364e-03, ...,\n           -1.32360598e-02, -4.97587898e-04,  1.26575744e-02]],\n \n         [[-3.64933652e-03, -4.82958928e-03,  9.14632156e-03, ...,\n           -4.02110023e-03, -1.57763090e-04,  1.70223936e-02],\n          [ 6.29810616e-04, -2.43360968e-03,  6.34040590e-03, ...,\n            3.36989964e-04,  6.95535308e-03,  1.70514290e-03],\n          [-3.24976468e-03,  2.30040471e-03,  4.15086048e-03, ...,\n           -4.70836135e-03, -1.04901823e-03, -6.13272097e-03],\n          ...,\n          [-1.13391438e-02, -4.96303104e-03, -2.09907000e-03, ...,\n            9.20415949e-03, -3.33326217e-03, -1.52914249e-03],\n          [-3.99327371e-03,  1.23460982e-02, -3.62788007e-04, ...,\n           -9.96733084e-03, -2.58089555e-03,  4.87980247e-03],\n          [-1.03235664e-03,  1.21477433e-03,  4.89632657e-04, ...,\n           -1.49842957e-02, -3.02216387e-03,  1.48773128e-02]]]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32, numpy=\n array([ 3.75776812e-02,  2.33153161e-02,  4.87732068e-02, -1.31543912e-02,\n         1.56386122e-01,  1.23117670e-01, -9.89586115e-03,  7.15063512e-02,\n         6.51094466e-02, -4.66837473e-02,  1.08425960e-01,  7.74275884e-02,\n         1.10251516e-01,  1.92952771e-02,  8.01710784e-02,  2.36119628e-02,\n         7.75422528e-02,  5.69755211e-02,  4.56143469e-02,  1.09167516e-01,\n         6.84632063e-02, -2.48736758e-02,  1.80764552e-02, -4.36790064e-02,\n         2.33979343e-04,  3.46214063e-02,  6.20072754e-03,  3.63811068e-02,\n        -1.55856647e-03, -4.96735498e-02,  3.23276311e-01, -1.96249466e-02,\n         3.84185798e-02,  1.20205760e-01,  1.09417096e-01, -5.51194139e-02,\n         1.10063709e-01, -7.15378523e-02, -4.00965102e-03, -5.51721174e-03,\n         4.30769622e-02,  5.59166037e-02,  2.27040742e-02, -5.66086136e-02,\n         3.19737196e-02,  5.85606098e-02, -3.15547064e-02,  7.50048980e-02,\n         1.69006623e-02,  1.00639649e-01,  1.32110983e-01,  1.37882242e-02,\n         6.91086054e-02,  8.38134661e-02,  9.01862830e-02,  1.04315668e-01,\n         2.41040811e-02, -1.04453005e-02,  1.57044113e-01, -8.38551857e-03,\n         4.52776887e-02,  2.68810600e-01,  4.62417714e-02,  1.61446109e-01,\n        -8.97112023e-03,  7.49935722e-03,  5.90045098e-03, -2.37324424e-02,\n         5.56173995e-02,  7.36198798e-02, -1.26277376e-02,  9.59626958e-02,\n         6.56266604e-03,  2.39787474e-01, -7.63456523e-02,  5.05909026e-02,\n        -1.48703540e-02, -1.69649515e-02,  6.16798475e-02,  8.90753344e-02,\n        -1.60123035e-02,  6.03094324e-02,  9.99760032e-02,  9.37769562e-02,\n         1.19709454e-01, -7.12951645e-03,  1.57594681e-01,  1.17099583e-01,\n         4.32997309e-02,  3.10191326e-03,  4.34731580e-02, -1.44030163e-02,\n         2.50148028e-02,  3.80369974e-03,  3.57966982e-02, -2.04993486e-02,\n         4.41434085e-02,  1.19379260e-01, -7.77974278e-02,  8.29918310e-02,\n        -1.09645920e-02,  6.94096508e-03,  9.36392620e-02,  2.11961791e-02,\n         3.53810154e-02,  5.99042177e-02,  8.37488249e-02,  1.61172628e-01,\n         3.94175723e-02,  4.38806564e-02, -4.46698554e-02,  7.57642016e-02,\n         2.54059248e-02,  1.07697628e-01, -2.26416271e-02, -7.51747862e-02,\n         1.16588682e-01,  4.07809131e-02,  6.54405504e-02, -7.98612386e-02,\n        -5.34622893e-02,  1.35034159e-01, -8.23003519e-03,  8.71877298e-02,\n         6.22871146e-02, -7.36035183e-02, -6.41531274e-02, -3.40898372e-02,\n         4.07952182e-02, -1.76432151e-02,  1.54758960e-01,  4.22448628e-02,\n         5.93631677e-02, -8.93671066e-02, -3.87337580e-02,  3.13355103e-02,\n        -1.28580444e-02,  9.12920609e-02, -3.70050780e-02,  7.26855099e-02,\n         1.26496330e-01,  6.18727617e-02,  7.48781580e-03,  2.12940797e-02,\n         8.53048190e-02,  1.16060473e-01,  8.07167664e-02, -1.11688130e-01,\n         7.46915489e-02, -4.41960469e-02,  3.92803811e-02, -2.14993507e-02,\n        -2.69011334e-02,  1.03029246e-02,  8.14891532e-02, -9.10045579e-02,\n         5.43500483e-02, -8.82004276e-02,  8.82978830e-03,  4.33639102e-02,\n         7.13211223e-02, -4.12511006e-02,  3.98692861e-02, -6.19892590e-02,\n        -6.01806827e-02,  2.01278832e-02, -3.58758192e-03,  3.78665552e-02,\n         1.38902327e-03,  8.03192034e-02, -3.72954980e-02, -6.32626098e-03,\n         8.30466226e-02,  4.92896251e-02,  1.26651511e-01,  7.76356179e-03,\n        -2.94745415e-02, -7.80526432e-04,  6.92223758e-02,  1.30362123e-01,\n         1.84790239e-01,  1.91820972e-02, -1.49902813e-02,  1.10085964e-01,\n         6.89911991e-02, -3.02578788e-04, -1.15961619e-01,  5.62943071e-02,\n         4.55947258e-02,  3.66118290e-02,  5.18099628e-02,  4.19007167e-02,\n         3.45699228e-02, -2.12627202e-02,  2.03489400e-02, -3.77459219e-03,\n        -7.30822235e-02,  3.36929522e-02,  9.80711281e-02,  7.44810402e-02,\n        -1.37623236e-01,  9.87377837e-02,  7.38442093e-02,  5.82326315e-02,\n        -4.73434590e-02, -2.95034284e-03,  7.63397291e-03,  3.93807627e-02,\n         6.29925206e-02,  4.16261982e-03,  1.30529091e-01, -7.88602140e-03,\n         9.76980031e-02,  1.15821004e-01,  1.81243010e-02, -7.63205364e-02,\n         7.11237895e-04,  7.99729973e-02, -3.18900235e-02,  4.17382717e-02,\n         1.13078449e-02,  3.89297232e-02, -1.72820743e-02, -5.58638088e-02,\n        -4.65179211e-04, -1.73023494e-03, -7.81372562e-03, -5.39793679e-03,\n         2.23870226e-03,  1.73662715e-02, -1.98377687e-02, -2.58054938e-02,\n         1.09198786e-01,  7.00220047e-03,  1.10002654e-02, -6.27600923e-02,\n        -3.32509121e-03,  2.23918274e-01,  1.01886550e-02, -2.82219443e-02,\n         7.38132671e-02,  2.91661564e-02,  3.13512050e-02, -2.51422524e-02,\n        -1.21693537e-02,  1.09122060e-01, -1.74833387e-02, -2.37340331e-02,\n         1.10582992e-01,  6.28124774e-02,  6.17114380e-02,  2.62949709e-02,\n         2.30104420e-02, -2.80443416e-03,  1.04754241e-02, -4.64987308e-02,\n         1.21549122e-01,  3.67750637e-02,  2.87013669e-02, -1.27652809e-01,\n        -1.33002065e-02,  1.30174324e-01,  1.72204357e-02,  1.41560405e-01,\n        -1.06372379e-01,  6.09373860e-02,  3.22992653e-02,  3.37665468e-01,\n         7.69582912e-02,  6.41081259e-02, -9.23939794e-02,  1.75577775e-01,\n        -7.04026893e-02,  1.75782852e-02, -3.75610068e-02,  9.56635773e-02,\n        -5.07870689e-02, -2.75258590e-02, -6.19521625e-02,  2.95777898e-03,\n         1.06505699e-01, -7.43966773e-02,  3.04472782e-02, -1.99559182e-02,\n         4.39408347e-02,  5.16868979e-02,  4.41786535e-02,  7.64883077e-03,\n         8.63148868e-02,  9.03494209e-02,  5.30875959e-02,  5.55970967e-02,\n         6.36413768e-02,  5.47764935e-02, -1.24607049e-02,  4.54404112e-03,\n         2.73986440e-02, -4.45177443e-02,  1.14658087e-01,  1.37271164e-02,\n         2.32506655e-02,  2.34096195e-03,  2.09581060e-03,  4.96438053e-03,\n         4.18750420e-02,  2.29809508e-02,  2.97142528e-02,  3.22302803e-02,\n         9.11867023e-02, -5.36765791e-02,  7.12390319e-02,  8.89192987e-03,\n         7.42134005e-02,  1.41858786e-01, -8.39287117e-02,  9.21422243e-02,\n         8.63370523e-02,  7.90292323e-02,  4.39192131e-02,  1.96809378e-02,\n        -2.77533829e-02,  1.15503237e-01,  1.58360928e-01, -8.54258910e-02,\n         8.24679807e-03,  5.85923018e-03,  9.32543650e-02, -4.55506667e-02,\n         9.26764756e-02,  1.15752965e-01, -2.97578033e-02, -2.86657400e-02,\n        -1.52926147e-02,  2.70879250e-02, -1.46033755e-02, -2.50229500e-02,\n         3.27791199e-02,  8.65786448e-02,  2.88362987e-03,  4.49427143e-02,\n         1.09195178e-02,  5.43190241e-02, -7.34395534e-03,  6.12054355e-02,\n         9.58566293e-02,  9.73507762e-02,  1.42083680e-02, -1.69505030e-02,\n        -1.49713429e-02, -5.62502677e-03,  4.46661711e-02,  7.96836764e-02,\n        -1.16956914e-02,  8.77603516e-02, -4.11723927e-02,  5.75434640e-02,\n         3.29620466e-02, -1.90446731e-02, -2.89780404e-02, -2.39247046e-02,\n         9.70396772e-02,  4.75698523e-02,  7.76923820e-02, -2.59347353e-02,\n         2.60097738e-02,  7.36013651e-02,  6.40694574e-02,  4.28020656e-02,\n         1.26856104e-01,  1.59579590e-01,  8.83957446e-02, -5.42309927e-03,\n         2.63387710e-02,  1.85682960e-02,  6.53882697e-02,  1.09975189e-02,\n         6.88788518e-02,  5.74412979e-02,  5.93333468e-02,  5.48945591e-02,\n         6.09451346e-02,  8.59644338e-02,  2.02770546e-01,  4.29083183e-02,\n         7.97930509e-02,  9.53034610e-02, -1.28447879e-02, -1.04903094e-02,\n         1.73322037e-02, -5.24674766e-02, -3.19139324e-02,  4.93030250e-02,\n         2.71479483e-03,  1.62390828e-01,  1.20086335e-01,  1.26804143e-01,\n        -5.88331409e-02,  1.53022245e-01,  5.09878583e-02, -1.86842903e-02,\n         6.00987040e-02,  5.23871966e-02, -7.51864463e-02,  1.64992996e-02,\n        -5.40164784e-02,  3.61461658e-04,  8.46449882e-02, -1.95790883e-02,\n         5.58903217e-02,  4.40694839e-02, -4.93288301e-02,  6.25271946e-02,\n         8.78013205e-03, -4.18124459e-04,  1.23570431e-02, -3.78970653e-02,\n        -6.56176805e-02, -2.17026584e-02,  6.12869449e-02, -2.76663955e-02,\n        -2.71973237e-02,  8.20611976e-03,  6.85504675e-02, -3.78268920e-02,\n        -4.82786186e-02, -2.73993164e-02, -3.26478891e-02, -3.15340282e-03,\n         8.87275264e-02,  1.06631555e-01,  3.64949666e-02,  1.62101001e-01,\n        -4.19401079e-02, -1.07982382e-01,  4.45775203e-02,  5.77827320e-02,\n         2.23185256e-01, -3.65598798e-02, -1.03740953e-02,  7.37543330e-02,\n        -5.21152234e-03, -1.00183100e-01,  9.83282644e-03, -1.42585650e-01,\n        -2.24055368e-02,  8.60606954e-02,  8.35221857e-02, -4.14283127e-02,\n        -6.31701499e-02,  4.49785119e-04,  1.11907467e-01,  3.42719327e-03,\n         4.70628962e-02, -6.42979667e-02,  6.84684664e-02, -4.96568903e-03,\n        -7.05677737e-03,  9.94443987e-03,  1.10456072e-01,  6.95149153e-02,\n         2.07045719e-01,  2.15834957e-02, -8.14040601e-02,  5.66582233e-02,\n         3.21405888e-01,  1.33668007e-02,  4.54747789e-02,  3.44119929e-02,\n         2.47866027e-02, -1.98359475e-01,  5.16267866e-02,  8.08398426e-02,\n         3.98843661e-02, -6.38091052e-03,  1.49523010e-02,  8.16762224e-02,\n         1.53234810e-01,  5.12944162e-02,  3.50128068e-03,  1.35419620e-02,\n         4.13721949e-02,  1.52683899e-01, -3.00547630e-02,  3.43827270e-02,\n         3.84746045e-02, -2.30445340e-02,  1.39713809e-01,  1.05473228e-01,\n         1.98547374e-02,  9.14304703e-02,  7.07473198e-04, -8.16547126e-03,\n         4.24766019e-02,  1.28319293e-01,  2.55543962e-02,  2.28070617e-02,\n        -3.57526243e-02, -2.44863424e-02,  1.94404684e-02, -1.58098768e-02,\n        -9.37856138e-02,  4.62724715e-02,  3.18735167e-02, -1.16503248e-02,\n         1.63484275e-01,  5.40490299e-02, -9.96737741e-03,  2.60503963e-03,\n        -7.91937932e-02,  7.22831637e-02, -1.29031166e-01,  5.37225716e-02],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32, numpy=\n array([[[[-6.0689339e-04,  1.4231558e-03,  7.3316055e-03, ...,\n            2.6178278e-03,  4.3357504e-04, -2.9434897e-03],\n          [-4.8605306e-03,  4.9075234e-04, -1.2511463e-03, ...,\n            1.1560039e-03, -1.2683710e-04, -1.5934335e-02],\n          [ 8.2136225e-03,  7.4967248e-03,  4.4443067e-03, ...,\n           -1.0573096e-02, -1.3803735e-02, -2.0028956e-03],\n          ...,\n          [ 2.0234480e-03, -5.8380342e-03, -2.3825851e-03, ...,\n            1.3192679e-03, -8.6920448e-03, -8.2156417e-04],\n          [ 7.9414165e-03, -8.0086980e-03,  7.7348115e-04, ...,\n            9.1014095e-03,  6.8836473e-04, -9.8630879e-03],\n          [ 7.3094536e-03, -1.1607672e-03,  5.1686643e-03, ...,\n           -1.8046533e-03, -7.7111748e-05,  3.5778836e-03]],\n \n         [[-1.5249068e-03, -3.1574788e-03,  4.5571220e-03, ...,\n           -8.1634670e-03, -8.1500728e-03, -5.8323247e-03],\n          [-1.2195857e-03, -5.4482273e-03, -8.4939813e-03, ...,\n            1.3417795e-03, -1.3884302e-03, -1.6707450e-02],\n          [-2.9635590e-03,  6.9369110e-03,  5.6030410e-03, ...,\n           -1.8767575e-02, -4.3791626e-03, -7.9249451e-03],\n          ...,\n          [-6.8472535e-04, -2.8301261e-03,  1.2871494e-03, ...,\n            1.8639782e-02, -1.4961872e-02, -1.3423259e-03],\n          [-1.5397326e-02, -1.0199412e-03,  1.0786263e-02, ...,\n            1.0112063e-02, -6.9852192e-03, -6.4757215e-03],\n          [ 1.0720392e-02, -3.2282835e-03, -1.0194050e-04, ...,\n           -4.7621364e-03,  1.0120733e-02,  5.2126674e-03]],\n \n         [[ 4.6802587e-03, -1.5438132e-03,  9.8896343e-03, ...,\n           -2.5150599e-03, -3.5095667e-03, -5.6064618e-03],\n          [ 1.0596886e-02, -1.0044489e-02,  1.8063044e-03, ...,\n           -2.9223200e-03,  8.0995419e-04, -2.0323446e-02],\n          [-1.7607687e-03,  2.3017763e-03, -1.5706603e-03, ...,\n           -1.1904801e-02, -5.6624818e-03, -2.7024033e-03],\n          ...,\n          [-8.9463210e-03, -7.5343349e-03,  7.4039923e-04, ...,\n            1.2268053e-02, -1.3198548e-02, -3.2026084e-05],\n          [-3.9164912e-02, -7.5820535e-03,  1.3907746e-03, ...,\n           -6.6482779e-03, -1.0733108e-02, -9.9823792e-03],\n          [ 6.8326667e-03, -2.0392942e-03,  4.8815343e-03, ...,\n           -2.2594277e-03,  1.3842926e-03,  7.3076310e-03]]],\n \n \n        [[[ 4.3825014e-03, -8.4557459e-03,  6.0976552e-05, ...,\n           -1.5721424e-03, -6.2847957e-03, -6.0747643e-03],\n          [-1.9328265e-02,  4.7232583e-03,  5.4184967e-03, ...,\n           -7.3021773e-04,  5.4419693e-03, -1.3912930e-02],\n          [ 5.5229296e-03,  4.6481588e-03, -7.7780159e-03, ...,\n            2.7554642e-04, -1.3050669e-02, -3.9565605e-03],\n          ...,\n          [ 3.2179512e-04, -3.4885360e-03, -1.0874616e-02, ...,\n            8.2677584e-03, -1.1178210e-02,  7.5714481e-03],\n          [ 4.0616496e-03, -7.2419085e-03, -1.3705336e-03, ...,\n            1.0747767e-02, -7.4744545e-04, -1.1671883e-02],\n          [ 1.2030830e-03,  7.4582128e-04,  3.5220869e-03, ...,\n           -1.7955588e-04,  2.9120417e-03,  6.2693246e-03]],\n \n         [[-5.1713228e-04, -1.3903971e-02, -2.6596109e-03, ...,\n           -8.2665663e-03, -1.9626400e-02, -7.4289762e-03],\n          [-6.0759084e-03, -9.6884841e-04,  6.0364590e-03, ...,\n           -9.3692346e-03,  4.3691956e-03, -1.3639066e-02],\n          [-9.0519208e-03,  9.6781654e-03,  8.6934352e-03, ...,\n            9.1049206e-03, -9.7425170e-03, -7.6264590e-03],\n          ...,\n          [-3.8311486e-03,  7.1369117e-04, -1.1714270e-02, ...,\n            1.6859323e-02, -1.6344676e-02,  1.2910481e-02],\n          [ 9.2128823e-03, -1.5336460e-03,  5.2924841e-03, ...,\n            3.0931132e-03, -9.8296702e-03, -5.3623371e-04],\n          [ 3.9525158e-03,  9.3688892e-04,  3.7328359e-03, ...,\n           -5.5979183e-03,  1.0980255e-02,  1.2100901e-02]],\n \n         [[-1.7740640e-03, -1.1662573e-02,  1.6112716e-03, ...,\n            1.2962105e-04, -1.1654774e-02, -5.4892940e-03],\n          [ 1.5333769e-02, -6.9049154e-03,  9.9986177e-03, ...,\n           -1.2304787e-02,  1.6369390e-03, -1.8881094e-02],\n          [-1.9262903e-04,  2.8037187e-03, -1.7493139e-03, ...,\n            2.6137648e-02, -9.3979454e-03, -4.5618960e-03],\n          ...,\n          [-6.4418763e-03, -2.7641824e-03, -8.7552797e-03, ...,\n            1.0848792e-02, -1.2664125e-02,  1.1188427e-02],\n          [-1.4976529e-02, -8.0800392e-03,  1.1753010e-03, ...,\n           -1.4212406e-02, -1.4587110e-02, -5.5110687e-04],\n          [ 4.9651507e-03,  1.9658136e-03,  4.4132387e-03, ...,\n           -3.2392361e-03,  3.2961257e-03,  1.0079567e-02]]],\n \n \n        [[[-1.0813643e-02, -1.2109953e-03,  4.0739947e-03, ...,\n           -3.7609618e-03, -8.4045194e-03, -4.1101421e-03],\n          [-1.6155045e-02,  3.3042664e-04,  2.8314698e-03, ...,\n            3.8799408e-03,  1.0882318e-02, -1.2172839e-02],\n          [ 1.3959367e-02,  4.1269679e-03, -5.6302696e-03, ...,\n            8.7758545e-03, -3.2438899e-03, -3.4345812e-03],\n          ...,\n          [ 9.1708098e-03, -6.2859273e-03, -1.2609004e-02, ...,\n            1.6977531e-03, -9.6289860e-03,  5.1916544e-05],\n          [-9.7496519e-03, -2.1479633e-03, -1.9520836e-02, ...,\n           -9.5207070e-04,  6.3222921e-03, -1.7897552e-02],\n          [ 3.9352821e-03,  4.7744825e-03,  2.0808049e-03, ...,\n            9.7545320e-03, -1.2982457e-02,  4.6863281e-03]],\n \n         [[-6.3140173e-03, -2.0006283e-03, -4.0322897e-04, ...,\n            7.0220581e-03, -1.3161357e-02, -7.2546205e-03],\n          [-7.1486277e-03,  7.5339980e-04,  1.3867128e-03, ...,\n           -6.8810834e-03,  4.9706153e-03, -1.2716940e-02],\n          [-1.4313880e-03,  7.5597558e-03, -2.7771997e-03, ...,\n            3.3415109e-02, -3.9894031e-03, -6.9557610e-03],\n          ...,\n          [ 4.2238324e-03, -4.8141801e-03, -1.0185721e-02, ...,\n           -6.8885856e-04, -1.6479922e-02, -1.7523849e-04],\n          [-6.4725848e-03, -3.6925147e-03, -8.9367675e-03, ...,\n           -5.6522340e-03, -8.0006220e-04, -1.0039192e-02],\n          [ 5.7996372e-03,  3.7618543e-03,  1.6103454e-03, ...,\n           -5.3754007e-04, -1.7849701e-02,  5.1220311e-03]],\n \n         [[ 6.7127135e-04, -3.2391129e-03,  7.8760099e-04, ...,\n            7.2297198e-03, -5.2612908e-03, -4.2801294e-03],\n          [ 5.4616379e-03, -1.8265145e-03,  8.0440696e-03, ...,\n           -5.5095245e-04, -1.2544187e-03, -1.7182948e-02],\n          [ 4.7172702e-04,  6.6203335e-03, -5.3720833e-03, ...,\n            4.0812425e-02, -3.6152611e-03, -5.1507959e-03],\n          ...,\n          [-1.1449282e-03, -6.4645633e-03, -1.2517356e-02, ...,\n           -6.6413991e-03, -1.0809171e-02, -2.6953095e-03],\n          [-1.4766367e-02, -5.9053781e-03, -8.8806386e-04, ...,\n           -1.0663199e-02, -5.6493361e-03, -4.9214857e-03],\n          [ 4.3073772e-03,  3.5157984e-03,  4.7004086e-04, ...,\n           -2.4001813e-04, -1.6719183e-02,  4.8483759e-03]]]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32, numpy=\n array([ 9.72232521e-02, -7.32301399e-02,  1.51504159e-01,  1.07368380e-01,\n        -7.87555948e-02, -2.41246987e-02,  8.48031044e-02,  6.24934174e-02,\n         2.88872004e-01,  1.55830339e-01,  1.34004295e-01,  6.98199943e-02,\n        -2.17193335e-01,  1.05869681e-01,  2.22256556e-01, -2.87815742e-02,\n         6.47584051e-02, -1.38827130e-01,  2.05309093e-01,  1.19278319e-01,\n        -4.18131426e-02, -1.11328689e-02, -8.71208981e-02,  7.55662546e-02,\n         2.64069051e-01,  9.73907709e-02, -6.56019151e-02, -1.39584467e-01,\n        -1.62646696e-01, -8.77466798e-03,  2.09752887e-01,  1.20625667e-01,\n         2.33027115e-01,  1.42718498e-02,  1.27024665e-01,  4.33076657e-02,\n         1.07562847e-01,  2.56101247e-02, -7.22438991e-02,  5.70154898e-02,\n         3.50982435e-02, -4.41192165e-02, -7.65857995e-02,  1.81532875e-01,\n         1.80326328e-01,  1.68747574e-01,  9.78041515e-02,  1.80262048e-02,\n        -1.43519500e-02,  7.41873607e-02,  9.39584598e-02,  1.19459517e-01,\n        -1.26094073e-01,  3.07464510e-01, -9.78475213e-02,  1.50838152e-01,\n         1.11390026e-02, -1.34227380e-01, -1.33490497e-02,  1.45182744e-01,\n         4.95087057e-02,  2.92521685e-01,  1.36722494e-02, -9.93172526e-02,\n         2.24625063e-03,  1.12465046e-01,  1.66944731e-02,  1.58153400e-02,\n        -2.61690557e-01, -5.42517416e-02,  8.32710341e-02, -8.74206871e-02,\n        -1.08093813e-01, -1.34222642e-01,  8.93189833e-02,  1.58900097e-01,\n         7.23217875e-02,  5.35155870e-02, -9.78990793e-02, -2.22737305e-02,\n         1.66057318e-01,  1.46783590e-02,  4.88309897e-02, -6.18785433e-02,\n        -2.42688484e-03, -3.50905024e-02, -5.70284128e-02,  2.11776167e-01,\n        -5.87092265e-02, -4.17218693e-02,  1.87396958e-01, -8.78316686e-02,\n         1.81572422e-01,  3.10360221e-03, -1.06422320e-01,  2.61189342e-01,\n         7.73651451e-02,  2.53047198e-01,  6.77449033e-02,  6.99037164e-02,\n        -1.21446617e-01,  3.27433422e-02, -7.32980669e-02, -1.29718170e-01,\n        -1.93338841e-01, -1.47859663e-01, -7.92617872e-02,  8.76382142e-02,\n         8.39660875e-03,  3.94587256e-02,  6.21666610e-02, -4.53170575e-03,\n         1.29386917e-01, -5.56114456e-03,  1.52212933e-01,  1.17498152e-02,\n         1.94093555e-01,  1.87042123e-03,  1.20573968e-01, -6.69639334e-02,\n         9.79068875e-02, -4.42622863e-02, -1.39063355e-02,  8.04094896e-02,\n         4.76720696e-03, -1.97302345e-02,  6.30021691e-02,  1.96513757e-01,\n         2.64776796e-01, -2.85423905e-01,  5.21171093e-02,  1.59660056e-01,\n         1.38520092e-01,  1.69852033e-01, -7.98163265e-02,  1.81670785e-01,\n         1.28256023e-01,  4.19358723e-02,  4.15071874e-04, -8.13728198e-02,\n         2.45410889e-01,  6.02891929e-02, -9.57015157e-02,  2.09470391e-02,\n         3.80575983e-03,  9.27622095e-02,  2.49510258e-01,  1.26563221e-01,\n        -3.85253169e-02,  3.05033624e-01, -5.89955784e-02, -2.51873839e-03,\n         1.39833361e-01,  2.53028609e-03,  2.35346526e-01,  8.09649676e-02,\n        -5.55666983e-02,  2.54182275e-02,  3.57310995e-02,  4.15629782e-02,\n         1.82599664e-01,  1.36219010e-01,  5.86013608e-02, -1.21335210e-02,\n         1.89356357e-02, -3.40443361e-03, -9.08635929e-02,  3.12733389e-02,\n         2.38715827e-01, -6.42500892e-02,  1.45969167e-01,  2.76238956e-02,\n        -8.74618143e-02,  7.40503967e-02, -3.93775478e-03,  2.25105122e-01,\n        -4.39890362e-02,  4.22569476e-02, -1.67123273e-01,  1.04955435e-01,\n         7.86387734e-03,  2.39473656e-01,  8.34665895e-02,  6.21325858e-02,\n         1.67894647e-01,  1.11196324e-01,  1.74066991e-01,  1.51154652e-01,\n         1.20348893e-01, -7.58293644e-03,  1.00768656e-01, -7.46568590e-02,\n         7.41088763e-02,  7.29339570e-02, -7.33533353e-02, -3.00517529e-02,\n         1.03525870e-01, -1.10795863e-01, -7.80599564e-02,  1.27777651e-01,\n        -4.17723134e-02,  1.30707458e-01,  5.09697646e-02, -9.98334959e-02,\n         2.53053069e-01, -6.56349882e-02,  2.10435748e-01,  9.95820090e-02,\n        -8.78535807e-02,  2.11530685e-01,  5.90504613e-03,  1.75386027e-01,\n         1.16541289e-01,  6.41804785e-02,  2.31669009e-01,  7.78236287e-03,\n        -1.32983744e-01,  2.63812905e-03,  2.08072722e-01,  7.67128775e-03,\n        -5.03700264e-02,  2.54232138e-02,  9.41740945e-02,  8.43535885e-02,\n        -2.45006084e-02,  3.12013805e-01,  4.90255244e-02,  2.03415439e-01,\n        -1.84839759e-02,  1.43999279e-01,  6.25193343e-02,  4.50861268e-02,\n        -2.16302112e-01,  5.03829300e-01,  7.39305988e-02, -1.63852684e-02,\n         5.60812131e-02, -6.62712678e-02, -2.29798537e-02, -1.23708293e-01,\n         3.37800831e-01,  1.37842536e-01,  9.20393169e-02, -3.44086215e-02,\n        -1.37066379e-01,  4.82112378e-01,  1.31646454e-01,  9.09326151e-02,\n        -1.06240317e-01,  1.95967779e-01, -1.06115602e-01, -1.98599119e-02,\n        -1.66443273e-01, -1.14921972e-01,  8.88420790e-02,  6.68230057e-02,\n         2.21923590e-01, -4.26545478e-02,  8.01347718e-02,  1.99960604e-01,\n        -1.43557638e-01, -3.83671336e-02,  1.82051972e-01,  1.25957280e-01,\n         2.93219537e-01, -5.74299358e-02, -1.57354832e-01,  5.85990511e-02,\n         2.97562450e-01,  2.89446954e-02,  4.40704599e-02, -1.35887312e-02,\n         9.70638841e-02, -8.05276707e-02, -2.61094987e-01,  2.56522536e-01,\n        -2.07009956e-01, -8.21785703e-02,  4.49509859e-01, -4.25433461e-03,\n         4.78458256e-02,  9.33266059e-02,  2.19508559e-01,  1.06764831e-01,\n        -5.11930324e-02,  7.28791952e-02, -9.17416811e-02,  5.22106476e-02,\n        -5.28104231e-02,  1.62669525e-01,  5.91234155e-02,  1.96272805e-01,\n        -7.36410916e-02, -8.18443149e-02,  1.54231995e-01,  3.45540904e-02,\n         7.77417421e-02,  3.94710861e-02, -1.87491804e-01,  9.31138769e-02,\n         8.36969912e-02,  5.33706583e-02,  5.67062795e-02,  2.17190430e-01,\n        -1.05187416e-01,  2.08498254e-01,  1.04223259e-01, -1.85986385e-01,\n        -9.21867788e-02,  6.70548007e-02, -5.55852577e-02,  1.85056180e-01,\n         8.37340429e-02,  4.81872410e-02,  1.02761738e-01, -1.27836660e-01,\n        -1.25223845e-01, -7.84098506e-02,  3.93300280e-02, -1.70610026e-01,\n         4.39143926e-02,  1.45399451e-01,  1.83793366e-01, -4.63268310e-02,\n        -9.10706446e-02,  1.30470559e-01,  3.45735438e-03, -9.39716399e-02,\n         2.15176061e-01, -2.00502500e-01, -6.55688122e-02, -1.82779312e-01,\n         7.54564404e-02,  2.45926872e-01,  9.01386812e-02,  4.41717096e-02,\n         9.59510636e-03,  2.99080610e-02, -3.50802809e-01,  1.49108902e-01,\n        -6.89550117e-02,  2.92768590e-02, -2.59770513e-01, -7.68297017e-02,\n        -1.77641049e-01,  4.57492262e-01,  1.85859293e-01, -1.14464037e-01,\n        -3.51907238e-02,  1.69105232e-01, -1.92702394e-02, -5.00965156e-02,\n         6.37099263e-04,  2.93955747e-02,  2.19126306e-02, -1.08865000e-01,\n         3.13707888e-02, -1.31874785e-01,  1.34321097e-02,  1.25567932e-02,\n         1.07721753e-01, -1.53308257e-01, -7.10092187e-02, -7.27935582e-02,\n         2.85641074e-01,  4.47935127e-02, -5.52070104e-02, -1.30422609e-02,\n         1.23817161e-01, -1.74044929e-02, -1.99130312e-01,  7.32331499e-02,\n         7.54711181e-02, -1.73574343e-01,  6.64412277e-03,  3.79290991e-02,\n        -1.63129531e-02, -1.05556346e-01,  5.55862300e-02, -1.77324861e-02,\n         3.48688327e-02,  5.44465706e-02, -7.10303709e-02, -7.02253059e-02,\n        -5.71787693e-02,  4.69734930e-02, -8.38443041e-02,  4.36032638e-02,\n         1.34468764e-01,  1.42802104e-01, -9.68526676e-02, -3.62464488e-02,\n        -3.52542698e-02, -1.72721241e-02, -9.06758308e-02, -9.79517773e-02,\n         7.56007954e-02,  1.20084636e-01, -1.15540870e-01, -1.40418530e-01,\n         1.22560948e-01,  2.40212176e-02,  9.04814005e-02, -1.91712096e-01,\n        -3.55947725e-02, -2.78878361e-02, -1.06624737e-01, -8.56165662e-02,\n        -1.23428777e-01,  1.16616353e-01,  2.84349471e-01,  6.05120044e-03,\n         1.65983543e-01, -9.34971347e-02,  1.19639456e-01, -1.85345355e-02,\n        -6.61099553e-02, -7.99963158e-03,  4.71975878e-02, -7.14084134e-02,\n         7.11026741e-03,  2.66952932e-01,  1.45430416e-01,  8.56207535e-02,\n         3.38725746e-01,  3.96331958e-02, -1.45800680e-01,  1.66975930e-01,\n        -1.08074434e-01,  1.59030661e-01,  2.25047082e-01,  1.42287627e-01,\n        -1.36478236e-02,  7.06764534e-02,  2.60705262e-01,  1.15524724e-01,\n         2.21833289e-01,  5.80034144e-02, -1.56820610e-01,  3.02231193e-01,\n         2.71723896e-01, -5.04713785e-03,  7.28831291e-02,  8.91094729e-02,\n         1.82268739e-01,  8.79880041e-02,  3.89606878e-02, -1.72649994e-02,\n         1.50122255e-01,  4.28993702e-02,  5.88023104e-02, -9.18406695e-02,\n         7.47657269e-02,  4.44494486e-01,  1.49463758e-01, -4.35398147e-02,\n        -2.31067967e-02,  3.25366631e-02,  2.95995384e-01,  1.08001344e-01,\n         1.26205295e-01,  3.68891031e-01,  1.21568717e-01,  1.12235777e-01,\n         1.88650295e-01, -3.62094343e-02, -5.28821200e-02,  1.80033222e-01,\n         7.66739175e-02,  1.70229360e-01,  1.44137412e-01,  9.78369042e-02,\n        -1.48568690e-01,  7.30241612e-02,  2.60537237e-01,  8.17443207e-02,\n         9.07354206e-02, -1.46276236e-01,  3.93006764e-02,  1.46489998e-04,\n        -1.17084712e-01, -5.83123378e-02, -5.44318892e-02,  7.57218078e-02,\n         2.11517498e-01,  1.14996850e-01,  1.53895184e-01, -4.45573479e-02,\n        -1.30707353e-01,  5.69508709e-02,  3.54849011e-01,  4.87805624e-03,\n         3.44373435e-02,  1.65210336e-01,  2.33957656e-02,  1.49531543e-01,\n         7.07088113e-02,  2.92125288e-02,  2.49673471e-01,  9.26863849e-02,\n         6.01426139e-02,  8.50257501e-02,  4.29810844e-02,  3.52593660e-01,\n        -1.15543187e-01,  2.04325944e-01,  1.29647478e-01, -8.82687271e-02,\n        -4.20641303e-02,  3.08955684e-02,  1.20931409e-01,  6.39748514e-01],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32, numpy=\n array([[[[-1.12610648e-03, -1.39102731e-02, -4.15291899e-04, ...,\n           -2.39090272e-03,  5.59418183e-03, -1.67810004e-02],\n          [ 1.13813886e-02, -6.83033955e-04, -8.76976922e-03, ...,\n           -8.44151899e-03, -3.70610040e-03,  1.24894250e-02],\n          [ 2.18971120e-03,  1.76711346e-03,  1.23967733e-02, ...,\n            1.34555586e-02,  1.07239345e-02,  5.72480168e-03],\n          ...,\n          [-8.70769471e-03,  4.26163105e-03, -7.06244586e-03, ...,\n            1.54851489e-02,  4.63837199e-03,  7.84500409e-03],\n          [-6.56485325e-04,  7.68889068e-03, -1.45298289e-03, ...,\n           -7.35005224e-03, -3.10419098e-04,  8.47741403e-03],\n          [ 5.95664885e-03, -3.27580003e-03, -5.32116368e-03, ...,\n           -2.03936966e-03,  1.35865889e-03,  5.91904763e-03]],\n \n         [[ 2.12769513e-03, -5.14811045e-03, -5.62243350e-03, ...,\n           -1.31134260e-02,  4.88865655e-03, -1.01500070e-02],\n          [ 1.17016789e-02, -7.15120370e-03, -8.39244854e-03, ...,\n           -6.67586317e-03, -5.10410266e-03,  1.43477311e-02],\n          [ 1.43261172e-03,  2.12484729e-02,  9.50200390e-03, ...,\n           -1.57885591e-03,  1.82978567e-02,  3.79953301e-03],\n          ...,\n          [-7.53166946e-03,  2.99661653e-03, -4.78781341e-03, ...,\n            9.03842878e-03,  4.02625185e-03,  9.22073331e-03],\n          [-3.35529796e-03,  3.79348244e-03,  1.26775482e-03, ...,\n           -5.82869118e-03, -8.80411360e-03,  7.26862065e-03],\n          [ 1.36666895e-05, -3.68203758e-03, -3.14675691e-03, ...,\n            2.22825771e-03,  7.57493125e-03,  1.02467118e-02]],\n \n         [[-8.14719591e-03, -2.00505066e-03, -8.80919211e-03, ...,\n           -1.66545529e-02,  1.98264781e-04, -3.52188433e-03],\n          [ 7.94519577e-03, -3.68036679e-03, -8.26357119e-03, ...,\n           -2.82117142e-03, -4.27439064e-03,  1.01640765e-02],\n          [ 4.09189891e-03,  1.91902220e-02,  1.09405918e-02, ...,\n           -2.29260847e-02,  2.60341149e-02,  7.29925465e-03],\n          ...,\n          [-2.73322989e-03,  3.00269900e-03, -9.70353652e-03, ...,\n           -1.42436460e-04,  9.75819398e-03,  2.72412854e-03],\n          [-4.47429763e-03, -3.83435097e-03,  7.17543124e-04, ...,\n            3.04877380e-04, -2.06050486e-03,  9.71180678e-04],\n          [ 2.62887310e-03,  1.30386110e-02, -6.04727073e-03, ...,\n            2.77841953e-03,  1.50414938e-02,  9.90516134e-03]]],\n \n \n        [[[-8.03796295e-03, -1.13971503e-02,  6.58952724e-03, ...,\n           -4.81049181e-04,  6.43613422e-03, -1.79823674e-02],\n          [ 1.67138800e-02, -7.45473197e-03, -8.19350593e-03, ...,\n           -6.07861159e-03,  3.93439055e-04,  1.27271740e-02],\n          [ 8.36025982e-04, -4.70938452e-04,  1.17752301e-02, ...,\n            1.79414582e-02,  3.75301763e-03,  1.27263891e-04],\n          ...,\n          [-8.50693882e-03,  6.07048161e-03, -2.66443216e-03, ...,\n            1.42896026e-02,  2.49904767e-03,  1.79498718e-04],\n          [-2.46597454e-03,  7.29254400e-03,  2.94409954e-04, ...,\n           -3.63253732e-03, -3.63337738e-03,  5.07437345e-03],\n          [-1.16811926e-03, -3.15029873e-03, -8.24172632e-04, ...,\n           -8.88785627e-03, -5.48729068e-03,  3.78749473e-03]],\n \n         [[-4.39948402e-03, -3.47739854e-03,  2.43245202e-04, ...,\n           -9.84785520e-03,  5.21054072e-03, -1.97509974e-02],\n          [ 1.85958110e-02, -1.33645898e-02, -1.02767739e-02, ...,\n           -2.41490663e-03,  4.60673073e-05,  1.20157907e-02],\n          [-5.91966091e-03,  9.24645830e-03,  1.21005550e-02, ...,\n           -1.05297462e-04,  1.10596651e-02,  2.63926852e-03],\n          ...,\n          [-9.06946044e-03, -1.54343119e-03, -6.10274205e-04, ...,\n            1.09442594e-02,  3.13927210e-03, -3.25301266e-03],\n          [-5.20819006e-03,  5.09213936e-03,  3.94632714e-03, ...,\n           -4.01240168e-03, -9.69846640e-03,  7.10429577e-03],\n          [-6.86919549e-03, -2.47165700e-03,  1.92192208e-03, ...,\n           -4.51413356e-03,  1.72176212e-03,  4.92478209e-03]],\n \n         [[-6.22658757e-03, -3.43090994e-03, -8.35243613e-03, ...,\n           -1.15216067e-02,  1.53592986e-03, -3.64543474e-03],\n          [ 1.28823500e-02, -6.69739628e-03, -8.30523577e-03, ...,\n            3.48634925e-03,  1.83020544e-03,  8.27968866e-03],\n          [-1.76686270e-03,  9.79864504e-03,  1.12368744e-02, ...,\n           -2.05144864e-02,  1.31868133e-02,  1.41431252e-02],\n          ...,\n          [ 1.68414356e-03, -3.16251907e-03, -4.50956682e-03, ...,\n            1.08172344e-02,  8.38247407e-03,  9.00894694e-04],\n          [-8.00926052e-03, -5.72536979e-03,  1.28548976e-03, ...,\n            2.47490546e-03, -3.73221375e-03,  2.54621520e-03],\n          [-4.48723370e-03,  1.57816298e-02, -3.63634876e-03, ...,\n            3.34846694e-03,  1.33423917e-02,  4.46979143e-03]]],\n \n \n        [[[-3.98385944e-03, -7.72264041e-03,  5.79379732e-03, ...,\n            4.73350612e-03,  8.24284181e-03, -9.89352167e-03],\n          [ 8.95213708e-03, -1.56957563e-03, -2.67244899e-03, ...,\n           -4.31917328e-03, -7.22031249e-03,  7.08149793e-03],\n          [-6.78315479e-03,  7.97655899e-03,  1.22533282e-02, ...,\n            1.35311754e-02,  1.13436170e-02,  1.00916745e-02],\n          ...,\n          [-5.42715983e-03,  6.82500936e-03,  2.06867745e-03, ...,\n            1.15423445e-02,  1.35911524e-03, -5.60511416e-03],\n          [ 4.61982097e-03,  8.86386912e-03, -5.09008206e-03, ...,\n            2.30618101e-03, -2.10025930e-03, -1.28205456e-02],\n          [-1.71301316e-03, -1.08229686e-02, -6.23088004e-03, ...,\n           -9.36139748e-03,  4.23580030e-04,  6.02343772e-03]],\n \n         [[-7.84571003e-03, -7.72077264e-03, -4.10368491e-04, ...,\n           -4.64761769e-03,  4.84151114e-03, -8.10108799e-03],\n          [ 7.03802519e-03, -6.14652690e-03, -3.35389609e-03, ...,\n           -7.66926736e-04, -4.10724850e-03,  3.30237439e-03],\n          [-1.26976296e-02,  4.06949688e-03,  1.57761611e-02, ...,\n            2.05063750e-03,  1.35973999e-02,  2.45495401e-02],\n          ...,\n          [-4.94020339e-03, -6.04472240e-04,  3.70768807e-03, ...,\n            5.28301718e-03,  5.77830663e-03, -5.73270582e-03],\n          [ 6.47951430e-03,  5.04286960e-03, -5.76301245e-03, ...,\n           -3.84971802e-03, -1.53160375e-02, -1.36713414e-02],\n          [-6.13218453e-03, -7.85352290e-03, -2.66636047e-03, ...,\n           -2.86623905e-03,  8.13857932e-03,  3.23665445e-03]],\n \n         [[-1.03016887e-02, -7.32906163e-03, -1.26632303e-02, ...,\n           -1.51485833e-03, -4.73790773e-04, -1.50191726e-03],\n          [ 2.47099600e-03, -3.58005147e-03, -8.92815471e-04, ...,\n            2.19221620e-04, -5.89206407e-04,  1.02671504e-03],\n          [-7.04030367e-03,  2.31858669e-03,  1.17366202e-02, ...,\n           -1.00303544e-02,  9.60416254e-03,  3.07981335e-02],\n          ...,\n          [ 1.02732638e-02, -4.61639231e-03, -3.90697533e-04, ...,\n            5.76351210e-03,  5.99978026e-03, -6.46614050e-03],\n          [ 1.79310958e-03, -1.10877156e-02, -4.39016009e-03, ...,\n           -3.89495096e-03, -1.43652763e-02, -1.33516490e-02],\n          [-4.76273848e-03,  9.76819452e-03, -6.88574556e-03, ...,\n            7.87178893e-03,  2.10002940e-02,  2.46465113e-03]]]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32, numpy=\n array([ 2.13278726e-01,  2.32329801e-01,  1.57151520e-02, -6.70147175e-03,\n         2.54097760e-01, -1.00149654e-01, -8.36583301e-02,  2.02481151e-01,\n        -1.42848700e-01,  1.77087430e-02, -2.12744266e-01, -2.57271100e-02,\n        -1.41537204e-01,  1.14469334e-01,  6.01538382e-02,  3.76103938e-01,\n        -3.72519553e-01,  1.40026376e-01, -4.37423550e-02, -1.54562220e-01,\n         1.70293525e-01,  4.88189682e-02, -2.26634011e-01,  2.06878632e-02,\n        -1.39044002e-01,  2.94346511e-01,  6.97177928e-03, -9.19279605e-02,\n         1.69544652e-01, -3.45243275e-01,  2.24639952e-01,  2.49377400e-01,\n         1.09288786e-02,  2.59293765e-01,  3.07276517e-01, -7.85489976e-02,\n        -1.98504012e-02,  3.29028279e-01, -1.72007695e-01, -8.32872316e-02,\n         4.12283033e-01,  4.27057028e-01, -6.69640023e-04, -1.90807786e-02,\n         1.99608058e-02,  1.33982033e-01,  6.11348823e-02, -1.53467342e-01,\n        -4.02507395e-01, -5.18702641e-02,  2.81949788e-01, -9.81014594e-03,\n         3.08913160e-02,  6.16866648e-02, -1.52534977e-01,  3.53446037e-01,\n         2.12723255e-01, -5.65962374e-01,  2.13322058e-01,  3.33331265e-02,\n         2.18498871e-01, -3.64092626e-02,  6.26419559e-02,  2.63942719e-01,\n         6.76808506e-02,  1.76065147e-01,  1.32455960e-01,  8.09534490e-02,\n         4.20970976e-01,  2.45447010e-01,  9.40517634e-02,  2.62030274e-01,\n         2.07859039e-01, -1.26895547e-01, -7.72396773e-02,  6.01348840e-02,\n        -4.77694571e-02,  6.82831630e-02,  1.21956528e-03,  1.36679605e-01,\n         2.72098005e-01,  3.63185108e-02,  3.45317125e-01,  1.09104455e-01,\n         1.36538848e-01,  1.74370810e-01, -5.80689192e-01, -1.29161194e-01,\n        -2.64734268e-01,  7.22823024e-01,  5.51068932e-02,  6.82976171e-02,\n        -2.05691177e-02,  6.75572380e-02, -1.18106611e-01,  1.66148528e-01,\n         2.65065789e-01, -1.40838578e-01, -1.13926053e-01, -7.64771700e-01,\n         2.43444085e-01, -5.57120815e-02, -2.55624652e-01,  3.85610074e-01,\n        -1.44879567e-02,  1.63364053e-01,  1.00237662e-02,  3.97323340e-01,\n        -7.04556704e-01, -2.19556034e-01,  2.09620565e-01,  2.83576787e-01,\n         6.42748699e-02,  3.93527038e-02,  8.13223980e-03, -1.37996048e-01,\n         9.31574106e-02,  6.41047806e-02,  9.02974159e-02, -4.22395229e-01,\n        -4.66141850e-02,  2.09921628e-01,  2.69864768e-01,  1.75364614e-01,\n         3.92805934e-01,  2.61148997e-02,  3.43184263e-01, -1.36680976e-01,\n         7.66678825e-02, -7.67129511e-02,  1.39000401e-01, -6.38619214e-02,\n         7.51705691e-02,  1.81327790e-01, -7.02414438e-02, -1.87789407e-02,\n        -3.80455293e-02,  1.47579476e-01,  1.84434429e-02, -5.12089878e-02,\n         2.01663211e-01, -7.26428106e-02,  1.55012429e-01,  1.10663034e-01,\n        -1.65615901e-01,  2.40110960e-02,  4.79917049e-01,  2.09958673e-01,\n         4.97291982e-02,  2.68139869e-01,  1.82850733e-01,  3.03464174e-01,\n        -4.91772629e-02,  1.32150194e-02,  2.09124073e-01,  1.07672758e-01,\n         2.45396182e-01,  8.34394395e-02,  8.05274025e-02,  7.64415264e-02,\n         8.56247693e-02,  4.96439356e-03,  1.25398323e-01,  2.49067828e-01,\n         1.94723964e-01,  4.58731391e-02, -8.88940915e-02,  4.20743972e-01,\n        -8.21111277e-02, -1.45499691e-01,  2.78597444e-01, -3.63252424e-02,\n         2.06234962e-01,  2.28693098e-01,  9.95886791e-03, -1.10201925e-01,\n        -9.74908993e-02, -3.03504437e-01,  9.26612690e-02, -2.10227557e-02,\n         2.59572864e-01,  7.11193904e-02,  2.11067975e-01,  1.62974775e-01,\n         1.62764654e-01, -4.78741825e-02,  4.58417088e-02,  3.80256511e-02,\n         1.25822708e-01,  3.20309401e-01, -3.58499885e-02,  2.67319262e-01,\n        -6.52918458e-01,  2.48101741e-01,  1.16309874e-01,  5.64715303e-02,\n         2.23180756e-01, -8.33447203e-02,  1.39551774e-01,  5.74671514e-02,\n         1.91890463e-01, -3.99313681e-03,  3.11073303e-01, -2.77877524e-02,\n         2.16852233e-01, -1.26395524e-01,  3.49284828e-01,  7.58803263e-02,\n         4.14046675e-01, -3.06704771e-02,  2.85062250e-02,  1.91049010e-01,\n         1.57977223e-01,  5.55282575e-04, -1.60641447e-01,  1.95794493e-01,\n         2.21998945e-01, -8.28020796e-02, -3.21320929e-02,  7.23793432e-02,\n        -1.76688388e-01,  2.45178908e-01,  4.32021171e-03, -1.53720647e-01,\n         6.39710426e-02, -3.81282091e-01,  7.59766817e-01,  9.65218022e-02,\n         3.92138250e-02,  8.57566670e-03,  1.83055371e-01,  3.34058814e-02,\n        -2.64758289e-01,  2.18491539e-01, -1.21293664e-02,  1.67465046e-01,\n         2.70698696e-01, -1.49033427e-01,  1.86509117e-02, -3.34587023e-02,\n        -7.18312711e-02,  3.57771628e-02, -1.69287875e-01,  2.23012403e-01,\n        -1.97449282e-01, -1.28948867e-01, -3.15834507e-02,  6.38716891e-02,\n         1.08870104e-01,  1.73344433e-01,  1.98710099e-01,  4.36336957e-02,\n         2.62033671e-01, -1.00803122e-01,  5.24610758e-01,  1.70146555e-01,\n         1.07129000e-01, -3.67283463e-01,  2.68015917e-02, -6.59564137e-01,\n         1.46105260e-01,  2.75709815e-02, -1.88518927e-01,  1.20801501e-01,\n        -1.32401049e-01,  4.11784947e-01,  2.12306082e-02,  7.02527463e-02,\n        -2.83787459e-01,  7.73022547e-02,  2.01812275e-02,  1.66517496e-01,\n         4.79313701e-01,  3.42917681e-01, -3.34198438e-02, -3.41561764e-01,\n         1.75815612e-01, -3.59087467e-01,  6.10887222e-02, -4.92576547e-02,\n         1.54441878e-01, -7.28115290e-02,  3.67573738e-01,  8.98908004e-02,\n        -2.22484455e-01,  5.81515320e-02,  1.30673215e-01, -2.24278972e-01,\n        -5.72047308e-02, -1.62113011e-01,  1.30246252e-01,  1.02766171e-01,\n         1.65047705e-01,  3.65190685e-01,  1.18729115e-01,  7.38752782e-02,\n        -2.02514175e-02, -5.72904348e-02, -2.32023701e-01, -3.10960591e-01,\n        -1.28202811e-01,  2.23797023e-01,  5.58057018e-02,  1.12005360e-01,\n        -3.44750062e-02, -3.89980584e-01,  3.46000075e-01, -3.05351943e-01,\n         9.16355625e-02, -1.46230668e-01, -6.39387146e-02, -1.09759547e-01,\n        -4.87965085e-02, -1.36795759e-01,  1.92397572e-02,  2.09345818e-01,\n         2.11968094e-01,  3.74513566e-01, -4.97587882e-02,  1.39423952e-01,\n        -2.47978363e-02, -7.22238719e-02,  1.84289142e-01,  1.36896312e-01,\n        -1.90947816e-01,  1.06812447e-01, -7.82530941e-03, -6.60004541e-02,\n         3.47046047e-01,  9.81120840e-02,  4.30794597e-01,  1.01599535e-02,\n        -1.26542211e-01,  1.18208684e-01, -5.22569954e-01,  7.63861686e-02,\n         4.94845808e-02,  4.33381796e-01,  6.63721487e-02,  2.66792089e-01,\n        -5.35082147e-02, -3.33328694e-01,  1.38470262e-01,  4.62355763e-01,\n         2.12002605e-01,  4.70723026e-02,  1.55368000e-01,  1.08918361e-01,\n         6.87757283e-02,  2.72534370e-01,  1.44902080e-01, -1.68686491e-02,\n         7.86768943e-02,  2.82390267e-02,  3.11506540e-01, -4.58674312e-01,\n        -3.98369227e-03,  1.14671439e-01,  3.40675823e-02,  9.54895765e-02,\n        -3.78678828e-01,  7.72286505e-02,  1.18462615e-01, -3.06991097e-02,\n        -1.63062274e-01,  6.27826527e-02,  1.62692279e-01,  1.10148959e-01,\n         2.50042826e-01,  1.92564636e-01, -3.29279825e-02,  8.73595774e-02,\n         5.85996322e-02,  4.35281582e-02, -5.02812490e-02, -1.85523495e-01,\n        -8.36205706e-02, -6.77409768e-03,  1.48594499e-01,  1.30210623e-01,\n         2.62326032e-01, -2.87208073e-02,  3.24752778e-02, -9.87079740e-03,\n         7.52181709e-02,  1.08591266e-01,  4.58415896e-02,  6.27836958e-02,\n         1.52184710e-01, -7.43376371e-03,  1.08495593e-01,  2.77632445e-01,\n         1.19359670e-02,  1.36060016e-02,  2.08717749e-01,  4.05857526e-02,\n         2.60712862e-01,  4.02474791e-01,  2.10542947e-01,  4.15131897e-01,\n         7.40829855e-02,  9.92712453e-02, -1.69865116e-01, -8.31194893e-02,\n        -7.56599426e-01,  2.82342285e-01, -5.45069203e-03,  3.20360482e-01,\n        -1.06196120e-01, -5.31354211e-02,  5.09625748e-02, -3.74190547e-02,\n         8.99988338e-02,  2.25224167e-01, -4.02766854e-01, -4.26091775e-02,\n         2.09927917e-01,  1.28805548e-01, -2.68443525e-01, -6.91533275e-03,\n         3.41932654e-01,  1.39936641e-01,  3.62053931e-01,  9.15973783e-02,\n         2.09024772e-01, -1.56470269e-01,  1.46654800e-01, -2.16415867e-01,\n        -1.32667748e-02,  1.49863943e-01, -1.07473610e-02, -6.58824146e-02,\n        -1.97233647e-01, -2.01405942e-01, -2.77925264e-02, -1.50060147e-01,\n         9.42247361e-02,  2.04853266e-01,  4.35391963e-02,  1.78399041e-01,\n         1.83343783e-01, -1.22696958e-01,  2.04414040e-01,  2.90846854e-01,\n         1.21619359e-01, -1.94642991e-01,  2.86005214e-02,  5.31951547e-01,\n        -2.70389646e-01, -1.51390404e-01, -4.79240343e-03,  2.29093239e-01,\n        -2.53238827e-01, -4.02558781e-02,  1.50439456e-01, -4.31613736e-02,\n         9.37732160e-02,  4.89119172e-01,  3.58660631e-02, -4.56337705e-02,\n        -6.33749040e-03,  8.82187635e-02,  2.08025292e-01,  4.15855139e-01,\n         1.72531709e-01,  1.02334842e-01,  3.62423569e-01,  9.70884785e-02,\n        -4.27529395e-01, -7.44454384e-01, -2.86756486e-01,  2.41579264e-01,\n         2.55721688e-01,  2.33342111e-01,  3.72318149e-01, -5.91950044e-02,\n        -9.17105675e-01,  2.30280116e-01, -1.02454223e-01, -1.57411680e-01,\n         2.57903012e-03,  3.25026102e-02,  1.92252338e-01, -9.96460691e-02,\n         1.06442891e-01,  1.07584395e-01, -4.43986133e-02, -4.05258000e-01,\n        -1.85224727e-01,  1.79304972e-01, -3.31959456e-01, -1.42133012e-02,\n         1.39563635e-01,  2.79828519e-01,  1.24286108e-01,  1.88625589e-01,\n         3.03091377e-01,  2.97313422e-01,  3.22930291e-02,  6.05367683e-02,\n         1.70370415e-01,  2.86071420e-01,  1.00513615e-01,  6.52797073e-02,\n         3.56945515e-01,  5.85718872e-03, -1.53081819e-01, -1.08499629e-02,\n        -4.60346401e-01,  1.64361209e-01,  2.35172257e-01, -1.76724881e-01],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32, numpy=\n array([[[[ 4.07882966e-04,  4.82621184e-03, -9.11695044e-03, ...,\n            1.51569908e-02, -2.73966836e-03, -4.83311387e-03],\n          [ 3.48924054e-03,  2.60589737e-03, -8.10198858e-03, ...,\n            8.86987546e-04, -7.08238874e-03, -7.96321197e-04],\n          [-1.12759033e-02, -6.43836753e-03, -8.05236772e-03, ...,\n            1.90410519e-03,  3.87627119e-03,  1.96338408e-02],\n          ...,\n          [-1.49949323e-02,  1.20318262e-02,  9.87236737e-04, ...,\n           -2.89861043e-03,  4.03315574e-03, -5.93429338e-03],\n          [-7.53336679e-03,  6.08530594e-03,  9.90014290e-04, ...,\n            1.63243050e-04, -3.09621776e-03, -3.01808352e-03],\n          [-2.62522907e-03,  3.51887429e-03, -2.58309790e-03, ...,\n           -6.02027494e-03, -8.85035843e-03,  9.85825085e-04]],\n \n         [[ 3.02846078e-04,  2.64736195e-03, -1.37099335e-02, ...,\n            1.49135189e-02, -4.65666235e-04, -6.84898719e-03],\n          [-9.60117579e-03,  5.01031429e-03, -1.13053191e-02, ...,\n            8.86707660e-03,  6.21224102e-03,  2.48882570e-03],\n          [-1.13499342e-02, -4.72285505e-03, -8.57903156e-03, ...,\n           -3.04920389e-03,  9.67197306e-03,  1.90250184e-02],\n          ...,\n          [-4.48127137e-03,  5.53716440e-03,  4.68912302e-03, ...,\n           -4.91005788e-03,  6.40070112e-03, -8.28019064e-03],\n          [-6.00830279e-03, -7.51605199e-04,  8.29616503e-04, ...,\n            2.07461347e-03,  3.22994636e-03, -1.69727230e-03],\n          [-2.81035155e-03,  1.36994272e-02, -2.04460253e-03, ...,\n           -5.17683988e-03, -9.04289633e-03, -1.79657899e-03]],\n \n         [[ 3.09069827e-03, -7.85107608e-04, -1.12262042e-02, ...,\n            1.38624888e-02, -1.50248851e-03, -2.83075101e-03],\n          [-1.78263709e-02, -4.06336552e-03, -1.11764381e-02, ...,\n            1.76884588e-02, -1.14172359e-03,  1.50952139e-03],\n          [-1.26420632e-02, -1.19902752e-03, -1.03873471e-02, ...,\n           -1.54873531e-03,  3.21487361e-03,  1.84067711e-02],\n          ...,\n          [ 4.99952445e-03,  8.96072667e-03,  2.35912274e-03, ...,\n           -6.95799524e-03,  9.11425054e-03, -1.76598097e-03],\n          [-3.11188446e-03, -4.81695170e-03,  2.14671413e-03, ...,\n            8.69878661e-03,  4.66441829e-03,  3.96351190e-03],\n          [-1.56560540e-03,  1.01758400e-02, -1.81451999e-03, ...,\n           -7.93038029e-03, -7.92690739e-03,  9.85421613e-03]]],\n \n \n        [[[ 2.21009902e-03,  5.53841889e-03, -1.51329748e-02, ...,\n            1.92244705e-02, -2.94085429e-03, -2.05214042e-03],\n          [ 4.31180233e-03, -4.67703957e-03, -9.83539224e-03, ...,\n           -1.16664264e-03, -7.47026131e-03,  8.98758229e-03],\n          [-1.00150378e-02, -3.31197702e-03, -4.38165246e-03, ...,\n            1.28704909e-04,  6.90069282e-05,  5.32614533e-03],\n          ...,\n          [-1.65325180e-02,  5.79520175e-03, -1.08136714e-03, ...,\n           -3.87264206e-03,  3.87065555e-03, -8.52444116e-03],\n          [-1.10835498e-02,  6.04208373e-03,  6.12878706e-03, ...,\n           -4.66409931e-03, -1.65367487e-03, -4.93706390e-03],\n          [-5.82469627e-03,  2.77068373e-03,  2.59765168e-03, ...,\n           -9.57501866e-03, -3.32338479e-03,  3.07677989e-03]],\n \n         [[ 3.59932799e-03,  1.19693214e-02, -1.60994809e-02, ...,\n            1.80910621e-02,  3.29896147e-06, -6.10836258e-04],\n          [-7.41903298e-03, -5.61146066e-03, -1.38216335e-02, ...,\n            2.96197180e-03,  2.69207382e-03,  7.01817824e-03],\n          [-8.48251395e-03,  9.95878596e-04,  6.25522574e-04, ...,\n           -3.68017773e-03,  1.27349654e-02,  4.87677474e-03],\n          ...,\n          [-8.20725970e-03,  1.95525214e-03,  2.69707199e-03, ...,\n           -4.49266564e-03,  6.45229314e-03, -1.28380703e-02],\n          [-5.50353806e-03,  1.61080994e-03,  1.39699667e-03, ...,\n           -2.52319360e-03, -4.11227718e-03, -5.21757640e-03],\n          [-2.69698747e-03,  2.70054420e-03,  3.79564380e-03, ...,\n           -7.66708190e-03, -9.20492224e-03, -4.52092383e-03]],\n \n         [[ 2.48262100e-03,  6.13112841e-03, -1.47995697e-02, ...,\n            1.69335436e-02,  1.16662926e-03,  1.17623655e-03],\n          [-1.69411730e-02, -8.71018134e-03, -1.29530905e-02, ...,\n            1.87354535e-03, -6.88433880e-04, -1.68701622e-03],\n          [-6.59902534e-03,  3.46401357e-03, -3.11806914e-03, ...,\n           -4.05590283e-03,  7.37769250e-03,  5.51742781e-03],\n          ...,\n          [ 9.51702707e-04,  1.64372730e-03,  1.49127271e-03, ...,\n           -7.65326899e-03,  1.19846454e-02, -9.10102390e-03],\n          [ 2.88849953e-03, -5.35335718e-03, -3.08376068e-04, ...,\n            3.87483276e-04, -4.36716061e-03, -2.73425644e-03],\n          [-1.54271268e-03,  4.30263951e-03, -4.31923472e-05, ...,\n           -5.21167647e-03, -8.79874174e-03, -2.11346560e-04]]],\n \n \n        [[[-2.93453527e-03,  4.62880917e-03, -9.69677325e-03, ...,\n            1.48544796e-02,  3.87839018e-03,  1.85810984e-03],\n          [ 5.63784037e-03, -6.04889123e-03, -5.59245935e-03, ...,\n           -4.98529337e-03, -1.92062720e-03,  4.07473044e-03],\n          [-4.84194839e-03,  2.70295335e-04, -3.02166585e-03, ...,\n           -1.49492722e-03, -1.31164724e-02,  2.69274553e-03],\n          ...,\n          [-1.79702230e-02, -3.29879066e-03, -6.12194091e-03, ...,\n           -7.89267663e-03, -1.74841529e-03, -2.64002383e-03],\n          [-1.11466460e-02,  5.13884844e-03,  1.51880039e-02, ...,\n           -4.87166690e-03,  2.45198980e-03, -3.42866592e-03],\n          [-6.12380216e-03,  8.87359586e-03, -3.45014478e-03, ...,\n           -1.09723555e-02,  2.66773975e-03, -4.79325093e-03]],\n \n         [[-6.14633691e-03,  1.40388934e-02, -9.19489283e-03, ...,\n            1.60656814e-02,  8.88941437e-03,  6.56510517e-03],\n          [-5.72578516e-03, -3.33702704e-03, -8.64025764e-03, ...,\n            1.98416272e-03,  7.86264334e-03,  2.50778953e-03],\n          [-3.77366156e-03,  9.24240507e-04,  7.29091058e-04, ...,\n           -4.96890210e-03,  3.85057204e-03, -7.15580711e-04],\n          ...,\n          [-1.22111076e-02, -2.42003356e-03, -4.81141545e-03, ...,\n           -4.22020489e-03, -1.01946108e-02, -5.80721162e-03],\n          [-9.04400460e-03,  1.94370467e-03,  7.34009594e-03, ...,\n           -5.14973374e-03,  3.66879918e-04, -5.37241250e-03],\n          [-3.53684562e-04,  1.53788179e-02, -8.76262202e-04, ...,\n           -7.13666994e-03, -4.27656015e-03, -1.42133404e-02]],\n \n         [[-6.67408668e-03,  9.11168288e-03, -9.18486901e-03, ...,\n            1.21346870e-02,  8.51947255e-03,  7.15135690e-03],\n          [-1.64910071e-02, -2.04550894e-03, -7.30760070e-03, ...,\n           -2.89353775e-03,  1.16095145e-03, -6.60440046e-03],\n          [-7.68517129e-05,  4.32093628e-03, -6.93119306e-04, ...,\n           -3.48316878e-03, -1.47545757e-03, -2.85687298e-03],\n          ...,\n          [-4.66702832e-03, -1.39780261e-03,  3.67210196e-05, ...,\n           -4.95146902e-04,  7.23151665e-04, -6.29519764e-03],\n          [ 3.63699184e-03, -4.63909749e-03,  5.31478412e-03, ...,\n           -1.50180201e-03, -7.48660788e-03, -1.94359326e-03],\n          [-1.03440508e-03,  1.57131702e-02, -1.98940560e-03, ...,\n           -5.38550608e-04, -5.37431426e-03, -9.20103770e-03]]]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32, numpy=\n array([ 1.88378572e-01, -5.00367463e-01,  5.42223394e-01,  2.46460959e-01,\n         1.54070303e-01, -2.36067176e-01, -1.17649809e-02,  3.57512347e-02,\n         7.58338273e-02,  1.63476378e-01, -1.34778112e-01, -8.40371996e-02,\n         2.50966042e-01, -1.67475253e-01, -2.82782689e-02,  4.04935628e-01,\n         4.92739901e-02,  3.43014568e-01, -2.48226017e-01, -8.52851495e-02,\n         1.14871934e-01, -8.34204536e-03, -7.97333941e-02,  2.08946794e-01,\n         2.29255453e-01, -3.57115209e-01,  3.54569256e-01,  4.94823381e-02,\n         2.67764390e-01,  1.16772160e-01,  2.48077109e-01,  1.90769851e-01,\n         2.27198079e-01,  2.07326874e-01,  4.33389664e-01, -2.14167282e-01,\n         4.27582026e-01,  2.85164490e-02, -1.01103269e-01, -1.16591506e-01,\n         1.57484300e-02, -1.92446187e-01,  5.09383738e-01, -1.01140939e-01,\n        -1.70503020e-01,  1.82109758e-01,  6.56641722e-02,  1.48948044e-01,\n        -3.34800705e-02,  2.34267697e-01,  1.17521681e-01, -1.11647561e-01,\n         1.67274535e-01, -2.18229502e-01, -1.26343705e-02,  6.51194036e-01,\n         4.33828384e-01,  5.46606928e-02,  2.51551688e-01,  2.21985489e-01,\n         2.07218826e-02, -2.58966297e-01,  1.32507131e-01,  1.47451892e-01,\n        -1.49491996e-01, -2.45121513e-02, -1.93051482e-03,  3.55903581e-02,\n        -2.87072897e-01,  2.04153866e-01,  5.73086739e-02,  1.29463181e-01,\n         1.23248003e-01, -1.90378204e-01, -5.25708608e-02, -8.20820853e-02,\n         7.67999232e-01,  2.17065111e-01,  4.36754115e-02, -6.61685169e-02,\n         1.55227348e-01,  6.22618794e-01,  1.98174551e-01,  1.35126531e-01,\n         1.28762200e-01, -4.85090315e-02,  4.69657898e-01,  4.86439429e-02,\n         3.53257835e-01,  5.42547941e-01, -2.09446177e-01,  1.35347247e-01,\n         1.24633104e-01,  2.24511325e-01, -1.93987757e-01, -1.69384852e-02,\n         6.01346325e-03,  2.04649493e-01, -3.49047594e-02,  3.08254182e-01,\n         2.74999171e-01,  6.06408417e-01,  1.30885825e-01, -7.84191936e-02,\n         2.30702013e-01,  6.21092655e-02,  1.07052080e-01, -2.47026488e-01,\n         3.11956823e-01,  2.79106498e-01,  2.18370885e-01,  1.03057109e-01,\n         5.74263111e-02,  1.94899786e+00, -9.07675177e-02,  7.99698196e-03,\n         1.57724932e-01,  6.45613790e-01,  1.95434928e-01,  6.70031726e-01,\n        -1.17897667e-01,  2.08516628e-01,  1.35548666e-01,  5.05479157e-01,\n         2.27726430e-01, -2.85992660e-02,  7.31752664e-02,  2.16317698e-01,\n         1.04756050e-01,  7.34472694e-03, -9.85700116e-02, -2.09823661e-02,\n        -1.19556062e-01,  4.82213408e-01,  2.29107309e-02,  7.89495856e-02,\n         1.66905075e-01,  2.03251719e-01,  2.19120562e-01,  3.17676008e-01,\n         2.38415692e-03,  4.65317338e-04,  5.53632751e-02,  2.63843596e-01,\n        -2.68867999e-01, -6.39354065e-02,  3.14108700e-01, -1.04466088e-01,\n         2.15531588e-01,  2.69292265e-01,  6.31085504e-03,  2.11812276e-02,\n         3.03376783e-02,  1.34254768e-01,  1.78778559e-01,  9.43155289e+00,\n         2.75686860e-01,  1.31738886e-01,  1.86976731e-01,  8.58496577e-02,\n         1.49559855e-01, -2.00136185e-01, -3.02753776e-01,  6.99055195e-01,\n         1.03292175e-01,  5.93839027e-02,  6.86080813e-01, -1.75999016e-01,\n         1.97002128e-01,  2.17421085e-01,  1.00237295e-01,  2.00453222e-01,\n         5.19402325e-01,  3.03252012e-01,  1.08759388e-01,  2.52096474e-01,\n         9.80204344e-02,  6.79299794e-03,  4.11590002e-02,  3.55982512e-01,\n         9.82239395e-02, -1.27133965e-01,  5.89193106e-02, -2.11274922e-02,\n        -9.46500301e-02,  3.23439181e-01,  5.33190012e-01,  2.81205267e-01,\n        -7.00195059e-02,  4.50011939e-02,  5.85235953e-02, -2.72989403e-02,\n         1.23330429e-01,  1.57047048e-01,  6.30159453e-02, -1.79739177e-01,\n         1.68211743e-01, -2.27243900e-02,  1.42132714e-01,  2.76279449e-01,\n         4.04965654e-02, -1.99054778e-01,  3.65843683e-01,  2.73112729e-02,\n        -1.31376877e-01,  9.32270288e-02, -7.60697424e-02,  1.93399116e-02,\n         1.85015425e-03,  4.15253565e-02,  2.56592147e-02, -6.57611638e-02,\n         3.76626439e-02,  2.72211343e-01,  1.40291587e-01,  3.34124207e-01,\n        -4.18526232e-02, -9.58744287e-02,  2.74706990e-01, -1.55596823e-01,\n         3.64604779e-02, -1.32014111e-01,  1.34014981e-02, -4.87616658e-02,\n         1.06156722e-01,  5.36326528e-01,  3.72983187e-01,  3.38566378e-02,\n        -8.60288590e-02,  7.98788011e-01, -1.82336777e-01,  1.64008543e-01,\n         1.88149005e-01,  1.57651126e-01,  8.60020444e-02,  5.53895067e-03,\n         2.04019845e-02,  3.66550952e-01,  1.50018111e-01,  6.82196319e-02,\n         1.26301348e-01,  2.67437845e-01,  1.32230207e-01,  6.47044480e-02,\n         2.42651135e-01,  7.22966492e-02,  1.25669807e-01, -6.93451017e-02,\n         4.69704807e-01,  1.46840796e-01,  1.52075395e-01,  1.08058611e-02,\n         7.89734907e-03, -1.83315612e-02,  1.02967513e+00,  2.86464036e-01,\n         2.72089839e-01,  2.99726725e-02,  1.12274528e-01,  3.48836958e-01,\n        -6.20776117e-02,  2.17991203e-01,  4.28738832e-01, -4.16037142e-02,\n         3.66377145e-01,  1.08046882e-01, -2.55869627e-02, -4.00224268e-01,\n        -2.40779221e-01,  7.19139993e-01, -5.15835034e-03,  4.65480566e-01,\n         1.19287916e-01, -1.20954335e-01,  2.40479499e-01,  4.27416801e-01,\n         5.59932351e-01,  1.26139030e-01, -1.14825912e-01,  1.22176088e-01,\n         2.95618027e-02, -7.13887066e-02,  2.86794007e-01,  2.46085122e-01,\n         1.81327894e-01,  3.12949091e-01, -9.33331028e-02,  1.16926350e-03,\n        -2.63316602e-01,  3.58326316e-01,  1.62727624e-01,  3.76439899e-01,\n         3.91090691e-01,  1.10005677e-01,  1.11292467e-01,  2.07414851e-02,\n         1.14994623e-01,  6.28707856e-02,  5.34125865e-01, -2.11611167e-01,\n         5.91500774e-02, -1.58647493e-01, -3.78553174e-03,  3.52953747e-02,\n        -2.05919351e-02,  1.53826475e-01,  1.20451070e-01,  1.26241818e-01,\n        -6.20619431e-02, -1.44232456e-02, -1.47647366e-01, -1.82920575e-01,\n         2.08717108e-01,  3.31947088e-01, -8.80439430e-02,  1.55856574e+00,\n         2.22004980e-01,  2.32446954e-01,  5.67621551e-02,  3.97473991e-01,\n         5.03389001e-01, -1.44024760e-01,  1.91921026e-01, -1.38310969e-01,\n        -6.44080155e-03,  3.39745015e-01,  4.69098449e-01,  2.28332952e-02,\n         6.56073332e-01,  2.45110661e-01, -1.68141842e-01,  7.19269738e-02,\n        -1.22520126e-01, -2.34832615e-01,  1.96609661e-01,  8.65012854e-02,\n         1.28099874e-01, -9.01192203e-02,  4.89099592e-01, -1.34810328e-01,\n         1.06568784e-01,  1.27098143e-01,  4.38580155e-01,  4.55901057e-01,\n         4.61729169e-02,  1.65377498e-01,  3.57421547e-01,  9.74082127e-02,\n        -2.31282339e-01,  8.46065506e-02,  2.89760232e-01, -9.88961682e-02,\n         9.01378930e-01,  2.52402127e-01, -2.15688810e-01, -5.59736192e-02,\n        -1.13818742e-01, -6.13411656e-03, -1.34001493e-01,  1.61142886e-01,\n        -4.80620004e-02,  1.38647243e-01, -3.01776342e-02,  1.83977813e-01,\n        -1.18016593e-01,  3.89179081e-01,  6.13915585e-02,  1.58884943e-01,\n         5.89057468e-02,  5.19612193e-01,  1.23953849e-01,  5.55581629e-01,\n         5.01390219e-01, -3.79978642e-02, -1.41257286e-01,  7.03621209e-02,\n        -1.73130766e-01,  3.50397527e-01, -1.34628639e-01, -2.30107084e-02,\n         5.94515465e-02,  3.11439663e-01,  4.39245671e-01, -8.34068432e-02,\n         8.05468298e-03,  8.24325830e-02, -6.42216429e-02,  1.73451111e-01,\n         9.77575108e-02, -9.48499516e-02,  6.00330293e-01, -2.85697937e-01,\n         6.63027644e-01,  2.28900835e-01, -5.50854504e-02,  1.96810931e-01,\n         1.31078482e-01,  1.16677739e-01,  2.73458511e-01, -1.60894319e-01,\n         1.54009443e-02,  1.65677398e-01,  4.01737362e-01,  1.94677606e-01,\n        -3.40434372e-01,  4.65704620e-01, -2.72638332e-02,  9.78857726e-02,\n         3.00865740e-01,  8.31336603e-02,  6.52762055e-02,  1.93404332e-01,\n         4.39618938e-02,  1.97853178e-01,  6.20303333e-01, -2.47850232e-02,\n         3.16133723e-02,  4.27223206e-01, -1.78643763e-01,  2.27603521e-02,\n         1.01879463e-01, -1.77166402e-01, -9.12419409e-02,  6.85135424e-02,\n         2.09163412e-01,  2.12687582e-01,  2.60186791e-01, -1.92657262e-02,\n         1.86271384e-01,  8.68155286e-02, -1.64935917e-01, -6.73513263e-02,\n         3.51010323e-01,  7.94189423e-02,  3.47567499e-01, -3.27353388e-01,\n         2.28438899e-01,  1.91672705e-02,  4.34933186e-01,  2.34846137e-02,\n         5.02053857e-01,  2.87198114e+00,  3.45609011e-03, -3.42143588e-02,\n         3.52340311e-01,  4.58314866e-01, -1.96509138e-01, -1.23567872e-01,\n         1.77132830e-01,  6.33585453e-02, -2.15546321e-03,  3.33019942e-01,\n        -5.46362288e-02,  1.94658935e-01,  2.31957555e-01, -1.94247887e-01,\n         7.14528114e-02,  4.19912785e-02,  3.27715337e-01, -1.10557206e-01,\n        -1.07955880e-01,  1.31638125e-01,  2.46899307e-01,  7.21076829e-03,\n         2.77655154e-01, -1.57194927e-01,  6.24649376e-02, -1.32860814e-03,\n        -1.57012362e-02,  2.07436994e-01, -9.61607322e-03, -4.07151505e-02,\n         3.30549240e-01, -5.71658760e-02,  5.70487320e-01, -1.03581101e-01,\n        -1.79280698e-01,  3.29045117e-01,  5.74648529e-02, -8.48884210e-02,\n         1.71312660e-01, -5.71665429e-02,  3.82779270e-01,  2.49197200e-01,\n        -1.67588264e-01, -5.42712435e-02,  4.65058923e-01, -3.14851582e-01,\n         3.38773549e-01, -1.19772919e-01,  7.56180510e-02,  2.71076292e-01,\n         1.29012525e-01,  1.41996786e-01,  3.30376983e-01,  2.09672466e-01,\n         2.74694502e-01,  1.87327504e-01,  2.14084148e-01,  1.21977694e-01,\n         5.93084395e-01,  2.13688929e-02,  8.09027970e-01,  3.09404194e-01,\n         3.44152540e-01,  1.66218415e-01,  1.36155128e-01,  2.33373582e-01,\n         8.00405815e-03,  1.03328384e-01,  6.38187110e-01, -2.65396535e-02],\n       dtype=float32)&gt;]\n\n\n\nconv_base.trainable = True\nprint(\"This is the number of trainable weights \"\n      \"before freezing the conv base:\", len(conv_base.trainable_weights))\n\nThis is the number of trainable weights before freezing the conv base: 26\n\n\n\nconv_base.trainable = False\nprint(\"This is the number of trainable weights \"\n      \"after freezing the conv base:\", len(conv_base.trainable_weights))\n\nThis is the number of trainable weights after freezing the conv base: 0\n\n\nNow we can create a new model that chains together which contains a data augmentation stage, our frozen convolutional base and a dense classifier.\n\ntf.keras.backend.clear_session()\ndata_augmentation = tf.keras.Sequential(\n    [\n        tf.keras.layers.RandomFlip(\"horizontal\"),\n        tf.keras.layers.RandomRotation(0.1),\n        tf.keras.layers.RandomZoom(0.2),\n    ]\n)\ninputs = tf.keras.Input(shape=(180, 180, 3))\nx = data_augmentation(inputs)\n# Apply input value scaling.\nx = tf.keras.applications.vgg16.preprocess_input(x)\nx = conv_base(x) # Fix weight\nx = tf.keras.layers.Flatten()(x)\nx = tf.keras.layers.Dense(256)(x)\nx = tf.keras.layers.Dropout(0.5)(x)\noutputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = tf.keras.Model(inputs, outputs)\nmodel.summary()\n\nmodel.compile(loss=\"binary_crossentropy\",\n              optimizer=\"rmsprop\",\n              metrics=[\"accuracy\"])\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 180, 180, 3)]     0         \n                                                                 \n sequential (Sequential)     (None, 180, 180, 3)       0         \n                                                                 \n tf.__operators__.getitem (S  (None, 180, 180, 3)      0         \n licingOpLambda)                                                 \n                                                                 \n tf.nn.bias_add (TFOpLambda)  (None, 180, 180, 3)      0         \n                                                                 \n vgg16 (Functional)          (None, None, None, 512)   14714688  \n                                                                 \n flatten (Flatten)           (None, 12800)             0         \n                                                                 \n dense (Dense)               (None, 256)               3277056   \n                                                                 \n dropout (Dropout)           (None, 256)               0         \n                                                                 \n dense_1 (Dense)             (None, 1)                 257       \n                                                                 \n=================================================================\nTotal params: 17,992,001\nTrainable params: 3,277,313\nNon-trainable params: 14,714,688\n_________________________________________________________________\n\n\nWith this setup, only the weights from the two Dense layers that we added will be trained. That’s a total of four weight tensors: two per layer (the main weight matrix and the bias vector). Note that in order for these changes to take effect, you must first compile the model. If you ever modify weight trainability after compilation, you should then recompile the model, or these changes will be ignored.\nLet’s train our model. Thanks to data augmentation, it will take much longer for the model to start overfitting, so we can train for more epochs—let’s do 50.\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\n        filepath=\"feature_extraction_with_data_augmentation.keras\",\n        save_best_only=True,\n        monitor=\"val_loss\")\n]\nhistory = model.fit(\n    train_dataset,\n    epochs=50,\n    validation_data=validation_dataset,\n    callbacks=callbacks)\n\nEpoch 1/50\n63/63 [==============================] - 9s 104ms/step - loss: 22.4581 - accuracy: 0.8925 - val_loss: 14.6266 - val_accuracy: 0.9340\nEpoch 2/50\n63/63 [==============================] - 5s 78ms/step - loss: 8.1973 - accuracy: 0.9400 - val_loss: 6.9847 - val_accuracy: 0.9630\nEpoch 3/50\n63/63 [==============================] - 6s 90ms/step - loss: 7.4329 - accuracy: 0.9510 - val_loss: 2.8821 - val_accuracy: 0.9810\nEpoch 4/50\n63/63 [==============================] - 5s 70ms/step - loss: 4.7731 - accuracy: 0.9595 - val_loss: 4.6078 - val_accuracy: 0.9720\nEpoch 5/50\n63/63 [==============================] - 6s 92ms/step - loss: 3.4647 - accuracy: 0.9690 - val_loss: 4.2702 - val_accuracy: 0.9710\nEpoch 6/50\n63/63 [==============================] - 6s 84ms/step - loss: 4.4984 - accuracy: 0.9680 - val_loss: 5.5084 - val_accuracy: 0.9700\nEpoch 7/50\n63/63 [==============================] - 6s 92ms/step - loss: 4.8084 - accuracy: 0.9630 - val_loss: 3.6594 - val_accuracy: 0.9810\nEpoch 8/50\n63/63 [==============================] - 7s 114ms/step - loss: 3.7968 - accuracy: 0.9655 - val_loss: 2.8214 - val_accuracy: 0.9780\nEpoch 9/50\n63/63 [==============================] - 8s 128ms/step - loss: 2.4507 - accuracy: 0.9740 - val_loss: 3.3047 - val_accuracy: 0.9830\nEpoch 10/50\n63/63 [==============================] - 8s 113ms/step - loss: 2.1578 - accuracy: 0.9760 - val_loss: 4.1814 - val_accuracy: 0.9750\nEpoch 11/50\n63/63 [==============================] - 5s 68ms/step - loss: 2.8886 - accuracy: 0.9730 - val_loss: 4.0381 - val_accuracy: 0.9750\nEpoch 12/50\n63/63 [==============================] - 5s 74ms/step - loss: 2.0621 - accuracy: 0.9820 - val_loss: 4.3079 - val_accuracy: 0.9790\nEpoch 13/50\n63/63 [==============================] - 7s 115ms/step - loss: 2.1753 - accuracy: 0.9795 - val_loss: 3.8336 - val_accuracy: 0.9780\nEpoch 14/50\n63/63 [==============================] - 5s 74ms/step - loss: 1.1900 - accuracy: 0.9830 - val_loss: 3.7860 - val_accuracy: 0.9790\nEpoch 15/50\n63/63 [==============================] - 6s 91ms/step - loss: 1.7755 - accuracy: 0.9765 - val_loss: 2.8147 - val_accuracy: 0.9810\nEpoch 16/50\n63/63 [==============================] - 6s 89ms/step - loss: 2.4548 - accuracy: 0.9740 - val_loss: 2.5295 - val_accuracy: 0.9820\nEpoch 17/50\n63/63 [==============================] - 5s 68ms/step - loss: 1.9622 - accuracy: 0.9780 - val_loss: 2.7645 - val_accuracy: 0.9800\nEpoch 18/50\n63/63 [==============================] - 6s 97ms/step - loss: 1.1975 - accuracy: 0.9835 - val_loss: 2.6837 - val_accuracy: 0.9840\nEpoch 19/50\n63/63 [==============================] - 8s 127ms/step - loss: 1.7905 - accuracy: 0.9795 - val_loss: 2.7016 - val_accuracy: 0.9790\nEpoch 20/50\n63/63 [==============================] - 6s 85ms/step - loss: 1.4599 - accuracy: 0.9815 - val_loss: 2.9001 - val_accuracy: 0.9800\nEpoch 21/50\n63/63 [==============================] - 7s 108ms/step - loss: 1.7051 - accuracy: 0.9800 - val_loss: 2.4754 - val_accuracy: 0.9810\nEpoch 22/50\n63/63 [==============================] - 6s 84ms/step - loss: 0.8508 - accuracy: 0.9860 - val_loss: 2.6499 - val_accuracy: 0.9820\nEpoch 23/50\n63/63 [==============================] - 5s 69ms/step - loss: 0.9975 - accuracy: 0.9830 - val_loss: 3.4150 - val_accuracy: 0.9770\nEpoch 24/50\n63/63 [==============================] - 6s 88ms/step - loss: 0.8632 - accuracy: 0.9845 - val_loss: 3.1597 - val_accuracy: 0.9790\nEpoch 25/50\n63/63 [==============================] - 6s 98ms/step - loss: 1.0432 - accuracy: 0.9875 - val_loss: 2.8918 - val_accuracy: 0.9820\nEpoch 26/50\n63/63 [==============================] - 6s 87ms/step - loss: 1.1397 - accuracy: 0.9860 - val_loss: 3.3315 - val_accuracy: 0.9830\nEpoch 27/50\n63/63 [==============================] - 4s 67ms/step - loss: 0.8705 - accuracy: 0.9835 - val_loss: 3.6293 - val_accuracy: 0.9740\nEpoch 28/50\n63/63 [==============================] - 6s 86ms/step - loss: 0.8030 - accuracy: 0.9870 - val_loss: 3.4704 - val_accuracy: 0.9770\nEpoch 29/50\n63/63 [==============================] - 5s 68ms/step - loss: 0.6485 - accuracy: 0.9895 - val_loss: 3.6099 - val_accuracy: 0.9760\nEpoch 30/50\n63/63 [==============================] - 5s 68ms/step - loss: 1.0861 - accuracy: 0.9880 - val_loss: 3.6421 - val_accuracy: 0.9760\nEpoch 31/50\n63/63 [==============================] - 7s 113ms/step - loss: 1.0628 - accuracy: 0.9875 - val_loss: 2.8915 - val_accuracy: 0.9750\nEpoch 32/50\n63/63 [==============================] - 10s 145ms/step - loss: 0.8727 - accuracy: 0.9845 - val_loss: 3.2785 - val_accuracy: 0.9750\nEpoch 33/50\n63/63 [==============================] - 6s 80ms/step - loss: 0.8732 - accuracy: 0.9880 - val_loss: 3.1621 - val_accuracy: 0.9780\nEpoch 34/50\n63/63 [==============================] - 7s 106ms/step - loss: 0.6747 - accuracy: 0.9905 - val_loss: 2.4626 - val_accuracy: 0.9790\nEpoch 35/50\n63/63 [==============================] - 8s 108ms/step - loss: 0.6020 - accuracy: 0.9865 - val_loss: 3.8568 - val_accuracy: 0.9720\nEpoch 36/50\n63/63 [==============================] - 6s 90ms/step - loss: 0.7389 - accuracy: 0.9895 - val_loss: 3.6797 - val_accuracy: 0.9750\nEpoch 37/50\n63/63 [==============================] - 5s 69ms/step - loss: 0.6137 - accuracy: 0.9885 - val_loss: 3.6034 - val_accuracy: 0.9790\nEpoch 38/50\n63/63 [==============================] - 5s 75ms/step - loss: 0.6838 - accuracy: 0.9890 - val_loss: 2.9538 - val_accuracy: 0.9800\nEpoch 39/50\n63/63 [==============================] - 5s 77ms/step - loss: 0.8906 - accuracy: 0.9840 - val_loss: 3.2922 - val_accuracy: 0.9720\nEpoch 40/50\n63/63 [==============================] - 5s 68ms/step - loss: 0.4902 - accuracy: 0.9915 - val_loss: 3.2208 - val_accuracy: 0.9740\nEpoch 41/50\n63/63 [==============================] - 6s 88ms/step - loss: 0.4827 - accuracy: 0.9905 - val_loss: 2.6416 - val_accuracy: 0.9760\nEpoch 42/50\n63/63 [==============================] - 5s 70ms/step - loss: 0.4320 - accuracy: 0.9915 - val_loss: 2.7477 - val_accuracy: 0.9790\nEpoch 43/50\n63/63 [==============================] - 5s 69ms/step - loss: 0.6763 - accuracy: 0.9880 - val_loss: 2.7176 - val_accuracy: 0.9740\nEpoch 44/50\n63/63 [==============================] - 5s 77ms/step - loss: 0.8718 - accuracy: 0.9830 - val_loss: 2.7467 - val_accuracy: 0.9760\nEpoch 45/50\n63/63 [==============================] - 5s 69ms/step - loss: 0.5806 - accuracy: 0.9875 - val_loss: 2.5397 - val_accuracy: 0.9780\nEpoch 46/50\n63/63 [==============================] - 6s 88ms/step - loss: 0.4621 - accuracy: 0.9890 - val_loss: 2.6212 - val_accuracy: 0.9770\nEpoch 47/50\n63/63 [==============================] - 5s 69ms/step - loss: 0.3543 - accuracy: 0.9920 - val_loss: 3.2013 - val_accuracy: 0.9730\nEpoch 48/50\n63/63 [==============================] - 6s 89ms/step - loss: 0.6806 - accuracy: 0.9885 - val_loss: 2.8002 - val_accuracy: 0.9790\nEpoch 49/50\n63/63 [==============================] - 5s 69ms/step - loss: 0.5521 - accuracy: 0.9885 - val_loss: 2.8219 - val_accuracy: 0.9780\nEpoch 50/50\n63/63 [==============================] - 6s 89ms/step - loss: 0.5166 - accuracy: 0.9890 - val_loss: 2.2039 - val_accuracy: 0.9790\n\n\nLet’s plot the results again. As you can see, we reach a validation accuracy of about 98%. This is a strong improvement over the previous model.\n\nacc = history.history[\"accuracy\"]\nval_acc = history.history[\"val_accuracy\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\ntest_model = tf.keras.models.load_model(\n    \"feature_extraction_with_data_augmentation.keras\")\ntest_loss, test_acc = test_model.evaluate(test_dataset)\nprint(f\"Test accuracy: {test_acc:.3f}\")\n\n63/63 [==============================] - 3s 42ms/step - loss: 2.5062 - accuracy: 0.9775\nTest accuracy: 0.978\n\n\nWe get a test accuracy of 97.8%!\n\n\n11.2.5 Fine-tuning a pretrained model\nWe stated earlier that it’s necessary to freeze the convolution base of VGG16 in order to be able to train a randomly initialized classifier on top. For the same reason, it’s only possible to fine-tune the top layers of the convolutional base once the classifier on top has already been trained. If the classifier isn’t already trained, the error signal propagating through the network during training will be too large, and the representations previously learned by the layers being fine-tuned will be destroyed. Thus the steps for fine-tuning a network are as follows:\n\nAdd our custom network on top of an already-trained base network.\nFreeze the base network.\nTrain the part we added.\nUnfreeze some layers in the base network. (Note that usually you should not unfreeze “batch normalization” layers, which are not relevant here since there are no such layers in VGG16. Batch normalization and its impact on finetuning is explained in the lecture.)\nJointly train both these layers and the part we added.\n\nYou already completed the first three steps when doing feature extraction. Let’s proceed with step 4: we’ll unfreeze our conv_base and then freeze individual layers inside it.\n\nconv_base.summary()\n\nModel: \"vgg16\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, None, None, 3)]   0         \n                                                                 \n block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n                                                                 \n block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n                                                                 \n block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n                                                                 \n block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n                                                                 \n block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n                                                                 \n=================================================================\nTotal params: 14,714,688\nTrainable params: 7,079,424\nNon-trainable params: 7,635,264\n_________________________________________________________________\n\n\nWe’ll fine-tune the last three convolutional layers, which means all layers up to block4_pool should be frozen, and the layers block5_conv1, block5_conv2, and block5_conv3 should be trainable.\n\nconv_base.trainable = True\nfor layer in conv_base.layers[:-4]:\n    layer.trainable = False\n\nNow we can begin fine-tuning the model. We’ll do this with the RMSprop optimizer, using a very low learning rate. The reason for using a low learning rate is that we want to limit the magnitude of the modifications we make to the representations of the three layers we’re fine-tuning. Updates that are too large may harm these representations.\n\nmodel.compile(loss=\"binary_crossentropy\",\n              optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-5),\n              metrics=[\"accuracy\"])\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\n        filepath=\"fine_tuning.keras\",\n        save_best_only=True,\n        monitor=\"val_loss\")\n]\nhistory = model.fit(\n    train_dataset,\n    epochs=30,\n    validation_data=validation_dataset,\n    callbacks=callbacks)\n\nEpoch 1/30\n63/63 [==============================] - 10s 100ms/step - loss: 0.5517 - accuracy: 0.9925 - val_loss: 2.2310 - val_accuracy: 0.9800\nEpoch 2/30\n63/63 [==============================] - 5s 72ms/step - loss: 0.3904 - accuracy: 0.9900 - val_loss: 2.4200 - val_accuracy: 0.9800\nEpoch 3/30\n63/63 [==============================] - 6s 91ms/step - loss: 0.4373 - accuracy: 0.9900 - val_loss: 2.3136 - val_accuracy: 0.9770\nEpoch 4/30\n63/63 [==============================] - 5s 71ms/step - loss: 0.4454 - accuracy: 0.9925 - val_loss: 2.3906 - val_accuracy: 0.9760\nEpoch 5/30\n63/63 [==============================] - 6s 91ms/step - loss: 0.1972 - accuracy: 0.9955 - val_loss: 2.4696 - val_accuracy: 0.9790\nEpoch 6/30\n63/63 [==============================] - 5s 69ms/step - loss: 0.2334 - accuracy: 0.9940 - val_loss: 2.4251 - val_accuracy: 0.9780\nEpoch 7/30\n63/63 [==============================] - 6s 91ms/step - loss: 0.4201 - accuracy: 0.9920 - val_loss: 2.1063 - val_accuracy: 0.9820\nEpoch 8/30\n63/63 [==============================] - 5s 70ms/step - loss: 0.3471 - accuracy: 0.9930 - val_loss: 3.3687 - val_accuracy: 0.9690\nEpoch 9/30\n63/63 [==============================] - 5s 71ms/step - loss: 0.3701 - accuracy: 0.9895 - val_loss: 2.5398 - val_accuracy: 0.9780\nEpoch 10/30\n63/63 [==============================] - 6s 87ms/step - loss: 0.0996 - accuracy: 0.9955 - val_loss: 2.6732 - val_accuracy: 0.9780\nEpoch 11/30\n63/63 [==============================] - 5s 70ms/step - loss: 0.2559 - accuracy: 0.9915 - val_loss: 2.1091 - val_accuracy: 0.9830\nEpoch 12/30\n63/63 [==============================] - 6s 91ms/step - loss: 0.0957 - accuracy: 0.9945 - val_loss: 1.8280 - val_accuracy: 0.9790\nEpoch 13/30\n63/63 [==============================] - 5s 69ms/step - loss: 0.2371 - accuracy: 0.9915 - val_loss: 1.8461 - val_accuracy: 0.9820\nEpoch 14/30\n63/63 [==============================] - 7s 110ms/step - loss: 0.2336 - accuracy: 0.9945 - val_loss: 2.0797 - val_accuracy: 0.9770\nEpoch 15/30\n63/63 [==============================] - 7s 99ms/step - loss: 0.1045 - accuracy: 0.9955 - val_loss: 2.1444 - val_accuracy: 0.9770\nEpoch 16/30\n63/63 [==============================] - 8s 106ms/step - loss: 0.2312 - accuracy: 0.9945 - val_loss: 2.4450 - val_accuracy: 0.9780\nEpoch 17/30\n63/63 [==============================] - 6s 88ms/step - loss: 0.1132 - accuracy: 0.9975 - val_loss: 2.5122 - val_accuracy: 0.9760\nEpoch 18/30\n63/63 [==============================] - 6s 89ms/step - loss: 0.1684 - accuracy: 0.9930 - val_loss: 2.7627 - val_accuracy: 0.9740\nEpoch 19/30\n63/63 [==============================] - 5s 71ms/step - loss: 0.1350 - accuracy: 0.9960 - val_loss: 2.6271 - val_accuracy: 0.9800\nEpoch 20/30\n63/63 [==============================] - 6s 90ms/step - loss: 0.3152 - accuracy: 0.9935 - val_loss: 2.2119 - val_accuracy: 0.9790\nEpoch 21/30\n63/63 [==============================] - 5s 72ms/step - loss: 0.1630 - accuracy: 0.9965 - val_loss: 2.1776 - val_accuracy: 0.9830\nEpoch 22/30\n63/63 [==============================] - 6s 94ms/step - loss: 0.1027 - accuracy: 0.9960 - val_loss: 1.9486 - val_accuracy: 0.9800\nEpoch 23/30\n63/63 [==============================] - 5s 70ms/step - loss: 0.1776 - accuracy: 0.9950 - val_loss: 2.1171 - val_accuracy: 0.9800\nEpoch 24/30\n63/63 [==============================] - 6s 88ms/step - loss: 0.0134 - accuracy: 0.9990 - val_loss: 1.8702 - val_accuracy: 0.9800\nEpoch 25/30\n63/63 [==============================] - 5s 71ms/step - loss: 0.0881 - accuracy: 0.9975 - val_loss: 1.8833 - val_accuracy: 0.9780\nEpoch 26/30\n63/63 [==============================] - 5s 70ms/step - loss: 0.1303 - accuracy: 0.9950 - val_loss: 1.9189 - val_accuracy: 0.9770\nEpoch 27/30\n63/63 [==============================] - 6s 91ms/step - loss: 0.1189 - accuracy: 0.9965 - val_loss: 1.9842 - val_accuracy: 0.9810\nEpoch 28/30\n63/63 [==============================] - 6s 92ms/step - loss: 0.1629 - accuracy: 0.9955 - val_loss: 1.8205 - val_accuracy: 0.9780\nEpoch 29/30\n63/63 [==============================] - 5s 71ms/step - loss: 0.0629 - accuracy: 0.9980 - val_loss: 1.8468 - val_accuracy: 0.9790\nEpoch 30/30\n63/63 [==============================] - 6s 91ms/step - loss: 0.0244 - accuracy: 0.9975 - val_loss: 1.9959 - val_accuracy: 0.9770\n\n\n\nmodel = tf.keras.models.load_model(\"fine_tuning.keras\")\ntest_loss, test_acc = model.evaluate(test_dataset)\nprint(f\"Test accuracy: {test_acc:.3f}\")\n\n63/63 [==============================] - 3s 41ms/step - loss: 2.0598 - accuracy: 0.9755\nTest accuracy: 0.975\n\n\nHere, we get a test accuracy of 97.5%! By leveraging modern deep learning techniques, we managed to reach this result using only a small fraction of the training data that was available for the competition (about 10%). There is a huge difference between being able to train on 20,000 samples compared to 2,000 samples!"
  },
  {
    "objectID": "11_Transfer_learning.html#using-tensorflow-hub",
    "href": "11_Transfer_learning.html#using-tensorflow-hub",
    "title": "11  Transfer learning and self-supervised learning",
    "section": "11.3 Using Tensorflow Hub",
    "text": "11.3 Using Tensorflow Hub\nTensorFlow Hub is a repository of pre-trained TensorFlow models.\n\n11.3.1 Download the classifier\nSelect a MobileNetV2 pre-trained model from TensorFlow Hub and wrap it as a tf.Keras layer with hub.KerasLayer. Any compatible image classifier model from TensorFlow Hub will work here, including the examples provided in the drop-down below.\n\nmobilenet_v2 =\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\"\nclassifier_model = mobilenet_v2\n\nIMAGE_SHAPE = (224, 224)\n\nclassifier = tf.keras.Sequential([\n    hub.KerasLayer(classifier_model, input_shape=IMAGE_SHAPE+(3,))\n])\nclassifier.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n keras_layer (KerasLayer)    (None, 1001)              3540265   \n                                                                 \n=================================================================\nTotal params: 3,540,265\nTrainable params: 0\nNon-trainable params: 3,540,265\n_________________________________________________________________\n\n\n\ngrace_hopper = tf.keras.utils.get_file('image.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg')\ngrace_hopper = Image.open(grace_hopper).resize(IMAGE_SHAPE)\nplt.imshow(grace_hopper);\n\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg\n61306/61306 [==============================] - 0s 1us/step\n\n\n\n\n\nAdd a batch dimension (with np.newaxis) and pass the image to the model:\n\ngrace_hopper = np.array(grace_hopper)/255.0\nresult = classifier.predict(grace_hopper[np.newaxis, ...])\n\n1/1 [==============================] - 1s 686ms/step\n\n\nThe top class ID can be found with tf.math.argmax():\n\npredicted_class = tf.math.argmax(result[0], axis=-1)\n\nTake the predicted_class ID and fetch the ImageNet dataset labels to decode the predictions:\n\nlabels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\nimagenet_labels = np.array(open(labels_path).read().splitlines())\nplt.imshow(grace_hopper)\nplt.axis('off')\npredicted_class_name = imagenet_labels[predicted_class]\n_ = plt.title(\"Prediction: \" + predicted_class_name.title())\n\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\n10484/10484 [==============================] - 0s 0us/step\n\n\n\n\n\n\n\n11.3.2 Transfer learning\nIn this example, you will use the TensorFlow flowers dataset:\n\ndata_root = tf.keras.utils.get_file(\n  'flower_photos',\n  'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n   untar=True)\n\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n228813984/228813984 [==============================] - 6s 0us/step\n\n\nFirst, load this data into the model using the image data off disk with tf.keras.utils.image_dataset_from_directory(), which will generate a tf.data.Dataset:\n\nbatch_size = 32\nimg_height = 224\nimg_width = 224\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n  str(data_root),\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n  str(data_root),\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size\n)\n\nFound 3670 files belonging to 5 classes.\nUsing 2936 files for training.\nFound 3670 files belonging to 5 classes.\nUsing 734 files for validation.\n\n\nThe flowers dataset has five classes:\n\nclass_names = np.array(train_ds.class_names)\nprint(class_names)\n\n['daisy' 'dandelion' 'roses' 'sunflowers' 'tulips']\n\n\nSecond, because TensorFlow Hub’s convention for image models is to expect float inputs in the [0, 1] range, use the tf.keras.layers.Rescaling() preprocessing layer to achieve this.\n\nnormalization_layer = tf.keras.layers.Rescaling(1./255)\ntrain_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\nval_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n\nThird, finish the input pipeline by using buffered prefetching with Dataset.prefetch(), so you can yield the data from disk without I/O blocking issues.These are some of the most important tf.data methods you should use when loading data. Interested readers can learn more about them, as well as how to cache data to disk and other techniques, in the Better performance with the tf.data API guide.\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n\nfor image_batch, labels_batch in train_ds:\n  print(image_batch.shape)\n  print(labels_batch.shape)\n  break\n\n(32, 224, 224, 3)\n(32,)\n\n\n\n11.3.2.1 Run the classifier on a batch of images\n\nresult_batch = classifier.predict(train_ds)\npredicted_class_names = imagenet_labels[tf.math.argmax(result_batch, axis=-1)]\npredicted_class_names\n\n92/92 [==============================] - 7s 66ms/step\n\n\narray(['cardoon', 'obelisk', \"yellow lady's slipper\", ...,\n       \"jack-o'-lantern\", 'daisy', 'ice cream'], dtype='&lt;U30')\n\n\nCheck how these predictions line up with the images:\n\nplt.figure(figsize=(10,9))\nplt.subplots_adjust(hspace=0.5)\nfor n in range(30):\n    plt.subplot(6,5,n+1)\n    plt.imshow(image_batch[n])\n    plt.title(predicted_class_names[n])\n    plt.axis('off')\n_ = plt.suptitle(\"ImageNet predictions\")\n\n\n\n\nThe results are far from perfect, but reasonable considering that these are not the classes the model was trained for (except for “daisy”).\n\n\n11.3.2.2 Download the headless model\nTensorFlow Hub also distributes models without the top classification layer. These can be used to easily perform transfer learning.\n\n# Note that this time we select feature vector instead of classification\nmobilenet_v2 = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\nfeature_extractor_model = mobilenet_v2\n\nfeature_extractor_layer = hub.KerasLayer(\n    feature_extractor_model,\n    input_shape=(224, 224, 3),\n    trainable=False)\n\nCreate the feature extractor by wrapping the pre-trained model as a Keras layer with hub.KerasLayer. Use the trainable=False argument to freeze the variables, so that the training only modifies the new classifier layer:\n\nfeature_batch = feature_extractor_layer(image_batch)\nprint(feature_batch.shape)\n\n(32, 1280)\n\n\n\n\n11.3.2.3 Attach a classification head\n\ntf.keras.backend.clear_session()\nnum_classes = len(class_names)\n\nmodel = tf.keras.Sequential([\n    feature_extractor_layer,\n    tf.keras.layers.Dense(num_classes)\n])\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n keras_layer_1 (KerasLayer)  (None, 1280)              2257984   \n                                                                 \n dense (Dense)               (None, 5)                 6405      \n                                                                 \n=================================================================\nTotal params: 2,264,389\nTrainable params: 6,405\nNon-trainable params: 2,257,984\n_________________________________________________________________\n\n\n\n\n11.3.2.4 Train the model\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # Note there are no softmax at the model\n    metrics=['acc'])\n\n\nNUM_EPOCHS = 10\n\nhistory = model.fit(train_ds,\n                    validation_data=val_ds,\n                    epochs=NUM_EPOCHS)\n\nEpoch 1/10\n92/92 [==============================] - 9s 48ms/step - loss: 0.7141 - acc: 0.7330 - val_loss: 0.4273 - val_acc: 0.8474\nEpoch 2/10\n92/92 [==============================] - 2s 25ms/step - loss: 0.3607 - acc: 0.8777 - val_loss: 0.3511 - val_acc: 0.8760\nEpoch 3/10\n92/92 [==============================] - 3s 30ms/step - loss: 0.2834 - acc: 0.9070 - val_loss: 0.3192 - val_acc: 0.8924\nEpoch 4/10\n92/92 [==============================] - 2s 25ms/step - loss: 0.2365 - acc: 0.9278 - val_loss: 0.3020 - val_acc: 0.9005\nEpoch 5/10\n92/92 [==============================] - 2s 25ms/step - loss: 0.2030 - acc: 0.9397 - val_loss: 0.2917 - val_acc: 0.9046\nEpoch 6/10\n92/92 [==============================] - 2s 25ms/step - loss: 0.1770 - acc: 0.9523 - val_loss: 0.2854 - val_acc: 0.9060\nEpoch 7/10\n92/92 [==============================] - 2s 26ms/step - loss: 0.1561 - acc: 0.9608 - val_loss: 0.2817 - val_acc: 0.9046\nEpoch 8/10\n92/92 [==============================] - 3s 27ms/step - loss: 0.1388 - acc: 0.9680 - val_loss: 0.2801 - val_acc: 0.9074\nEpoch 9/10\n92/92 [==============================] - 2s 25ms/step - loss: 0.1243 - acc: 0.9714 - val_loss: 0.2797 - val_acc: 0.9060\nEpoch 10/10\n92/92 [==============================] - 2s 25ms/step - loss: 0.1119 - acc: 0.9768 - val_loss: 0.2802 - val_acc: 0.9060\n\n\nObtain the ordered list of class names from the model predictions:\n\npredicted_batch = model.predict(image_batch)\npredicted_id = tf.math.argmax(predicted_batch, axis=-1)\npredicted_label_batch = class_names[predicted_id]\nprint(predicted_label_batch)\n\n1/1 [==============================] - 0s 483ms/step\n['roses' 'sunflowers' 'tulips' 'roses' 'tulips' 'sunflowers' 'daisy'\n 'dandelion' 'dandelion' 'tulips' 'tulips' 'dandelion' 'dandelion' 'roses'\n 'sunflowers' 'tulips' 'roses' 'sunflowers' 'sunflowers' 'roses' 'tulips'\n 'roses' 'roses' 'tulips' 'daisy' 'dandelion' 'sunflowers' 'tulips'\n 'dandelion' 'roses' 'dandelion' 'sunflowers']\n\n\n\nplt.figure(figsize=(10,9))\nplt.subplots_adjust(hspace=0.5)\n\nfor n in range(30):\n    plt.subplot(6,5,n+1)\n    plt.imshow(image_batch[n])\n    plt.title(predicted_label_batch[n].title())\n    plt.axis('off')\n_ = plt.suptitle(\"Model predictions\")\n\n\n\n\n\n\n11.3.2.5 Export and reload your model\n\nt = time.time()\n\nexport_path = \"/tmp/saved_models/{}\".format(int(t))\nmodel.save(export_path)\n\nexport_path\n\n'/tmp/saved_models/1683952799'\n\n\n\nreloaded = tf.keras.models.load_model(export_path)\nresult_batch = model.predict(image_batch)\nreloaded_result_batch = reloaded.predict(image_batch)\nabs(reloaded_result_batch - result_batch).max()\n\n1/1 [==============================] - 0s 479ms/step\n1/1 [==============================] - 1s 1s/step\n\n\n0.0"
  },
  {
    "objectID": "11_Transfer_learning.html#pretrain-an-image-model-with-self-supervised-learning-using-simsiam-optional",
    "href": "11_Transfer_learning.html#pretrain-an-image-model-with-self-supervised-learning-using-simsiam-optional",
    "title": "11  Transfer learning and self-supervised learning",
    "section": "11.4 Pretrain an image model with Self-supervised learning using SimSiam (Optional)",
    "text": "11.4 Pretrain an image model with Self-supervised learning using SimSiam (Optional)\nTensorFlow Similarity is a python package focused on making similarity learning and self-supervised learning quick and easy. Self-supervised learning is an approach to pre-training models using unlabeled data. The key insight is that you can train a self-supervised model to learn data representations by contrasting multiple augmented views of the same example. These learned representations capture data invariants, e.g., object translation, color jitter, noise, etc. Training a simple linear classifier on top of the frozen representations is easier and requires fewer labels because the pre-trained model already produces meaningful and generally useful features. Overall, self-supervised pre-training learns representations which are more generic and robust than other approaches to augmented training and pre-training.\nTensorflow Similarity provides a set of network architectures, losses, and data augmentations that are common across a number of self-supervised learning techniques. The Tensorflow Similarity package attempts to provide a consistent terminology across these techniques; however, this leads to slightly different naming conventions, as many papers use different terms to describe the same components. The main terms used by Tensorflow Similarity are:\n\nView: A view represents an augmented example.\nBackbone: Refers to the model that learns the Representation that we will use for downstream tasks.\nProjector: Is an MLP model that projects the backbone representation of a view to an Embedding that is contrasted with other views using a specialized contrastive loss.\nPredictor: Is an optional MLP model that is used, in conjunction with gradient stopping, in some recent architectures to further improve the representation quality.\nStop Gradient: Is used by some algorithms to ensure that we only propagate the update from the main view and not the contrasting view.\n\n\n\n\ncontrastive_model_terms.png\n\n\nWe will demonstrates how to use Tensorflow Similarity to boost classification accuracy by pre-training a ResNet18 model using contrastive learning on the cifar10 dataset. As you will see, the pre-trained model achieves about ~1.6x the accuracy of the model trained without pre-training. For example, using SimSiam pre-training, you can achieve 80% accuracy versus 50% accuracy when training the same architecture from scratch.\n\n11.4.1 Dataset Preperation\nThe following section:\n\nLoads the CIFAR10 data from tensorflow datasets.\nCreates the train, val, test, and query/index splits.\n\nThe TensorFlow Datasets’ CIFAR10 dataset provides a test and train split. However, we are going to partition the train data into the following additional splits:\n\nValidation: Data used for validation metrics during the pre-training phase.\nQuery and Index: Data used to compute matching metrics. The query data is used to retrieve the nearest indexed examples.\n\nIn particular, the Query and Index split allows us to track the matching classification performance during training.\nAn increasing match accuracy is a strong indication that the model is learning useful features, however, it does require that we have labeled data. If a dataset only has a small number of labeled examples, they can be passed as the query and index to help monitor the potential matching classification performance during training.\n\nDATA_PATH = Path(\"tfsim_contrastive_model\")\nif not DATA_PATH.exists():\n    DATA_PATH.mkdir(parents=True)\n\n((x_raw_train, y_raw_train), (x_test, y_test)), ds_info = tfds.load(\n    \"cifar10\",\n    split=[\"train\", \"test\"],\n    batch_size=-1,\n    shuffle_files=True,\n    as_supervised=True,\n    with_info=True,\n)\n\nDownloading and preparing dataset 162.17 MiB (download: 162.17 MiB, generated: 132.40 MiB, total: 294.58 MiB) to /root/tensorflow_datasets/cifar10/3.0.2...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset cifar10 downloaded and prepared to /root/tensorflow_datasets/cifar10/3.0.2. Subsequent calls will reuse this data.\n\n\n\n# Compute the indicies for query, index, val, and train splits\nquery_idxs, index_idxs, val_idxs, train_idxs = [], [], [], []\nfor cid in range(ds_info.features[\"label\"].num_classes):\n    idxs = tf.random.shuffle(tf.where(y_raw_train == cid))\n    idxs = tf.reshape(idxs, (-1,))\n    query_idxs.extend(idxs[:200])  # 200 query examples per class\n    index_idxs.extend(idxs[200:400])  # 200 index examples per class\n    val_idxs.extend(idxs[400:500])  # 100 validation examples per class\n    train_idxs.extend(idxs[500:])  # The remaining are used for training\n\nrandom.shuffle(query_idxs)\nrandom.shuffle(index_idxs)\nrandom.shuffle(val_idxs)\nrandom.shuffle(train_idxs)\n\n\ndef create_split(idxs):\n    x, y = [], []\n    for idx in idxs:\n        x.append(x_raw_train[int(idx)])\n        y.append(y_raw_train[int(idx)])\n    return tf.convert_to_tensor(np.array(x)), tf.convert_to_tensor(np.array(y))\n\n\nx_query, y_query = create_split(query_idxs)\nx_index, y_index = create_split(index_idxs)\nx_val, y_val = create_split(val_idxs)\nx_train, y_train = create_split(train_idxs)\n\n\npd.DataFrame(\n        [\n            [\"train\", x_train.shape, y_train.shape],\n            [\"val\", x_val.shape, y_val.shape],\n            [\"query\", x_query.shape, y_query.shape],\n            [\"index\", x_index.shape, y_index.shape],\n            [\"test\", x_test.shape, y_test.shape],\n        ],\n        columns=[\"Dataset\", \"Examples\", \"Labels\"],\n)\n\n\n  \n    \n      \n\n\n\n\n\n\nDataset\nExamples\nLabels\n\n\n\n\n0\ntrain\n(45000, 32, 32, 3)\n(45000)\n\n\n1\nval\n(1000, 32, 32, 3)\n(1000)\n\n\n2\nquery\n(2000, 32, 32, 3)\n(2000)\n\n\n3\nindex\n(2000, 32, 32, 3)\n(2000)\n\n\n4\ntest\n(10000, 32, 32, 3)\n(10000)\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n11.4.2 Self-Supervised Training Setup\n\n11.4.2.1 Self-Supervised Algorithm Selection\nTensorFlow Similarity currently supports three different self-supervised models.\n\nSimCLR: Only requires the Backbone and the projector and uses a contrastive cross-entropy loss.\nSimSiam: Requires the Backbone, projector, and predictor and only compares the cosine distance between augmented views from the same example.\nBarlow Twins: Only requires the Backbone and the projector and uses a loss that compares the feature covariance instead of contrasting the views.\nVicReg: Only requires the Backbone and projector and uses a loss that enforces the learned representations to be invariant to random augmentations while preserving the covariance and variance information.\n\nThe ALGORITHM parameter is used throughout this notebook to set up the various architectures and the parameters defined below are set up to reproduce the results published in the papers.\n\nALGORITHM = \"simsiam\"  # @param [\"barlow\", \"simsiam\", \"simclr\", \"vigreg\"]\n\n\nCIFAR_IMG_SIZE = 32\nBATCH_SIZE = 512\nPRE_TRAIN_EPOCHS = 200\nPRE_TRAIN_STEPS_PER_EPOCH = len(x_train) // BATCH_SIZE\nVAL_STEPS_PER_EPOCH = 20\nWEIGHT_DECAY = 5e-4\nDIM = 2048  # The layer size for the projector and predictor models.\nWARMUP_LR = 0.0\nWARMUP_STEPS = 0\nTEMPERATURE = None\n\nif ALGORITHM == \"simsiam\":\n    INIT_LR = 3e-2 * int(BATCH_SIZE / 256)\nelif ALGORITHM == \"barlow\":\n    INIT_LR = 1e-3  # Initial LR for the learning rate schedule.\n    WARMUP_STEPS = 1000\nelif ALGORITHM == \"simclr\":\n    INIT_LR = 1e-3  # Initial LR for the learning rate schedule, see section B.1 in the paper.\n    TEMPERATURE = 0.5  # Tuned for CIFAR10, see section B.9 in the paper.\nelif ALGORITHM == \"vicreg\":\n    INIT_LR = 1e-3 \n\n\n\n11.4.2.2 Augmented View Configuration\nSelf-supervised networks require at least two augmented “views” of each example. This can be created using a DataSet and an augmentation function. The DataSet treats each example in the batch as its own class and then the augment function produces two separate views for each example.\nThis means the resulting batch will yield tuples containing the two views, i.e., Tuple[(BATCH_SIZE, 32, 32, 3), (BATCH_SIZE, 32, 32, 3)]. TensorFlow Similarity provides several random augmentation functions, and here we combine augmenters from the simCLR module to replicate the augmentations used in simsiam.\n\ndef img_scaling(img):\n    return tf.keras.applications.imagenet_utils.preprocess_input(\n        img, \n        data_format=None, \n        mode='torch')\n\n\n@tf.function\ndef simsiam_augmenter(img, blur=True, area_range=(0.2, 1.0)):\n    \"\"\"SimSiam augmenter.\n\n    The SimSiam augmentations are based on the SimCLR augmentations, but have\n    some important differences.\n    * The crop area lower bound is 20% instead of 8%.\n    * The color jitter and grayscale are applied separately instead of together.\n    * The color jitter ranges are much smaller.\n    * Blur is not applied for the cifar10 dataset.\n\n    args:\n        img: Single image tensor of shape (H, W, C)\n        blur: If true, apply blur. Should be disabled for cifar10.\n        area_range: The upper and lower bound of the random crop percentage.\n\n    returns:\n        A single image tensor of shape (H, W, C) with values between 0.0 and 1.0.\n    \"\"\"\n    # random resize and crop. Increase the size before we crop.\n    img = tfsim.augmenters.augmentation_utils.cropping.crop_and_resize(\n        img, CIFAR_IMG_SIZE, CIFAR_IMG_SIZE, area_range=area_range\n    )\n    \n    # The following transforms expect the data to be [0, 1]\n    img /= 255.\n    \n    # random color jitter\n    def _jitter_transform(x):\n        return tfsim.augmenters.augmentation_utils.color_jitter.color_jitter_rand(\n            x,\n            np.random.uniform(0.0, 0.4),\n            np.random.uniform(0.0, 0.4),\n            np.random.uniform(0.0, 0.4),\n            np.random.uniform(0.0, 0.1),\n            \"multiplicative\",\n        )\n\n    img = tfsim.augmenters.augmentation_utils.random_apply.random_apply(_jitter_transform, p=0.8, x=img)\n\n    # # random grayscale\n    def _grascayle_transform(x):\n        return tfsim.augmenters.augmentation_utils.color_jitter.to_grayscale(x)\n\n    img = tfsim.augmenters.augmentation_utils.random_apply.random_apply(_grascayle_transform, p=0.2, x=img)\n\n    # optional random gaussian blur\n    if blur:\n        img = tfsim.augmenters.augmentation_utils.blur.random_blur(img, p=0.5)\n\n    # random horizontal flip\n    img = tf.image.random_flip_left_right(img)\n    \n    # scale the data back to [0, 255]\n    img = img * 255.\n    img = tf.clip_by_value(img, 0., 255.)\n\n    return img\n\n\n@tf.function()\ndef process(img):\n    view1 = simsiam_augmenter(img, blur=False)\n    view1 = img_scaling(view1)\n    view2 = simsiam_augmenter(img, blur=False)\n    view2 = img_scaling(view2)\n    return (view1, view2)\n\n\n# Note that there is no label in the train_ds, instead the inputs are two views!\ntrain_ds = tf.data.Dataset.from_tensor_slices(x_train)\ntrain_ds = train_ds.repeat()\ntrain_ds = train_ds.shuffle(1024)\ntrain_ds = train_ds.map(process, num_parallel_calls=tf.data.AUTOTUNE)\ntrain_ds = train_ds.batch(BATCH_SIZE)\ntrain_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n\nval_ds = tf.data.Dataset.from_tensor_slices(x_val)\nval_ds = val_ds.repeat()\nval_ds = val_ds.shuffle(1024)\nval_ds = val_ds.map(process, num_parallel_calls=tf.data.AUTOTUNE)\nval_ds = val_ds.batch(BATCH_SIZE)\nval_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n\nThe following cell plots the pairs of augmented views side by side. This can be a useful sanity check as many augmentation functions are set up for the larger ImageNet examples and can be overly aggressive for smaller images found in CIFAR.\n\ndisplay_imgs = next(train_ds.as_numpy_iterator())\nmax_pixel = np.max([display_imgs[0].max(), display_imgs[1].max()])\nmin_pixel = np.min([display_imgs[0].min(), display_imgs[1].min()])\n\ntfsim.visualization.visualize_views(\n    views=display_imgs,\n    num_imgs=16,\n    views_per_col=8,\n    max_pixel_value=max_pixel,\n    min_pixel_value=min_pixel,\n)\n\n\n\n\n\n\n\n11.4.3 Contrastive Model Setup\nThe following section creates the sub-models used by the different algorithms. There are various architectures for building self-supervised models which may include some of the following:\n\nBackbone: This is the base model and is typically an existing architecture like ResNet or EfficientNet.\nProjector: This is a small multi-layer Neural Net and provides the embedding features at the end of training.\nPredictor: This model is used by BYOL and SimSiam and provides an additional small multi-layer Neural Net.\n\nTypically, the projector and predictor networks are only 2 or 3 layers with batch normalization. Additionally, many papers show a single encoder block, but this often contains both the Backbone and the Projector network.\n\n\n\ncontrastive_loss_functions.png\n\n\nThe diagram above shows three self-supervised architectures supported by TensorFlow Similarity. As you can see, they all share a common structure: * Processing multiple views of the same example. * Using a backbone model for learning the representation output. * Using a projector for the embedding output. * Additionally, note that the loss is symmetric, so we compute it twice during each step. First for view 1 and then a second time for view 2. These two losses are then summed up to compute the final aggregate loss.\n\n11.4.3.1 Backbone Model\nThe backbone uses a custom version of ResNet18 in order to reproduce the SimSiam CIFAR10 results.\nThe ResNet models provided in tf.keras.applications use larger [(1x1), (3x3), (1x1)] blocks that can’t be used to reproduce the SimSiam CIFAR10 results.\n\ntf.keras.backend.clear_session()\ndef get_backbone(img_size, activation=\"relu\", preproc_mode=\"torch\"):\n    input_shape = (img_size, img_size, 3)\n\n    backbone = tfsim.architectures.ResNet18Sim(\n        input_shape,\n        include_top=False,  # Take the pooling layer as the output.\n        pooling=\"avg\",\n    )\n    return backbone\n\n\nbackbone = get_backbone(CIFAR_IMG_SIZE)\nbackbone.summary()\n\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n\n\nModel: \"resnet18sim\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n resnet18 (Functional)       (None, 4, 4, 512)         11182784  \n                                                                 \n avg_pool (GlobalAveragePool  (None, 512)              0         \n ing2D)                                                          \n                                                                 \n=================================================================\nTotal params: 11,182,784\nTrainable params: 11,173,056\nNon-trainable params: 9,728\n_________________________________________________________________\n\n\n\n\n11.4.3.2 Projector Model\nThis MLP is common to all the self-supervised models and is typically a stack of 3 layers of the same size. However, SimSiam only uses 2 layers for the smaller CIFAR images. Having too much capacity in the models can make it difficult for the loss to stabilize and converge. Additionally, the SimSiam paper found that disabling the center and scale parameters can lead to a small boost in the final loss.\nNote this is the model output that is returned by ContrastiveModel.predict() and represents the distance based embedding. This embedding can be used for the KNN lookups and matching classification metrics. However, when using the pre-train model for downstream tasks, only the ContrastiveModel.backbone is used.\n\nprojector = None # Passing None will automatically build the default projector.\n\n\n\n11.4.3.3 Predictor model\nThe predictor model is used by BYOL and SimSiam, and is an additional 2 layer MLP containing a bottleneck in the hidden layer.\n\npredictor = None # Passing None will automatically build the default predictor.\n\n\n\n\n11.4.4 Self-Supervised Algorithms\nThe following section builds the ContrastiveModel based on the ALGORITHM set at the start of the Notebook.\nThe model training is very sensitive to the learning rate decay and weight decay. * SimSiam: Requires using SGD with weight decay from TF Addons. Adding weight decay as a kernel_regularizer doesn’t seem to be able to reproduce the published results in the paper. * Barlow Twins: We can use LAMB and avoid the need for the learning rate schedule. Lamb is similar to the LARS optimizer used in the Barlow paper, but includes the use of ADAM. Alternatively, we can use SGD but the optimizer requires a warm up period, otherwise the loss explodes. * SimCLR: We can also use LAMB as the original paper uses LARS. However, LAMB seems to require smaller learning rates than shown for LARS in the original paper.\n\ncontrastive_model = tfsim.models.create_contrastive_model(\n    backbone=backbone,\n    projector=projector,\n    predictor=predictor,\n    algorithm=ALGORITHM,\n    name=ALGORITHM,\n)\n\nif ALGORITHM == \"simsiam\":\n    loss = tfsim.losses.SimSiamLoss(projection_type=\"cosine_distance\", name=ALGORITHM)\n    lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=INIT_LR,\n        decay_steps=PRE_TRAIN_EPOCHS * PRE_TRAIN_STEPS_PER_EPOCH,\n    )\n    wd_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=WEIGHT_DECAY,\n        decay_steps=PRE_TRAIN_EPOCHS * PRE_TRAIN_STEPS_PER_EPOCH,\n    )\n    optimizer = tfa.optimizers.SGDW(learning_rate=lr_decayed_fn, weight_decay=wd_decayed_fn, momentum=0.9)\nelif ALGORITHM == \"barlow\":\n    loss = tfsim.losses.Barlow(name=ALGORITHM)\n    optimizer = tfa.optimizers.LAMB(learning_rate=INIT_LR)\nelif ALGORITHM == \"simclr\":\n    loss = tfsim.losses.SimCLRLoss(name=ALGORITHM, temperature=TEMPERATURE)\n    optimizer = tfa.optimizers.LAMB(learning_rate=INIT_LR)\nelif ALGORITHM == \"vicreg\":\n    loss = tfsim.losses.VicReg(name=ALGORITHM)\n    optimizer = tfa.optimizers.LAMB(learning_rate=INIT_LR)\nelse:\n    raise ValueError(f\"{ALGORITHM} is not supported.\")\n\n\ncontrastive_model.compile(\n    optimizer=optimizer,\n    loss=loss,\n)\n\ncontrastive_model.summary()\n\nlog_dir = DATA_PATH / \"models\" / \"logs\" / f\"{loss.name}_{time.time()}\"\nchkpt_dir = DATA_PATH / \"models\" / \"checkpoints\" / f\"{loss.name}_{time.time()}\"\n\n\nevb = tfsim.callbacks.EvalCallback(\n    img_scaling(tf.cast(x_query, tf.float32)),\n    y_query,\n    img_scaling(tf.cast(x_index, tf.float32)),\n    y_index,\n    metrics=[\"binary_accuracy\"],\n    k=1,\n    tb_logdir=log_dir,\n)\n\nmcp = tf.keras.callbacks.ModelCheckpoint(\n    filepath=chkpt_dir,\n    monitor=\"val_loss\",\n    mode=\"min\",\n    save_best_only=True,\n    save_weights_only=True,\n)\n\n[Backbone]\nModel: \"resnet18sim\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n resnet18 (Functional)       (None, 4, 4, 512)         11182784  \n                                                                 \n avg_pool (GlobalAveragePool  (None, 512)              0         \n ing2D)                                                          \n                                                                 \n=================================================================\nTotal params: 11,182,784\nTrainable params: 11,173,056\nNon-trainable params: 9,728\n_________________________________________________________________\n\n[Projector]\nModel: \"projector\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n projector_input (InputLayer  [(None, 512)]            0         \n )                                                               \n                                                                 \n projector_layer_0 (Dense)   (None, 512)               262144    \n                                                                 \n batch_normalization_0 (Batc  (None, 512)              2048      \n hNormalization)                                                 \n                                                                 \n relu_activation_0 (Activati  (None, 512)              0         \n on)                                                             \n                                                                 \n projector_output (Dense)    (None, 512)               262144    \n                                                                 \n batch_normalization_ouput (  (None, 512)              1024      \n BatchNormalization)                                             \n                                                                 \n proj_std (ActivationStdLogg  (None, 512)              0         \n ingLayer)                                                       \n                                                                 \n=================================================================\nTotal params: 527,360\nTrainable params: 525,312\nNon-trainable params: 2,048\n_________________________________________________________________\n\n[Predictor]\nModel: \"projector\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n projector_input (InputLayer  [(None, 512)]            0         \n )                                                               \n                                                                 \n projector_layer_0 (Dense)   (None, 512)               262144    \n                                                                 \n batch_normalization_0 (Batc  (None, 512)              2048      \n hNormalization)                                                 \n                                                                 \n relu_activation_0 (Activati  (None, 512)              0         \n on)                                                             \n                                                                 \n projector_output (Dense)    (None, 512)               262144    \n                                                                 \n batch_normalization_ouput (  (None, 512)              1024      \n BatchNormalization)                                             \n                                                                 \n proj_std (ActivationStdLogg  (None, 512)              0         \n ingLayer)                                                       \n                                                                 \n=================================================================\nTotal params: 527,360\nTrainable params: 525,312\nNon-trainable params: 2,048\n_________________________________________________________________\nTensorBoard logging enable in tfsim_contrastive_model/models/logs/simsiam_1683956643.5111115/index\n\n\n\n11.4.4.1 Training\nThe model training provides a number of metrics. - loss: This represents the total loss over the contrastive batch. Separate contrastive and regularization losses will also be shown if there are trainable variables in the model layers. - proj_std and pred_std: These are added as metric logging layers in the model and show the std of the activations of the final layer in the projector or predictor models. - binary_accuracy: This is the nearest neighbor matching classification accuracy. A new index is built at the end of each epoch and the accuracy is computed using the query and index examples.\n\n# The training wil take long time, you can reduce the epoch if you really want to train\nhistory = contrastive_model.fit(\n    train_ds,\n    epochs=PRE_TRAIN_EPOCHS,\n    steps_per_epoch=PRE_TRAIN_STEPS_PER_EPOCH,\n    validation_data=val_ds,\n    validation_steps=VAL_STEPS_PER_EPOCH,\n    callbacks=[evb, mcp],\n    verbose=1,\n)\n\nEpoch 1/200\n87/87 [==============================] - 38s 270ms/step - loss: 0.4331 - projector_loss: 0.2165 - proj_std: 0.0393 - pred_std: 0.0344 - val_loss: 0.2020 - val_projector_loss: 0.1010 - val_proj_std: 0.0280 - val_pred_std: 0.0244 - binary_accuracy: 0.1860\nEpoch 2/200\n87/87 [==============================] - 22s 248ms/step - loss: 0.2563 - projector_loss: 0.1281 - proj_std: 0.0413 - pred_std: 0.0396 - val_loss: 0.0945 - val_projector_loss: 0.0473 - val_proj_std: 0.0246 - val_pred_std: 0.0218 - binary_accuracy: 0.1875\nEpoch 3/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.2271 - projector_loss: 0.1136 - proj_std: 0.0413 - pred_std: 0.0400 - val_loss: 0.1029 - val_projector_loss: 0.0515 - val_proj_std: 0.0280 - val_pred_std: 0.0257 - binary_accuracy: 0.2000\nEpoch 4/200\n87/87 [==============================] - 21s 248ms/step - loss: 0.2130 - projector_loss: 0.1065 - proj_std: 0.0414 - pred_std: 0.0402 - val_loss: 0.0924 - val_projector_loss: 0.0462 - val_proj_std: 0.0287 - val_pred_std: 0.0269 - binary_accuracy: 0.2165\nEpoch 5/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1939 - projector_loss: 0.0969 - proj_std: 0.0414 - pred_std: 0.0403 - val_loss: 0.0851 - val_projector_loss: 0.0425 - val_proj_std: 0.0278 - val_pred_std: 0.0256 - binary_accuracy: 0.2180\nEpoch 6/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1877 - projector_loss: 0.0938 - proj_std: 0.0416 - pred_std: 0.0405 - val_loss: 0.1097 - val_projector_loss: 0.0549 - val_proj_std: 0.0312 - val_pred_std: 0.0297 - binary_accuracy: 0.2385\nEpoch 7/200\n87/87 [==============================] - 21s 247ms/step - loss: 0.1888 - projector_loss: 0.0944 - proj_std: 0.0417 - pred_std: 0.0407 - val_loss: 0.0817 - val_projector_loss: 0.0409 - val_proj_std: 0.0274 - val_pred_std: 0.0251 - binary_accuracy: 0.2220\nEpoch 8/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.2047 - projector_loss: 0.1024 - proj_std: 0.0417 - pred_std: 0.0407 - val_loss: 0.1345 - val_projector_loss: 0.0673 - val_proj_std: 0.0312 - val_pred_std: 0.0291 - binary_accuracy: 0.2395\nEpoch 9/200\n87/87 [==============================] - 22s 249ms/step - loss: 0.2034 - projector_loss: 0.1017 - proj_std: 0.0414 - pred_std: 0.0406 - val_loss: 0.0812 - val_projector_loss: 0.0406 - val_proj_std: 0.0243 - val_pred_std: 0.0223 - binary_accuracy: 0.2385\nEpoch 10/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1956 - projector_loss: 0.0978 - proj_std: 0.0421 - pred_std: 0.0413 - val_loss: 0.1016 - val_projector_loss: 0.0508 - val_proj_std: 0.0302 - val_pred_std: 0.0289 - binary_accuracy: 0.2435\nEpoch 11/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1816 - projector_loss: 0.0908 - proj_std: 0.0422 - pred_std: 0.0415 - val_loss: 0.0855 - val_projector_loss: 0.0427 - val_proj_std: 0.0287 - val_pred_std: 0.0274 - binary_accuracy: 0.2430\nEpoch 12/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1761 - projector_loss: 0.0880 - proj_std: 0.0423 - pred_std: 0.0417 - val_loss: 0.0903 - val_projector_loss: 0.0452 - val_proj_std: 0.0302 - val_pred_std: 0.0289 - binary_accuracy: 0.2600\nEpoch 13/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1749 - projector_loss: 0.0875 - proj_std: 0.0423 - pred_std: 0.0417 - val_loss: 0.0641 - val_projector_loss: 0.0321 - val_proj_std: 0.0240 - val_pred_std: 0.0225 - binary_accuracy: 0.2655\nEpoch 14/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1735 - projector_loss: 0.0867 - proj_std: 0.0424 - pred_std: 0.0418 - val_loss: 0.0893 - val_projector_loss: 0.0447 - val_proj_std: 0.0293 - val_pred_std: 0.0279 - binary_accuracy: 0.2455\nEpoch 15/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1757 - projector_loss: 0.0878 - proj_std: 0.0423 - pred_std: 0.0416 - val_loss: 0.0802 - val_projector_loss: 0.0401 - val_proj_std: 0.0280 - val_pred_std: 0.0266 - binary_accuracy: 0.2640\nEpoch 16/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1916 - projector_loss: 0.0958 - proj_std: 0.0422 - pred_std: 0.0414 - val_loss: 0.1757 - val_projector_loss: 0.0879 - val_proj_std: 0.0350 - val_pred_std: 0.0347 - binary_accuracy: 0.2585\nEpoch 17/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1808 - projector_loss: 0.0904 - proj_std: 0.0422 - pred_std: 0.0414 - val_loss: 0.1341 - val_projector_loss: 0.0670 - val_proj_std: 0.0372 - val_pred_std: 0.0359 - binary_accuracy: 0.2530\nEpoch 18/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1718 - projector_loss: 0.0859 - proj_std: 0.0424 - pred_std: 0.0418 - val_loss: 0.0824 - val_projector_loss: 0.0412 - val_proj_std: 0.0293 - val_pred_std: 0.0280 - binary_accuracy: 0.2675\nEpoch 19/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1667 - projector_loss: 0.0834 - proj_std: 0.0428 - pred_std: 0.0422 - val_loss: 0.0959 - val_projector_loss: 0.0480 - val_proj_std: 0.0322 - val_pred_std: 0.0312 - binary_accuracy: 0.2705\nEpoch 20/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1638 - projector_loss: 0.0819 - proj_std: 0.0428 - pred_std: 0.0423 - val_loss: 0.0923 - val_projector_loss: 0.0462 - val_proj_std: 0.0325 - val_pred_std: 0.0313 - binary_accuracy: 0.2855\nEpoch 21/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1668 - projector_loss: 0.0834 - proj_std: 0.0428 - pred_std: 0.0423 - val_loss: 0.1053 - val_projector_loss: 0.0527 - val_proj_std: 0.0328 - val_pred_std: 0.0318 - binary_accuracy: 0.2835\nEpoch 22/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1738 - projector_loss: 0.0869 - proj_std: 0.0431 - pred_std: 0.0426 - val_loss: 0.1155 - val_projector_loss: 0.0578 - val_proj_std: 0.0343 - val_pred_std: 0.0336 - binary_accuracy: 0.2845\nEpoch 23/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1756 - projector_loss: 0.0878 - proj_std: 0.0431 - pred_std: 0.0426 - val_loss: 0.1889 - val_projector_loss: 0.0945 - val_proj_std: 0.0400 - val_pred_std: 0.0405 - binary_accuracy: 0.2765\nEpoch 24/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1764 - projector_loss: 0.0882 - proj_std: 0.0427 - pred_std: 0.0420 - val_loss: 0.1169 - val_projector_loss: 0.0585 - val_proj_std: 0.0312 - val_pred_std: 0.0306 - binary_accuracy: 0.2970\nEpoch 25/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1821 - projector_loss: 0.0911 - proj_std: 0.0429 - pred_std: 0.0423 - val_loss: 0.1300 - val_projector_loss: 0.0650 - val_proj_std: 0.0340 - val_pred_std: 0.0331 - binary_accuracy: 0.3180\nEpoch 26/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1840 - projector_loss: 0.0920 - proj_std: 0.0430 - pred_std: 0.0425 - val_loss: 0.1157 - val_projector_loss: 0.0579 - val_proj_std: 0.0291 - val_pred_std: 0.0294 - binary_accuracy: 0.2845\nEpoch 27/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1860 - projector_loss: 0.0930 - proj_std: 0.0429 - pred_std: 0.0423 - val_loss: 0.1579 - val_projector_loss: 0.0789 - val_proj_std: 0.0339 - val_pred_std: 0.0329 - binary_accuracy: 0.2710\nEpoch 28/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1861 - projector_loss: 0.0931 - proj_std: 0.0432 - pred_std: 0.0427 - val_loss: 0.1323 - val_projector_loss: 0.0662 - val_proj_std: 0.0340 - val_pred_std: 0.0338 - binary_accuracy: 0.2980\nEpoch 29/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1830 - projector_loss: 0.0915 - proj_std: 0.0432 - pred_std: 0.0428 - val_loss: 0.1267 - val_projector_loss: 0.0633 - val_proj_std: 0.0340 - val_pred_std: 0.0333 - binary_accuracy: 0.3075\nEpoch 30/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1862 - projector_loss: 0.0931 - proj_std: 0.0432 - pred_std: 0.0427 - val_loss: 0.1254 - val_projector_loss: 0.0627 - val_proj_std: 0.0314 - val_pred_std: 0.0303 - binary_accuracy: 0.3210\nEpoch 31/200\n87/87 [==============================] - 21s 238ms/step - loss: 0.1826 - projector_loss: 0.0913 - proj_std: 0.0434 - pred_std: 0.0430 - val_loss: 0.1583 - val_projector_loss: 0.0791 - val_proj_std: 0.0383 - val_pred_std: 0.0378 - binary_accuracy: 0.3000\nEpoch 32/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1903 - projector_loss: 0.0951 - proj_std: 0.0436 - pred_std: 0.0432 - val_loss: 0.1707 - val_projector_loss: 0.0853 - val_proj_std: 0.0364 - val_pred_std: 0.0364 - binary_accuracy: 0.3200\nEpoch 33/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1926 - projector_loss: 0.0963 - proj_std: 0.0436 - pred_std: 0.0431 - val_loss: 0.1496 - val_projector_loss: 0.0748 - val_proj_std: 0.0350 - val_pred_std: 0.0340 - binary_accuracy: 0.3320\nEpoch 34/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1801 - projector_loss: 0.0901 - proj_std: 0.0432 - pred_std: 0.0427 - val_loss: 0.1618 - val_projector_loss: 0.0809 - val_proj_std: 0.0370 - val_pred_std: 0.0364 - binary_accuracy: 0.3070\nEpoch 35/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1798 - projector_loss: 0.0899 - proj_std: 0.0430 - pred_std: 0.0425 - val_loss: 0.1671 - val_projector_loss: 0.0835 - val_proj_std: 0.0349 - val_pred_std: 0.0352 - binary_accuracy: 0.2975\nEpoch 36/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1831 - projector_loss: 0.0916 - proj_std: 0.0431 - pred_std: 0.0426 - val_loss: 0.1298 - val_projector_loss: 0.0649 - val_proj_std: 0.0319 - val_pred_std: 0.0309 - binary_accuracy: 0.3370\nEpoch 37/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1802 - projector_loss: 0.0901 - proj_std: 0.0434 - pred_std: 0.0431 - val_loss: 0.1643 - val_projector_loss: 0.0821 - val_proj_std: 0.0354 - val_pred_std: 0.0350 - binary_accuracy: 0.3445\nEpoch 38/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1813 - projector_loss: 0.0906 - proj_std: 0.0430 - pred_std: 0.0425 - val_loss: 0.1720 - val_projector_loss: 0.0860 - val_proj_std: 0.0377 - val_pred_std: 0.0367 - binary_accuracy: 0.3370\nEpoch 39/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1772 - projector_loss: 0.0886 - proj_std: 0.0433 - pred_std: 0.0429 - val_loss: 0.1259 - val_projector_loss: 0.0629 - val_proj_std: 0.0318 - val_pred_std: 0.0303 - binary_accuracy: 0.3230\nEpoch 40/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1709 - projector_loss: 0.0854 - proj_std: 0.0434 - pred_std: 0.0430 - val_loss: 0.1529 - val_projector_loss: 0.0764 - val_proj_std: 0.0362 - val_pred_std: 0.0359 - binary_accuracy: 0.3370\nEpoch 41/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1729 - projector_loss: 0.0865 - proj_std: 0.0435 - pred_std: 0.0431 - val_loss: 0.1516 - val_projector_loss: 0.0758 - val_proj_std: 0.0370 - val_pred_std: 0.0365 - binary_accuracy: 0.3600\nEpoch 42/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1723 - projector_loss: 0.0862 - proj_std: 0.0435 - pred_std: 0.0432 - val_loss: 0.1646 - val_projector_loss: 0.0823 - val_proj_std: 0.0389 - val_pred_std: 0.0386 - binary_accuracy: 0.3535\nEpoch 43/200\n87/87 [==============================] - 21s 246ms/step - loss: 0.1728 - projector_loss: 0.0864 - proj_std: 0.0430 - pred_std: 0.0425 - val_loss: 0.1873 - val_projector_loss: 0.0937 - val_proj_std: 0.0401 - val_pred_std: 0.0405 - binary_accuracy: 0.3490\nEpoch 44/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1678 - projector_loss: 0.0839 - proj_std: 0.0432 - pred_std: 0.0427 - val_loss: 0.1385 - val_projector_loss: 0.0693 - val_proj_std: 0.0342 - val_pred_std: 0.0327 - binary_accuracy: 0.3655\nEpoch 45/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1682 - projector_loss: 0.0841 - proj_std: 0.0434 - pred_std: 0.0430 - val_loss: 0.1455 - val_projector_loss: 0.0727 - val_proj_std: 0.0370 - val_pred_std: 0.0363 - binary_accuracy: 0.3780\nEpoch 46/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1669 - projector_loss: 0.0834 - proj_std: 0.0432 - pred_std: 0.0427 - val_loss: 0.1654 - val_projector_loss: 0.0827 - val_proj_std: 0.0358 - val_pred_std: 0.0348 - binary_accuracy: 0.3840\nEpoch 47/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1697 - projector_loss: 0.0849 - proj_std: 0.0430 - pred_std: 0.0425 - val_loss: 0.1990 - val_projector_loss: 0.0995 - val_proj_std: 0.0389 - val_pred_std: 0.0390 - binary_accuracy: 0.3765\nEpoch 48/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1671 - projector_loss: 0.0836 - proj_std: 0.0428 - pred_std: 0.0423 - val_loss: 0.1517 - val_projector_loss: 0.0758 - val_proj_std: 0.0340 - val_pred_std: 0.0318 - binary_accuracy: 0.3775\nEpoch 49/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1632 - projector_loss: 0.0816 - proj_std: 0.0428 - pred_std: 0.0423 - val_loss: 0.1786 - val_projector_loss: 0.0893 - val_proj_std: 0.0382 - val_pred_std: 0.0369 - binary_accuracy: 0.3845\nEpoch 50/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1627 - projector_loss: 0.0813 - proj_std: 0.0428 - pred_std: 0.0423 - val_loss: 0.1471 - val_projector_loss: 0.0736 - val_proj_std: 0.0347 - val_pred_std: 0.0328 - binary_accuracy: 0.4110\nEpoch 51/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1591 - projector_loss: 0.0796 - proj_std: 0.0429 - pred_std: 0.0425 - val_loss: 0.1310 - val_projector_loss: 0.0655 - val_proj_std: 0.0316 - val_pred_std: 0.0292 - binary_accuracy: 0.3970\nEpoch 52/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1578 - projector_loss: 0.0789 - proj_std: 0.0430 - pred_std: 0.0426 - val_loss: 0.1748 - val_projector_loss: 0.0874 - val_proj_std: 0.0391 - val_pred_std: 0.0389 - binary_accuracy: 0.4205\nEpoch 53/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1561 - projector_loss: 0.0780 - proj_std: 0.0429 - pred_std: 0.0424 - val_loss: 0.1505 - val_projector_loss: 0.0752 - val_proj_std: 0.0336 - val_pred_std: 0.0322 - binary_accuracy: 0.4275\nEpoch 54/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1574 - projector_loss: 0.0787 - proj_std: 0.0432 - pred_std: 0.0429 - val_loss: 0.1279 - val_projector_loss: 0.0640 - val_proj_std: 0.0344 - val_pred_std: 0.0335 - binary_accuracy: 0.4220\nEpoch 55/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1572 - projector_loss: 0.0786 - proj_std: 0.0435 - pred_std: 0.0432 - val_loss: 0.1413 - val_projector_loss: 0.0706 - val_proj_std: 0.0372 - val_pred_std: 0.0363 - binary_accuracy: 0.4610\nEpoch 56/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1560 - projector_loss: 0.0780 - proj_std: 0.0436 - pred_std: 0.0434 - val_loss: 0.1440 - val_projector_loss: 0.0720 - val_proj_std: 0.0393 - val_pred_std: 0.0389 - binary_accuracy: 0.4540\nEpoch 57/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1567 - projector_loss: 0.0783 - proj_std: 0.0437 - pred_std: 0.0434 - val_loss: 0.1498 - val_projector_loss: 0.0749 - val_proj_std: 0.0392 - val_pred_std: 0.0392 - binary_accuracy: 0.4270\nEpoch 58/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1547 - projector_loss: 0.0774 - proj_std: 0.0438 - pred_std: 0.0435 - val_loss: 0.1457 - val_projector_loss: 0.0729 - val_proj_std: 0.0405 - val_pred_std: 0.0403 - binary_accuracy: 0.4550\nEpoch 59/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1526 - projector_loss: 0.0763 - proj_std: 0.0437 - pred_std: 0.0434 - val_loss: 0.1520 - val_projector_loss: 0.0760 - val_proj_std: 0.0398 - val_pred_std: 0.0395 - binary_accuracy: 0.4690\nEpoch 60/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1548 - projector_loss: 0.0774 - proj_std: 0.0437 - pred_std: 0.0434 - val_loss: 0.1378 - val_projector_loss: 0.0689 - val_proj_std: 0.0387 - val_pred_std: 0.0379 - binary_accuracy: 0.4755\nEpoch 61/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1517 - projector_loss: 0.0759 - proj_std: 0.0438 - pred_std: 0.0435 - val_loss: 0.1398 - val_projector_loss: 0.0699 - val_proj_std: 0.0391 - val_pred_std: 0.0390 - binary_accuracy: 0.4665\nEpoch 62/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1527 - projector_loss: 0.0764 - proj_std: 0.0438 - pred_std: 0.0435 - val_loss: 0.1313 - val_projector_loss: 0.0656 - val_proj_std: 0.0401 - val_pred_std: 0.0399 - binary_accuracy: 0.4750\nEpoch 63/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1516 - projector_loss: 0.0758 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1436 - val_projector_loss: 0.0718 - val_proj_std: 0.0415 - val_pred_std: 0.0416 - binary_accuracy: 0.4770\nEpoch 64/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1492 - projector_loss: 0.0746 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1353 - val_projector_loss: 0.0676 - val_proj_std: 0.0411 - val_pred_std: 0.0409 - binary_accuracy: 0.4915\nEpoch 65/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1493 - projector_loss: 0.0747 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1234 - val_projector_loss: 0.0617 - val_proj_std: 0.0389 - val_pred_std: 0.0386 - binary_accuracy: 0.4995\nEpoch 66/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1486 - projector_loss: 0.0743 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1466 - val_projector_loss: 0.0733 - val_proj_std: 0.0416 - val_pred_std: 0.0420 - binary_accuracy: 0.4970\nEpoch 67/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1471 - projector_loss: 0.0736 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1229 - val_projector_loss: 0.0614 - val_proj_std: 0.0393 - val_pred_std: 0.0394 - binary_accuracy: 0.5015\nEpoch 68/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1470 - projector_loss: 0.0735 - proj_std: 0.0441 - pred_std: 0.0439 - val_loss: 0.1330 - val_projector_loss: 0.0665 - val_proj_std: 0.0408 - val_pred_std: 0.0411 - binary_accuracy: 0.5005\nEpoch 69/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1464 - projector_loss: 0.0732 - proj_std: 0.0441 - pred_std: 0.0439 - val_loss: 0.1417 - val_projector_loss: 0.0708 - val_proj_std: 0.0425 - val_pred_std: 0.0427 - binary_accuracy: 0.5100\nEpoch 70/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1463 - projector_loss: 0.0731 - proj_std: 0.0441 - pred_std: 0.0439 - val_loss: 0.1383 - val_projector_loss: 0.0692 - val_proj_std: 0.0412 - val_pred_std: 0.0417 - binary_accuracy: 0.4915\nEpoch 71/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1468 - projector_loss: 0.0734 - proj_std: 0.0441 - pred_std: 0.0439 - val_loss: 0.1460 - val_projector_loss: 0.0730 - val_proj_std: 0.0422 - val_pred_std: 0.0422 - binary_accuracy: 0.5235\nEpoch 72/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1464 - projector_loss: 0.0732 - proj_std: 0.0441 - pred_std: 0.0439 - val_loss: 0.1293 - val_projector_loss: 0.0647 - val_proj_std: 0.0412 - val_pred_std: 0.0415 - binary_accuracy: 0.5345\nEpoch 73/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1458 - projector_loss: 0.0729 - proj_std: 0.0441 - pred_std: 0.0439 - val_loss: 0.1368 - val_projector_loss: 0.0684 - val_proj_std: 0.0416 - val_pred_std: 0.0418 - binary_accuracy: 0.5325\nEpoch 74/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1434 - projector_loss: 0.0717 - proj_std: 0.0441 - pred_std: 0.0439 - val_loss: 0.1350 - val_projector_loss: 0.0675 - val_proj_std: 0.0421 - val_pred_std: 0.0421 - binary_accuracy: 0.5440\nEpoch 75/200\n87/87 [==============================] - 21s 246ms/step - loss: 0.1463 - projector_loss: 0.0732 - proj_std: 0.0441 - pred_std: 0.0439 - val_loss: 0.1316 - val_projector_loss: 0.0658 - val_proj_std: 0.0412 - val_pred_std: 0.0411 - binary_accuracy: 0.5295\nEpoch 76/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1458 - projector_loss: 0.0729 - proj_std: 0.0441 - pred_std: 0.0439 - val_loss: 0.1388 - val_projector_loss: 0.0694 - val_proj_std: 0.0424 - val_pred_std: 0.0425 - binary_accuracy: 0.5340\nEpoch 77/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1461 - projector_loss: 0.0731 - proj_std: 0.0441 - pred_std: 0.0439 - val_loss: 0.1302 - val_projector_loss: 0.0651 - val_proj_std: 0.0426 - val_pred_std: 0.0426 - binary_accuracy: 0.5590\nEpoch 78/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1450 - projector_loss: 0.0725 - proj_std: 0.0441 - pred_std: 0.0439 - val_loss: 0.1519 - val_projector_loss: 0.0759 - val_proj_std: 0.0423 - val_pred_std: 0.0420 - binary_accuracy: 0.5380\nEpoch 79/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1448 - projector_loss: 0.0724 - proj_std: 0.0441 - pred_std: 0.0439 - val_loss: 0.1457 - val_projector_loss: 0.0729 - val_proj_std: 0.0426 - val_pred_std: 0.0427 - binary_accuracy: 0.5455\nEpoch 80/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1439 - projector_loss: 0.0719 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1351 - val_projector_loss: 0.0676 - val_proj_std: 0.0417 - val_pred_std: 0.0417 - binary_accuracy: 0.5480\nEpoch 81/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1440 - projector_loss: 0.0720 - proj_std: 0.0441 - pred_std: 0.0439 - val_loss: 0.1343 - val_projector_loss: 0.0671 - val_proj_std: 0.0419 - val_pred_std: 0.0417 - binary_accuracy: 0.5445\nEpoch 82/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1462 - projector_loss: 0.0731 - proj_std: 0.0441 - pred_std: 0.0439 - val_loss: 0.1346 - val_projector_loss: 0.0673 - val_proj_std: 0.0427 - val_pred_std: 0.0426 - binary_accuracy: 0.5795\nEpoch 83/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1454 - projector_loss: 0.0727 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1358 - val_projector_loss: 0.0679 - val_proj_std: 0.0418 - val_pred_std: 0.0418 - binary_accuracy: 0.5830\nEpoch 84/200\n87/87 [==============================] - 21s 246ms/step - loss: 0.1475 - projector_loss: 0.0738 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1478 - val_projector_loss: 0.0739 - val_proj_std: 0.0423 - val_pred_std: 0.0424 - binary_accuracy: 0.5725\nEpoch 85/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1459 - projector_loss: 0.0730 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1394 - val_projector_loss: 0.0697 - val_proj_std: 0.0421 - val_pred_std: 0.0420 - binary_accuracy: 0.5680\nEpoch 86/200\n87/87 [==============================] - 22s 249ms/step - loss: 0.1424 - projector_loss: 0.0712 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1341 - val_projector_loss: 0.0671 - val_proj_std: 0.0418 - val_pred_std: 0.0416 - binary_accuracy: 0.5705\nEpoch 87/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1415 - projector_loss: 0.0707 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1399 - val_projector_loss: 0.0699 - val_proj_std: 0.0423 - val_pred_std: 0.0424 - binary_accuracy: 0.5815\nEpoch 88/200\n87/87 [==============================] - 21s 246ms/step - loss: 0.1400 - projector_loss: 0.0700 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1293 - val_projector_loss: 0.0646 - val_proj_std: 0.0425 - val_pred_std: 0.0426 - binary_accuracy: 0.5570\nEpoch 89/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1407 - projector_loss: 0.0704 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1391 - val_projector_loss: 0.0695 - val_proj_std: 0.0430 - val_pred_std: 0.0428 - binary_accuracy: 0.5780\nEpoch 90/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1388 - projector_loss: 0.0694 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1393 - val_projector_loss: 0.0696 - val_proj_std: 0.0431 - val_pred_std: 0.0432 - binary_accuracy: 0.6075\nEpoch 91/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1397 - projector_loss: 0.0699 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1335 - val_projector_loss: 0.0667 - val_proj_std: 0.0430 - val_pred_std: 0.0429 - binary_accuracy: 0.5855\nEpoch 92/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1409 - projector_loss: 0.0705 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1368 - val_projector_loss: 0.0684 - val_proj_std: 0.0425 - val_pred_std: 0.0424 - binary_accuracy: 0.5965\nEpoch 93/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1427 - projector_loss: 0.0713 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1381 - val_projector_loss: 0.0690 - val_proj_std: 0.0423 - val_pred_std: 0.0422 - binary_accuracy: 0.5900\nEpoch 94/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1414 - projector_loss: 0.0707 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1419 - val_projector_loss: 0.0710 - val_proj_std: 0.0427 - val_pred_std: 0.0426 - binary_accuracy: 0.5900\nEpoch 95/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1399 - projector_loss: 0.0699 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1391 - val_projector_loss: 0.0696 - val_proj_std: 0.0422 - val_pred_std: 0.0422 - binary_accuracy: 0.5820\nEpoch 96/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1428 - projector_loss: 0.0714 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1449 - val_projector_loss: 0.0725 - val_proj_std: 0.0431 - val_pred_std: 0.0429 - binary_accuracy: 0.6065\nEpoch 97/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1434 - projector_loss: 0.0717 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1355 - val_projector_loss: 0.0678 - val_proj_std: 0.0425 - val_pred_std: 0.0427 - binary_accuracy: 0.6130\nEpoch 98/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1415 - projector_loss: 0.0708 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1397 - val_projector_loss: 0.0699 - val_proj_std: 0.0429 - val_pred_std: 0.0429 - binary_accuracy: 0.6110\nEpoch 99/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1396 - projector_loss: 0.0698 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1418 - val_projector_loss: 0.0709 - val_proj_std: 0.0432 - val_pred_std: 0.0431 - binary_accuracy: 0.6000\nEpoch 100/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1403 - projector_loss: 0.0702 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1440 - val_projector_loss: 0.0720 - val_proj_std: 0.0429 - val_pred_std: 0.0425 - binary_accuracy: 0.6150\nEpoch 101/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1426 - projector_loss: 0.0713 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1401 - val_projector_loss: 0.0700 - val_proj_std: 0.0429 - val_pred_std: 0.0428 - binary_accuracy: 0.6175\nEpoch 102/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1418 - projector_loss: 0.0709 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1502 - val_projector_loss: 0.0751 - val_proj_std: 0.0432 - val_pred_std: 0.0431 - binary_accuracy: 0.6390\nEpoch 103/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1404 - projector_loss: 0.0702 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1440 - val_projector_loss: 0.0720 - val_proj_std: 0.0432 - val_pred_std: 0.0430 - binary_accuracy: 0.6070\nEpoch 104/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1418 - projector_loss: 0.0709 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1422 - val_projector_loss: 0.0711 - val_proj_std: 0.0429 - val_pred_std: 0.0428 - binary_accuracy: 0.6300\nEpoch 105/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1411 - projector_loss: 0.0706 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1431 - val_projector_loss: 0.0715 - val_proj_std: 0.0425 - val_pred_std: 0.0423 - binary_accuracy: 0.6075\nEpoch 106/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1416 - projector_loss: 0.0708 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1377 - val_projector_loss: 0.0688 - val_proj_std: 0.0432 - val_pred_std: 0.0432 - binary_accuracy: 0.6260\nEpoch 107/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1414 - projector_loss: 0.0707 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1400 - val_projector_loss: 0.0700 - val_proj_std: 0.0430 - val_pred_std: 0.0430 - binary_accuracy: 0.6285\nEpoch 108/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1407 - projector_loss: 0.0703 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1405 - val_projector_loss: 0.0702 - val_proj_std: 0.0432 - val_pred_std: 0.0430 - binary_accuracy: 0.6435\nEpoch 109/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1383 - projector_loss: 0.0691 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1346 - val_projector_loss: 0.0673 - val_proj_std: 0.0428 - val_pred_std: 0.0427 - binary_accuracy: 0.6365\nEpoch 110/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1402 - projector_loss: 0.0701 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1347 - val_projector_loss: 0.0673 - val_proj_std: 0.0431 - val_pred_std: 0.0431 - binary_accuracy: 0.6470\nEpoch 111/200\n87/87 [==============================] - 22s 258ms/step - loss: 0.1370 - projector_loss: 0.0685 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1397 - val_projector_loss: 0.0698 - val_proj_std: 0.0433 - val_pred_std: 0.0433 - binary_accuracy: 0.6400\nEpoch 112/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1385 - projector_loss: 0.0692 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1423 - val_projector_loss: 0.0711 - val_proj_std: 0.0432 - val_pred_std: 0.0431 - binary_accuracy: 0.6525\nEpoch 113/200\n87/87 [==============================] - 22s 249ms/step - loss: 0.1394 - projector_loss: 0.0697 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1423 - val_projector_loss: 0.0712 - val_proj_std: 0.0432 - val_pred_std: 0.0431 - binary_accuracy: 0.6445\nEpoch 114/200\n87/87 [==============================] - 21s 246ms/step - loss: 0.1366 - projector_loss: 0.0683 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1409 - val_projector_loss: 0.0704 - val_proj_std: 0.0432 - val_pred_std: 0.0432 - binary_accuracy: 0.6450\nEpoch 115/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1363 - projector_loss: 0.0682 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1413 - val_projector_loss: 0.0706 - val_proj_std: 0.0434 - val_pred_std: 0.0434 - binary_accuracy: 0.6425\nEpoch 116/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1333 - projector_loss: 0.0666 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1338 - val_projector_loss: 0.0669 - val_proj_std: 0.0428 - val_pred_std: 0.0427 - binary_accuracy: 0.6395\nEpoch 117/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1333 - projector_loss: 0.0667 - proj_std: 0.0439 - pred_std: 0.0437 - val_loss: 0.1361 - val_projector_loss: 0.0680 - val_proj_std: 0.0433 - val_pred_std: 0.0431 - binary_accuracy: 0.6480\nEpoch 118/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1338 - projector_loss: 0.0669 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1302 - val_projector_loss: 0.0651 - val_proj_std: 0.0423 - val_pred_std: 0.0422 - binary_accuracy: 0.6430\nEpoch 119/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1330 - projector_loss: 0.0665 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1380 - val_projector_loss: 0.0690 - val_proj_std: 0.0433 - val_pred_std: 0.0432 - binary_accuracy: 0.6525\nEpoch 120/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1337 - projector_loss: 0.0669 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1344 - val_projector_loss: 0.0672 - val_proj_std: 0.0434 - val_pred_std: 0.0434 - binary_accuracy: 0.6635\nEpoch 121/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1321 - projector_loss: 0.0661 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1361 - val_projector_loss: 0.0680 - val_proj_std: 0.0431 - val_pred_std: 0.0430 - binary_accuracy: 0.6595\nEpoch 122/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1307 - projector_loss: 0.0654 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1336 - val_projector_loss: 0.0668 - val_proj_std: 0.0433 - val_pred_std: 0.0432 - binary_accuracy: 0.6400\nEpoch 123/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1327 - projector_loss: 0.0664 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1344 - val_projector_loss: 0.0672 - val_proj_std: 0.0433 - val_pred_std: 0.0432 - binary_accuracy: 0.6575\nEpoch 124/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1310 - projector_loss: 0.0655 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1282 - val_projector_loss: 0.0641 - val_proj_std: 0.0431 - val_pred_std: 0.0429 - binary_accuracy: 0.6550\nEpoch 125/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1307 - projector_loss: 0.0654 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1335 - val_projector_loss: 0.0667 - val_proj_std: 0.0433 - val_pred_std: 0.0432 - binary_accuracy: 0.6600\nEpoch 126/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1305 - projector_loss: 0.0652 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1272 - val_projector_loss: 0.0636 - val_proj_std: 0.0427 - val_pred_std: 0.0427 - binary_accuracy: 0.6545\nEpoch 127/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1298 - projector_loss: 0.0649 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1386 - val_projector_loss: 0.0693 - val_proj_std: 0.0433 - val_pred_std: 0.0432 - binary_accuracy: 0.6620\nEpoch 128/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1312 - projector_loss: 0.0656 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1303 - val_projector_loss: 0.0651 - val_proj_std: 0.0433 - val_pred_std: 0.0432 - binary_accuracy: 0.6870\nEpoch 129/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1284 - projector_loss: 0.0642 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1347 - val_projector_loss: 0.0673 - val_proj_std: 0.0432 - val_pred_std: 0.0432 - binary_accuracy: 0.6735\nEpoch 130/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1296 - projector_loss: 0.0648 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1309 - val_projector_loss: 0.0655 - val_proj_std: 0.0433 - val_pred_std: 0.0433 - binary_accuracy: 0.6800\nEpoch 131/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1291 - projector_loss: 0.0645 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1257 - val_projector_loss: 0.0629 - val_proj_std: 0.0433 - val_pred_std: 0.0433 - binary_accuracy: 0.6825\nEpoch 132/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1268 - projector_loss: 0.0634 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1282 - val_projector_loss: 0.0641 - val_proj_std: 0.0432 - val_pred_std: 0.0431 - binary_accuracy: 0.6795\nEpoch 133/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1270 - projector_loss: 0.0635 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1253 - val_projector_loss: 0.0626 - val_proj_std: 0.0432 - val_pred_std: 0.0430 - binary_accuracy: 0.6865\nEpoch 134/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1269 - projector_loss: 0.0634 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1302 - val_projector_loss: 0.0651 - val_proj_std: 0.0433 - val_pred_std: 0.0432 - binary_accuracy: 0.6885\nEpoch 135/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1255 - projector_loss: 0.0628 - proj_std: 0.0440 - pred_std: 0.0438 - val_loss: 0.1349 - val_projector_loss: 0.0674 - val_proj_std: 0.0435 - val_pred_std: 0.0435 - binary_accuracy: 0.6965\nEpoch 136/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1269 - projector_loss: 0.0634 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1297 - val_projector_loss: 0.0649 - val_proj_std: 0.0434 - val_pred_std: 0.0434 - binary_accuracy: 0.6930\nEpoch 137/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1254 - projector_loss: 0.0627 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1278 - val_projector_loss: 0.0639 - val_proj_std: 0.0435 - val_pred_std: 0.0434 - binary_accuracy: 0.6775\nEpoch 138/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1263 - projector_loss: 0.0631 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1348 - val_projector_loss: 0.0674 - val_proj_std: 0.0435 - val_pred_std: 0.0434 - binary_accuracy: 0.6900\nEpoch 139/200\n87/87 [==============================] - 21s 246ms/step - loss: 0.1250 - projector_loss: 0.0625 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1274 - val_projector_loss: 0.0637 - val_proj_std: 0.0434 - val_pred_std: 0.0434 - binary_accuracy: 0.6880\nEpoch 140/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1234 - projector_loss: 0.0617 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1226 - val_projector_loss: 0.0613 - val_proj_std: 0.0433 - val_pred_std: 0.0432 - binary_accuracy: 0.6775\nEpoch 141/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1227 - projector_loss: 0.0614 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1255 - val_projector_loss: 0.0628 - val_proj_std: 0.0435 - val_pred_std: 0.0434 - binary_accuracy: 0.6895\nEpoch 142/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1218 - projector_loss: 0.0609 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1240 - val_projector_loss: 0.0620 - val_proj_std: 0.0432 - val_pred_std: 0.0431 - binary_accuracy: 0.6900\nEpoch 143/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1217 - projector_loss: 0.0608 - proj_std: 0.0439 - pred_std: 0.0438 - val_loss: 0.1304 - val_projector_loss: 0.0652 - val_proj_std: 0.0434 - val_pred_std: 0.0434 - binary_accuracy: 0.6875\nEpoch 144/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1212 - projector_loss: 0.0606 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1211 - val_projector_loss: 0.0605 - val_proj_std: 0.0431 - val_pred_std: 0.0431 - binary_accuracy: 0.6980\nEpoch 145/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1196 - projector_loss: 0.0598 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1245 - val_projector_loss: 0.0623 - val_proj_std: 0.0435 - val_pred_std: 0.0434 - binary_accuracy: 0.6955\nEpoch 146/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1220 - projector_loss: 0.0610 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1237 - val_projector_loss: 0.0618 - val_proj_std: 0.0434 - val_pred_std: 0.0433 - binary_accuracy: 0.7090\nEpoch 147/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1200 - projector_loss: 0.0600 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1272 - val_projector_loss: 0.0636 - val_proj_std: 0.0432 - val_pred_std: 0.0431 - binary_accuracy: 0.6925\nEpoch 148/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1196 - projector_loss: 0.0598 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1201 - val_projector_loss: 0.0601 - val_proj_std: 0.0432 - val_pred_std: 0.0431 - binary_accuracy: 0.7090\nEpoch 149/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1169 - projector_loss: 0.0585 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1287 - val_projector_loss: 0.0643 - val_proj_std: 0.0434 - val_pred_std: 0.0434 - binary_accuracy: 0.7080\nEpoch 150/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1189 - projector_loss: 0.0594 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1247 - val_projector_loss: 0.0623 - val_proj_std: 0.0433 - val_pred_std: 0.0433 - binary_accuracy: 0.6920\nEpoch 151/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1189 - projector_loss: 0.0595 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1223 - val_projector_loss: 0.0612 - val_proj_std: 0.0434 - val_pred_std: 0.0433 - binary_accuracy: 0.7150\nEpoch 152/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1185 - projector_loss: 0.0592 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1249 - val_projector_loss: 0.0625 - val_proj_std: 0.0434 - val_pred_std: 0.0434 - binary_accuracy: 0.6980\nEpoch 153/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1159 - projector_loss: 0.0579 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1202 - val_projector_loss: 0.0601 - val_proj_std: 0.0436 - val_pred_std: 0.0436 - binary_accuracy: 0.7050\nEpoch 154/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1174 - projector_loss: 0.0587 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1250 - val_projector_loss: 0.0625 - val_proj_std: 0.0434 - val_pred_std: 0.0434 - binary_accuracy: 0.7150\nEpoch 155/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1163 - projector_loss: 0.0582 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1177 - val_projector_loss: 0.0589 - val_proj_std: 0.0436 - val_pred_std: 0.0435 - binary_accuracy: 0.7115\nEpoch 156/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1124 - projector_loss: 0.0562 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1245 - val_projector_loss: 0.0622 - val_proj_std: 0.0435 - val_pred_std: 0.0435 - binary_accuracy: 0.7035\nEpoch 157/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1158 - projector_loss: 0.0579 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1191 - val_projector_loss: 0.0596 - val_proj_std: 0.0431 - val_pred_std: 0.0431 - binary_accuracy: 0.7035\nEpoch 158/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1127 - projector_loss: 0.0564 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1242 - val_projector_loss: 0.0621 - val_proj_std: 0.0437 - val_pred_std: 0.0437 - binary_accuracy: 0.7035\nEpoch 159/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1152 - projector_loss: 0.0576 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1201 - val_projector_loss: 0.0600 - val_proj_std: 0.0436 - val_pred_std: 0.0435 - binary_accuracy: 0.7055\nEpoch 160/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1124 - projector_loss: 0.0562 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1243 - val_projector_loss: 0.0622 - val_proj_std: 0.0437 - val_pred_std: 0.0436 - binary_accuracy: 0.7245\nEpoch 161/200\n87/87 [==============================] - 21s 246ms/step - loss: 0.1135 - projector_loss: 0.0568 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1233 - val_projector_loss: 0.0617 - val_proj_std: 0.0436 - val_pred_std: 0.0436 - binary_accuracy: 0.7105\nEpoch 162/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1122 - projector_loss: 0.0561 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1195 - val_projector_loss: 0.0597 - val_proj_std: 0.0436 - val_pred_std: 0.0435 - binary_accuracy: 0.7190\nEpoch 163/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1099 - projector_loss: 0.0550 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1217 - val_projector_loss: 0.0608 - val_proj_std: 0.0437 - val_pred_std: 0.0436 - binary_accuracy: 0.7170\nEpoch 164/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1111 - projector_loss: 0.0556 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1177 - val_projector_loss: 0.0589 - val_proj_std: 0.0436 - val_pred_std: 0.0436 - binary_accuracy: 0.7055\nEpoch 165/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1083 - projector_loss: 0.0542 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1178 - val_projector_loss: 0.0589 - val_proj_std: 0.0438 - val_pred_std: 0.0437 - binary_accuracy: 0.7205\nEpoch 166/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1091 - projector_loss: 0.0545 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1170 - val_projector_loss: 0.0585 - val_proj_std: 0.0437 - val_pred_std: 0.0437 - binary_accuracy: 0.7255\nEpoch 167/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1076 - projector_loss: 0.0538 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1120 - val_projector_loss: 0.0560 - val_proj_std: 0.0436 - val_pred_std: 0.0436 - binary_accuracy: 0.7190\nEpoch 168/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1066 - projector_loss: 0.0533 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1171 - val_projector_loss: 0.0585 - val_proj_std: 0.0437 - val_pred_std: 0.0437 - binary_accuracy: 0.7255\nEpoch 169/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1093 - projector_loss: 0.0547 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1143 - val_projector_loss: 0.0571 - val_proj_std: 0.0438 - val_pred_std: 0.0438 - binary_accuracy: 0.7240\nEpoch 170/200\n87/87 [==============================] - 22s 248ms/step - loss: 0.1079 - projector_loss: 0.0539 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1140 - val_projector_loss: 0.0570 - val_proj_std: 0.0437 - val_pred_std: 0.0436 - binary_accuracy: 0.7210\nEpoch 171/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1034 - projector_loss: 0.0517 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1124 - val_projector_loss: 0.0562 - val_proj_std: 0.0438 - val_pred_std: 0.0437 - binary_accuracy: 0.7370\nEpoch 172/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1052 - projector_loss: 0.0526 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1166 - val_projector_loss: 0.0583 - val_proj_std: 0.0439 - val_pred_std: 0.0438 - binary_accuracy: 0.7315\nEpoch 173/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1062 - projector_loss: 0.0531 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1145 - val_projector_loss: 0.0573 - val_proj_std: 0.0438 - val_pred_std: 0.0437 - binary_accuracy: 0.7240\nEpoch 174/200\n87/87 [==============================] - 21s 247ms/step - loss: 0.1046 - projector_loss: 0.0523 - proj_std: 0.0440 - pred_std: 0.0440 - val_loss: 0.1142 - val_projector_loss: 0.0571 - val_proj_std: 0.0437 - val_pred_std: 0.0436 - binary_accuracy: 0.7270\nEpoch 175/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1053 - projector_loss: 0.0526 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1130 - val_projector_loss: 0.0565 - val_proj_std: 0.0438 - val_pred_std: 0.0437 - binary_accuracy: 0.7325\nEpoch 176/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1030 - projector_loss: 0.0515 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1129 - val_projector_loss: 0.0565 - val_proj_std: 0.0438 - val_pred_std: 0.0438 - binary_accuracy: 0.7355\nEpoch 177/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1027 - projector_loss: 0.0513 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1119 - val_projector_loss: 0.0559 - val_proj_std: 0.0439 - val_pred_std: 0.0438 - binary_accuracy: 0.7380\nEpoch 178/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1033 - projector_loss: 0.0516 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1148 - val_projector_loss: 0.0574 - val_proj_std: 0.0439 - val_pred_std: 0.0438 - binary_accuracy: 0.7430\nEpoch 179/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.1044 - projector_loss: 0.0522 - proj_std: 0.0440 - pred_std: 0.0440 - val_loss: 0.1157 - val_projector_loss: 0.0578 - val_proj_std: 0.0438 - val_pred_std: 0.0438 - binary_accuracy: 0.7455\nEpoch 180/200\n87/87 [==============================] - 21s 246ms/step - loss: 0.1028 - projector_loss: 0.0514 - proj_std: 0.0440 - pred_std: 0.0440 - val_loss: 0.1123 - val_projector_loss: 0.0561 - val_proj_std: 0.0438 - val_pred_std: 0.0438 - binary_accuracy: 0.7335\nEpoch 181/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1003 - projector_loss: 0.0502 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1136 - val_projector_loss: 0.0568 - val_proj_std: 0.0438 - val_pred_std: 0.0438 - binary_accuracy: 0.7340\nEpoch 182/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1011 - projector_loss: 0.0506 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1103 - val_projector_loss: 0.0551 - val_proj_std: 0.0438 - val_pred_std: 0.0438 - binary_accuracy: 0.7295\nEpoch 183/200\n87/87 [==============================] - 21s 239ms/step - loss: 0.1010 - projector_loss: 0.0505 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1116 - val_projector_loss: 0.0558 - val_proj_std: 0.0439 - val_pred_std: 0.0438 - binary_accuracy: 0.7395\nEpoch 184/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.1012 - projector_loss: 0.0506 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1156 - val_projector_loss: 0.0578 - val_proj_std: 0.0439 - val_pred_std: 0.0438 - binary_accuracy: 0.7435\nEpoch 185/200\n87/87 [==============================] - 21s 245ms/step - loss: 0.1005 - projector_loss: 0.0502 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1113 - val_projector_loss: 0.0557 - val_proj_std: 0.0439 - val_pred_std: 0.0438 - binary_accuracy: 0.7320\nEpoch 186/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1007 - projector_loss: 0.0503 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1117 - val_projector_loss: 0.0559 - val_proj_std: 0.0439 - val_pred_std: 0.0438 - binary_accuracy: 0.7380\nEpoch 187/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.0993 - projector_loss: 0.0497 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1107 - val_projector_loss: 0.0553 - val_proj_std: 0.0439 - val_pred_std: 0.0438 - binary_accuracy: 0.7320\nEpoch 188/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.0994 - projector_loss: 0.0497 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1136 - val_projector_loss: 0.0568 - val_proj_std: 0.0439 - val_pred_std: 0.0438 - binary_accuracy: 0.7340\nEpoch 189/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1007 - projector_loss: 0.0504 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1111 - val_projector_loss: 0.0555 - val_proj_std: 0.0439 - val_pred_std: 0.0439 - binary_accuracy: 0.7290\nEpoch 190/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.0992 - projector_loss: 0.0496 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1089 - val_projector_loss: 0.0544 - val_proj_std: 0.0439 - val_pred_std: 0.0439 - binary_accuracy: 0.7365\nEpoch 191/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.0981 - projector_loss: 0.0491 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1110 - val_projector_loss: 0.0555 - val_proj_std: 0.0439 - val_pred_std: 0.0439 - binary_accuracy: 0.7340\nEpoch 192/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.0991 - projector_loss: 0.0495 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1134 - val_projector_loss: 0.0567 - val_proj_std: 0.0439 - val_pred_std: 0.0439 - binary_accuracy: 0.7400\nEpoch 193/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.0991 - projector_loss: 0.0495 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1088 - val_projector_loss: 0.0544 - val_proj_std: 0.0439 - val_pred_std: 0.0439 - binary_accuracy: 0.7380\nEpoch 194/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.0998 - projector_loss: 0.0499 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1135 - val_projector_loss: 0.0568 - val_proj_std: 0.0439 - val_pred_std: 0.0439 - binary_accuracy: 0.7375\nEpoch 195/200\n87/87 [==============================] - 21s 240ms/step - loss: 0.1001 - projector_loss: 0.0501 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1099 - val_projector_loss: 0.0549 - val_proj_std: 0.0439 - val_pred_std: 0.0439 - binary_accuracy: 0.7400\nEpoch 196/200\n87/87 [==============================] - 21s 242ms/step - loss: 0.1001 - projector_loss: 0.0500 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1077 - val_projector_loss: 0.0539 - val_proj_std: 0.0439 - val_pred_std: 0.0439 - binary_accuracy: 0.7395\nEpoch 197/200\n87/87 [==============================] - 21s 243ms/step - loss: 0.0988 - projector_loss: 0.0494 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1110 - val_projector_loss: 0.0555 - val_proj_std: 0.0439 - val_pred_std: 0.0439 - binary_accuracy: 0.7390\nEpoch 198/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.0983 - projector_loss: 0.0492 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1099 - val_projector_loss: 0.0550 - val_proj_std: 0.0439 - val_pred_std: 0.0439 - binary_accuracy: 0.7385\nEpoch 199/200\n87/87 [==============================] - 21s 244ms/step - loss: 0.0997 - projector_loss: 0.0498 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1126 - val_projector_loss: 0.0563 - val_proj_std: 0.0439 - val_pred_std: 0.0439 - binary_accuracy: 0.7380\nEpoch 200/200\n87/87 [==============================] - 21s 241ms/step - loss: 0.1001 - projector_loss: 0.0500 - proj_std: 0.0440 - pred_std: 0.0439 - val_loss: 0.1115 - val_projector_loss: 0.0558 - val_proj_std: 0.0439 - val_pred_std: 0.0439 - binary_accuracy: 0.7380\n\n\n\nplt.figure(figsize=(15, 4))\nplt.subplot(1, 3, 1)\nplt.plot(history.history[\"loss\"])\nplt.grid()\nplt.title(f\"{loss.name} - loss\")\n\nplt.subplot(1, 3, 2)\nplt.plot(history.history[\"proj_std\"], label=\"proj\")\nif \"pred_std\" in history.history:\n    plt.plot(history.history[\"pred_std\"], label=\"pred\")\nplt.grid()\nplt.title(f\"{loss.name} - std metrics\")\nplt.legend()\n\nplt.subplot(1, 3, 3)\nplt.plot(history.history[\"binary_accuracy\"], label=\"acc\")\nplt.grid()\nplt.title(f\"{loss.name} - match metrics\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n11.4.4.2 Save and Reload\nThe ContrastiveModel contains a set of sub-models and custom train and test steps. Consequently, the ContrastiveModel implements a custom save function that performs the following:\n\nSaves each of the sub models, including the predictor if one exists.\nA JSON file containing the serialized Loss, Metrics, and Optimizer.\nThe Optimizer weighs as a npy file.\n\n\ncontrastive_model.save(DATA_PATH / \"models\" / \"trained_model\")\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 21). These functions will not be directly callable after loading.\n\n\n\ndel contrastive_model\n\n\ncontrastive_model = tf.keras.models.load_model(\n    DATA_PATH / \"models\" / \"trained_model\",\n    custom_objects={\n        \"ContrastiveModel\": tfsim.models.ContrastiveModel,\n        \"ActivationStdLoggingLayer\": tfsim.layers.ActivationStdLoggingLayer,\n    },\n)\n\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704780) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706280) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705460) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706540) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706150) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705110) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704630) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706080) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705240) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706770) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706380) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705580) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705560) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704590) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706210) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705010) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706070) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706100) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705480) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706130) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705300) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704420) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706590) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706020) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706370) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704670) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706700) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704490) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704370) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705100) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704650) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705130) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704840) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706450) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705650) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705280) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705930) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706810) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706110) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706440) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704790) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704360) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705830) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706430) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704530) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704860) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706510) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706710) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705670) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705170) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705800) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705080) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705590) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706640) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704600) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704480) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706310) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706270) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706560) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705740) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704830) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706780) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705070) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705050) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705140) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704500) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704760) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706300) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706000) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704680) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704450) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705900) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706230) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706690) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705440) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706290) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705000) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704970) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706550) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705190) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704610) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706660) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705920) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705880) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706530) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704510) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706420) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704700) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706340) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704340) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705360) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704320) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706050) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705060) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705380) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706460) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705490) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705820) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705520) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705730) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705870) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705610) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706240) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706220) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705470) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705410) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706010) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704330) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706750) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704750) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706120) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705600) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705020) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706490) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704580) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706260) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705290) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705910) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705120) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704800) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704690) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706740) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704660) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706330) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706730) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705540) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705860) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706470) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706500) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704820) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706160) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705840) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704640) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704770) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704540) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704620) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704410) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705390) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704390) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704930) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704440) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705680) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706600) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704990) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704300) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705330) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706620) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706680) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706190) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706630) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705700) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704740) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705340) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704880) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705750) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706610) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706670) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705230) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706390) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705320) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705250) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705030) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705790) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705090) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706140) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704950) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705200) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706250) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705630) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705780) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706090) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705400) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705180) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704890) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706580) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706180) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706030) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706790) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704710) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705210) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706480) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704520) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704870) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704570) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704960) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705940) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704910) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705040) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706350) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705310) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706520) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704310) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706360) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704980) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704850) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704550) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705760) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704430) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705710) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704380) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704470) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706650) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705970) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706570) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706760) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705160) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706060) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706800) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704940) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704560) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705430) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705370) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705350) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705260) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705660) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706410) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705220) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706400) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706170) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706320) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705450) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704400) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705550) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705270) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705620) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706200) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705420) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705640) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705990) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704900) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705980) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704730) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4706040) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705530) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705950) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704810) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705150) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705510) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705890) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705960) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704350) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705850) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705690) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705500) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704460) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705770) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705570) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4705810) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_4704920) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n\n\n\n\n\n11.4.5 Evaluation\nThis final section trains two different classifiers.\n\nNo Pre-training: Uses a ResNet18 model and a simple linear layer.\nPre-trained Uses the frozen pre-trained backbone from the ContrastiveModel and only trains the weights in the linear layer.\n\nThe original train data is partitioned into eval_train and eval_val splits and a simplified augmentation is applied to the training data. The models are then trained for 10 epochs and the classification accuracy is evaluated on the held out test split.\n\nTEST_EPOCHS = 10\nTEST_STEPS_PER_EPOCH = len(x_train) // BATCH_SIZE\n\n@tf.function\ndef eval_augmenter(img):\n    # random resize and crop. Increase the size before we crop.\n    img = tfsim.augmenters.augmentation_utils.cropping.crop_and_resize(\n        img, CIFAR_IMG_SIZE, CIFAR_IMG_SIZE, area_range=(0.2, 1.0)\n    )\n    # random horizontal flip\n    img = tf.image.random_flip_left_right(img)\n    img = tf.clip_by_value(img, 0., 255.)\n\n    return img\n\neval_train_ds = tf.data.Dataset.from_tensor_slices((x_train, tf.keras.utils.to_categorical(y_train, 10)))\neval_train_ds = eval_train_ds.repeat()\neval_train_ds = eval_train_ds.shuffle(1024)\neval_train_ds = eval_train_ds.map(lambda x, y: (eval_augmenter(x), y), tf.data.AUTOTUNE)\neval_train_ds = eval_train_ds.map(lambda x, y: (img_scaling(x), y), tf.data.AUTOTUNE)\neval_train_ds = eval_train_ds.batch(BATCH_SIZE)\neval_train_ds = eval_train_ds.prefetch(tf.data.AUTOTUNE)\n\neval_val_ds = tf.data.Dataset.from_tensor_slices((x_val, tf.keras.utils.to_categorical(y_val, 10)))\neval_val_ds = eval_val_ds.repeat()\neval_val_ds = eval_val_ds.shuffle(1024)\neval_val_ds = eval_val_ds.map(lambda x, y: (img_scaling(tf.cast(x, dtype=tf.float32)), y), tf.data.AUTOTUNE)\neval_val_ds = eval_val_ds.batch(BATCH_SIZE)\neval_val_ds = eval_val_ds.prefetch(tf.data.AUTOTUNE)\n\neval_test_ds = tf.data.Dataset.from_tensor_slices((x_test, tf.keras.utils.to_categorical(y_test, 10)))\neval_test_ds = eval_test_ds.map(lambda x, y: (img_scaling(tf.cast(x, dtype=tf.float32)), y), tf.data.AUTOTUNE)\neval_test_ds = eval_test_ds.batch(BATCH_SIZE)\neval_test_ds = eval_test_ds.prefetch(tf.data.AUTOTUNE)\n\ndef get_eval_model(img_size, backbone, total_steps, trainable=True, lr=1.8):\n    backbone.trainable = trainable\n    inputs = tf.keras.layers.Input((img_size, img_size, 3), name=\"eval_input\")\n    x = backbone(inputs, training=trainable)\n    o = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n    model = tf.keras.Model(inputs, o)\n    cosine_decayed_lr = tf.keras.experimental.CosineDecay(initial_learning_rate=lr, decay_steps=total_steps)\n    opt = tf.keras.optimizers.SGD(cosine_decayed_lr, momentum=0.9)\n    model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n    return model\n\n\n11.4.5.1 No Pretrain\n\nno_pt_eval_model = get_eval_model(\n    img_size=CIFAR_IMG_SIZE,\n    backbone=get_backbone(CIFAR_IMG_SIZE, DIM),\n    total_steps=TEST_EPOCHS * TEST_STEPS_PER_EPOCH,\n    trainable=True,\n    lr=1e-3,\n)\n\nno_pt_history = no_pt_eval_model.fit(\n    eval_train_ds,\n    batch_size=BATCH_SIZE,\n    epochs=TEST_EPOCHS,\n    steps_per_epoch=TEST_STEPS_PER_EPOCH,\n    validation_data=eval_val_ds,\n    validation_steps=VAL_STEPS_PER_EPOCH,\n)\n\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\nWARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n\n\nEpoch 1/10\n87/87 [==============================] - 21s 128ms/step - loss: 2.0807 - acc: 0.2465 - val_loss: 1.7362 - val_acc: 0.3757\nEpoch 2/10\n87/87 [==============================] - 10s 110ms/step - loss: 1.7352 - acc: 0.3550 - val_loss: 1.5761 - val_acc: 0.4279\nEpoch 3/10\n87/87 [==============================] - 10s 111ms/step - loss: 1.6550 - acc: 0.3953 - val_loss: 1.5116 - val_acc: 0.4521\nEpoch 4/10\n87/87 [==============================] - 10s 111ms/step - loss: 1.5952 - acc: 0.4177 - val_loss: 1.4707 - val_acc: 0.4760\nEpoch 5/10\n87/87 [==============================] - 10s 110ms/step - loss: 1.5585 - acc: 0.4317 - val_loss: 1.4327 - val_acc: 0.4843\nEpoch 6/10\n87/87 [==============================] - 10s 111ms/step - loss: 1.5225 - acc: 0.4487 - val_loss: 1.4095 - val_acc: 0.4852\nEpoch 7/10\n87/87 [==============================] - 10s 111ms/step - loss: 1.5051 - acc: 0.4555 - val_loss: 1.3930 - val_acc: 0.4961\nEpoch 8/10\n87/87 [==============================] - 10s 111ms/step - loss: 1.4894 - acc: 0.4624 - val_loss: 1.3834 - val_acc: 0.4981\nEpoch 9/10\n87/87 [==============================] - 10s 111ms/step - loss: 1.4859 - acc: 0.4631 - val_loss: 1.3785 - val_acc: 0.5000\nEpoch 10/10\n87/87 [==============================] - 10s 111ms/step - loss: 1.4803 - acc: 0.4672 - val_loss: 1.3764 - val_acc: 0.5022\n\n\n\n\n11.4.5.2 Pretrained\n\npt_eval_model = get_eval_model(\n    img_size=CIFAR_IMG_SIZE,\n    backbone=contrastive_model.backbone,\n    total_steps=TEST_EPOCHS * TEST_STEPS_PER_EPOCH,\n    trainable=False,\n    lr=30.0,\n)\npt_eval_model.summary()\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n eval_input (InputLayer)     [(None, 32, 32, 3)]       0         \n                                                                 \n resnet18sim (SimilarityMode  (None, 512)              11182784  \n l)                                                              \n                                                                 \n dense_1 (Dense)             (None, 10)                5130      \n                                                                 \n=================================================================\nTotal params: 11,187,914\nTrainable params: 5,130\nNon-trainable params: 11,182,784\n_________________________________________________________________\n\n\n\npt_history = pt_eval_model.fit(\n    eval_train_ds,\n    batch_size=BATCH_SIZE,\n    epochs=TEST_EPOCHS,\n    steps_per_epoch=TEST_STEPS_PER_EPOCH,\n    validation_data=eval_val_ds,\n    validation_steps=VAL_STEPS_PER_EPOCH,\n)\n\nEpoch 1/10\n87/87 [==============================] - 5s 35ms/step - loss: 32.3816 - acc: 0.1163 - val_loss: 36.2844 - val_acc: 0.1178\nEpoch 2/10\n87/87 [==============================] - 2s 26ms/step - loss: 29.9182 - acc: 0.1659 - val_loss: 10.1967 - val_acc: 0.1580\nEpoch 3/10\n87/87 [==============================] - 2s 27ms/step - loss: 3.0410 - acc: 0.6028 - val_loss: 0.7029 - val_acc: 0.7741\nEpoch 4/10\n87/87 [==============================] - 2s 26ms/step - loss: 0.7650 - acc: 0.7661 - val_loss: 0.6377 - val_acc: 0.7936\nEpoch 5/10\n87/87 [==============================] - 2s 26ms/step - loss: 0.7141 - acc: 0.7745 - val_loss: 0.5892 - val_acc: 0.7896\nEpoch 6/10\n87/87 [==============================] - 2s 27ms/step - loss: 0.6743 - acc: 0.7784 - val_loss: 0.5803 - val_acc: 0.7985\nEpoch 7/10\n87/87 [==============================] - 2s 27ms/step - loss: 0.6640 - acc: 0.7821 - val_loss: 0.5813 - val_acc: 0.7913\nEpoch 8/10\n87/87 [==============================] - 2s 27ms/step - loss: 0.6478 - acc: 0.7872 - val_loss: 0.5675 - val_acc: 0.7979\nEpoch 9/10\n87/87 [==============================] - 2s 27ms/step - loss: 0.6370 - acc: 0.7898 - val_loss: 0.5511 - val_acc: 0.8021\nEpoch 10/10\n87/87 [==============================] - 2s 27ms/step - loss: 0.6375 - acc: 0.7874 - val_loss: 0.5537 - val_acc: 0.7937\n\n\n\n\n11.4.5.3 Comparison\n\nprint(\"no pretrain\", no_pt_eval_model.evaluate(eval_test_ds))\nprint(\"pretrained\", pt_eval_model.evaluate(eval_test_ds))\n\n20/20 [==============================] - 1s 66ms/step - loss: 1.3856 - acc: 0.5010\nno pretrain [1.3856277465820312, 0.5009999871253967]\n20/20 [==============================] - 0s 21ms/step - loss: 0.6054 - acc: 0.7963\npretrained [0.6053985953330994, 0.7962999939918518]"
  },
  {
    "objectID": "11_Transfer_learning.html#zero-shot-image-classification-using-clip",
    "href": "11_Transfer_learning.html#zero-shot-image-classification-using-clip",
    "title": "11  Transfer learning and self-supervised learning",
    "section": "11.5 Zero-shot image classification using CLIP",
    "text": "11.5 Zero-shot image classification using CLIP\nZero-shot image classification is a task that involves classifying images into different categories using a model that was not explicitly trained on data containing labeled examples from those specific categories.\nTraditionally, image classification requires training a model on a specific set of labeled images, and this model learns to “map” certain image features to labels. When there’s a need to use such model for a classification task that introduces a new set of labels, fine-tuning is required to “recalibrate” the model. In contrast, zero-shot or open vocabulary image classification models are typically multi-modal models that have been trained on a large dataset of images and associated descriptions. These models learn aligned vision-language representations that can be used for many downstream tasks including zero-shot image classification.\nThis is a more flexible approach to image classification that allows models to generalize to new and unseen categories without the need for additional training data and enables users to query images with free-form text descriptions of their target objects .\n\n11.5.1 Zero-shot image classification pipeline\nStart by loading the model and associated processor from a checkpoint on the Hugging Face Hub. Here we’ll use the checkpoint:\n\ncheckpoint = \"openai/clip-vit-large-patch14\"\n\nmodel = TFAutoModelForZeroShotImageClassification.from_pretrained(checkpoint)\nprocessor = AutoProcessor.from_pretrained(checkpoint)\n\n\n\n\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n\n\n\n\n\nAll model checkpoint layers were used when initializing TFCLIPModel.\n\nAll the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-large-patch14.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n\n\n\n\n\n\n\n\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSay, if we want to classifiy the following image:\n\nurl = \"https://unsplash.com/photos/xBRQfR2bqNI/download?ixid=MnwxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjc4Mzg4ODEx&force=true&w=640\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nimage\n\n\n\n\nUse the processor to prepare the inputs for the model. The processor combines an image processor that prepares the image for the model by resizing and normalizing it, and a tokenizer that takes care of the text inputs.\n\ncandidate_labels = [\"tree\", \"car\", \"bike\", \"cat\"]\ninputs = processor(images=image, text=candidate_labels, return_tensors=\"tf\", padding=True)\n\nPass the inputs through the model, and post-process the results:\n\noutputs = model(**inputs)\n\n# Assuming that 'outputs' is a dictionary and 'logits_per_image' is a key that returns a batch of logits\nlogits = outputs['logits_per_image'][0]\n\n# Compute softmax probabilities\nprobs = tf.nn.softmax(logits, axis=-1).numpy()\n\n# Assuming that 'candidate_labels' is a list of labels\nresult = [\n    {\"score\": float(score), \"label\": candidate_label}\n    for score, candidate_label in sorted(zip(probs, candidate_labels), key=lambda x: -x[0])\n]\n\nresult\n\n[{'score': 0.998578667640686, 'label': 'car'},\n {'score': 0.0010499537456780672, 'label': 'bike'},\n {'score': 0.00034005637280642986, 'label': 'tree'},\n {'score': 3.1229159503709525e-05, 'label': 'cat'}]\n\n\n\n\n11.5.2 Fintune the model\nSee here for more information."
  },
  {
    "objectID": "11_Transfer_learning.html#bert",
    "href": "11_Transfer_learning.html#bert",
    "title": "11  Transfer learning and self-supervised learning",
    "section": "11.6 BERT",
    "text": "11.6 BERT\n\n11.6.1 Try out BERT\nFeel free to swap out the sentence below for one of your own. However, leave [MASK] in somewhere to allow BERT to predict the missing word:\n\nunmasker = pipeline('fill-mask', model='bert-base-uncased', framework=\"tf\")\nunmasker(\"Artificial Intelligence [MASK] take over the world.\")\n\n\n\n\nAll model checkpoint layers were used when initializing TFBertForMaskedLM.\n\nAll the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n\n\n[{'score': 0.3182392716407776,\n  'token': 2064,\n  'token_str': 'can',\n  'sequence': 'artificial intelligence can take over the world.'},\n {'score': 0.18299587070941925,\n  'token': 2097,\n  'token_str': 'will',\n  'sequence': 'artificial intelligence will take over the world.'},\n {'score': 0.056000713258981705,\n  'token': 2000,\n  'token_str': 'to',\n  'sequence': 'artificial intelligence to take over the world.'},\n {'score': 0.04519489407539368,\n  'token': 2015,\n  'token_str': '##s',\n  'sequence': 'artificial intelligences take over the world.'},\n {'score': 0.04515315219759941,\n  'token': 2052,\n  'token_str': 'would',\n  'sequence': 'artificial intelligence would take over the world.'}]\n\n\nFor chinese, we can use the model from https://github.com/ckiplab/ckip-transformers:\n\nunmasker = pipeline('fill-mask', model='ckiplab/albert-tiny-chinese')\nunmasker(\"中[MASK]大學\")\n\n[{'score': 0.41772598028182983,\n  'token': 1751,\n  'token_str': '國',\n  'sequence': '中 國 大 學'},\n {'score': 0.10670846700668335,\n  'token': 2255,\n  'token_str': '山',\n  'sequence': '中 山 大 學'},\n {'score': 0.07562493532896042,\n  'token': 3152,\n  'token_str': '文',\n  'sequence': '中 文 大 學'},\n {'score': 0.04743417352437973,\n  'token': 1333,\n  'token_str': '原',\n  'sequence': '中 原 大 學'},\n {'score': 0.04172985255718231,\n  'token': 4906,\n  'token_str': '科',\n  'sequence': '中 科 大 學'}]\n\n\n\n\n11.6.2 Fintune Bert (Optional)\nText classification is a common NLP task that assigns a label or class to text. Some of the largest companies run text classification in production for a wide range of practical applications. One of the most popular forms of text classification is sentiment analysis.\nThis guide will show you how to:\n\nFinetune DistilBERT on the IMDb dataset to determine whether a movie review is positive or negative.\nUse your finetuned model for inference.\n\n\n11.6.2.1 Load IMDb dataset\n\nimdb = load_dataset(\"imdb\")\n\n\n\n\n\n\n\n\n\n\nDownloading and preparing dataset imdb/plain_text to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0. Subsequent calls will reuse this data.\n\n\n\n\n\nThen take a look at an example:\n\nimdb[\"test\"][0]\n\n{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n 'label': 0}\n\n\nThere are two fields in this dataset:\n\ntext: the movie review text.\nlabel: a value that is either 0 for a negative review or 1 for a positive review.\n\n\n\n11.6.2.2 Preprocess\nThe next step is to load a DistilBERT tokenizer to preprocess the text field:\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a preprocessing function to tokenize text and truncate sequences to be no longer than DistilBERT’s maximum input length:\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)\n\nTo apply the preprocessing function over the entire dataset, use Datasets map function. You can speed up map by setting batched=True to process multiple elements of the dataset at once:\n\ntokenized_imdb = imdb.map(preprocess_function, batched=True)\n\n\n\n\n\n\n\n\n\n\nNow create a batch of examples using DataCollatorWithPadding. It’s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n\n\n\n11.6.2.3 Evaluate\nIncluding a metric during training is often helpful for evaluating your model’s performance. You can quickly load a evaluation method with the Evaluate library. For this task, load the accuracy metric (see the Evaluate quick tour to learn more about how to load and compute a metric). Then create a function that passes your predictions and labels to compute to calculate the accuracy:\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)\n\n\n\n\n\n\n11.6.2.4 Train\nBefore you start training your model, create a map of the expected ids to their labels with id2label and label2id:\n\nid2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\nlabel2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n\nIf you aren’t familiar with finetuning a model with Keras, take a look at the basic tutorial here! To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n\nbatch_size = 16\nnum_epochs = 3\nbatches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\ntotal_train_steps = int(batches_per_epoch * num_epochs)\noptimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n\nThen you can load DistilBERT with TFAutoModelForSequenceClassification along with the number of expected labels, and the label mappings:\n\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n)\n\n\n\n\nSome layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nConvert your datasets to the tf.data.Dataset format with prepare_tf_dataset():\n\ntf_train_set = model.prepare_tf_dataset(\n    tokenized_imdb[\"train\"],\n    shuffle=True,\n    batch_size=16,\n    collate_fn=data_collator,\n)\n\ntf_validation_set = model.prepare_tf_dataset(\n    tokenized_imdb[\"test\"],\n    shuffle=False,\n    batch_size=16,\n    collate_fn=data_collator,\n)\n\nmodel.compile(optimizer=optimizer)\n\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nNo loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n\n\nThe last two things to setup before you start training is to compute the accuracy from the predictions, and provide a way to push your model to the Hub. Both are done by using Keras callbacks. Pass your compute_metrics function to KerasMetricCallback.\n\nmetric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n\ncallbacks = [metric_callback]\n\n\nmodel.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n\nEpoch 1/3\n1562/1562 [==============================] - 427s 256ms/step - loss: 0.2530 - val_loss: 0.1836 - accuracy: 0.9288\nEpoch 2/3\n1562/1562 [==============================] - 305s 195ms/step - loss: 0.1344 - val_loss: 0.1844 - accuracy: 0.9325\nEpoch 3/3\n1562/1562 [==============================] - 297s 190ms/step - loss: 0.0657 - val_loss: 0.2097 - accuracy: 0.9324\n\n\n&lt;keras.callbacks.History at 0x7f01f00cb2e0&gt;\n\n\n\n\n11.6.2.5 Inference\nGreat, now that you’ve finetuned a model, you can use it for inference! Grab some text you’d like to run inference on:\n\ntext = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n\nGet the class with the highest probability, and use the model’s id2label mapping to convert it to a text label:\n\ninputs = tokenizer(text, return_tensors=\"tf\")\nlogits = model(**inputs).logits\npredicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\nmodel.config.id2label[predicted_class_id]\n\n'POSITIVE'"
  },
  {
    "objectID": "11_Transfer_learning.html#gpt",
    "href": "11_Transfer_learning.html#gpt",
    "title": "11  Transfer learning and self-supervised learning",
    "section": "11.7 GPT",
    "text": "11.7 GPT\n\n11.7.1 Generate text\nTo see how greedy search works, let’s start by loading the 1.5-billion-parameter version of GPT-2 with a language modeling head. we’ll use “Transformers are the” as the input prompt and run the decoding for eight timesteps. At each timestep, we pick out the model’s logits for the last token in the prompt and wrap them with a softmax to get a probability distribution. We then pick the next token with the highest probability, add it to the input sequence, and run the process again. The following code does the job, and also stores the five most probable tokens at each timestep so we can visualize the alternatives:\n\n# Select device\ndevice = \"GPU:0\" if tf.config.list_physical_devices('GPU') else \"CPU\"\n\n# Select model and tokenizer\nmodel_name = \"gpt2-xl\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = TFAutoModelForCausalLM.from_pretrained(model_name)\n\n# Set device for the model\nwith tf.device(device):\n    input_txt = \"Transformers are the\"\n    input_ids = tokenizer(input_txt, return_tensors=\"tf\")[\"input_ids\"]\n    iterations = []\n    n_steps = 8\n    choices_per_step = 5\n\n    for _ in range(n_steps):\n        iteration = dict()\n        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n        output = model(input_ids=input_ids)\n        next_token_logits = output.logits[0, -1, :]\n        next_token_probs = tf.nn.softmax(next_token_logits, axis=-1)\n        sorted_ids = tf.argsort(next_token_probs, direction='DESCENDING')\n        \n        for choice_idx in range(choices_per_step):\n            token_id = sorted_ids[choice_idx]\n            token_prob = next_token_probs[token_id].numpy()\n            token_choice = (\n                f\"{tokenizer.decode([token_id.numpy()])} ({100 * token_prob:.2f}%)\"\n            )\n            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n        \n        input_ids = tf.concat([input_ids, tf.reshape(sorted_ids[0], (1, 1))], axis=-1)\n        iterations.append(iteration)\n\npd.DataFrame(iterations)\n\n\n\n\nAll model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n\nAll the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-xl.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n\n\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nInput\nChoice 1\nChoice 2\nChoice 3\nChoice 4\nChoice 5\n\n\n\n\n0\nTransformers are the\nmost (8.54%)\nonly (4.96%)\nbest (4.66%)\nTransformers (4.37%)\nultimate (2.16%)\n\n\n1\nTransformers are the most\npopular (16.78%)\npowerful (5.37%)\ncommon (4.96%)\nfamous (3.71%)\nsuccessful (3.19%)\n\n\n2\nTransformers are the most popular\ntoy (10.62%)\ntoys (7.23%)\nTransformers (6.60%)\nof (5.46%)\nand (3.76%)\n\n\n3\nTransformers are the most popular toy\nline (34.39%)\nin (18.20%)\nof (11.71%)\nbrand (6.09%)\nline (2.69%)\n\n\n4\nTransformers are the most popular toy line\nin (46.27%)\nof (15.10%)\n, (4.94%)\non (4.40%)\never (2.72%)\n\n\n5\nTransformers are the most popular toy line in\nthe (65.99%)\nhistory (12.39%)\nAmerica (6.91%)\nJapan (2.45%)\nNorth (1.40%)\n\n\n6\nTransformers are the most popular toy line in the\nworld (69.24%)\nUnited (4.56%)\nhistory (4.29%)\nUS (4.23%)\nU (2.30%)\n\n\n7\nTransformers are the most popular toy line in ...\n, (39.73%)\n. (30.64%)\nand (9.87%)\nwith (2.32%)\ntoday (1.74%)\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nImplementing greedy search wasn’t too hard, but we’ll want to use the built-in generate() function from Transformers to explore more sophisticated decoding methods. To reproduce our simple example, let’s make sure sampling is switched off (it’s off by default, unless the specific configuration of the model you are loading the checkpoint from states otherwise) and specify the max_new_tokens for the number of newly generated tokens:\n\nn_steps = 50\ninput_ids = tokenizer(input_txt, return_tensors=\"tf\")[\"input_ids\"]\noutput = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\nprint(\"========Generated Text==========\")\nprint(tokenizer.decode(output[0]))\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n========Generated Text==========\nTransformers are the most popular toy line in the world, and the Transformers are the most popular toy line in the world.\n\nThe Transformers are the most popular toy line in the world, and the Transformers are the most popular toy line in the world.\n\n\n\n\nInstead of decoding the token with the highest probability at each step, beam search keeps track of the top-b most probable next tokens:\n\noutput = model.generate(input_ids, max_new_tokens=n_steps, num_beams=5, do_sample=False)\nprint(\"========Generated Text==========\")\nprint(tokenizer.decode(output[0]))\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n========Generated Text==========\nTransformers are the most popular toy line in the world, and they've been around for over 30 years, so it's no surprise that there are a lot of people who love them. But there are also a lot of people who don't like them.\n\n\n\n\nThe simplest sampling method is to randomly sample from the probability distribution of the model’s outputs over the full vocabulary at each timestep. By tuning T we can control the shape of the probability distribution when sampling. To see how we can use temperature to influence the generated text, let’s sample with T=2 by setting the temperature parameter in the generate() function:\n\noutput = model.generate(input_ids, max_new_tokens=n_steps, do_sample=True, temperature=2.0, top_k=0)\nprint(\"========Generated Text==========\")\nprint(tokenizer.decode(output[0]))\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n========Generated Text==========\nTransformers are the planes transplanted 1937Ep11 Discord KeyKnightStreet archive cadm albums NashvilleoorIIInsolseraizoiner yells evidenced Chaos 2010 casing away AccelerarcPg Celtic Scrolls Brotherhood AllMonitor 227 Medic Noiseporter BeatCyptophile xFCPrdenon combined\n\n\nThere’s always a trade-off between coherence (low temperature) and diversity (high temperature) that one has to tune to the use case at hand:\n\noutput = model.generate(input_ids, max_new_tokens=n_steps, do_sample=True, temperature=0.5, top_k=0)\nprint(\"========Generated Text==========\")\nprint(tokenizer.decode(output[0]))\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n========Generated Text==========\nTransformers are the best.\n\nI know. I know. I'm not saying that just because I'm a Transformers fan. I'm saying that because I'm a fan of the best toys. I'm a fan of the best action figures. I'm a\n\n\nThe idea behind top-k sampling is to avoid the low-probability choices by only sampling from the k tokens with the highest probability. This puts a fixed cut on the long tail of the distribution and ensures that we only sample from likely choices\n\noutput = model.generate(input_ids, max_new_tokens=n_steps, do_sample=True, top_k=50)\nprint(\"========Generated Text==========\")\nprint(tokenizer.decode(output[0]))\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n========Generated Text==========\nTransformers are the latest to be affected, with the Energon series in particular facing an imminent extinction at the hands of the G1 Transformer Prime Sentinel Prime.\n\nI'm not much for reading, but I guess you can appreciate the significance of the number\n\n\nWe can also try chinese using the following model:\n\n# casual language model (GPT2)\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\nmodel = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese').to(device) # or other models above\n\n\ninput_txt = \"中山大學是座落於西子灣的一間學校\"\nn_steps = 300\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n\n\noutput = model.generate(input_ids, max_new_tokens=n_steps, num_beams=1, do_sample=True, top_k=50)\nprint(\"========Generated Text==========\")\nprint(tokenizer.decode(output[0]))\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:102 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n\n\n========Generated Text==========\n[CLS] 中 山 大 學 是 座 落 於 西 子 灣 的 一 間 學 校 [SEP] 校 地 為 大 同 裡 的 前 身 為 臺 南 州 立 大 學 。 學 校 是 臺 南 地 區 最 著 名 的 學 術 機 構 之 一 。 臺 南 州 立 大 學 創 立 於 1979 年 ， 是 臺 南 州 立 大 學 的 校 名 ， 當 時 是 臺 南 州 立 大 學 附 屬 立 法 機 關 。 學 生 名 稱 原 名 [UNK] 臺 南 大 學 ， 簡 稱 為 [UNK] 臺 南 師 範 大 學 [UNK] 。 2003 年 成 立 ， 名 稱 亦 改 為 [UNK] 高 雄 師 範 學 院 [UNK] 。 2000 年 10 月 6 日 立 法 院 三 讀 通 過 立 法 院 組 織 法 修 正 案 （ ） 草 案 。 2003 年 9 月 16 日 臺 南 大 學 更 名 ， 並 更 名 為 臺 南 市 立 大 學 。 2001 年 11 月 24 日 原 南 州 醫 學 院 升 格 為 高 雄 市 立 大 學 ； 2002 年 12 月 29 日 校 內 改 制 為 省 立 醫 院 。 2003 年 臺 灣 高 等 法 院 正 副 院 長 改 選 舉 公 民 投 票 於 12 月 25 日 舉 行 結 束 ， 總 統 陳 水 扁 應 將 新 任 董 事 長 與 副 董 事 長 人 選 交 接 確 定 。 2006 年 12 月 29 日 校 務 會 議 通 過 「 南 臺 科 學 工 業 園 區 事 業 廢 棄 物 處 理 特 別 基 金 」 辦 理 管 理 許 可 。 臺"
  },
  {
    "objectID": "11_Transfer_learning.html#references",
    "href": "11_Transfer_learning.html#references",
    "title": "11  Transfer learning and self-supervised learning",
    "section": "11.8 References",
    "text": "11.8 References\n1.https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub\n2.https://github.com/fchollet/deep-learning-with-python-notebooks_\n3.https://github.com/tensorflow/similarity/tree/master\n4.https://huggingface.co/docs/transformers/v4.29.1/en/tasks/sequence_classification\n5.https://huggingface.co/docs/transformers/tasks/zero_shot_image_classification"
  },
  {
    "objectID": "12_Representation_learning.html",
    "href": "12_Representation_learning.html",
    "title": "12  Representation learning",
    "section": "",
    "text": "13 References"
  },
  {
    "objectID": "12_Representation_learning.html#setup",
    "href": "12_Representation_learning.html#setup",
    "title": "12  Representation learning",
    "section": "12.1 Setup",
    "text": "12.1 Setup\n\n%pip install --upgrade keras-cv -qq\n%pip install diffusers -qq\n%pip install transformers ftfy accelerate -qq\n%pip install torchinfo -qq\n%pip install opentsne -qq\n\n\n# Python ≥3.7 is recommended\nimport sys\nassert sys.version_info &gt;= (3, 7)\n\n# Scikit-Learn ≥1.01 is recommended\nfrom packaging import version\nimport sklearn\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\n# Tensorflow ≥2.8.0 is recommended\nimport tensorflow as tf\nassert version.parse(tf.__version__) &gt;= version.parse(\"2.8.0\")\n\n# Difussion model\nimport keras_cv\nimport torch\nfrom torch import autocast\nfrom tqdm.auto import tqdm\nfrom torchinfo import summary\nfrom torchvision import transforms as tfms\nfrom diffusers import StableDiffusionPipeline\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom diffusers import LMSDiscreteScheduler, PNDMScheduler\n\n# Image related\nfrom PIL import Image\nfrom skimage.io import imread, imshow, show\nfrom skimage.color import rgba2rgb\nfrom skimage.transform import resize\n\n# Common imports\nfrom sklearn.decomposition import PCA\nfrom openTSNE import TSNE\nimport requests\nimport shutil\nimport numpy as np\nimport os\nfrom pathlib import Path\nimport time\n\n# To plot pretty figures\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport matplotlib as mpl\nfrom mpl_toolkits.mplot3d import Axes3D\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\ntf.random.set_seed(42)  # extra code – ensures reproducibility on CPU \n\n\nif not tf.config.list_physical_devices('GPU'):\n    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n    if \"google.colab\" in sys.modules:\n        print(\"Go to Runtime &gt; Change runtime and select a GPU hardware \"\n              \"accelerator.\")\n    if \"kaggle_secrets\" in sys.modules:\n        print(\"Go to Settings &gt; Accelerator and select GPU.\")\n\nA couple utility functions to plot grayscale \\(28 \\times 28\\) image:\n\n# Visualize 2d manifold from  encodings using tSNE\n\ndef plot_embeddings_tsne(X_data, y_data, encodings):\n  np.random.seed(42)\n  tsne = TSNE(negative_gradient_method=\"fft\")\n  X_data_2D = tsne.fit(encodings)\n  X_data_2D = (X_data_2D - X_data_2D.min()) / (X_data_2D.max() - X_data_2D.min())\n\n  # adapted from https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html\n  plt.figure(figsize=(10, 8))\n  cmap = plt.cm.tab10\n  plt.scatter(X_data_2D[:, 0], X_data_2D[:, 1], c=y_data, s=10, cmap=cmap)\n  image_positions = np.array([[1., 1.]])\n  for index, position in enumerate(X_data_2D):\n      dist = np.sum((position - image_positions) ** 2, axis=1)\n      if np.min(dist) &gt; 0.02: # if far enough from other images\n          image_positions = np.r_[image_positions, [position]]\n          imagebox = matplotlib.offsetbox.AnnotationBbox(\n              matplotlib.offsetbox.OffsetImage(X_data[index], cmap=\"binary\"),\n              position, bboxprops={\"edgecolor\": cmap(y_data[index]), \"lw\": 2})\n          plt.gca().add_artist(imagebox)\n  plt.axis(\"off\");\n\ndef plot_reconstructions(model, images, n_images=5):\n    reconstructions = np.clip(model.predict(images[:n_images]), 0, 1)\n    fig = plt.figure(figsize=(n_images * 1.5, 3))\n    for image_index in range(n_images):\n        plt.subplot(2, n_images, 1 + image_index)\n        plt.imshow(images[image_index], cmap=\"binary\")\n        plt.axis(\"off\")\n        plt.subplot(2, n_images, 1 + n_images + image_index)\n        plt.imshow(reconstructions[image_index], cmap=\"binary\")\n        plt.axis(\"off\")  \n\ndef plot_multiple_images(images, n_cols=None):\n    n_cols = n_cols or len(images)\n    n_rows = (len(images) - 1) // n_cols + 1\n    if images.shape[-1] == 1:\n        images = images.squeeze(axis=-1)\n    plt.figure(figsize=(n_cols, n_rows))\n    for index, image in enumerate(images):\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(image, cmap=\"binary\")\n        plt.axis(\"off\") \n\ndef plot_images(images):\n    plt.figure(figsize=(20, 20))\n    for i in range(len(images)):\n        ax = plt.subplot(1, len(images), i + 1)\n        plt.imshow(images[i])\n        plt.axis(\"off\")\n\n# To visualize statistics of the hidden units adapted from https://github.com/probml/pyprobml/blob/master/notebooks/book1/20/ae_mnist_tf.ipynb\n\ndef plot_percent_hist(ax, data, bins):\n    counts, _ = np.histogram(data, bins=bins)\n    widths = bins[1:] - bins[:-1]\n    x = bins[:-1] + widths / 2\n    ax.bar(x, counts / len(data), width=widths*0.8)\n    ax.xaxis.set_ticks(bins)\n    ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(\n        lambda y, position: \"{}%\".format(int(np.round(100 * y)))))\n    ax.grid(True)\n\ndef plot_activations_histogram2(encoder, height=1, n_bins=10):\n    X_valid_codings = encoder(X_valid).numpy()\n    activation_means = X_valid_codings.mean(axis=0)\n    mean = activation_means.mean()\n    bins = np.linspace(0, 1, n_bins + 1)\n\n    fig, ax1 = plt.subplots()\n    plot_percent_hist(ax1, X_valid_codings.ravel(), bins)\n    ax1.plot([mean, mean], [0, height], \"k--\", label=\"Overall Mean = {:.2f}\".format(mean))\n    ax1.legend(loc=\"upper center\", fontsize=14)\n    ax1.set_xlabel(\"Activation\")\n    ax1.set_ylabel(\"% Activations\")\n    ax1.axis([0, 1, 0, height])\n    plt.show()\n    \n    fig, ax2 = plt.subplots()\n    plot_percent_hist(ax2, activation_means, bins)\n    ax2.plot([mean, mean], [0, height], \"k--\", label=\"Overall Mean = {:.2f}\".format(mean))\n    ax2.set_xlabel(\"Neuron Mean Activation\")\n    ax2.set_ylabel(\"% Neurons\")\n    ax2.axis([0, 1, 0, height])\n    plt.show()\n\ndef plot_activations_heatmap(encoder, N=100):\n    X = encoder(X_valid).numpy()\n    plt.figure(figsize=(10,5))\n    plt.imshow(X[:N,:])\n    plt.colorbar() \n\n# Set device\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndef pil_to_latent(input_im):\n    # Single image -&gt; single latent in a batch (so size 1, 4, 64, 64)\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n    return 0.18215 * latent.latent_dist.sample()\n\ndef latents_to_pil(latents):\n    # bath of latents -&gt; list of images\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\ndef download_from_pokemondb(input_url, out_file):\n  r = requests.get(input_url, stream=True)\n  if r.status_code == 200:\n    with open(out_file, 'wb') as f:\n        r.raw.decode_content = True\n        shutil.copyfileobj(r.raw, f)"
  },
  {
    "objectID": "12_Representation_learning.html#autoencoder",
    "href": "12_Representation_learning.html#autoencoder",
    "title": "12  Representation learning",
    "section": "12.2 Autoencoder",
    "text": "12.2 Autoencoder\n\n12.2.1 PCA with a linear Autoencoder\nIf the autoencoder uses only linear activations and the cost function is the mean squared error (MSE), then it ends up performing Principal Component Analysis. The following code builds a simple linear autoencoder to perform PCA on a 3D dataset, projecting it to 2D\n\nnp.random.seed(42)\n\ndef generate_3d_data(m, w1=0.1, w2=0.3, noise=0.1):\n    angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n    data = np.empty((m, 3))\n    data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n    data[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n    data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * np.random.randn(m)\n    return data\n\nX_train = generate_3d_data(60)\nX_train = X_train - X_train.mean(axis=0, keepdims=0)\n\n\nfig = plt.figure(figsize=(6, 3.8))\nax = fig.add_subplot(111, projection='3d')\n\nax.plot(X_train[:, 0], X_train[:, 1], X_train[:, 2], \"k.\")\nax.set_xlabel(\"$x_1$\", fontsize=18, labelpad=10)\nax.set_ylabel(\"$x_2$\", fontsize=18, labelpad=10)\nax.set_zlabel(\"$x_3$\", fontsize=18, labelpad=10);\n\nText(0.5, 0, '$x_3$')\n\n\n\n\n\nIf the autoencoder uses only linear activations and the cost function is the mean squared error (MSE), then it ends up performing principal component analysis:\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n\nencoder = tf.keras.Sequential([tf.keras.layers.Dense(2)])\ndecoder = tf.keras.Sequential([tf.keras.layers.Dense(3)])\nautoencoder = tf.keras.Sequential([encoder, decoder])\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=1)\nautoencoder.compile(loss=\"mse\", optimizer=optimizer)\n\n\nhistory = autoencoder.fit(X_train, X_train, epochs=500, verbose=False)\ncodings = encoder.predict(X_train)\n\n2/2 [==============================] - 0s 6ms/step\n\n\n\nfig = plt.figure(figsize=(4,3))\nplt.plot(codings[:,0], codings[:, 1], \"b.\")\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\nplt.grid(True);\n\n\n\n\nAs you can see, the autoencoder found the best 2D plane to project the data onto, preserving as much variance in the data as it could (just like PCA). Note that we exchage the axis in this figure, since autoencoder will not order the data according to the eigenvalues.\n\npca = PCA(n_components=2)\ncodings2 = pca.fit_transform(X_train)\n\n\nfig = plt.figure(figsize=(4,3))\nplt.plot(codings2[:,0], -codings2[:, 1], \"b.\")\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.ylabel(\"$z_2$\", fontsize=18)\nplt.grid(True);\n\n\n\n\n\n\n12.2.2 Stacked Autoencoders\nLet’s load the fashion MNIST dataset, scale it, and split it into a training set, a validation set, and a test set:\n\nfashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\nX_train_full = X_train_full.astype(np.float32) / 255\nX_test = X_test.astype(np.float32) / 255\nX_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 [==============================] - 1s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 [==============================] - 1s 0us/step\n\n\n\n12.2.2.1 MLP Layer\nYou can implement a stacked autoencoder very much like a regular deep MLP. The following code builds a stacked autoencoder for Fashion MNIST. we split the autoencoder model into two submodels: the encoder and the decoder.\n\nThe encoder takes 28 × 28–pixel grayscale images, flattens them so that each image is represented as a vector of size 784, then processes these vectors through two Dense layers of diminishing sizes (100 units then 30 units), both using the ReLU activation function. For each input image, the encoder outputs a vector of size 30.\nThe decoder takes codings of size 30 (output by the encoder) and processes them through two Dense layers of increasing sizes (100 units then 784 units), and it reshapes the final vectors into 28 × 28 arrays so the decoder’s outputs have the same shape as the encoder’s inputs.\nWhen compiling the stacked autoencoder, we use MSE loss and Nadam optimization.\nWe train the model using X_train as both the inputs and the targets (and similarly, we use X_valid as both the validation inputs and targets)\n\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n\nstacked_encoder = tf.keras.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(30, activation=\"relu\"),\n])\nstacked_decoder = tf.keras.Sequential([\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(28 * 28),\n    tf.keras.layers.Reshape([28, 28])\n])\nstacked_ae = tf.keras.Sequential([stacked_encoder, stacked_decoder])\n\nstacked_ae.compile(loss=\"mse\", optimizer=\"nadam\")                   \nhistory = stacked_ae.fit(X_train, X_train, epochs=20,\n                         validation_data=(X_valid, X_valid))\n\nEpoch 1/20\n1719/1719 [==============================] - 22s 9ms/step - loss: 0.0241 - val_loss: 0.0187\nEpoch 2/20\n1719/1719 [==============================] - 15s 9ms/step - loss: 0.0173 - val_loss: 0.0166\nEpoch 3/20\n1719/1719 [==============================] - 14s 8ms/step - loss: 0.0161 - val_loss: 0.0159\nEpoch 4/20\n1719/1719 [==============================] - 14s 8ms/step - loss: 0.0156 - val_loss: 0.0155\nEpoch 5/20\n1719/1719 [==============================] - 16s 9ms/step - loss: 0.0152 - val_loss: 0.0152\nEpoch 6/20\n1719/1719 [==============================] - 15s 9ms/step - loss: 0.0149 - val_loss: 0.0148\nEpoch 7/20\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.0146 - val_loss: 0.0147\nEpoch 8/20\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.0144 - val_loss: 0.0146\nEpoch 9/20\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0143 - val_loss: 0.0144\nEpoch 10/20\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0142 - val_loss: 0.0143\nEpoch 11/20\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.0141 - val_loss: 0.0142\nEpoch 12/20\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.0140 - val_loss: 0.0142\nEpoch 13/20\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.0139 - val_loss: 0.0141\nEpoch 14/20\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0139 - val_loss: 0.0140\nEpoch 15/20\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0138 - val_loss: 0.0139\nEpoch 16/20\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.0138 - val_loss: 0.0141\nEpoch 17/20\n1719/1719 [==============================] - 11s 6ms/step - loss: 0.0137 - val_loss: 0.0139\nEpoch 18/20\n1719/1719 [==============================] - 17s 10ms/step - loss: 0.0137 - val_loss: 0.0139\nEpoch 19/20\n1719/1719 [==============================] - 12s 7ms/step - loss: 0.0136 - val_loss: 0.0138\nEpoch 20/20\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0136 - val_loss: 0.0138\n\n\n\n\n12.2.2.2 Visualizing the Reconstructions\nOne way to ensure that an autoencoder is properly trained is to compare the inputs and the outputs: the differences should not be too significant. Let’s plot a few images from the validation set, as well as their reconstructions:\n\nplot_reconstructions(stacked_ae, X_valid);\n\n1/1 [==============================] - 0s 134ms/step\n\n\n\n\n\nThe reconstructions are recognizable, but a bit too lossy. We may need to train the model for longer, or make the encoder and decoder deeper, or make the codings larger. But if we make the network too powerful, it will manage to make perfect reconstructions without having learned any useful patterns in the data. For now, let’s go with this model.\n\n\n12.2.2.3 Visualizing Fashion MNIST\nNow that we have trained a stacked autoencoder, we can use it to reduce the dataset’s dimensionality. For visualization, this does not give great results compared to other dimensionality reduction algorithms, but one big advantage of autoencoders is that they can handle large datasets, with many instances and many features. So one strategy is to use an autoencoder to reduce the dimensionality down to a reasonable level, then use another dimensionality reduction algorithm for visualization. Let’s use this strategy to visualize Fashion MNIST. First, we use the encoder from our stacked autoencoder to reduce the dimensionality down to 30, then we use Scikit-Learn’s implementation of the t-SNE algorithm to reduce the dimensionality down to 2 for visualization:\n\nZ = stacked_encoder.predict(X_valid)\nprint(Z.shape)\nplot_embeddings_tsne(X_valid, y_valid, Z)\nplt.tight_layout();\n\n157/157 [==============================] - 0s 2ms/step\n(5000, 30)\n\n\n\n\n\nThe t-SNE algorithm identified several clusters which match the classes reasonably well (each class is represented with a different color).\n\n\n12.2.2.4 Using Convolutional Layers Instead of Dense Layers\nIf you are dealing with images, then the autoencoders we have seen so far will not work well (unless the images are very small): convolutional neural networks are far better suited than dense networks to work with images. So if you want to build an autoencoder for images, you will need to build a convolutional autoencoder. The encoder is a regular CNN composed of convolutional layers and pooling layers. It typically reduces the spatial dimensionality of the inputs (i.e., height and width) while increasing the depth (i.e., the number of feature maps). The decoder must do the reverse (upscale the image and reduce its depth back to the original dimensions), and for this you can use transpose convolutional layers (alternatively, you could combine upsampling layers with convolutional layers).\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n\nconv_encoder = tf.keras.models.Sequential([\n    tf.keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]),\n    tf.keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"relu\"),\n    tf.keras.layers.MaxPool2D(pool_size=2),\n    tf.keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"relu\"),\n    tf.keras.layers.MaxPool2D(pool_size=2),\n    tf.keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"relu\"),\n    tf.keras.layers.MaxPool2D(pool_size=2)\n])\nconv_decoder = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"VALID\", activation=\"relu\",\n                                 input_shape=[3, 3, 64]),\n    tf.keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"SAME\", activation=\"relu\"),\n    tf.keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"SAME\", activation=\"relu\"),\n    tf.keras.layers.Reshape([28, 28])\n])\n\nconv_ae = tf.keras.Sequential([conv_encoder, conv_decoder])\n\nconv_ae.compile(loss=\"mse\", optimizer=\"nadam\")\nhistory = conv_ae.fit(X_train, X_train, epochs=10,\n                      validation_data=(X_valid, X_valid))\n\nEpoch 1/10\n1719/1719 [==============================] - 24s 8ms/step - loss: 0.0179 - val_loss: 0.0117\nEpoch 2/10\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.0102 - val_loss: 0.0095\nEpoch 3/10\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.0088 - val_loss: 0.0081\nEpoch 4/10\n1719/1719 [==============================] - 14s 8ms/step - loss: 0.0079 - val_loss: 0.0076\nEpoch 5/10\n1719/1719 [==============================] - 14s 8ms/step - loss: 0.0073 - val_loss: 0.0069\nEpoch 6/10\n1719/1719 [==============================] - 18s 10ms/step - loss: 0.0068 - val_loss: 0.0065\nEpoch 7/10\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.0065 - val_loss: 0.0062\nEpoch 8/10\n1719/1719 [==============================] - 14s 8ms/step - loss: 0.0062 - val_loss: 0.0069\nEpoch 9/10\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.0060 - val_loss: 0.0060\nEpoch 10/10\n1719/1719 [==============================] - 13s 8ms/step - loss: 0.0058 - val_loss: 0.0058\n\n\n\nplot_reconstructions(conv_ae, X_valid)\nplt.show()\n\n1/1 [==============================] - 0s 370ms/step\n\n\n\n\n\n\nZ = conv_encoder.predict(X_valid)\nN = Z.shape[0]\nZZ = np.reshape(Z, (N,-1))\nplot_embeddings_tsne(X_valid, y_valid, ZZ)\nplt.tight_layout();\n\n157/157 [==============================] - 1s 3ms/step\n\n\n\n\n\n\n\n\n12.2.3 Stacked denoising Autoencoder\nAnother way to force the autoencoder to learn useful features is to add noise to its inputs, training it to recover the original, noise-free inputs. The noise can be pure Gaussian noise added to the inputs, or it can be randomly switched-off inputs, just like in dropout (Bernoulli)\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n\n# If you want, you can try replacing the Dropout layer with tf.keras.layers.GaussianNoise(0.2)\ndropout_encoder = tf.keras.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(30, activation=\"relu\")\n])\ndropout_decoder = tf.keras.Sequential([\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(28 * 28),\n    tf.keras.layers.Reshape([28, 28])\n])\ndropout_ae = tf.keras.Sequential([dropout_encoder, dropout_decoder])\n\n# extra code – compiles and fits the model\ndropout_ae.compile(loss=\"mse\", optimizer=\"nadam\")\nhistory = dropout_ae.fit(X_train, X_train, epochs=10,\n                         validation_data=(X_valid, X_valid))\n\nEpoch 1/10\n1719/1719 [==============================] - 12s 6ms/step - loss: 0.0287 - val_loss: 0.0214\nEpoch 2/10\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.0220 - val_loss: 0.0196\nEpoch 3/10\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.0208 - val_loss: 0.0187\nEpoch 4/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0201 - val_loss: 0.0183\nEpoch 5/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0196 - val_loss: 0.0179\nEpoch 6/10\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.0194 - val_loss: 0.0178\nEpoch 7/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0191 - val_loss: 0.0176\nEpoch 8/10\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.0190 - val_loss: 0.0174\nEpoch 9/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0188 - val_loss: 0.0173\nEpoch 10/10\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.0187 - val_loss: 0.0170\n\n\n\ndropout = tf.keras.layers.Dropout(0.5)\nplot_reconstructions(dropout_ae, dropout(X_valid, training=True))\nplt.show()\n\n1/1 [==============================] - 0s 25ms/step\n\n\n\n\n\n\n\n12.2.4 Sparse Autoencoder\nAnother kind of constraint that often leads to good feature extraction is sparsity: by adding an appropriate term to the cost function, the autoencoder is pushed to reduce the number of active neurons in the coding layer. For example, it may be pushed to have on average only 5% significantly active neurons in the coding layer. This forces the autoencoder to represent each input as a combination of a small number of activations. As a result, each neuron in the coding layer typically ends up representing a useful feature.\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n# Normal autoencoder\n\nstacked_encoder = tf.keras.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(300, activation=\"relu\"),\n])\nstacked_decoder = tf.keras.Sequential([\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(28 * 28),\n    tf.keras.layers.Reshape([28, 28])\n])\nstacked_ae = tf.keras.Sequential([stacked_encoder, stacked_decoder])\n\nstacked_ae.compile(loss=\"mse\", optimizer=\"nadam\")                   \nhistory = stacked_ae.fit(X_train, X_train, epochs=10,\n                         validation_data=(X_valid, X_valid))\n\nEpoch 1/10\n1719/1719 [==============================] - 12s 6ms/step - loss: 0.0207 - val_loss: 0.0142\nEpoch 2/10\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.0128 - val_loss: 0.0121\nEpoch 3/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0114 - val_loss: 0.0117\nEpoch 4/10\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.0107 - val_loss: 0.0109\nEpoch 5/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0104 - val_loss: 0.0101\nEpoch 6/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0103 - val_loss: 0.0102\nEpoch 7/10\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.0103 - val_loss: 0.0102\nEpoch 8/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0102 - val_loss: 0.0104\nEpoch 9/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0102 - val_loss: 0.0102\nEpoch 10/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0101 - val_loss: 0.0104\n\n\n\nplot_reconstructions(stacked_ae, X_valid)\nplot_activations_heatmap(stacked_encoder)\nplot_activations_histogram2(stacked_encoder, height=0.35);\n\n1/1 [==============================] - 0s 287ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThree figures show neuron activity (in the bottleneck layer) for an autoencoder applied to Fashion MNIST. Heatmap of 300 neuron activations (columns) across 100 examples (rows). Histogram of activation levels derived from this heatmap. Histogram of the mean activation per neuron, averaged over all examples in the validation set. You can see that some neurons fire almost all the time (right side of the histogram).\nA simple approach is to use the sigmoid activation function in the coding layer (to constrain the codings to values between 0 and 1), use a large coding layer (e.g., with 300 units), and add some L1 regularization to the coding layer’s activations.\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n\nsparse_l1_encoder = tf.keras.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(300, activation=\"sigmoid\"),\n    tf.keras.layers.ActivityRegularization(l1=1e-4)\n])\nsparse_l1_decoder = tf.keras.Sequential([\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(28 * 28),\n    tf.keras.layers.Reshape([28, 28])\n])\nsparse_l1_ae = tf.keras.Sequential([sparse_l1_encoder, sparse_l1_decoder])\n\n# extra code – compiles and fits the model\nsparse_l1_ae.compile(loss=\"mse\", optimizer=\"nadam\")\nhistory = sparse_l1_ae.fit(X_train, X_train, epochs=10,\n                           validation_data=(X_valid, X_valid))\n\nEpoch 1/10\n1719/1719 [==============================] - 17s 8ms/step - loss: 0.0275 - val_loss: 0.0191\nEpoch 2/10\n1719/1719 [==============================] - 12s 7ms/step - loss: 0.0170 - val_loss: 0.0160\nEpoch 3/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0148 - val_loss: 0.0146\nEpoch 4/10\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.0137 - val_loss: 0.0133\nEpoch 5/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0129 - val_loss: 0.0128\nEpoch 6/10\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.0124 - val_loss: 0.0122\nEpoch 7/10\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.0119 - val_loss: 0.0118\nEpoch 8/10\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.0116 - val_loss: 0.0115\nEpoch 9/10\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.0113 - val_loss: 0.0113\nEpoch 10/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.0111 - val_loss: 0.0110\n\n\nThis ActivityRegularization layer just returns its inputs, but as a side effect it adds a training loss equal to the sum of the absolute values of its inputs. This only affects training. Equivalently, you could remove the ActivityRegularization layer and set activity_regularizer=tf.keras.regularizers.l1(1e-4) in the previous layer. This penalty will encourage the neural network to produce codings close to 0, but since it will also be penalized if it does not reconstruct the inputs correctly, it will have to output at least a few nonzero values. Using the L1 norm rather than the L2 norm will push the neural network to preserve the most important codings while eliminating the ones that are not needed for the input image (rather than just reducing all codings).\n\nplot_reconstructions(sparse_l1_ae, X_valid)\nplot_activations_heatmap(sparse_l1_encoder)\nplot_activations_histogram2(sparse_l1_encoder);\n\n1/1 [==============================] - 0s 364ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother approach, which often yields better results, is to measure the actual sparsity of the coding layer at each training iteration, and penalize the model when the measured sparsity differs from a target sparsity. We do so by computing the average activation of each neuron in the coding layer, over the whole training batch. Once we have the mean activation per neuron, we want to penalize the neurons that are too active, or not active enough, by adding a sparsity loss to the cost function.\nOnce we have computed the sparsity loss for each neuron in the coding layer, we sum up these losses and add the result to the cost function. In order to control the relative importance of the sparsity loss and the reconstruction loss, we can multiply the sparsity loss by a sparsity weight hyperparameter. If this weight is too high, the model will stick closely to the target sparsity, but it may not reconstruct the inputs properly, making the model useless. Conversely, if it is too low, the model will mostly ignore the sparsity objective and will not learn any interesting features.\nWe now have all we need to implement a sparse autoencoder based on the KL divergence. First, let’s create a custom regularizer to apply KL divergence regularization:\n\nkl_divergence = tf.keras.losses.kullback_leibler_divergence\n\nclass KLDivergenceRegularizer(tf.keras.regularizers.Regularizer):\n    def __init__(self, weight, target):\n        self.weight = weight\n        self.target = target\n\n    def __call__(self, inputs):\n        mean_activities = tf.reduce_mean(inputs, axis=0)\n        return self.weight * (\n            kl_divergence(self.target, mean_activities) +\n            kl_divergence(1. - self.target, 1. - mean_activities))\n\nNow let’s use this regularizer to push the model to have about 10% sparsity in the coding layer:\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n\nkld_reg = KLDivergenceRegularizer(weight=5e-3, target=0.1)\nsparse_kl_encoder = tf.keras.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(300, activation=\"sigmoid\",\n                          activity_regularizer=kld_reg)\n])\nsparse_kl_decoder = tf.keras.Sequential([\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(28 * 28),\n    tf.keras.layers.Reshape([28, 28])\n])\nsparse_kl_ae = tf.keras.Sequential([sparse_kl_encoder, sparse_kl_decoder])\n\n# extra code – compiles and fits the model\nsparse_kl_ae.compile(loss=\"mse\", optimizer=\"nadam\")\nhistory = sparse_kl_ae.fit(X_train, X_train, epochs=10,\n                           validation_data=(X_valid, X_valid))\n\nEpoch 1/10\n1719/1719 [==============================] - 20s 10ms/step - loss: 0.0258 - val_loss: 0.0172\nEpoch 2/10\n1719/1719 [==============================] - 14s 8ms/step - loss: 0.0149 - val_loss: 0.0134\nEpoch 3/10\n1719/1719 [==============================] - 12s 7ms/step - loss: 0.0125 - val_loss: 0.0143\nEpoch 4/10\n1719/1719 [==============================] - 11s 7ms/step - loss: 0.0112 - val_loss: 0.0108\nEpoch 5/10\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.0105 - val_loss: 0.0102\nEpoch 6/10\n1719/1719 [==============================] - 11s 6ms/step - loss: 0.0100 - val_loss: 0.0098\nEpoch 7/10\n1719/1719 [==============================] - 11s 7ms/step - loss: 0.0097 - val_loss: 0.0096\nEpoch 8/10\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.0095 - val_loss: 0.0099\nEpoch 9/10\n1719/1719 [==============================] - 11s 7ms/step - loss: 0.0094 - val_loss: 0.0094\nEpoch 10/10\n1719/1719 [==============================] - 11s 6ms/step - loss: 0.0092 - val_loss: 0.0092\n\n\n\nplot_reconstructions(sparse_kl_ae, X_valid)\nplot_activations_heatmap(sparse_kl_encoder)\nplot_activations_histogram2(sparse_kl_encoder);\n\nWARNING:tensorflow:5 out of the last 162 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7f618ee73640&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n1/1 [==============================] - 0s 110ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter training this sparse autoencoder on Fashion MNIST, the activations of the neurons in the coding layer are mostly close to 0 (about 70% of all activations are lower than 0.1), and all neurons have a mean activation around 0.1 (about 80% of all neurons have a mean activation between 0.1 and 0.2)"
  },
  {
    "objectID": "12_Representation_learning.html#variational-autoencoder",
    "href": "12_Representation_learning.html#variational-autoencoder",
    "title": "12  Representation learning",
    "section": "12.3 Variational Autoencoder",
    "text": "12.3 Variational Autoencoder\nWe’re going to be implementing a VAE that can generate MNIST digits. It’s going to have three parts:\n\nAn encoder network that turns a real image into a mean and a variance in the latent space\nA sampling layer that takes such a mean and variance, and uses them to sample a random point from the latent space\nA decoder network that turns points from the latent space back into images\n\n\n12.3.1 Latent-space-sampling layer\nFirst, we will need a custom layer to sample the codings:\n\nclass Sampling(tf.keras.layers.Layer):\n    def call(self, inputs):\n        mean, log_var = inputs\n        return tf.random.normal(tf.shape(log_var)) * tf.exp(log_var / 2) + mean \n\n\n\n12.3.2 VAE implementation\n\n12.3.2.1 VAE encoder\nNext, we can create the encoder, using the functional API because the model is not entirely sequential:\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n\ncodings_size = 10\n\ninputs = tf.keras.layers.Input(shape=[28, 28])\nZ = tf.keras.layers.Flatten()(inputs)\nZ = tf.keras.layers.Dense(150, activation=\"relu\")(Z)\nZ = tf.keras.layers.Dense(100, activation=\"relu\")(Z)\ncodings_mean = tf.keras.layers.Dense(codings_size)(Z)  # μ\ncodings_log_var = tf.keras.layers.Dense(codings_size)(Z)  # γ\ncodings = Sampling()([codings_mean, codings_log_var])\nvariational_encoder = tf.keras.Model(\n    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\n\nvariational_encoder.summary()\n\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 28, 28)]     0           []                               \n                                                                                                  \n flatten (Flatten)              (None, 784)          0           ['input_1[0][0]']                \n                                                                                                  \n dense (Dense)                  (None, 150)          117750      ['flatten[0][0]']                \n                                                                                                  \n dense_1 (Dense)                (None, 100)          15100       ['dense[0][0]']                  \n                                                                                                  \n dense_2 (Dense)                (None, 10)           1010        ['dense_1[0][0]']                \n                                                                                                  \n dense_3 (Dense)                (None, 10)           1010        ['dense_1[0][0]']                \n                                                                                                  \n sampling (Sampling)            (None, 10)           0           ['dense_2[0][0]',                \n                                                                  'dense_3[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 134,870\nTrainable params: 134,870\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\nNote that the Dense layers that output codings_mean (\\(\\mu\\)) and codings_log_var (\\(\\gamma\\)) have the same inputs (i.e., the outputs of the second Dense layer). We then pass both codings_mean and codings_log_var to the Sampling layer. Finally, the variational_encoder model has three outputs. Only the codings are required, but we add codings_mean and codings_log_var as well, in case we want to inspect their values.\n\n\n12.3.2.2 VAE decoder\nNow let’s build the decoder:\n\ndecoder_inputs = tf.keras.layers.Input(shape=[codings_size])\nx = tf.keras.layers.Dense(100, activation=\"relu\")(decoder_inputs)\nx = tf.keras.layers.Dense(150, activation=\"relu\")(x)\nx = tf.keras.layers.Dense(28 * 28)(x)\noutputs = tf.keras.layers.Reshape([28, 28])(x)\nvariational_decoder = tf.keras.Model(inputs=[decoder_inputs], outputs=[outputs])\nvariational_decoder.summary()\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 10)]              0         \n                                                                 \n dense_4 (Dense)             (None, 100)               1100      \n                                                                 \n dense_5 (Dense)             (None, 150)               15150     \n                                                                 \n dense_6 (Dense)             (None, 784)               118384    \n                                                                 \n reshape (Reshape)           (None, 28, 28)            0         \n                                                                 \n=================================================================\nTotal params: 134,634\nTrainable params: 134,634\nNon-trainable params: 0\n_________________________________________________________________\n\n\nFor this decoder, we could have used the sequential API instead of the functional API, since it is really just a simple stack of layers, virtually identical to many of the decoders we have built so far.\n\n\n12.3.2.3 VAE model\nFinally, let’s build the variational autoencoder model:\n\n_, _, codings = variational_encoder(inputs)\nreconstructions = variational_decoder(codings)\nvariational_ae = tf.keras.Model(inputs=[inputs], outputs=[reconstructions])\n\nWe ignore the first two outputs of the encoder (we only want to feed the codings to the decoder). Lastly, we must add the latent loss and the reconstruction loss:\n\nlatent_loss = -0.5 * tf.reduce_sum( 1 + codings_log_var - tf.exp(codings_log_var) - tf.square(codings_mean), axis=-1)\n# Indeed, the variational autoencoder’s reconstruction loss  is  supposed  to  be  \n# the  sum  of  the  pixel  reconstruction  errors,  but  when  Keras\n# computes the  \"mse\"  loss it computes the mean over all 784 pixels, rather than the\n# sum. So, the reconstruction loss is 784 times smaller than we need it to be.\nvariational_ae.add_loss(tf.reduce_mean(latent_loss) / 784.)\n\n\nvariational_ae.compile(loss=\"mse\", optimizer=\"nadam\")\nhistory = variational_ae.fit(X_train, X_train, epochs=25, batch_size=128,\n                             validation_data=(X_valid, X_valid))\n\nEpoch 1/25\n430/430 [==============================] - 7s 8ms/step - loss: 0.0507 - val_loss: 0.0384\nEpoch 2/25\n430/430 [==============================] - 5s 11ms/step - loss: 0.0365 - val_loss: 0.0357\nEpoch 3/25\n430/430 [==============================] - 7s 17ms/step - loss: 0.0347 - val_loss: 0.0345\nEpoch 4/25\n430/430 [==============================] - 4s 9ms/step - loss: 0.0338 - val_loss: 0.0338\nEpoch 5/25\n430/430 [==============================] - 4s 9ms/step - loss: 0.0332 - val_loss: 0.0333\nEpoch 6/25\n430/430 [==============================] - 4s 10ms/step - loss: 0.0328 - val_loss: 0.0330\nEpoch 7/25\n430/430 [==============================] - 4s 8ms/step - loss: 0.0325 - val_loss: 0.0330\nEpoch 8/25\n430/430 [==============================] - 5s 12ms/step - loss: 0.0323 - val_loss: 0.0327\nEpoch 9/25\n430/430 [==============================] - 4s 10ms/step - loss: 0.0320 - val_loss: 0.0325\nEpoch 10/25\n430/430 [==============================] - 4s 9ms/step - loss: 0.0319 - val_loss: 0.0320\nEpoch 11/25\n430/430 [==============================] - 4s 10ms/step - loss: 0.0317 - val_loss: 0.0320\nEpoch 12/25\n430/430 [==============================] - 3s 8ms/step - loss: 0.0315 - val_loss: 0.0317\nEpoch 13/25\n430/430 [==============================] - 4s 10ms/step - loss: 0.0315 - val_loss: 0.0316\nEpoch 14/25\n430/430 [==============================] - 4s 9ms/step - loss: 0.0313 - val_loss: 0.0315\nEpoch 15/25\n430/430 [==============================] - 3s 8ms/step - loss: 0.0313 - val_loss: 0.0318\nEpoch 16/25\n430/430 [==============================] - 3s 8ms/step - loss: 0.0312 - val_loss: 0.0315\nEpoch 17/25\n430/430 [==============================] - 7s 17ms/step - loss: 0.0312 - val_loss: 0.0314\nEpoch 18/25\n430/430 [==============================] - 5s 10ms/step - loss: 0.0311 - val_loss: 0.0314\nEpoch 19/25\n430/430 [==============================] - 5s 12ms/step - loss: 0.0310 - val_loss: 0.0313\nEpoch 20/25\n430/430 [==============================] - 5s 11ms/step - loss: 0.0310 - val_loss: 0.0313\nEpoch 21/25\n430/430 [==============================] - 4s 9ms/step - loss: 0.0309 - val_loss: 0.0311\nEpoch 22/25\n430/430 [==============================] - 3s 8ms/step - loss: 0.0309 - val_loss: 0.0312\nEpoch 23/25\n430/430 [==============================] - 3s 8ms/step - loss: 0.0308 - val_loss: 0.0312\nEpoch 24/25\n430/430 [==============================] - 4s 10ms/step - loss: 0.0308 - val_loss: 0.0311\nEpoch 25/25\n430/430 [==============================] - 4s 9ms/step - loss: 0.0308 - val_loss: 0.0310\n\n\n\nplot_reconstructions(variational_ae, X_valid);\n\n1/1 [==============================] - 0s 134ms/step\n\n\n\n\n\n\n\n\n12.3.3 Generating Fashion MNIST Images\nNow let’s use this variational autoencoder to generate images that look like fashion items. All we need to do is sample random codings from a Gaussian distribution and decode them:\n\ncodings = tf.random.normal(shape=[3 * 7, codings_size])\nimages = variational_decoder(codings).numpy()\n\nplot_multiple_images(images, 7);\n\n\n\n\nVariational autoencoders make it possible to perform semantic interpolation: instead of interpolating between two images at the pixel level, which would look as if the two images were just overlaid, we can interpolate at the codings level. For example, let’s take a few codings along an arbitrary line in latent space and decode them. We get a sequence of images that gradually go from sweaters to pants:\n\ncodings = np.zeros([7, codings_size])\ncodings[:, 4] = np.linspace(-0.8, 0.8, 7)  # axis 4 looks best in this case\nimages = variational_decoder(codings).numpy()\n\n\nplot_multiple_images(images);"
  },
  {
    "objectID": "12_Representation_learning.html#generative-adversarial-networks",
    "href": "12_Representation_learning.html#generative-adversarial-networks",
    "title": "12  Representation learning",
    "section": "12.4 Generative Adversarial Networks",
    "text": "12.4 Generative Adversarial Networks\nLet’s go ahead and build a simple GAN for Fashion MNIST. First, we need to build the generator and the discriminator. The generator is similar to an autoencoder’s decoder, and the discriminator is a regular binary classifier: it takes an image as input and ends with a Dense layer containing a single unit and using the sigmoid activation function. For the second phase of each training iteration, we also need the full GAN model containing the generator followed by the discriminator:\n\n12.4.1 Simple DCGAN\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n\ncodings_size = 100\n\ngenerator = tf.keras.Sequential([\n    tf.keras.layers.Dense(7 * 7 * 128),\n    tf.keras.layers.Reshape([7, 7, 128]),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2,\n                                    padding=\"same\", activation=\"relu\"),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2,\n                                    padding=\"same\", activation=\"tanh\"),\n])\ndiscriminator = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\",\n                        activation=tf.keras.layers.LeakyReLU(0.2)),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"same\",\n                        activation=tf.keras.layers.LeakyReLU(0.2)),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\ngan = tf.keras.Sequential([generator, discriminator])\n\nThe generator takes codings of size 100, projects them to 6,272 dimensions (7 * 7 * 128), and reshapes the result to get a 7 × 7 × 128 tensor. This tensor is batch normalized and fed to a transposed convolutional layer with a stride of 2, which upsamples it from 7 × 7 to 14 × 14 and reduces its depth from 128 to 64. The result is batch normalized again and fed to another transposed convolutional layer with a stride of 2, which upsamples it from 14 × 14 to 28 × 28 and reduces the depth from 64 to 1. This layer uses the tanh activation function, so the outputs will range from -1 to 1. For this reason, before training the GAN, we need to rescale the training set to that same range. We also need to reshape it to add the channel dimension:\n\nX_train_dcgan = X_train.reshape(-1, 28, 28, 1) * 2. - 1. # reshape and rescale\n\nThe discriminator looks much like a regular CNN for binary classification, except instead of using max pooling layers to downsample the image, we use strided convo lutions ( strides=2 ). Note that we use the leaky ReLU activation function. Overall, we respected the DCGAN guidelines, except we replaced the BatchNormalization layers in the discriminator with Dropout layers.\nNext, we need to compile these models. As the discriminator is a binary classifier, we can naturally use the binary cross-entropy loss. The gan model is also a binary classifier, so it can use the binary cross-entropy loss as well. However, the generator will only be trained through the gan model, so we do not need to compile it at all. Importantly, the discriminator should not be trained during the second phase, so we make it non-trainable before compiling the gan model:\n\ndiscriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n\n# In the gan model, we should not train the discriminator\ndiscriminator.trainable = False\ngan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n\n\nThe trainable attribute is taken into account by Keras only when compiling a model, so after running this code, the discriminator is trainable if we call its fit() method or its train_on_batch() method (which we will be using), while it is not trainable when we call these methods on the gan model.\n\nSince the training loop is unusual, we cannot use the regular fit() method. Instead, we will write a custom training loop. For this, we first need to create a Dataset to iterate through the images:\n\nbatch_size = 32\ndataset = tf.data.Dataset.from_tensor_slices(X_train_dcgan)\ndataset = dataset.shuffle(1000)\ndataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n\n\ndef train_gan(gan, dataset, batch_size, codings_size, n_epochs):\n    generator, discriminator = gan.layers\n    for epoch in range(n_epochs):\n        print(f\"Epoch {epoch + 1}/{n_epochs}\")  # extra code\n        for X_batch in dataset:\n            # phase 1 - training the discriminator\n            noise = tf.random.normal(shape=[batch_size, codings_size])\n            generated_images = generator(noise)\n            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\n            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n            discriminator.train_on_batch(X_fake_and_real, y1)\n            # phase 2 - training the generator\n            noise = tf.random.normal(shape=[batch_size, codings_size])\n            y2 = tf.constant([[1.]] * batch_size)\n            gan.train_on_batch(noise, y2)\n        # extra code — plot images during training\n        plot_multiple_images(generated_images.numpy(), 8)\n        plt.show()\n        plt.close()\n\ntrain_gan(gan, dataset, batch_size, codings_size, n_epochs=50)\n\nAs discussed earlier, you can see the two phases at each iteration:\n\nIn phase one we feed Gaussian noise to the generator to produce fake images, • and we complete this batch by concatenating an equal number of real images. The targets y1 are set to 0 for fake images and 1 for real images. Then we train the discriminator on this batch. Remember that the discriminator is trainable in this phase, but we are not touching the generator.\nIn phase two, we feed the GAN some Gaussian noise. Its generator will start by producing fake images, then the discriminator will try to guess whether these images are fake or real. In this phase, we are trying to improve the generator, which means that we want the discriminator to fail: this is why the targets y2 are all set to 1, although the images are fake. In this phase, the discriminator is not trainable, so the only part of the gan model that will improve is the generator.\n\nAfter training, you can randomly sample some codings from a Gaussian distribution, and feed them to the generator to produce new images:\n\nnoise = tf.random.normal(shape=[batch_size, codings_size])\ngenerated_images = generator.predict(noise)\nplot_multiple_images(generated_images, 8)\n\n1/1 [==============================] - 0s 142ms/step\n\n\n\n\n\nSee stylegan if you are looking for state-of-the-art solutions."
  },
  {
    "objectID": "12_Representation_learning.html#diffusion-models",
    "href": "12_Representation_learning.html#diffusion-models",
    "title": "12  Representation learning",
    "section": "12.5 Diffusion Models",
    "text": "12.5 Diffusion Models\nStarting with an image from the dataset, at each time step \\(t\\), the diffusion process adds Gaussian noise with mean 0 and variance \\(\\beta_t\\). The model is then trained to reverse that process. More specifically, given a noisy image produced by the forward process, and given the time \\(t\\), the model is trained to predict the total noise that was added to the original image, scaled to variance 1.\nThe DDPM paper increased \\(\\beta_t\\) from \\(\\beta_1\\) = 0.0001 to $_T = \\(0.02 (\\)T$ is the max step), but the Improved DDPM paper suggested using the following \\(\\cos^2(\\ldots)\\) schedule instead, which gradually decreases \\(\\bar{\\alpha_t} = \\prod_{i=0}^{t} \\alpha_i\\) from 1 to 0, where \\(\\alpha_t = 1 - \\beta_t\\):\n\ndef variance_schedule(T, s=0.008, max_beta=0.999):\n    t = np.arange(T + 1)\n    f = np.cos((t / T + s) / (1 + s) * np.pi / 2) ** 2\n    alpha = np.clip(f[1:] / f[:-1], 1 - max_beta, 1)\n    alpha = np.append(1, alpha).astype(np.float32)  # add α₀ = 1\n    beta = 1 - alpha\n    alpha_cumprod = np.cumprod(alpha)\n    return alpha, alpha_cumprod, beta  # αₜ , α̅ₜ , βₜ for t = 0 to T\n\nnp.random.seed(42)  # for reproducibility\nT = 4000\nalpha, alpha_cumprod, beta = variance_schedule(T)\n\nIn the DDPM paper, the authors used \\(T = 1,000\\), while in the Improved DDPM, they bumped this up to \\(T = 4,000\\), so we use this value. The variable alpha is a vector containing \\(\\alpha_0, \\alpha_1, ..., \\alpha_T\\). The variable alpha_cumprod is a vector containing \\(\\bar{\\alpha_0}, \\bar{\\alpha_1}, ..., \\bar{\\alpha_T}\\).\n\nplt.figure(figsize=(6, 3))\nplt.plot(beta, \"r--\", label=r\"$\\beta_t$\")\nplt.plot(alpha_cumprod, \"b\", label=r\"$\\bar{\\alpha}_t$\")\nplt.axis([0, T, 0, 1])\nplt.grid(True)\nplt.xlabel(r\"t\")\nplt.legend();\n\n\n\n\nTo train our model to reverse the diffusion process, we will need noisy images from different time steps of the forward process. For this, let’s create a prepare_batch() function that will take a batch of clean images from the dataset and prepare them:\n\ndef prepare_batch(X):\n    X = tf.cast(X[..., tf.newaxis], tf.float32) * 2 - 1  # scale from -1 to +1\n    X_shape = tf.shape(X)\n    t = tf.random.uniform([X_shape[0]], minval=1, maxval=T + 1, dtype=tf.int32)\n    alpha_cm = tf.gather(alpha_cumprod, t)\n    alpha_cm = tf.reshape(alpha_cm, [X_shape[0]] + [1] * (len(X_shape) - 1))\n    noise = tf.random.normal(X_shape)\n    return {\n        \"X_noisy\": alpha_cm ** 0.5 * X + (1 - alpha_cm) ** 0.5 * noise,\n        \"time\": t,\n    }, noise\n\nLet’s go through this code: - For simplicity we will use Fashion MNIST, so the function must first add a channel axis. It will also help to scale the pixel values from -1 to 1, so it’s closer to the final Gaussian distribution with mean 0 and variance 1.\n\nNext, the function creates t, a vector containing a random time step for each image in the batch, between 1 and T. Then it uses tf.gather() to get the value of alpha_cumprod for each of the time steps in the vector t. This gives us the vector alpha_cm, containing one value of \\(\\bar{\\alpha_t}\\) for each image.\nThe next line reshapes the alpha_cm from [batch size] to [batch size, 1, 1, 1]. This is needed to ensure alpha_cm can be broadcasted with the batch X . Then we generate some Gaussian noise with mean 0 and variance 1.\nLastly, we use apply the diffusion process to the images. Note that x  **  0.5 is equal to the square root of x. The function returns a tuple containing the inputs and the targets. The inputs are represented as a Python dict containing the noisy images and the time steps used to generate them. The targets are the Gaussian noise used to generate each image.\n\nNext, we’ll create a training dataset and a validation set that will apply the prepare_batch() function to every batch. As earlier, X_train and X_valid contain the Fashion MNIST images with pixel values ranging from 0 to 1:\n\ndef prepare_dataset(X, batch_size=32, shuffle=False):\n    ds = tf.data.Dataset.from_tensor_slices(X)\n    if shuffle:\n        ds = ds.shuffle(10_000)\n    return ds.batch(batch_size).map(prepare_batch).prefetch(1)\n\ntf.random.set_seed(43)  # ensures reproducibility on CPU\ntrain_set = prepare_dataset(X_train, batch_size=32, shuffle=True)\nvalid_set = prepare_dataset(X_valid, batch_size=32)\n\nAs a quick sanity check, let’s take a look at a few training samples, along with the corresponding noise to predict, and the original images (which we get by subtracting the appropriately scaled noise from the appropriately scaled noisy image):\n\n# Just a quick sanity check\n\ndef subtract_noise(X_noisy, time, noise):\n    X_shape = tf.shape(X_noisy)\n    alpha_cm = tf.gather(alpha_cumprod, time)\n    alpha_cm = tf.reshape(alpha_cm, [X_shape[0]] + [1] * (len(X_shape) - 1))\n    return (X_noisy - (1 - alpha_cm) ** 0.5 * noise) / alpha_cm ** 0.5\n\nX_dict, Y_noise = list(train_set.take(1))[0]  # get the first batch\nX_original = subtract_noise(X_dict[\"X_noisy\"], X_dict[\"time\"], Y_noise)\n\nprint(\"Original images\")\nplot_multiple_images(X_original[:8].numpy())\nplt.show()\nprint(\"Time steps:\", X_dict[\"time\"].numpy()[:8])\nprint(\"Noisy images\")\nplot_multiple_images(X_dict[\"X_noisy\"][:8].numpy())\nplt.show()\nprint(\"Noise to predict\")\nplot_multiple_images(Y_noise[:8].numpy())\n\nOriginal images\n\n\n\n\n\nTime steps: [3405  312 3441 1991 2443 1657 3308 1151]\nNoisy images\n\n\n\n\n\nNoise to predict\n\n\n\n\n\nNow we’re ready to build the diffusion model itself. It will need to process both images and times. We will encode the times using a sinusoidal encoding, as suggested in the DDPM paper, just like in the Attention is all you need paper. Given a vector of m integers representing time indices (integers), the layer returns an m × d matrix, where d is the chosen embedding size.\n\n# Implements a custom time encoding layer\n\nembed_size = 64\n\nclass TimeEncoding(tf.keras.layers.Layer):\n    def __init__(self, T, embed_size, dtype=tf.float32, **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        assert embed_size % 2 == 0, \"embed_size must be even\"\n        p, i = np.meshgrid(np.arange(T + 1), 2 * np.arange(embed_size // 2))\n        t_emb = np.empty((T + 1, embed_size))\n        t_emb[:, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n        t_emb[:, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n        self.time_encodings = tf.constant(t_emb.astype(self.dtype))\n\n    def call(self, inputs):\n        return tf.gather(self.time_encodings, inputs)\n\nNow let’s build the model. In the Improved DDPM paper, they use a UNet model. We’ll create a UNet-like model, that processes the image through Conv2D + BatchNormalization layers and skip connections, gradually downsampling the image (using MaxPooling layers with strides=2), then growing it back again (using Upsampling2D layers). Skip connections are also added across the downsampling part and the upsampling part. We also add the time encodings to the output of each block, after passing them through a Dense layer to resize them to the right dimension.\n\nNote: an image’s time encoding is added to every pixel in the image, along the last axis (channels). So the number of units in the Conv2D layer must correspond to the embedding size, and we must reshape the time_enc tensor to add the width and height dimensions.\nThis UNet implementation was inspired by keras.io’s image segmentation example, as well as from the official diffusion models implementation. Compared to the first implementation, I added a few things, especially time encodings and skip connections across down/up parts. Compared to the second implementation, I removed a few things, especially the attention layers. It seemed like overkill for Fashion MNIST, but feel free to add them.\n\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n\ndef build_diffusion_model():\n    X_noisy = tf.keras.layers.Input(shape=[28, 28, 1], name=\"X_noisy\")\n    time_input = tf.keras.layers.Input(shape=[], dtype=tf.int32, name=\"time\")\n    time_enc = TimeEncoding(T, embed_size)(time_input)\n\n    dim = 16\n    Z = tf.keras.layers.ZeroPadding2D((3, 3))(X_noisy)\n    Z = tf.keras.layers.Conv2D(dim, 3)(Z)\n    Z = tf.keras.layers.BatchNormalization()(Z)\n    Z = tf.keras.layers.Activation(\"relu\")(Z)\n\n    time = tf.keras.layers.Dense(dim)(time_enc)  # adapt time encoding\n    Z = time[:, tf.newaxis, tf.newaxis, :] + Z  # add time data to every pixel\n\n    skip = Z\n    cross_skips = []  # skip connections across the down & up parts of the UNet\n\n    for dim in (32, 64, 128):\n        Z = tf.keras.layers.Activation(\"relu\")(Z)\n        Z = tf.keras.layers.SeparableConv2D(dim, 3, padding=\"same\")(Z)\n        Z = tf.keras.layers.BatchNormalization()(Z)\n\n        Z = tf.keras.layers.Activation(\"relu\")(Z)\n        Z = tf.keras.layers.SeparableConv2D(dim, 3, padding=\"same\")(Z)\n        Z = tf.keras.layers.BatchNormalization()(Z)\n\n        cross_skips.append(Z)\n        Z = tf.keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(Z)\n        skip_link = tf.keras.layers.Conv2D(dim, 1, strides=2,\n                                           padding=\"same\")(skip)\n        Z = tf.keras.layers.add([Z, skip_link])\n\n        time = tf.keras.layers.Dense(dim)(time_enc)\n        Z = time[:, tf.newaxis, tf.newaxis, :] + Z\n        skip = Z\n\n    for dim in (64, 32, 16):\n        Z = tf.keras.layers.Activation(\"relu\")(Z)\n        Z = tf.keras.layers.Conv2DTranspose(dim, 3, padding=\"same\")(Z)\n        Z = tf.keras.layers.BatchNormalization()(Z)\n\n        Z = tf.keras.layers.Activation(\"relu\")(Z)\n        Z = tf.keras.layers.Conv2DTranspose(dim, 3, padding=\"same\")(Z)\n        Z = tf.keras.layers.BatchNormalization()(Z)\n\n        Z = tf.keras.layers.UpSampling2D(2)(Z)\n\n        skip_link = tf.keras.layers.UpSampling2D(2)(skip)\n        skip_link = tf.keras.layers.Conv2D(dim, 1, padding=\"same\")(skip_link)\n        Z = tf.keras.layers.add([Z, skip_link])\n\n        time = tf.keras.layers.Dense(dim)(time_enc)\n        Z = time[:, tf.newaxis, tf.newaxis, :] + Z\n        Z = tf.keras.layers.concatenate([Z, cross_skips.pop()], axis=-1)\n        skip = Z\n\n    outputs = tf.keras.layers.Conv2D(1, 3, padding=\"same\")(Z)[:, 2:-2, 2:-2]\n    return tf.keras.Model(inputs=[X_noisy, time_input], outputs=[outputs])\n\n\nmodel = build_diffusion_model()\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=\"nadam\")\nmodel.summary()\n\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n X_noisy (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n                                                                                                  \n time (InputLayer)              [(None,)]            0           []                               \n                                                                                                  \n zero_padding2d (ZeroPadding2D)  (None, 34, 34, 1)   0           ['X_noisy[0][0]']                \n                                                                                                  \n time_encoding (TimeEncoding)   (None, 64)           0           ['time[0][0]']                   \n                                                                                                  \n conv2d (Conv2D)                (None, 32, 32, 16)   160         ['zero_padding2d[0][0]']         \n                                                                                                  \n dense (Dense)                  (None, 16)           1040        ['time_encoding[0][0]']          \n                                                                                                  \n batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n alization)                                                                                       \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 1, 1, 16)    0           ['dense[0][0]']                  \n ingOpLambda)                                                                                     \n                                                                                                  \n activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n                                                                                                  \n tf.__operators__.add (TFOpLamb  (None, 32, 32, 16)  0           ['tf.__operators__.getitem[0][0]'\n da)                                                             , 'activation[0][0]']            \n                                                                                                  \n activation_1 (Activation)      (None, 32, 32, 16)   0           ['tf.__operators__.add[0][0]']   \n                                                                                                  \n separable_conv2d (SeparableCon  (None, 32, 32, 32)  688         ['activation_1[0][0]']           \n v2D)                                                                                             \n                                                                                                  \n batch_normalization_1 (BatchNo  (None, 32, 32, 32)  128         ['separable_conv2d[0][0]']       \n rmalization)                                                                                     \n                                                                                                  \n activation_2 (Activation)      (None, 32, 32, 32)   0           ['batch_normalization_1[0][0]']  \n                                                                                                  \n separable_conv2d_1 (SeparableC  (None, 32, 32, 32)  1344        ['activation_2[0][0]']           \n onv2D)                                                                                           \n                                                                                                  \n batch_normalization_2 (BatchNo  (None, 32, 32, 32)  128         ['separable_conv2d_1[0][0]']     \n rmalization)                                                                                     \n                                                                                                  \n dense_1 (Dense)                (None, 32)           2080        ['time_encoding[0][0]']          \n                                                                                                  \n max_pooling2d (MaxPooling2D)   (None, 16, 16, 32)   0           ['batch_normalization_2[0][0]']  \n                                                                                                  \n conv2d_1 (Conv2D)              (None, 16, 16, 32)   544         ['tf.__operators__.add[0][0]']   \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 1, 1, 32)    0           ['dense_1[0][0]']                \n icingOpLambda)                                                                                   \n                                                                                                  \n add (Add)                      (None, 16, 16, 32)   0           ['max_pooling2d[0][0]',          \n                                                                  'conv2d_1[0][0]']               \n                                                                                                  \n tf.__operators__.add_1 (TFOpLa  (None, 16, 16, 32)  0           ['tf.__operators__.getitem_1[0][0\n mbda)                                                           ]',                              \n                                                                  'add[0][0]']                    \n                                                                                                  \n activation_3 (Activation)      (None, 16, 16, 32)   0           ['tf.__operators__.add_1[0][0]'] \n                                                                                                  \n separable_conv2d_2 (SeparableC  (None, 16, 16, 64)  2400        ['activation_3[0][0]']           \n onv2D)                                                                                           \n                                                                                                  \n batch_normalization_3 (BatchNo  (None, 16, 16, 64)  256         ['separable_conv2d_2[0][0]']     \n rmalization)                                                                                     \n                                                                                                  \n activation_4 (Activation)      (None, 16, 16, 64)   0           ['batch_normalization_3[0][0]']  \n                                                                                                  \n separable_conv2d_3 (SeparableC  (None, 16, 16, 64)  4736        ['activation_4[0][0]']           \n onv2D)                                                                                           \n                                                                                                  \n batch_normalization_4 (BatchNo  (None, 16, 16, 64)  256         ['separable_conv2d_3[0][0]']     \n rmalization)                                                                                     \n                                                                                                  \n dense_2 (Dense)                (None, 64)           4160        ['time_encoding[0][0]']          \n                                                                                                  \n max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 64)    0           ['batch_normalization_4[0][0]']  \n                                                                                                  \n conv2d_2 (Conv2D)              (None, 8, 8, 64)     2112        ['tf.__operators__.add_1[0][0]'] \n                                                                                                  \n tf.__operators__.getitem_2 (Sl  (None, 1, 1, 64)    0           ['dense_2[0][0]']                \n icingOpLambda)                                                                                   \n                                                                                                  \n add_1 (Add)                    (None, 8, 8, 64)     0           ['max_pooling2d_1[0][0]',        \n                                                                  'conv2d_2[0][0]']               \n                                                                                                  \n tf.__operators__.add_2 (TFOpLa  (None, 8, 8, 64)    0           ['tf.__operators__.getitem_2[0][0\n mbda)                                                           ]',                              \n                                                                  'add_1[0][0]']                  \n                                                                                                  \n activation_5 (Activation)      (None, 8, 8, 64)     0           ['tf.__operators__.add_2[0][0]'] \n                                                                                                  \n separable_conv2d_4 (SeparableC  (None, 8, 8, 128)   8896        ['activation_5[0][0]']           \n onv2D)                                                                                           \n                                                                                                  \n batch_normalization_5 (BatchNo  (None, 8, 8, 128)   512         ['separable_conv2d_4[0][0]']     \n rmalization)                                                                                     \n                                                                                                  \n activation_6 (Activation)      (None, 8, 8, 128)    0           ['batch_normalization_5[0][0]']  \n                                                                                                  \n separable_conv2d_5 (SeparableC  (None, 8, 8, 128)   17664       ['activation_6[0][0]']           \n onv2D)                                                                                           \n                                                                                                  \n batch_normalization_6 (BatchNo  (None, 8, 8, 128)   512         ['separable_conv2d_5[0][0]']     \n rmalization)                                                                                     \n                                                                                                  \n dense_3 (Dense)                (None, 128)          8320        ['time_encoding[0][0]']          \n                                                                                                  \n max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 128)   0           ['batch_normalization_6[0][0]']  \n                                                                                                  \n conv2d_3 (Conv2D)              (None, 4, 4, 128)    8320        ['tf.__operators__.add_2[0][0]'] \n                                                                                                  \n tf.__operators__.getitem_3 (Sl  (None, 1, 1, 128)   0           ['dense_3[0][0]']                \n icingOpLambda)                                                                                   \n                                                                                                  \n add_2 (Add)                    (None, 4, 4, 128)    0           ['max_pooling2d_2[0][0]',        \n                                                                  'conv2d_3[0][0]']               \n                                                                                                  \n tf.__operators__.add_3 (TFOpLa  (None, 4, 4, 128)   0           ['tf.__operators__.getitem_3[0][0\n mbda)                                                           ]',                              \n                                                                  'add_2[0][0]']                  \n                                                                                                  \n activation_7 (Activation)      (None, 4, 4, 128)    0           ['tf.__operators__.add_3[0][0]'] \n                                                                                                  \n conv2d_transpose (Conv2DTransp  (None, 4, 4, 64)    73792       ['activation_7[0][0]']           \n ose)                                                                                             \n                                                                                                  \n batch_normalization_7 (BatchNo  (None, 4, 4, 64)    256         ['conv2d_transpose[0][0]']       \n rmalization)                                                                                     \n                                                                                                  \n activation_8 (Activation)      (None, 4, 4, 64)     0           ['batch_normalization_7[0][0]']  \n                                                                                                  \n conv2d_transpose_1 (Conv2DTran  (None, 4, 4, 64)    36928       ['activation_8[0][0]']           \n spose)                                                                                           \n                                                                                                  \n batch_normalization_8 (BatchNo  (None, 4, 4, 64)    256         ['conv2d_transpose_1[0][0]']     \n rmalization)                                                                                     \n                                                                                                  \n up_sampling2d_1 (UpSampling2D)  (None, 8, 8, 128)   0           ['tf.__operators__.add_3[0][0]'] \n                                                                                                  \n dense_4 (Dense)                (None, 64)           4160        ['time_encoding[0][0]']          \n                                                                                                  \n up_sampling2d (UpSampling2D)   (None, 8, 8, 64)     0           ['batch_normalization_8[0][0]']  \n                                                                                                  \n conv2d_4 (Conv2D)              (None, 8, 8, 64)     8256        ['up_sampling2d_1[0][0]']        \n                                                                                                  \n tf.__operators__.getitem_4 (Sl  (None, 1, 1, 64)    0           ['dense_4[0][0]']                \n icingOpLambda)                                                                                   \n                                                                                                  \n add_3 (Add)                    (None, 8, 8, 64)     0           ['up_sampling2d[0][0]',          \n                                                                  'conv2d_4[0][0]']               \n                                                                                                  \n tf.__operators__.add_4 (TFOpLa  (None, 8, 8, 64)    0           ['tf.__operators__.getitem_4[0][0\n mbda)                                                           ]',                              \n                                                                  'add_3[0][0]']                  \n                                                                                                  \n concatenate (Concatenate)      (None, 8, 8, 192)    0           ['tf.__operators__.add_4[0][0]', \n                                                                  'batch_normalization_6[0][0]']  \n                                                                                                  \n activation_9 (Activation)      (None, 8, 8, 192)    0           ['concatenate[0][0]']            \n                                                                                                  \n conv2d_transpose_2 (Conv2DTran  (None, 8, 8, 32)    55328       ['activation_9[0][0]']           \n spose)                                                                                           \n                                                                                                  \n batch_normalization_9 (BatchNo  (None, 8, 8, 32)    128         ['conv2d_transpose_2[0][0]']     \n rmalization)                                                                                     \n                                                                                                  \n activation_10 (Activation)     (None, 8, 8, 32)     0           ['batch_normalization_9[0][0]']  \n                                                                                                  \n conv2d_transpose_3 (Conv2DTran  (None, 8, 8, 32)    9248        ['activation_10[0][0]']          \n spose)                                                                                           \n                                                                                                  \n batch_normalization_10 (BatchN  (None, 8, 8, 32)    128         ['conv2d_transpose_3[0][0]']     \n ormalization)                                                                                    \n                                                                                                  \n up_sampling2d_3 (UpSampling2D)  (None, 16, 16, 192)  0          ['concatenate[0][0]']            \n                                                                                                  \n dense_5 (Dense)                (None, 32)           2080        ['time_encoding[0][0]']          \n                                                                                                  \n up_sampling2d_2 (UpSampling2D)  (None, 16, 16, 32)  0           ['batch_normalization_10[0][0]'] \n                                                                                                  \n conv2d_5 (Conv2D)              (None, 16, 16, 32)   6176        ['up_sampling2d_3[0][0]']        \n                                                                                                  \n tf.__operators__.getitem_5 (Sl  (None, 1, 1, 32)    0           ['dense_5[0][0]']                \n icingOpLambda)                                                                                   \n                                                                                                  \n add_4 (Add)                    (None, 16, 16, 32)   0           ['up_sampling2d_2[0][0]',        \n                                                                  'conv2d_5[0][0]']               \n                                                                                                  \n tf.__operators__.add_5 (TFOpLa  (None, 16, 16, 32)  0           ['tf.__operators__.getitem_5[0][0\n mbda)                                                           ]',                              \n                                                                  'add_4[0][0]']                  \n                                                                                                  \n concatenate_1 (Concatenate)    (None, 16, 16, 96)   0           ['tf.__operators__.add_5[0][0]', \n                                                                  'batch_normalization_4[0][0]']  \n                                                                                                  \n activation_11 (Activation)     (None, 16, 16, 96)   0           ['concatenate_1[0][0]']          \n                                                                                                  \n conv2d_transpose_4 (Conv2DTran  (None, 16, 16, 16)  13840       ['activation_11[0][0]']          \n spose)                                                                                           \n                                                                                                  \n batch_normalization_11 (BatchN  (None, 16, 16, 16)  64          ['conv2d_transpose_4[0][0]']     \n ormalization)                                                                                    \n                                                                                                  \n activation_12 (Activation)     (None, 16, 16, 16)   0           ['batch_normalization_11[0][0]'] \n                                                                                                  \n conv2d_transpose_5 (Conv2DTran  (None, 16, 16, 16)  2320        ['activation_12[0][0]']          \n spose)                                                                                           \n                                                                                                  \n batch_normalization_12 (BatchN  (None, 16, 16, 16)  64          ['conv2d_transpose_5[0][0]']     \n ormalization)                                                                                    \n                                                                                                  \n up_sampling2d_5 (UpSampling2D)  (None, 32, 32, 96)  0           ['concatenate_1[0][0]']          \n                                                                                                  \n dense_6 (Dense)                (None, 16)           1040        ['time_encoding[0][0]']          \n                                                                                                  \n up_sampling2d_4 (UpSampling2D)  (None, 32, 32, 16)  0           ['batch_normalization_12[0][0]'] \n                                                                                                  \n conv2d_6 (Conv2D)              (None, 32, 32, 16)   1552        ['up_sampling2d_5[0][0]']        \n                                                                                                  \n tf.__operators__.getitem_6 (Sl  (None, 1, 1, 16)    0           ['dense_6[0][0]']                \n icingOpLambda)                                                                                   \n                                                                                                  \n add_5 (Add)                    (None, 32, 32, 16)   0           ['up_sampling2d_4[0][0]',        \n                                                                  'conv2d_6[0][0]']               \n                                                                                                  \n tf.__operators__.add_6 (TFOpLa  (None, 32, 32, 16)  0           ['tf.__operators__.getitem_6[0][0\n mbda)                                                           ]',                              \n                                                                  'add_5[0][0]']                  \n                                                                                                  \n concatenate_2 (Concatenate)    (None, 32, 32, 48)   0           ['tf.__operators__.add_6[0][0]', \n                                                                  'batch_normalization_2[0][0]']  \n                                                                                                  \n conv2d_7 (Conv2D)              (None, 32, 32, 1)    433         ['concatenate_2[0][0]']          \n                                                                                                  \n tf.__operators__.getitem_7 (Sl  (None, 28, 28, 1)   0           ['conv2d_7[0][0]']               \n icingOpLambda)                                                                                   \n                                                                                                  \n==================================================================================================\nTotal params: 280,369\nTrainable params: 278,993\nNon-trainable params: 1,376\n__________________________________________________________________________________________________\n\n\n\n# adds a ModelCheckpoint callback\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_diffusion_model\", save_best_only=True)\n\nhistory = model.fit(train_set, validation_data=valid_set, epochs=100, callbacks=[checkpoint_cb])\n\nEpoch 1/100\n1719/1719 [==============================] - ETA: 0s - loss: 0.1045\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 90s 36ms/step - loss: 0.1045 - val_loss: 0.0704\nEpoch 2/100\n1719/1719 [==============================] - ETA: 0s - loss: 0.0614\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 58s 34ms/step - loss: 0.0614 - val_loss: 0.0570\nEpoch 3/100\n1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0525\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 57s 33ms/step - loss: 0.0525 - val_loss: 0.0513\nEpoch 4/100\n1719/1719 [==============================] - ETA: 0s - loss: 0.0487\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 53s 31ms/step - loss: 0.0487 - val_loss: 0.0482\nEpoch 5/100\n1719/1719 [==============================] - 44s 26ms/step - loss: 0.0469 - val_loss: 0.0485\nEpoch 6/100\n1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0455\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 54s 31ms/step - loss: 0.0454 - val_loss: 0.0454\nEpoch 7/100\n1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0445\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 54s 31ms/step - loss: 0.0446 - val_loss: 0.0448\nEpoch 8/100\n1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0436\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 52s 30ms/step - loss: 0.0436 - val_loss: 0.0431\nEpoch 9/100\n1719/1719 [==============================] - 44s 26ms/step - loss: 0.0430 - val_loss: 0.0431\nEpoch 10/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0424 - val_loss: 0.0433\nEpoch 11/100\n1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0423\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 54s 32ms/step - loss: 0.0423 - val_loss: 0.0419\nEpoch 12/100\n1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0420\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 54s 32ms/step - loss: 0.0420 - val_loss: 0.0413\nEpoch 13/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0416 - val_loss: 0.0418\nEpoch 14/100\n1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0414\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 53s 31ms/step - loss: 0.0414 - val_loss: 0.0412\nEpoch 15/100\n1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0411\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 55s 32ms/step - loss: 0.0411 - val_loss: 0.0405\nEpoch 16/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0410 - val_loss: 0.0423\nEpoch 17/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0409 - val_loss: 0.0418\nEpoch 18/100\n1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0407\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 51s 30ms/step - loss: 0.0407 - val_loss: 0.0402\nEpoch 19/100\n1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0406\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 53s 31ms/step - loss: 0.0406 - val_loss: 0.0399\nEpoch 20/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0405 - val_loss: 0.0403\nEpoch 21/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0403 - val_loss: 0.0405\nEpoch 22/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0402 - val_loss: 0.0404\nEpoch 23/100\n1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0400\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 52s 30ms/step - loss: 0.0400 - val_loss: 0.0393\nEpoch 24/100\n1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0401\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 54s 31ms/step - loss: 0.0401 - val_loss: 0.0392\nEpoch 25/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0399 - val_loss: 0.0396\nEpoch 26/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0399 - val_loss: 0.0404\nEpoch 27/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0399 - val_loss: 0.0395\nEpoch 28/100\n1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0397\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 52s 30ms/step - loss: 0.0397 - val_loss: 0.0392\nEpoch 29/100\n1719/1719 [==============================] - 42s 25ms/step - loss: 0.0397 - val_loss: 0.0392\nEpoch 30/100\n1719/1719 [==============================] - ETA: 0s - loss: 0.0396\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 53s 31ms/step - loss: 0.0396 - val_loss: 0.0391\nEpoch 31/100\n1719/1719 [==============================] - ETA: 0s - loss: 0.0396\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 55s 32ms/step - loss: 0.0396 - val_loss: 0.0388\nEpoch 32/100\n1719/1719 [==============================] - 44s 25ms/step - loss: 0.0394 - val_loss: 0.0391\nEpoch 33/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0395 - val_loss: 0.0391\nEpoch 34/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0394 - val_loss: 0.0404\nEpoch 35/100\n1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0393\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 53s 31ms/step - loss: 0.0393 - val_loss: 0.0387\nEpoch 36/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0393 - val_loss: 0.0388\nEpoch 37/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0393 - val_loss: 0.0395\nEpoch 38/100\n1719/1719 [==============================] - 45s 26ms/step - loss: 0.0392 - val_loss: 0.0387\nEpoch 39/100\n1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0391\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 54s 32ms/step - loss: 0.0391 - val_loss: 0.0384\nEpoch 40/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0391 - val_loss: 0.0388\nEpoch 41/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0393 - val_loss: 0.0386\nEpoch 42/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0391 - val_loss: 0.0386\nEpoch 43/100\n1719/1719 [==============================] - ETA: 0s - loss: 0.0391\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 51s 30ms/step - loss: 0.0391 - val_loss: 0.0383\nEpoch 44/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0391 - val_loss: 0.0387\nEpoch 45/100\n1719/1719 [==============================] - 42s 25ms/step - loss: 0.0391 - val_loss: 0.0392\nEpoch 46/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0390 - val_loss: 0.0387\nEpoch 47/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0389 - val_loss: 0.0400\nEpoch 48/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0389 - val_loss: 0.0391\nEpoch 49/100\n1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0389\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 53s 31ms/step - loss: 0.0389 - val_loss: 0.0383\nEpoch 50/100\n1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0389\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 51s 30ms/step - loss: 0.0389 - val_loss: 0.0383\nEpoch 51/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0388 - val_loss: 0.0411\nEpoch 52/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0387 - val_loss: 0.0384\nEpoch 53/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0387 - val_loss: 0.0387\nEpoch 54/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0387 - val_loss: 0.0384\nEpoch 55/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0386 - val_loss: 0.0383\nEpoch 56/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0387 - val_loss: 0.0384\nEpoch 57/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0387 - val_loss: 0.0393\nEpoch 58/100\n1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0386\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 52s 30ms/step - loss: 0.0386 - val_loss: 0.0380\nEpoch 59/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0387 - val_loss: 0.0391\nEpoch 60/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0385 - val_loss: 0.0389\nEpoch 61/100\n1718/1719 [============================&gt;.] - ETA: 0s - loss: 0.0386\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 54s 31ms/step - loss: 0.0387 - val_loss: 0.0380\nEpoch 62/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0386 - val_loss: 0.0386\nEpoch 63/100\n1719/1719 [==============================] - 44s 25ms/step - loss: 0.0386 - val_loss: 0.0380\nEpoch 64/100\n1719/1719 [==============================] - ETA: 0s - loss: 0.0384\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 52s 30ms/step - loss: 0.0384 - val_loss: 0.0377\nEpoch 65/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0385 - val_loss: 0.0386\nEpoch 66/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0386 - val_loss: 0.0388\nEpoch 67/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0384 - val_loss: 0.0392\nEpoch 68/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0383 - val_loss: 0.0380\nEpoch 69/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0384 - val_loss: 0.0389\nEpoch 70/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0385 - val_loss: 0.0395\nEpoch 71/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0384 - val_loss: 0.0386\nEpoch 72/100\n1719/1719 [==============================] - 44s 25ms/step - loss: 0.0383 - val_loss: 0.0380\nEpoch 73/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0384 - val_loss: 0.0384\nEpoch 74/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0383 - val_loss: 0.0384\nEpoch 75/100\n1719/1719 [==============================] - 42s 25ms/step - loss: 0.0383 - val_loss: 0.0399\nEpoch 76/100\n1719/1719 [==============================] - 44s 26ms/step - loss: 0.0383 - val_loss: 0.0378\nEpoch 77/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0382 - val_loss: 0.0384\nEpoch 78/100\n1719/1719 [==============================] - 45s 26ms/step - loss: 0.0383 - val_loss: 0.0378\nEpoch 79/100\n1719/1719 [==============================] - ETA: 0s - loss: 0.0384\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 55s 32ms/step - loss: 0.0384 - val_loss: 0.0377\nEpoch 80/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0384 - val_loss: 0.0387\nEpoch 81/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0382 - val_loss: 0.0388\nEpoch 82/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0381 - val_loss: 0.0378\nEpoch 83/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0383 - val_loss: 0.0380\nEpoch 84/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0381 - val_loss: 0.0384\nEpoch 85/100\n1719/1719 [==============================] - 42s 25ms/step - loss: 0.0381 - val_loss: 0.0401\nEpoch 86/100\n1719/1719 [==============================] - 43s 25ms/step - loss: 0.0382 - val_loss: 0.0386\nEpoch 87/100\n1719/1719 [==============================] - ETA: 0s - loss: 0.0381\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 51s 30ms/step - loss: 0.0381 - val_loss: 0.0375\nEpoch 88/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0381 - val_loss: 0.0382\nEpoch 89/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0380 - val_loss: 0.0384\nEpoch 90/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0381 - val_loss: 0.0376\nEpoch 91/100\n1719/1719 [==============================] - 42s 24ms/step - loss: 0.0380 - val_loss: 0.0385\nEpoch 92/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0381 - val_loss: 0.0379\nEpoch 93/100\n1719/1719 [==============================] - 42s 25ms/step - loss: 0.0379 - val_loss: 0.0386\nEpoch 94/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0381 - val_loss: 0.0376\nEpoch 95/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0380 - val_loss: 0.0378\nEpoch 96/100\n1717/1719 [============================&gt;.] - ETA: 0s - loss: 0.0381\n\n\nWARNING:absl:Function `_wrapped_model` contains input name(s) X_noisy with unsupported characters which will be renamed to x_noisy in the SavedModel.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1719/1719 [==============================] - 52s 30ms/step - loss: 0.0381 - val_loss: 0.0373\nEpoch 97/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0380 - val_loss: 0.0390\nEpoch 98/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0380 - val_loss: 0.0382\nEpoch 99/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0381 - val_loss: 0.0375\nEpoch 100/100\n1719/1719 [==============================] - 41s 24ms/step - loss: 0.0380 - val_loss: 0.0377\n\n\nNow that the model is trained, we can use it to generate new images. For this, we just generate Gaussian noise, and pretend this is the result of the diffusion process, and we’re at time \\(T\\). Then we use the model to predict the image at time \\(T - 1\\), then we call it again to get \\(T - 2\\), and so on, removing a bit of noise at each step. At the end, we get an image that looks like it’s from the Fashion MNIST dataset. The equation for this reverse process is at the top of page 4 in the DDPM paper (step 4 in algorithm 2).\n\ndef generate(model, batch_size=32):\n    X = tf.random.normal([batch_size, 28, 28, 1])\n    for t in range(T - 1, 0, -1):\n        print(f\"\\rt = {t}\", end=\" \")  # show progress\n        noise = (tf.random.normal if t &gt; 1 else tf.zeros)(tf.shape(X))\n        X_noise = model({\"X_noisy\": X, \"time\": tf.constant([t] * batch_size)})\n        X = (\n            1 / alpha[t] ** 0.5\n            * (X - beta[t] / (1 - alpha_cumprod[t]) ** 0.5 * X_noise)\n            + (1 - alpha[t]) ** 0.5 * noise\n        )\n    return X\n\nX_gen = generate(model)  # generated images\n\nt = 1 \n\n\n\nplot_multiple_images(X_gen.numpy(), 8)\n\n\n\n\nThere is no shortcut in the reverse process. This may take a minute or two. That’s the main drawback of diffusion models: generating images is slow since the model needs to be called many times. It’s possible to make this faster by using a smaller T value, or by using the same model prediction for several steps at a time, but the resulting images may not look as nice. That said, despite this speed limitation, diffusion models do produce high-quality and diverse images"
  },
  {
    "objectID": "12_Representation_learning.html#stable-diffusion-with-diffuser",
    "href": "12_Representation_learning.html#stable-diffusion-with-diffuser",
    "title": "12  Representation learning",
    "section": "12.6 Stable Diffusion with diffuser",
    "text": "12.6 Stable Diffusion with diffuser\n\n12.6.1 Delve into stable difussion\nStable Diffusion is based on a particular type of diffusion model called Latent Diffusion, proposed in High-Resolution Image Synthesis with Latent Diffusion Models. General diffusion models are machine learning systems that are trained to denoise random gaussian noise step by step, to get to a sample of interest, such as an image. For a more detailed overview of how they work, check this colab.\nDiffusion models have shown to achieve state-of-the-art results for generating image data. But one downside of diffusion models is that the reverse denoising process is slow. In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images. Therefore, it is challenging to train these models and also use them for inference.\nLatent diffusion can reduce the memory and compute complexity by applying the diffusion process over a lower dimensional latent space, instead of using the actual pixel space. This is the key difference between standard diffusion and latent diffusion models: in latent diffusion the model is trained to generate latent (compressed) representations of the images.\nThere are three main components in latent diffusion.\n\nAn autoencoder (VAE): The encoder is used to convert the image into a low dimensional latent representation, which will serve as the input to the U-Net model. The decoder, conversely, transforms the latent representation back into an image. During latent diffusion training, the encoder is used to get the latent representations (latents) of the images for the forward diffusion process, which applies more and more noise at each step. During inference, the denoised latents generated by the reverse diffusion process are converted back into images using the VAE decoder.\nA U-Net: The U-Net has an encoder part and a decoder part both comprised of ResNet blocks.More specifically, the U-Net output predicts the noise residual which can be used to compute the predicted denoised image representation. To prevent the U-Net from losing important information while downsampling, short-cut connections are usually added between the downsampling ResNets of the encoder to the upsampling ResNets of the decoder. Additionally, the stable diffusion U-Net is able to condition its output on text-embeddings via cross-attention layers.\nA text-encoder, e.g. CLIP’s Text Encoder: The text-encoder is responsible for transforming the input prompt, e.g. “An astronout riding a horse” into an embedding space that can be understood by the U-Net. It is usually a simple transformer-based encoder that maps a sequence of input tokens to a sequence of latent text-embeddings. Stable Diffusion does not train the text-encoder during training and simply uses an CLIP’s already trained text encoder, CLIPTextModel.\n\n\n\n\n\nsource: https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png\n\nThe stable diffusion model takes both a latent seed and a text prompt as an input. The latent seed is then used to generate random latent image representations of size \\(64 \\times 64\\) where as the text prompt is transformed to text embeddings of size \\(77 \\times 768\\) via CLIP’s text encoder.\nNext the U-Net iteratively denoises the random latent image representations while being conditioned on the text embeddings. The output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm (\\(\\alpha_t\\) or \\(\\beta_t\\)). Many different scheduler algorithms can be used for this computation, each having its pros and cons. For more information, we recommend looking into Elucidating the Design Space of Diffusion-Based Generative Models\nThe denoising process is repeated ca. 50 times to step-by-step retrieve better latent image representations. Once complete, the latent image representation is decoded by the decoder part of the VAE.\n\n\n12.6.2 Write your own inference pipeline with diffusers\nLet’s go through the StableDiffusionPipeline step by step to see how we could have written it ourselves. We will start by loading the individual models involved.\nThe pre-trained model includes all the components required to setup a complete diffusion pipeline. They are stored in the following folders: - text_encoder: Stable Diffusion uses CLIP, but other diffusion models may use other encoders such as BERT. - tokenizer. It must match the one used by the text_encoder model. - scheduler: The scheduling algorithm used to progressively add noise to the image during training. - unet: The model used to generate the latent representation of the input. - vae: Autoencoder module that we’ll use to decode latent representations into real images.\nWe can load the components by referring to the folder they were saved, using the subfolder argument to from_pretrained().\n\n# 1. Load the autoencoder model which will be used to decode the latents into image space. \nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n\n# 2. Load the tokenizer and text encoder to tokenize and encode the text. \ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\n# 3. The UNet model for generating the latents.\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'logit_scale', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'text_projection.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias']\n- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\n12.6.2.1 VAE configuration\n\nvae.config\n\nFrozenDict([('in_channels', 3),\n            ('out_channels', 3),\n            ('down_block_types',\n             ['DownEncoderBlock2D',\n              'DownEncoderBlock2D',\n              'DownEncoderBlock2D',\n              'DownEncoderBlock2D']),\n            ('up_block_types',\n             ['UpDecoderBlock2D',\n              'UpDecoderBlock2D',\n              'UpDecoderBlock2D',\n              'UpDecoderBlock2D']),\n            ('block_out_channels', [128, 256, 512, 512]),\n            ('layers_per_block', 2),\n            ('act_fn', 'silu'),\n            ('latent_channels', 4),\n            ('norm_num_groups', 32),\n            ('sample_size', 512),\n            ('scaling_factor', 0.18215),\n            ('_class_name', 'AutoencoderKL'),\n            ('_diffusers_version', '0.2.2'),\n            ('_name_or_path', 'CompVis/stable-diffusion-v1-4')])\n\n\n\nsummary(vae, (1, 3, 512, 512))\n\n/usr/local/lib/python3.9/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  action_fn=lambda data: sys.getsizeof(data.storage()),\n/usr/local/lib/python3.9/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return super().__sizeof__() + self.nbytes()\n\n\n====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\nAutoencoderKL                                      [1, 3, 512, 512]          --\n├─Encoder: 1-1                                     [1, 8, 64, 64]            --\n│    └─Conv2d: 2-1                                 [1, 128, 512, 512]        3,584\n│    └─ModuleList: 2-2                             --                        --\n│    │    └─DownEncoderBlock2D: 3-1                [1, 128, 256, 256]        738,944\n│    │    └─DownEncoderBlock2D: 3-2                [1, 256, 128, 128]        2,690,304\n│    │    └─DownEncoderBlock2D: 3-3                [1, 512, 64, 64]          10,754,560\n│    │    └─DownEncoderBlock2D: 3-4                [1, 512, 64, 64]          9,443,328\n│    └─UNetMidBlock2D: 2-3                         [1, 512, 64, 64]          --\n│    │    └─ModuleList: 3-7                        --                        (recursive)\n│    │    └─ModuleList: 3-6                        --                        1,051,648\n│    │    └─ModuleList: 3-7                        --                        (recursive)\n│    └─GroupNorm: 2-4                              [1, 512, 64, 64]          1,024\n│    └─SiLU: 2-5                                   [1, 512, 64, 64]          --\n│    └─Conv2d: 2-6                                 [1, 8, 64, 64]            36,872\n├─Conv2d: 1-2                                      [1, 8, 64, 64]            72\n├─Conv2d: 1-3                                      [1, 4, 64, 64]            20\n├─Decoder: 1-4                                     [1, 3, 512, 512]          --\n│    └─Conv2d: 2-7                                 [1, 512, 64, 64]          18,944\n│    └─UNetMidBlock2D: 2-8                         [1, 512, 64, 64]          --\n│    │    └─ModuleList: 3-10                       --                        (recursive)\n│    │    └─ModuleList: 3-9                        --                        1,051,648\n│    │    └─ModuleList: 3-10                       --                        (recursive)\n│    └─ModuleList: 2-9                             --                        --\n│    │    └─UpDecoderBlock2D: 3-11                 [1, 512, 128, 128]        16,524,800\n│    │    └─UpDecoderBlock2D: 3-12                 [1, 512, 256, 256]        16,524,800\n│    │    └─UpDecoderBlock2D: 3-13                 [1, 256, 512, 512]        4,855,296\n│    │    └─UpDecoderBlock2D: 3-14                 [1, 128, 512, 512]        1,067,648\n│    └─GroupNorm: 2-10                             [1, 128, 512, 512]        256\n│    └─SiLU: 2-11                                  [1, 128, 512, 512]        --\n│    └─Conv2d: 2-12                                [1, 3, 512, 512]          3,459\n====================================================================================================\nTotal params: 83,653,863\nTrainable params: 83,653,863\nNon-trainable params: 0\nTotal mult-adds (T): 1.77\n====================================================================================================\nInput size (MB): 3.15\nForward/backward pass size (MB): 12640.19\nParams size (MB): 334.62\nEstimated Total Size (MB): 12977.95\n====================================================================================================\n\n\n\ndownload_from_pokemondb('https://img.pokemondb.net/sprites/go/normal/dragonite.png','dragonite.png')\nim = resize(rgba2rgb(imread('dragonite.png')), (512, 512)).astype(np.float32)\nplt.imshow(im)\n\n&lt;matplotlib.image.AxesImage at 0x7fbcc5b47370&gt;\n\n\n\n\n\n\n# Encode to the latent space\nencoded = pil_to_latent(im)\nprint(encoded.shape)\n# Let's visualize the four channels of this latent representation:\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(encoded[0][c].cpu(), cmap='Greys')\n\ntorch.Size([1, 4, 64, 64])\n\n\n\n\n\n\n# Decode this latent representation back into an image\ndecoded = latents_to_pil(encoded)[0]\nplt.imshow(decoded)\n\n&lt;matplotlib.image.AxesImage at 0x7fbccc515220&gt;\n\n\n\n\n\nWe start with a 3x512x512 image and it get compressed to a latent vector 4x64x64. Each 3x8x8 pixel volume in the input image gets compressed down to just 4 numbers(4x1x1). This greatly speed up the task!\n\n\n12.6.2.2 UNet Configuration\n\nunet.config\n\nFrozenDict([('sample_size', 64),\n            ('in_channels', 4),\n            ('out_channels', 4),\n            ('center_input_sample', False),\n            ('flip_sin_to_cos', True),\n            ('freq_shift', 0),\n            ('down_block_types',\n             ['CrossAttnDownBlock2D',\n              'CrossAttnDownBlock2D',\n              'CrossAttnDownBlock2D',\n              'DownBlock2D']),\n            ('mid_block_type', 'UNetMidBlock2DCrossAttn'),\n            ('up_block_types',\n             ['UpBlock2D',\n              'CrossAttnUpBlock2D',\n              'CrossAttnUpBlock2D',\n              'CrossAttnUpBlock2D']),\n            ('only_cross_attention', False),\n            ('block_out_channels', [320, 640, 1280, 1280]),\n            ('layers_per_block', 2),\n            ('downsample_padding', 1),\n            ('mid_block_scale_factor', 1),\n            ('act_fn', 'silu'),\n            ('norm_num_groups', 32),\n            ('norm_eps', 1e-05),\n            ('cross_attention_dim', 768),\n            ('encoder_hid_dim', None),\n            ('attention_head_dim', 8),\n            ('dual_cross_attention', False),\n            ('use_linear_projection', False),\n            ('class_embed_type', None),\n            ('num_class_embeds', None),\n            ('upcast_attention', False),\n            ('resnet_time_scale_shift', 'default'),\n            ('resnet_skip_time_act', False),\n            ('resnet_out_scale_factor', 1.0),\n            ('time_embedding_type', 'positional'),\n            ('time_embedding_act_fn', None),\n            ('timestep_post_act', None),\n            ('time_cond_proj_dim', None),\n            ('conv_in_kernel', 3),\n            ('conv_out_kernel', 3),\n            ('projection_class_embeddings_input_dim', None),\n            ('class_embeddings_concat', False),\n            ('mid_block_only_cross_attention', None),\n            ('cross_attention_norm', None),\n            ('_class_name', 'UNet2DConditionModel'),\n            ('_diffusers_version', '0.2.2'),\n            ('_name_or_path', 'CompVis/stable-diffusion-v1-4')])\n\n\n\nsummary(unet, [(1, 4, 64, 64), (1,), (1, 77, 768)])\n\n===================================================================================================================\nLayer (type:depth-idx)                                            Output Shape              Param #\n===================================================================================================================\nUNet2DConditionModel                                              [1, 4, 64, 64]            --\n├─Timesteps: 1-1                                                  [1, 320]                  --\n├─TimestepEmbedding: 1-2                                          [1, 1280]                 --\n│    └─Linear: 2-1                                                [1, 1280]                 410,880\n│    └─SiLU: 2-2                                                  [1, 1280]                 --\n│    └─Linear: 2-3                                                [1, 1280]                 1,639,680\n├─Conv2d: 1-3                                                     [1, 320, 64, 64]          11,840\n├─ModuleList: 1-4                                                 --                        --\n│    └─CrossAttnDownBlock2D: 2-4                                  [1, 320, 32, 32]          --\n│    │    └─ModuleList: 3-3                                       --                        (recursive)\n│    │    └─ModuleList: 3-4                                       --                        (recursive)\n│    │    └─ModuleList: 3-3                                       --                        (recursive)\n│    │    └─ModuleList: 3-4                                       --                        (recursive)\n│    │    └─ModuleList: 3-5                                       --                        921,920\n│    └─CrossAttnDownBlock2D: 2-5                                  [1, 640, 16, 16]          --\n│    │    └─ModuleList: 3-8                                       --                        (recursive)\n│    │    └─ModuleList: 3-9                                       --                        (recursive)\n│    │    └─ModuleList: 3-8                                       --                        (recursive)\n│    │    └─ModuleList: 3-9                                       --                        (recursive)\n│    │    └─ModuleList: 3-10                                      --                        3,687,040\n│    └─CrossAttnDownBlock2D: 2-6                                  [1, 1280, 8, 8]           --\n│    │    └─ModuleList: 3-13                                      --                        (recursive)\n│    │    └─ModuleList: 3-14                                      --                        (recursive)\n│    │    └─ModuleList: 3-13                                      --                        (recursive)\n│    │    └─ModuleList: 3-14                                      --                        (recursive)\n│    │    └─ModuleList: 3-15                                      --                        14,746,880\n│    └─DownBlock2D: 2-7                                           [1, 1280, 8, 8]           --\n│    │    └─ModuleList: 3-16                                      --                        62,277,120\n├─UNetMidBlock2DCrossAttn: 1-5                                    [1, 1280, 8, 8]           --\n│    └─ModuleList: 2-10                                           --                        (recursive)\n│    │    └─ResnetBlock2D: 3-17                                   [1, 1280, 8, 8]           31,138,560\n│    └─ModuleList: 2-9                                            --                        --\n│    │    └─Transformer2DModel: 3-18                              [1, 1280, 8, 8]           34,760,960\n│    └─ModuleList: 2-10                                           --                        (recursive)\n│    │    └─ResnetBlock2D: 3-19                                   [1, 1280, 8, 8]           31,138,560\n├─ModuleList: 1-6                                                 --                        --\n│    └─UpBlock2D: 2-11                                            [1, 1280, 16, 16]         --\n│    │    └─ModuleList: 3-20                                      --                        147,494,400\n│    │    └─ModuleList: 3-21                                      --                        14,746,880\n│    └─CrossAttnUpBlock2D: 2-12                                   [1, 1280, 32, 32]         --\n│    │    └─ModuleList: 3-26                                      --                        (recursive)\n│    │    └─ModuleList: 3-27                                      --                        (recursive)\n│    │    └─ModuleList: 3-26                                      --                        (recursive)\n│    │    └─ModuleList: 3-27                                      --                        (recursive)\n│    │    └─ModuleList: 3-26                                      --                        (recursive)\n│    │    └─ModuleList: 3-27                                      --                        (recursive)\n│    │    └─ModuleList: 3-28                                      --                        14,746,880\n│    └─CrossAttnUpBlock2D: 2-13                                   [1, 640, 64, 64]          --\n│    │    └─ModuleList: 3-33                                      --                        (recursive)\n│    │    └─ModuleList: 3-34                                      --                        (recursive)\n│    │    └─ModuleList: 3-33                                      --                        (recursive)\n│    │    └─ModuleList: 3-34                                      --                        (recursive)\n│    │    └─ModuleList: 3-33                                      --                        (recursive)\n│    │    └─ModuleList: 3-34                                      --                        (recursive)\n│    │    └─ModuleList: 3-35                                      --                        3,687,040\n│    └─CrossAttnUpBlock2D: 2-14                                   [1, 320, 64, 64]          --\n│    │    └─ModuleList: 3-40                                      --                        (recursive)\n│    │    └─ModuleList: 3-41                                      --                        (recursive)\n│    │    └─ModuleList: 3-40                                      --                        (recursive)\n│    │    └─ModuleList: 3-41                                      --                        (recursive)\n│    │    └─ModuleList: 3-40                                      --                        (recursive)\n│    │    └─ModuleList: 3-41                                      --                        (recursive)\n├─GroupNorm: 1-7                                                  [1, 320, 64, 64]          640\n├─SiLU: 1-8                                                       [1, 320, 64, 64]          --\n├─Conv2d: 1-9                                                     [1, 4, 64, 64]            11,524\n===================================================================================================================\nTotal params: 859,520,964\nTrainable params: 859,520,964\nNon-trainable params: 0\nTotal mult-adds (G): 222.30\n===================================================================================================================\nInput size (MB): 0.30\nForward/backward pass size (MB): 2530.96\nParams size (MB): 3438.08\nEstimated Total Size (MB): 5969.35\n===================================================================================================================\n\n\n\n\n12.6.2.3 Schedulers\nNow instead of loading the pre-defined scheduler, we’ll use the K-LMS scheduler instead.\n\nscheduler = LMSDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n\n\nscheduler.config\n\nFrozenDict([('num_train_timesteps', 1000),\n            ('beta_start', 0.00085),\n            ('beta_end', 0.012),\n            ('beta_schedule', 'scaled_linear'),\n            ('trained_betas', None),\n            ('prediction_type', 'epsilon'),\n            ('_class_name', 'PNDMScheduler'),\n            ('_diffusers_version', '0.7.0.dev0'),\n            ('set_alpha_to_one', False),\n            ('skip_prk_steps', True),\n            ('steps_offset', 1),\n            ('clip_sample', False)])\n\n\n\n# Plotting this noise schedule:\nplt.plot(scheduler.sigmas)\nplt.title('Noise Schedule')\nplt.xlabel('Sampling step')\nplt.ylabel('sigma')\nplt.show()\n\n\n\n\nThis ‘sigma’ is the amount of noise added to the latent representation.\n\n\n\n12.6.3 Actual inference\nNext we move the models to the GPU.\n\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nvae = vae.to(torch_device)\ntext_encoder = text_encoder.to(torch_device)\nunet = unet.to(torch_device) \n\nWe now define the parameters we’ll use to generate images.\n\nprompt = [\"photograph of an astronaut riding a horse\"]\n\nheight = 512                        # default height of Stable Diffusion\nwidth = 512                         # default width of Stable Diffusion\nnum_inference_steps = 100            # Number of denoising steps\nguidance_scale = 7.5                # Scale for classifier-free guidance\ngenerator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\n\nbatch_size = 1\n\nguidance_scale is a way to increase the adherence to the text as well as overall sample quality. In simple terms it force generation to better match with the prompt. Numbers like 7 or 8.5 give good results, if you use a very large number the images might look good, but will be less diverse.\n\n12.6.3.1 Text embedding\nFirst, we get the text_embeddings for the prompt. These embeddings will be used to condition the UNet model.\n\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad():\n  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n\nWe’ll also get the unconditional text embeddings, which are just the embeddings for the padding token (empty text). They need to have the same shape as the conditional text_embeddings (batch_size and seq_length)\n\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n\nwith torch.no_grad():\n  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]   \n\n\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings]) # Concatenate together\n\nSee classifier-free guidence for more detail.\n\n\n12.6.3.2 Diffussion models\nGenerate the intial random noise.\n\nlatents = torch.randn(\n  (batch_size, unet.in_channels, height // 8, width // 8),\n  generator=generator,\n)\nlatents = latents.to(torch_device)\n\nNext, we initialize the scheduler with our chosen num_inference_steps. This will compute the sigmas (similar to \\(\\alpha_t\\)) and exact time step values to be used during the denoising process.\n\nscheduler.set_timesteps(num_inference_steps)\n\n# The scheduler need to scale the latent\nlatents = latents * scheduler.init_noise_sigma\nlatents.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\nWe are ready to write the denoising loop:\n\nfor i, t in enumerate(tqdm(scheduler.timesteps)):\n  # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n  latent_model_input = torch.cat([latents] * 2)\n\n  latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n  # predict the noise residual\n  with torch.no_grad():\n    noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n  # perform guidance\n  noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) # We have two samples here\n  noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n  # compute the previous noisy sample x_t -&gt; x_t-1\n  latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n  # 3. optionally look at image\n  if (i + 1) % 10 == 0:\n      latents_viz = 1 / 0.18215 * latents\n      with torch.no_grad():\n        image = vae.decode(latents_viz).sample\n      image = (image / 2 + 0.5).clamp(0, 1)\n      image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n      images = (image * 255).round().astype(\"uint8\")\n      plt.imshow(images.squeeze())\n      plt.axis(\"off\")\n      plt.show()\n      del image, latents_viz\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\nnoise_pred_text = noise_pred.chunk(2)\n\n\nnoise_pred_text[0].shape\n\ntorch.Size([1, 4, 64, 64])\n\n\nAll schedulers provide one or multiple step() methods that can be used to compute the slightly less noisy image. The step() method may vary from one scheduler to another, but normally expects at least the model output, the timestep and the current noisy_sample.\n\n\n12.6.3.3 VAE\nWe now use the vae to decode the generated latents back into the image.\n\n# scale and decode the image latents with vae\nlatents = 1 / 0.18215 * latents\n\nwith torch.no_grad():\n  image = vae.decode(latents).sample\n\n\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image.detach().cpu().permute(0, 2, 3, 1).numpy()\nimages = (image * 255).round().astype(\"uint8\")\n\n\nplt.imshow(images.squeeze())\n\n&lt;matplotlib.image.AxesImage at 0x7fbd19ed9190&gt;\n\n\n\n\n\nSee here and here for more in depth discussion. If you are interested in training, see here.\n\n\n\n12.6.4 Stable diffusion with Pipeline of difusser\nThere are many built-in pipelines available.\nStableDiffusionPipeline is an end-to-end inference pipeline that you can use to generate images from text with just a few lines of code. First, we load the pre-trained weights of all components of the model. We use Stable Diffusion version 1.4 (CompVis/stable-diffusion-v1-4), but there are other variants that you may want to try:\n\nrunwayml/stable-diffusion-v1-5\nstabilityai/stable-diffusion-2-1-base\nstabilityai/stable-diffusion-2-1. This version can produce images with a resolution of \\(768 \\times 768\\), while the others work at \\(512 \\times 512\\).\n\nIn addition to the model id CompVis/stable-diffusion-v1-4, we’re also passing a specific revision and torch_dtype to the from_pretrained() method.\nWe’re loading the weights from the half-precision branch fp16 and also tell diffusers to expect the weights in float16 precision by passing torch_dtype=torch.float16. If you want to ensure the highest possible precision, please make sure to remove torch_dtype=torch.float16 at the cost of a higher memory usage.\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n\n/usr/local/lib/python3.9/dist-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n  warnings.warn(\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n\n\n\npipe = pipe.to(\"cuda\") # let's move the pipeline to GPU to have faster inference.\npipe\n\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.15.0\",\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPFeatureExtractor\"\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"PNDMScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n\n\n\nprompt = \"photograph of an astronaut riding a horse\"\nimage = pipe(prompt).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n\n# If you're in a google colab you can directly display it with \nimage\n\n\n\n\n\n\n\nYou can change the number of inference steps using the num_inference_steps argument. In general, results are better the more steps you use. If you want faster results you can use a smaller number. If you want deterministic output you can pass a random seed to the pipeline. Every time you use the same seed you’ll have the same image result.\n\ngenerator = torch.Generator(\"cuda\").manual_seed(1024)\nimage = pipe(prompt, num_inference_steps=35, generator=generator).images[0]\n\nimage\n\n\n\n\n\n\n\nTo generate multiple images for the same prompt, we simply use a list with the same prompt repeated several times. We’ll send the list to the pipeline instead of the string we used before.\n\nnum_images = 3\nprompt = [\"photograph of an astronaut riding a horse\"] * num_images\n\nimages = pipe(prompt).images\n\n\n\n\n\nplot_images(images)\n\n\n\n\nSee here and here for more examples or training."
  },
  {
    "objectID": "12_Representation_learning.html#high-performance-image-generation-using-stable-diffusion-in-kerascv",
    "href": "12_Representation_learning.html#high-performance-image-generation-using-stable-diffusion-in-kerascv",
    "title": "12  Representation learning",
    "section": "12.7 High-performance image generation using Stable Diffusion in KerasCV",
    "text": "12.7 High-performance image generation using Stable Diffusion in KerasCV\nIn this guide, we will show how to generate novel images based on a text prompt using the KerasCV implementation of text-to-image model, Stable Diffusion.\nStable Diffusion is a powerful, open-source text-to-image generation model. While there exist multiple open-source implementations that allow you to easily create images from textual prompts, KerasCV’s offers a few distinct advantages. These include XLA compilation and mixed precision support, which together achieve state-of-the-art generation speed.\nIn this guide, we will explore KerasCV’s Stable Diffusion implementation, show how to use these powerful performance boosts, and explore the performance benefits that they offer.\nFirst, we construct a model:\n\nmodel = keras_cv.models.StableDiffusion(img_width=512, img_height=512)\n\nBy using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n\n\nNext, we give it a prompt:\n\nimages = model.text_to_image(\"photograph of an astronaut riding a horse\", batch_size=3)\n\nplot_images(images)\n\nDownloading data from https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true\n1356917/1356917 [==============================] - 0s 0us/step\nDownloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_encoder.h5\n492466864/492466864 [==============================] - 5s 0us/step\nDownloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_diffusion_model.h5\n3439090152/3439090152 [==============================] - 53s 0us/step\n50/50 [==============================] - 275s 3s/step\nDownloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_decoder.h5\n198180272/198180272 [==============================] - 2s 0us/step\n\n\n\n\n\nBut that’s not all this model can do. Let’s try a more complex prompt:\n\nimages = model.text_to_image(\n    \"cute magical flying dog, fantasy art, \"\n    \"golden color, high quality, highly detailed, elegant, sharp focus, \"\n    \"concept art, character concepts, digital painting, mystery, adventure\",\n    batch_size=3,\n)\nplot_images(images)\n\n50/50 [==============================] - 157s 3s/step\n\n\n\n\n\n\n12.7.1 Perks of KerasCV\nWith several implementations of Stable Diffusion publicly available why should you use keras_cv.models.StableDiffusion?\nAside from the easy-to-use API, KerasCV’s Stable Diffusion model comes with some powerful advantages, including:\n\nGraph mode execution\nXLA compilation through jit_compile=True\nSupport for mixed precision computation\n\nWhen these are combined, the KerasCV Stable Diffusion model runs orders of magnitude faster than naive implementations. This section shows how to enable all of these features, and the resulting performance gain yielded from using them.\n\ntf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\nmodel = keras_cv.models.StableDiffusion(jit_compile=True)\n\n# Let's make sure to warm up the model\nimages = model.text_to_image(\n    \"Teddy bears conducting machine learning research\",\n    batch_size=3,\n)\nplot_images(images)\n\nBy using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n50/50 [==============================] - 543s 640ms/step\n\n\n\n\n\n\nimages = model.text_to_image(\n    \"Sunset on the beach and mountain\",\n    batch_size=3,\n)\nplot_images(images)\n\n50/50 [==============================] - 37s 750ms/step"
  },
  {
    "objectID": "13_Hyperparameter.html#setup",
    "href": "13_Hyperparameter.html#setup",
    "title": "13  Hyperparameter Tuning",
    "section": "13.1 Setup",
    "text": "13.1 Setup\n\n%pip install optuna -q\n%pip install keras-tuner -q\n%pip install autokeras -q\n%pip install wandb -q\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 365.7/365.7 kB 4.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 15.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 6.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 176.1/176.1 kB 14.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.6/148.6 kB 9.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 527.7/527.7 kB 32.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 39.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 26.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.3/184.3 kB 22.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206.5/206.5 kB 22.8 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 8.6 MB/s eta 0:00:00\n  Building wheel for pathtools (setup.py) ... done\n\n\n\n# Python ≥3.7 is recommended\nimport sys\nassert sys.version_info &gt;= (3, 7)\n\n# Scikit-Learn ≥1.01 is recommended\nfrom packaging import version\nimport sklearn\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_curve, roc_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.preprocessing import label_binarize\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\n# Tensorflow ≥2.8.0 is recommended\nimport tensorflow as tf\nassert version.parse(tf.__version__) &gt;= version.parse(\"2.8.0\")\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport optuna \nfrom optuna.samplers import GridSampler, RandomSampler, TPESampler\nimport keras_tuner as kt\nimport autokeras as ak\n\nimport wandb\nfrom wandb.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbEvalCallback, WandbCallback\n\n# Common imports\nimport numpy as np\nimport pandas as pd\nimport os\nfrom random import shuffle\nimport random\nimport time\nimport math\nimport pprint\n\n# To plot pretty figures\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport matplotlib as mpl\nfrom mpl_toolkits.mplot3d import Axes3D\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n\nif not tf.config.list_physical_devices('GPU'):\n    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n    if \"google.colab\" in sys.modules:\n        print(\"Go to Runtime &gt; Change runtime and select a GPU hardware \"\n              \"accelerator.\")\n    if \"kaggle_secrets\" in sys.modules:\n        print(\"Go to Settings &gt; Accelerator and select GPU.\")"
  },
  {
    "objectID": "13_Hyperparameter.html#hyparameter-tuning-for-sklearn",
    "href": "13_Hyperparameter.html#hyparameter-tuning-for-sklearn",
    "title": "13  Hyperparameter Tuning",
    "section": "13.2 Hyparameter tuning for sklearn",
    "text": "13.2 Hyparameter tuning for sklearn\nHyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc.\nIt is possible and recommended to search the hyper-parameter space for the best cross validation score. Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, we can use estimator.get_params().\nA search consists of: - an estimator (regressor or classifier such as sklearn.svm.SVC()); - a parameter space; - a method for searching or sampling candidates; - a cross-validation scheme; and - a score function.\n\nNote that it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values. It is recommended to read the docstring of the estimator class to get a finer understanding of their expected behavior.\n\nLet us load the example dataset first:\n\n# load data\ndigits = datasets.load_digits()\n\n# flatten the images\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\n\n# Split data into train and test subsets\nX_train, X_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.25, shuffle=False)\n\n\nX_train.shape\n\n(1347, 64)\n\n\n\n13.2.1 Grid search\nThe grid search provided by GridSearchCV() exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. Here our estimator is RandomForestClassifier.\n\n# random forest classifier object\nrfc = RandomForestClassifier(random_state=42)\n\n# define sample space\nparam_grid = {\n    'n_estimators': [100, 150, 200],\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [2, 3, 4],\n    'max_features': ['sqrt', 'log2'],\n    'max_depth': [5, 6, 7]\n    }\n\nFirst, let’s obtain the optimal hyperparameters using the grid search method and time the process. This means that we will test all 108 hyperparameter sets and pick out the one that yields the best results.\n\n# create grid search object\ngs = GridSearchCV(estimator=rfc,\n                  param_grid=param_grid,\n                  scoring='f1_micro',\n                  cv=5,\n                  verbose=2)\n\ngs.fit(X_train, y_train)\n\nHere, we will use Optuna instead:\n\ndef objective(trial):\n    \"\"\"return the f1-score\"\"\"\n\n    # search space\n    n_estimators =  trial.suggest_int('n_estimators', low=100, high=200, step=50)\n    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n    min_samples_split = trial.suggest_int('min_samples_split', low=2, high=4, step=1)\n    max_depth = trial.suggest_int('max_depth', low=5, high=7, step=1)\n    max_features = trial.suggest_categorical('max_features', ['sqrt','log2'])\n    # random forest classifier object\n    rfc = RandomForestClassifier(n_estimators=n_estimators, \n                                                  criterion=criterion,\n                                                  min_samples_split=min_samples_split,\n                                                  max_depth=max_depth,\n                                                  max_features=max_features,\n                                                  random_state=42)\n    score =  cross_val_score(estimator=rfc, \n                             X=X_train, \n                             y=y_train, \n                             scoring='f1_micro',\n                             cv=5).mean()\n    return score\n\n\n# create a study (aim to maximize score)\nsearch_space = param_grid\nstudy = optuna.create_study(sampler=GridSampler(param_grid), direction='maximize')\n\n# perform hyperparamter tuning (while timing the process)\ntime_start = time.time()\nstudy.optimize(objective, show_progress_bar=True, n_trials=108)\ntime_grid = time.time() - time_start\n\n# store result in a data frame \ncolumns = ['Number of iterations', 'Iteration Number of Optimal Hyperparamters', 'Score', 'Time Elapsed (s)']\nvalues_grid = [108, study.best_trial.number, study.best_trial.value, time_grid]\nresults_grid = pd.DataFrame([values_grid], columns = columns)\n\n[I 2023-05-24 03:32:56,934] A new study created in memory with name: no-name-185675b5-a671-4372-8e9a-468be41c2e10\n/usr/local/lib/python3.10/dist-packages/optuna/progress_bar.py:56: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n  self._init_valid()\n\n\n\n\n\n[I 2023-05-24 03:32:59,443] Trial 0 finished with value: 0.912405342145119 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 0 with value: 0.912405342145119.\n[I 2023-05-24 03:33:02,630] Trial 1 finished with value: 0.9250227178851714 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 1 with value: 0.9250227178851714.\n[I 2023-05-24 03:33:05,533] Trial 2 finished with value: 0.9324631694891918 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:06,777] Trial 3 finished with value: 0.9183367754371471 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:08,470] Trial 4 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:10,680] Trial 5 finished with value: 0.9235467437697921 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:13,703] Trial 6 finished with value: 0.9064628941208868 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:16,354] Trial 7 finished with value: 0.9005039239983479 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:19,684] Trial 8 finished with value: 0.9235439900867408 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:21,402] Trial 9 finished with value: 0.9257662123089634 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:24,212] Trial 10 finished with value: 0.9094203497177474 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:27,877] Trial 11 finished with value: 0.909436871816054 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:29,396] Trial 12 finished with value: 0.9146413327825968 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:31,885] Trial 13 finished with value: 0.9094396254991051 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:34,273] Trial 14 finished with value: 0.9220542475561062 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:35,409] Trial 15 finished with value: 0.9064601404378356 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:36,590] Trial 16 finished with value: 0.9005259534627564 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:38,963] Trial 17 finished with value: 0.9146358254164946 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:42,036] Trial 18 finished with value: 0.9027509293680298 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:44,686] Trial 19 finished with value: 0.9294781770618201 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:46,076] Trial 20 finished with value: 0.9161145532149249 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:48,029] Trial 21 finished with value: 0.9220597549222085 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:50,518] Trial 22 finished with value: 0.9116590940382763 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:52,397] Trial 23 finished with value: 0.90868787002616 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:56,365] Trial 24 finished with value: 0.9124136031942722 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:33:58,279] Trial 25 finished with value: 0.9168552939556657 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:01,031] Trial 26 finished with value: 0.9183477901693516 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:02,537] Trial 27 finished with value: 0.9109211069805866 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:04,567] Trial 28 finished with value: 0.9131488365689109 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:06,559] Trial 29 finished with value: 0.9146385790995456 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:08,323] Trial 30 finished with value: 0.9235467437697921 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:10,239] Trial 31 finished with value: 0.9027454220019276 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:12,345] Trial 32 finished with value: 0.9153738124741843 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:14,730] Trial 33 finished with value: 0.9198320253338842 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:17,433] Trial 34 finished with value: 0.9138758088943962 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:19,158] Trial 35 finished with value: 0.9309596585433016 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:21,127] Trial 36 finished with value: 0.9213190141814678 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:23,385] Trial 37 finished with value: 0.9034861627426685 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:25,632] Trial 38 finished with value: 0.9317114140162467 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:27,953] Trial 39 finished with value: 0.8990279498829684 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:29,098] Trial 40 finished with value: 0.8938372573316811 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:30,595] Trial 41 finished with value: 0.9109100922483823 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:31,638] Trial 42 finished with value: 0.8960677406030566 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:33,112] Trial 43 finished with value: 0.9057221533801461 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:38,448] Trial 44 finished with value: 0.9294754233787691 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:42,098] Trial 45 finished with value: 0.9086933773922621 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:44,185] Trial 46 finished with value: 0.9146413327825966 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:45,545] Trial 47 finished with value: 0.9294919454770756 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:49,061] Trial 48 finished with value: 0.9176042957455597 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:51,829] Trial 49 finished with value: 0.9235384827206389 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:53,027] Trial 50 finished with value: 0.9242764697783284 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:56,547] Trial 51 finished with value: 0.9235467437697921 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:58,264] Trial 52 finished with value: 0.9205782734407271 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:34:59,834] Trial 53 finished with value: 0.922048740190004 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:35:03,354] Trial 54 finished with value: 0.9287346826380283 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:35:04,961] Trial 55 finished with value: 0.9124136031942722 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:35:07,470] Trial 56 finished with value: 0.9302326862178163 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:35:08,708] Trial 57 finished with value: 0.9287401900041307 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:35:11,191] Trial 58 finished with value: 0.9116618477213272 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9324631694891918.\n[I 2023-05-24 03:35:12,661] Trial 59 finished with value: 0.9331956491807792 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:15,838] Trial 60 finished with value: 0.9175987883794576 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:16,986] Trial 61 finished with value: 0.8916040203772544 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:18,563] Trial 62 finished with value: 0.8967947129285418 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:21,396] Trial 63 finished with value: 0.9250227178851714 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:24,192] Trial 64 finished with value: 0.916869062370921 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:26,956] Trial 65 finished with value: 0.9183395291201982 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:29,498] Trial 66 finished with value: 0.9034944237918217 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:30,592] Trial 67 finished with value: 0.9131433292028088 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:32,428] Trial 68 finished with value: 0.9294864381109734 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:33,919] Trial 69 finished with value: 0.89605121850475 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:35,847] Trial 70 finished with value: 0.928007710312543 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:38,759] Trial 71 finished with value: 0.9324576621230897 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:40,130] Trial 72 finished with value: 0.9124053421451193 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:42,819] Trial 73 finished with value: 0.9086933773922622 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:44,966] Trial 74 finished with value: 0.9153738124741843 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:46,137] Trial 75 finished with value: 0.924292991876635 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:47,752] Trial 76 finished with value: 0.915371058791133 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:49,775] Trial 77 finished with value: 0.9257827344072698 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:51,653] Trial 78 finished with value: 0.9153765661572354 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:54,414] Trial 79 finished with value: 0.9317114140162467 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:57,526] Trial 80 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:35:59,378] Trial 81 finished with value: 0.9294891917940244 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:36:00,680] Trial 82 finished with value: 0.9064601404378356 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:36:03,287] Trial 83 finished with value: 0.9116480793060718 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:36:05,805] Trial 84 finished with value: 0.9072174032768829 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 59 with value: 0.9331956491807792.\n[I 2023-05-24 03:36:07,974] Trial 85 finished with value: 0.9346853917114141 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:10,077] Trial 86 finished with value: 0.9220570012391572 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:11,690] Trial 87 finished with value: 0.9168580476387168 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:12,870] Trial 88 finished with value: 0.9131626049841663 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:15,187] Trial 89 finished with value: 0.913148836568911 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:17,057] Trial 90 finished with value: 0.9079498829684702 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:19,437] Trial 91 finished with value: 0.9228032493460002 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:22,674] Trial 92 finished with value: 0.9101831199228968 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:24,856] Trial 93 finished with value: 0.9220652622883106 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:26,105] Trial 94 finished with value: 0.9309651659094038 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:28,253] Trial 95 finished with value: 0.9072063885446784 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:29,936] Trial 96 finished with value: 0.9072036348616275 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:32,011] Trial 97 finished with value: 0.8982789480930744 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:34,132] Trial 98 finished with value: 0.8975437147184359 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:36,927] Trial 99 finished with value: 0.928007710312543 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:38,974] Trial 100 finished with value: 0.9176042957455598 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:40,226] Trial 101 finished with value: 0.9153765661572353 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:42,948] Trial 102 finished with value: 0.9190885309100922 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:44,413] Trial 103 finished with value: 0.9294836844279224 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:45,530] Trial 104 finished with value: 0.9057276607462482 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:49,537] Trial 105 finished with value: 0.9294891917940244 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.9346853917114141.\n[I 2023-05-24 03:36:51,709] Trial 106 finished with value: 0.9354261324521549 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 106 with value: 0.9354261324521549.\n[I 2023-05-24 03:36:53,457] Trial 107 finished with value: 0.9279966955803387 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 106 with value: 0.9354261324521549.\n\n\n\nstudy.best_params\n\n{'n_estimators': 150,\n 'criterion': 'entropy',\n 'min_samples_split': 3,\n 'max_depth': 7,\n 'max_features': 'sqrt'}\n\n\n\n\n13.2.2 Random Search\nWhile using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favorable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n\nA budget can be chosen independent of the number of parameters and possible values.\nAdding parameters that do not influence the performance does not decrease efficiency.\n\nSpecifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for GridSearchCV. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the n_iter parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified.\n\n# create a random search object\nrs = RandomizedSearchCV(estimator=rfc,\n                  param_distributions=param_grid,\n                  scoring='f1_micro',\n                  cv=5,\n                  n_jobs=-1,\n                  verbose=2,\n                  n_iter=50)\n\nrs.fit(X_train, y_train)\n\nHere, we will use Optuna instead:\n\n# create a study (aim to maximize score)\nstudy = optuna.create_study(sampler=RandomSampler(), direction='maximize')\n\n# perform hyperparamter tuning (while timing the process)\ntime_start = time.time()\nstudy.optimize(objective, show_progress_bar=True, n_trials=50)\ntime_random = time.time() - time_start\n\n# store result in a data frame \ncolumns = ['Number of iterations', 'Iteration Number of Optimal Hyperparamters', 'Score', 'Time Elapsed (s)']\nvalues_random = [50, study.best_trial.number, study.best_trial.value, time_random]\nresults_random = pd.DataFrame([values_random], columns = columns)\n\n[I 2023-05-24 03:36:53,501] A new study created in memory with name: no-name-6b952135-06c4-4c62-bfe4-10f435063be0\n/usr/local/lib/python3.10/dist-packages/optuna/progress_bar.py:56: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n  self._init_valid()\n\n\n\n\n\n[I 2023-05-24 03:36:56,505] Trial 0 finished with value: 0.9116618477213272 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9116618477213272.\n[I 2023-05-24 03:36:58,816] Trial 1 finished with value: 0.9064628941208868 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 0 with value: 0.9116618477213272.\n[I 2023-05-24 03:37:00,477] Trial 2 finished with value: 0.9146413327825968 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9146413327825968.\n[I 2023-05-24 03:37:02,120] Trial 3 finished with value: 0.8938372573316811 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 2 with value: 0.9146413327825968.\n[I 2023-05-24 03:37:03,514] Trial 4 finished with value: 0.9294919454770756 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 4 with value: 0.9294919454770756.\n[I 2023-05-24 03:37:06,530] Trial 5 finished with value: 0.9094203497177474 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 4 with value: 0.9294919454770756.\n[I 2023-05-24 03:37:09,683] Trial 6 finished with value: 0.9146358254164946 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 4 with value: 0.9294919454770756.\n[I 2023-05-24 03:37:12,148] Trial 7 finished with value: 0.9176042957455597 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 4 with value: 0.9294919454770756.\n[I 2023-05-24 03:37:13,912] Trial 8 finished with value: 0.9287401900041307 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 4 with value: 0.9294919454770756.\n[I 2023-05-24 03:37:16,052] Trial 9 finished with value: 0.8967947129285418 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 4 with value: 0.9294919454770756.\n[I 2023-05-24 03:37:18,753] Trial 10 finished with value: 0.9190885309100922 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 4 with value: 0.9294919454770756.\n[I 2023-05-24 03:37:19,989] Trial 11 finished with value: 0.9287401900041307 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 4 with value: 0.9294919454770756.\n[I 2023-05-24 03:37:21,311] Trial 12 finished with value: 0.9309596585433016 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:22,570] Trial 13 finished with value: 0.9287401900041307 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:24,167] Trial 14 finished with value: 0.9168580476387168 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:25,251] Trial 15 finished with value: 0.9057276607462482 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:26,730] Trial 16 finished with value: 0.9109100922483823 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:29,249] Trial 17 finished with value: 0.9086933773922622 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:31,401] Trial 18 finished with value: 0.9153738124741843 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:32,559] Trial 19 finished with value: 0.9064601404378356 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:34,848] Trial 20 finished with value: 0.913148836568911 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:36,454] Trial 21 finished with value: 0.915371058791133 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:38,933] Trial 22 finished with value: 0.9183395291201982 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:41,200] Trial 23 finished with value: 0.8967947129285418 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:42,603] Trial 24 finished with value: 0.9109211069805866 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:44,407] Trial 25 finished with value: 0.9294864381109734 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:46,818] Trial 26 finished with value: 0.9072174032768829 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:48,624] Trial 27 finished with value: 0.9168580476387168 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:51,256] Trial 28 finished with value: 0.9294754233787691 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:52,510] Trial 29 finished with value: 0.924292991876635 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:54,237] Trial 30 finished with value: 0.9131626049841663 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:56,439] Trial 31 finished with value: 0.9079498829684702 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:58,165] Trial 32 finished with value: 0.9220570012391572 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:37:59,389] Trial 33 finished with value: 0.9027509293680298 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:00,889] Trial 34 finished with value: 0.89605121850475 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:02,171] Trial 35 finished with value: 0.9168552939556657 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:04,603] Trial 36 finished with value: 0.9072174032768829 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:07,636] Trial 37 finished with value: 0.9250227178851714 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:10,114] Trial 38 finished with value: 0.9072063885446784 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:11,235] Trial 39 finished with value: 0.9235467437697921 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:13,222] Trial 40 finished with value: 0.9250227178851714 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:14,296] Trial 41 finished with value: 0.9057276607462482 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:17,222] Trial 42 finished with value: 0.9294891917940244 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:18,473] Trial 43 finished with value: 0.9027509293680298 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:20,391] Trial 44 finished with value: 0.9309596585433016 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:22,972] Trial 45 finished with value: 0.9176042957455598 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:25,420] Trial 46 finished with value: 0.9072174032768829 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:27,583] Trial 47 finished with value: 0.9094203497177474 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:30,060] Trial 48 finished with value: 0.9116618477213272 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.9309596585433016.\n[I 2023-05-24 03:38:32,588] Trial 49 finished with value: 0.9101831199228968 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 12 with value: 0.9309596585433016.\n\n\n\nstudy.best_params\n\n{'n_estimators': 100,\n 'criterion': 'entropy',\n 'min_samples_split': 2,\n 'max_depth': 7,\n 'max_features': 'log2'}\n\n\n\n\n13.2.3 Bayesian Optimization\nFinally, we perform hyperparameter tuning with the Bayesian optimization and time the process. In Python, this can be accomplished with the Optuna module.\n\n# create a study (aim to maximize score)\nstudy = optuna.create_study(sampler=TPESampler(), direction='maximize')\n\n# perform hyperparamter tuning (while timing the process)\ntime_start = time.time()\nstudy.optimize(objective, show_progress_bar=True, n_trials=50)\ntime_bayesian = time.time() - time_start\n\n# store result in a data frame \nvalues_bayesian = [50, study.best_trial.number, study.best_trial.value, time_bayesian]\nresults_bayesian = pd.DataFrame([values_bayesian], columns = columns)\n\n[I 2023-05-24 03:39:05,603] A new study created in memory with name: no-name-fc37470e-3f6d-4b2a-ba76-90827f032e79\n/usr/local/lib/python3.10/dist-packages/optuna/progress_bar.py:56: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n  self._init_valid()\n\n\n\n\n\n[I 2023-05-24 03:39:10,555] Trial 0 finished with value: 0.90868787002616 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.90868787002616.\n[I 2023-05-24 03:39:13,367] Trial 1 finished with value: 0.9220570012391572 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 1 with value: 0.9220570012391572.\n[I 2023-05-24 03:39:14,553] Trial 2 finished with value: 0.9205782734407271 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.9220570012391572.\n[I 2023-05-24 03:39:16,294] Trial 3 finished with value: 0.9257662123089634 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 3 with value: 0.9257662123089634.\n[I 2023-05-24 03:39:17,825] Trial 4 finished with value: 0.9161145532149249 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.9257662123089634.\n[I 2023-05-24 03:39:19,972] Trial 5 finished with value: 0.8938372573316811 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 3 with value: 0.9257662123089634.\n[I 2023-05-24 03:39:21,584] Trial 6 finished with value: 0.9057221533801461 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.9257662123089634.\n[I 2023-05-24 03:39:24,523] Trial 7 finished with value: 0.909436871816054 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 3 with value: 0.9257662123089634.\n[I 2023-05-24 03:39:27,335] Trial 8 finished with value: 0.9101831199228968 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 5, 'max_features': 'log2'}. Best is trial 3 with value: 0.9257662123089634.\n[I 2023-05-24 03:39:29,828] Trial 9 finished with value: 0.9250227178851714 and parameters: {'n_estimators': 150, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 3 with value: 0.9257662123089634.\n[I 2023-05-24 03:39:31,891] Trial 10 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:39:33,954] Trial 11 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:39:36,098] Trial 12 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:39:40,490] Trial 13 finished with value: 0.912405342145119 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:39:42,994] Trial 14 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:39:44,695] Trial 15 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:39:47,372] Trial 16 finished with value: 0.912405342145119 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:39:49,691] Trial 17 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:39:52,292] Trial 18 finished with value: 0.9146385790995456 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:39:54,780] Trial 19 finished with value: 0.9235439900867408 and parameters: {'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:39:56,426] Trial 20 finished with value: 0.9124136031942722 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:39:58,119] Trial 21 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:39:59,827] Trial 22 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:40:01,533] Trial 23 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:40:03,474] Trial 24 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:40:05,210] Trial 25 finished with value: 0.9235467437697921 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:40:09,968] Trial 26 finished with value: 0.9235467437697921 and parameters: {'n_estimators': 200, 'criterion': 'entropy', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:40:11,895] Trial 27 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:40:14,346] Trial 28 finished with value: 0.9257662123089634 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:40:15,609] Trial 29 finished with value: 0.9287401900041307 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:40:17,691] Trial 30 finished with value: 0.9124136031942722 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 6, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:40:20,106] Trial 31 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:40:21,825] Trial 32 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:40:24,146] Trial 33 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:40:25,892] Trial 34 finished with value: 0.9287512047363349 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:40:27,630] Trial 35 finished with value: 0.9220570012391572 and parameters: {'n_estimators': 150, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 10 with value: 0.9287512047363349.\n[I 2023-05-24 03:40:28,882] Trial 36 finished with value: 0.9309651659094038 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 36 with value: 0.9309651659094038.\n[I 2023-05-24 03:40:30,523] Trial 37 finished with value: 0.9161145532149249 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 36 with value: 0.9309651659094038.\n[I 2023-05-24 03:40:32,396] Trial 38 finished with value: 0.9309651659094038 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 36 with value: 0.9309651659094038.\n[I 2023-05-24 03:40:33,900] Trial 39 finished with value: 0.9309651659094038 and parameters: {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 36 with value: 0.9309651659094038.\n[I 2023-05-24 03:40:35,399] Trial 40 finished with value: 0.9331956491807792 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 40 with value: 0.9331956491807792.\n[I 2023-05-24 03:40:36,926] Trial 41 finished with value: 0.9331956491807792 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 40 with value: 0.9331956491807792.\n[I 2023-05-24 03:40:38,393] Trial 42 finished with value: 0.9331956491807792 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 40 with value: 0.9331956491807792.\n[I 2023-05-24 03:40:39,915] Trial 43 finished with value: 0.9331956491807792 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 40 with value: 0.9331956491807792.\n[I 2023-05-24 03:40:41,399] Trial 44 finished with value: 0.9331956491807792 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 40 with value: 0.9331956491807792.\n[I 2023-05-24 03:40:42,900] Trial 45 finished with value: 0.9331956491807792 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 40 with value: 0.9331956491807792.\n[I 2023-05-24 03:40:44,694] Trial 46 finished with value: 0.9057221533801461 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 3, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 40 with value: 0.9331956491807792.\n[I 2023-05-24 03:40:46,780] Trial 47 finished with value: 0.9346853917114141 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 47 with value: 0.9346853917114141.\n[I 2023-05-24 03:40:48,298] Trial 48 finished with value: 0.9346853917114141 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 47 with value: 0.9346853917114141.\n[I 2023-05-24 03:40:49,700] Trial 49 finished with value: 0.922048740190004 and parameters: {'n_estimators': 100, 'criterion': 'entropy', 'min_samples_split': 2, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 47 with value: 0.9346853917114141.\n\n\n\nstudy.best_params\n\n{'n_estimators': 100,\n 'criterion': 'entropy',\n 'min_samples_split': 2,\n 'max_depth': 7,\n 'max_features': 'sqrt'}\n\n\nWe summarized the results below:\n\n# store all results in a single data frame\ndf = results_grid.append(results_random).append(results_bayesian)\ndf.index = ['Grid Search', 'Random Search', 'Bayesian Optimization']\ndf\n\nFutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = results_grid.append(results_random).append(results_bayesian)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nNumber of iterations\nIteration Number of Optimal Hyperparamters\nScore\nTime Elapsed (s)\n\n\n\n\nGrid Search\n108\n106\n0.935426\n236.524639\n\n\nRandom Search\n50\n12\n0.930960\n99.094361\n\n\nBayesian Optimization\n50\n47\n0.934685\n104.101121\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe grid search registered the highest score. However, the method required more trials and only managed to obtain the optimal hyperparameters at the 106th iteration. Also, its run time far exceeded that of the random search and the Bayesian optimization methods. The random search method required only 50 trials and needed only 12 iterations to find the best hyperparameter set. It also took the least amount of time to execute. However, the random search method registered the lowest score out of the 3 methods. The Bayesian optimization also performed 50 trials but was able to achieve the highest score after only 47 iterations, far less than the grid search. Although it executed the same number of trials as the random search, it has a longer run time since it is an informed search method."
  },
  {
    "objectID": "13_Hyperparameter.html#hyperparamter-tuning-using-kerastuner",
    "href": "13_Hyperparameter.html#hyperparamter-tuning-using-kerastuner",
    "title": "13  Hyperparameter Tuning",
    "section": "13.3 Hyperparamter tuning using KerasTuner",
    "text": "13.3 Hyperparamter tuning using KerasTuner\nTraining the weights of a model is relatively easy: you compute a loss function on a mini-batch of data and then use backpropagation to move the weights in the right direction. Updating hyperparameters, on the other hand, presents unique challenges. Consider these points:\n\nThe hyperparameter space is typically made up of discrete decisions and thus isn’t continuous or differentiable. Hence, you typically can’t do gradient descent in hyperparameter space. Instead, you must rely on gradient-free optimization techniques, which naturally are far less efficient than gradient descent.\nComputing the feedback signal of this optimization process (does this set of hyperparameters lead to a high-performing model on this task?) can be extremely expensive: it requires creating and training a new model from scratch on your dataset.\nThe feedback signal may be noisy: if a training run performs 0.2% better, is that because of a better model configuration, or because you got lucky with the initial weight values?\n\nThankfully, there’s a tool that makes hyperparameter tuning simpler: KerasTuner. Let’s check it out.\nKerasTuner lets you replace hard-coded hyperparameter values, such as units=32, with a range of possible choices, such as Int(name=\"units\", min_value=16, max_value=64, step=16). This set of choices in a given model is called the search space of the hyperparameter tuning process.\nTo specify a search space, define a model-building function. It takes an hp argument, from which you can sample hyperparameter ranges, and it returns a compiled Keras model.\n\n13.3.1 Tune model architecture\n\ndef build_model(hp):\n    # Sample hyperparameter values from the hp object. After sampling, these values (such as the \"units\" which is number of nuerons \n    # variable here) are just regular Python constants.\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Flatten())\n    # Tune the number of layers.\n    for i in range(hp.Int(\"num_layers\", 1, 3)):\n        model.add(        \n            tf.keras.layers.Dense(\n            # Tune number of units.\n            units=hp.Int(\"units\", min_value=16, max_value=64, step=16),\n            # Tune the activation function to use.\n            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n            )\n        )\n    # Tune whether to use dropout.\n    if hp.Boolean(\"dropout\"):\n        model.add(tf.keras.layers.Dropout(rate=0.25))\n    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n    # Define the optimizer learning rate as a hyperparameter.\n    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n    optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n    model.compile(\n        optimizer=optimizer,\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"])\n    return model\n\nIf you want to adopt a more modular and configurable approach to model-building, you can also subclass the HyperModel class and define a build() method, as follows.\n\nclass SimpleMLP(kt.HyperModel):\n    # Thanks to the object-oriented approach, we can configure model constants\n    # as constructor arguments (instead of hardcoding them in the model-building\n    # function).\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n    # The build() method is identical to our prior build_model() standalone function.\n    def build(self, hp):\n      # Sample hyperparameter values from the hp object. After sampling, these values (such as the \"units\" which is number of nuerons \n      # variable here) are just regular Python constants.\n        model = tf.keras.keras.Sequential()\n        model.add(tf.keras.layers.Flatten())\n        # Tune the number of layers.\n        for i in range(hp.Int(\"num_layers\", 1, 3)):\n            model.add(        \n              tf.keras.layers.Dense(\n              # Tune number of units.\n              units=hp.Int(\"units\", min_value=16, max_value=64, step=16),\n              # Tune the activation function to use.\n              activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n              )\n            )\n        # Tune whether to use dropout.\n        if hp.Boolean(\"dropout\"):\n            model.add(tf.keras.layers.Dropout(rate=0.25))\n        model.add(tf.keras.layers.Dense(self.num_classes, activation=\"softmax\"))\n        # Define the optimizer learning rate as a hyperparameter.\n        learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n        optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n        model.compile(\n            optimizer=optimizer,\n            loss=\"sparse_categorical_crossentropy\",\n            metrics=[\"accuracy\"])\n        return model\n\nhypermodel = SimpleMLP(num_classes=10)\n\nThe next step is to define a “tuner.” Schematically, you can think of a tuner as a for loop that will repeatedly  1. Pick a set of hyperparameter values 2. Call the model-building function with these values to create a model 3. Train the model and record its metrics\nKerasTuner has several built-in tuners available— RandomSearch, BayesianOptimization, and Hyperband. Let’s try BayesianOptimization, a tuner that attempts to make smart predictions for which new hyperparameter values are likely to perform best given the outcomes of previous choices:\n\ntuner = kt.BayesianOptimization(\n    build_model,\n    objective=\"val_accuracy\",\n    max_trials=3, # Increase this if you would like to do more search\n    executions_per_trial=2,\n    directory=\"mnist_kt_test\",\n    overwrite=True,\n)\n\n\nobjective: Specify the metric that the tuner will seek to optimize. Always specify validation metrics, since the goal of the search process is to find models that generalize!\nmax_trials: Maximum number of different model configurations (“trials”) to try before ending the search.\nexecutions_per_trial: To reduce metrics variance, you can train the same model multiple times and average the results. executions_per_trial is how many training rounds(executions) to run for each model configuration (trial).\ndirectory: Where to store search logs\noverwrite: Whether to overwrite data in directory to start a new search. Set this to True if you’ve modified the model-building function, or to False to resume a previously started search with the same model-building function.\n\nYou can display an overview of the search space via search_space_summary():\n\ntuner.search_space_summary()\n\nSearch space summary\nDefault search space size: 6\nnum_layers (Int)\n{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': 'linear'}\nunits (Int)\n{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': 'linear'}\nactivation (Choice)\n{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\ndropout (Boolean)\n{'default': False, 'conditions': []}\nlr (Float)\n{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\noptimizer (Choice)\n{'default': 'rmsprop', 'conditions': [], 'values': ['rmsprop', 'adam'], 'ordered': False}\n\n\nFinally, let’s launch the search. Don’t forget to pass validation data, and make sure not to use your test set as validation data — otherwise you’d quickly start overfitting to your test data, and you wouldn’t be able to trust your test metrics anymore:\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train = x_train.reshape((-1, 28 * 28)).astype(\"float32\") / 255\nx_test = x_test.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n# Reserve these for later.\nx_train_full = x_train[:]\ny_train_full = y_train[:]\nnum_val_samples = 10000\n# Set these aside as a validation set.\nx_train, x_val = x_train[:-num_val_samples], x_train[-num_val_samples:]\ny_train, y_val = y_train[:-num_val_samples], y_train[-num_val_samples:]\n\n# This takes the same arguments as fit() (it simply passes them\n# down to fit() for each new model).\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5),\n]\n# Use a large number of epochs (you don’t know in advance how\n# many epochs each model will need), and use an EarlyStopping\n# callback to stop training when you start overfitting.\ntuner.search(\n    x_train, y_train,\n    batch_size=128,\n    epochs=100,\n    validation_data=(x_val, y_val),\n    callbacks=callbacks,\n    verbose=2,\n)\n\nTrial 3 Complete [00h 03m 17s]\nval_accuracy: 0.9639500081539154\n\nBest val_accuracy So Far: 0.9647500216960907\nTotal elapsed time: 00h 07m 45s\n\n\nThe preceding example will run in just a few minutes, since we’re only looking at a few possible choices and we’re training on MNIST. However, with a typical search space and dataset, you’ll often find yourself letting the hyperparameter search run overnight or even over several days. If your search process crashes, you can always restart it — just specify overwrite=False in the tuner so that it can resume from the trial logs stored on disk.\nOnce the search is complete, you can query the best hyperparameter configurations, which you can use to create high-performing models that you can then retrain.\n\n13.3.1.1 Querying the best hyperparameter configurations\n\ntop_n = 3\nbest_hps = tuner.get_best_hyperparameters(top_n)\n\nUsually, when retraining these models, you may want to include the validation data as part of the training data, since you won’t be making any further hyperparameter changes, and thus you will no longer be evaluating performance on the validation data. In our example, we’d train these final models on the totality of the original MNIST training data, without reserving a validation set.\nBefore we can train on the full training data, though, there’s one last parameter we need to settle: the optimal number of epochs to train for. Typically, you’ll want to train the new models for longer than you did during the search: using an aggressive patience value in the EarlyStopping callback saves time during the search, but it may lead to under-fit models. Just use the validation set to find the best epoch:\n\ndef get_best_epoch(hp):\n    model = build_model(hp)\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor=\"val_loss\", mode=\"min\", patience=10)\n    ]\n    history = model.fit(\n        x_train, y_train,\n        validation_data=(x_val, y_val),\n        epochs=100,\n        batch_size=128,\n        callbacks=callbacks)\n    val_loss_per_epoch = history.history[\"val_loss\"]\n    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n    print(f\"Best epoch: {best_epoch}\")\n    return best_epoch, model\n\nFinally, train on the full dataset for just a bit longer than this epoch count, since you’re training on more data; 20% more in this case:\n\ndef get_best_trained_model(hp):\n    best_epoch, model = get_best_epoch(hp)\n    model.fit(\n        x_train_full, y_train_full,\n        batch_size=128, epochs=int(best_epoch * 1.2))\n    return model\n\nbest_models = []\nfor hp in best_hps:\n    model = get_best_trained_model(hp)\n    model.evaluate(x_test, y_test)\n    best_models.append(model)\n\nEpoch 1/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.6390 - accuracy: 0.8321 - val_loss: 0.3086 - val_accuracy: 0.9149\nEpoch 2/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.3483 - accuracy: 0.9005 - val_loss: 0.2482 - val_accuracy: 0.9301\nEpoch 3/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.3063 - accuracy: 0.9109 - val_loss: 0.2270 - val_accuracy: 0.9338\nEpoch 4/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.2823 - accuracy: 0.9172 - val_loss: 0.2175 - val_accuracy: 0.9370\nEpoch 5/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.2644 - accuracy: 0.9217 - val_loss: 0.1991 - val_accuracy: 0.9419\nEpoch 6/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.2518 - accuracy: 0.9260 - val_loss: 0.1919 - val_accuracy: 0.9449\nEpoch 7/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.2422 - accuracy: 0.9277 - val_loss: 0.1835 - val_accuracy: 0.9472\nEpoch 8/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.2323 - accuracy: 0.9309 - val_loss: 0.1832 - val_accuracy: 0.9470\nEpoch 9/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.2272 - accuracy: 0.9331 - val_loss: 0.1756 - val_accuracy: 0.9493\nEpoch 10/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.2183 - accuracy: 0.9343 - val_loss: 0.1736 - val_accuracy: 0.9484\nEpoch 11/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.2174 - accuracy: 0.9346 - val_loss: 0.1692 - val_accuracy: 0.9494\nEpoch 12/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.2089 - accuracy: 0.9369 - val_loss: 0.1663 - val_accuracy: 0.9506\nEpoch 13/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.2060 - accuracy: 0.9383 - val_loss: 0.1634 - val_accuracy: 0.9516\nEpoch 14/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.2036 - accuracy: 0.9385 - val_loss: 0.1627 - val_accuracy: 0.9515\nEpoch 15/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.1955 - accuracy: 0.9411 - val_loss: 0.1595 - val_accuracy: 0.9525\nEpoch 16/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1971 - accuracy: 0.9392 - val_loss: 0.1555 - val_accuracy: 0.9550\nEpoch 17/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1896 - accuracy: 0.9429 - val_loss: 0.1528 - val_accuracy: 0.9551\nEpoch 18/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1891 - accuracy: 0.9416 - val_loss: 0.1522 - val_accuracy: 0.9541\nEpoch 19/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1878 - accuracy: 0.9426 - val_loss: 0.1499 - val_accuracy: 0.9544\nEpoch 20/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1829 - accuracy: 0.9431 - val_loss: 0.1477 - val_accuracy: 0.9560\nEpoch 21/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1805 - accuracy: 0.9438 - val_loss: 0.1475 - val_accuracy: 0.9558\nEpoch 22/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1795 - accuracy: 0.9454 - val_loss: 0.1484 - val_accuracy: 0.9567\nEpoch 23/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.1794 - accuracy: 0.9450 - val_loss: 0.1475 - val_accuracy: 0.9557\nEpoch 24/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.1745 - accuracy: 0.9454 - val_loss: 0.1478 - val_accuracy: 0.9561\nEpoch 25/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1721 - accuracy: 0.9456 - val_loss: 0.1452 - val_accuracy: 0.9563\nEpoch 26/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1700 - accuracy: 0.9470 - val_loss: 0.1445 - val_accuracy: 0.9567\nEpoch 27/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1710 - accuracy: 0.9459 - val_loss: 0.1441 - val_accuracy: 0.9569\nEpoch 28/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1660 - accuracy: 0.9485 - val_loss: 0.1442 - val_accuracy: 0.9562\nEpoch 29/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1698 - accuracy: 0.9472 - val_loss: 0.1454 - val_accuracy: 0.9564\nEpoch 30/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1645 - accuracy: 0.9495 - val_loss: 0.1453 - val_accuracy: 0.9568\nEpoch 31/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1643 - accuracy: 0.9491 - val_loss: 0.1440 - val_accuracy: 0.9575\nEpoch 32/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.1615 - accuracy: 0.9498 - val_loss: 0.1406 - val_accuracy: 0.9583\nEpoch 33/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.1592 - accuracy: 0.9505 - val_loss: 0.1425 - val_accuracy: 0.9569\nEpoch 34/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1586 - accuracy: 0.9510 - val_loss: 0.1415 - val_accuracy: 0.9575\nEpoch 35/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1589 - accuracy: 0.9494 - val_loss: 0.1400 - val_accuracy: 0.9583\nEpoch 36/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1552 - accuracy: 0.9510 - val_loss: 0.1384 - val_accuracy: 0.9597\nEpoch 37/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1565 - accuracy: 0.9503 - val_loss: 0.1397 - val_accuracy: 0.9597\nEpoch 38/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1565 - accuracy: 0.9502 - val_loss: 0.1386 - val_accuracy: 0.9584\nEpoch 39/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1542 - accuracy: 0.9521 - val_loss: 0.1395 - val_accuracy: 0.9586\nEpoch 40/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1523 - accuracy: 0.9507 - val_loss: 0.1364 - val_accuracy: 0.9592\nEpoch 41/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.1529 - accuracy: 0.9516 - val_loss: 0.1383 - val_accuracy: 0.9581\nEpoch 42/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.1498 - accuracy: 0.9524 - val_loss: 0.1407 - val_accuracy: 0.9592\nEpoch 43/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1519 - accuracy: 0.9531 - val_loss: 0.1365 - val_accuracy: 0.9588\nEpoch 44/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1477 - accuracy: 0.9532 - val_loss: 0.1368 - val_accuracy: 0.9604\nEpoch 45/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1476 - accuracy: 0.9534 - val_loss: 0.1397 - val_accuracy: 0.9591\nEpoch 46/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1488 - accuracy: 0.9529 - val_loss: 0.1375 - val_accuracy: 0.9605\nEpoch 47/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1453 - accuracy: 0.9528 - val_loss: 0.1376 - val_accuracy: 0.9594\nEpoch 48/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1467 - accuracy: 0.9536 - val_loss: 0.1385 - val_accuracy: 0.9582\nEpoch 49/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1446 - accuracy: 0.9531 - val_loss: 0.1374 - val_accuracy: 0.9594\nEpoch 50/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.1442 - accuracy: 0.9544 - val_loss: 0.1375 - val_accuracy: 0.9590\nBest epoch: 40\nEpoch 1/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1570 - accuracy: 0.9517\nEpoch 2/48\n469/469 [==============================] - 2s 3ms/step - loss: 0.1515 - accuracy: 0.9519\nEpoch 3/48\n469/469 [==============================] - 2s 4ms/step - loss: 0.1514 - accuracy: 0.9518\nEpoch 4/48\n469/469 [==============================] - 2s 3ms/step - loss: 0.1493 - accuracy: 0.9534\nEpoch 5/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1486 - accuracy: 0.9533\nEpoch 6/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1473 - accuracy: 0.9533\nEpoch 7/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1482 - accuracy: 0.9540\nEpoch 8/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1469 - accuracy: 0.9540\nEpoch 9/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1439 - accuracy: 0.9549\nEpoch 10/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1460 - accuracy: 0.9542\nEpoch 11/48\n469/469 [==============================] - 2s 3ms/step - loss: 0.1441 - accuracy: 0.9545\nEpoch 12/48\n469/469 [==============================] - 2s 4ms/step - loss: 0.1451 - accuracy: 0.9542\nEpoch 13/48\n469/469 [==============================] - 2s 3ms/step - loss: 0.1434 - accuracy: 0.9551\nEpoch 14/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1422 - accuracy: 0.9554\nEpoch 15/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1445 - accuracy: 0.9537\nEpoch 16/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1421 - accuracy: 0.9547\nEpoch 17/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1391 - accuracy: 0.9544\nEpoch 18/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1382 - accuracy: 0.9556\nEpoch 19/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1403 - accuracy: 0.9557\nEpoch 20/48\n469/469 [==============================] - 2s 3ms/step - loss: 0.1404 - accuracy: 0.9551\nEpoch 21/48\n469/469 [==============================] - 2s 4ms/step - loss: 0.1373 - accuracy: 0.9558\nEpoch 22/48\n469/469 [==============================] - 2s 3ms/step - loss: 0.1385 - accuracy: 0.9556\nEpoch 23/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1371 - accuracy: 0.9557\nEpoch 24/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1383 - accuracy: 0.9561\nEpoch 25/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1359 - accuracy: 0.9559\nEpoch 26/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1372 - accuracy: 0.9557\nEpoch 27/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1352 - accuracy: 0.9569\nEpoch 28/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1364 - accuracy: 0.9574\nEpoch 29/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1362 - accuracy: 0.9561\nEpoch 30/48\n469/469 [==============================] - 2s 4ms/step - loss: 0.1334 - accuracy: 0.9574\nEpoch 31/48\n469/469 [==============================] - 2s 3ms/step - loss: 0.1348 - accuracy: 0.9562\nEpoch 32/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1327 - accuracy: 0.9569\nEpoch 33/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1332 - accuracy: 0.9575\nEpoch 34/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1331 - accuracy: 0.9576\nEpoch 35/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1321 - accuracy: 0.9571\nEpoch 36/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1350 - accuracy: 0.9564\nEpoch 37/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1319 - accuracy: 0.9568\nEpoch 38/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1330 - accuracy: 0.9578\nEpoch 39/48\n469/469 [==============================] - 2s 4ms/step - loss: 0.1290 - accuracy: 0.9582\nEpoch 40/48\n469/469 [==============================] - 2s 4ms/step - loss: 0.1278 - accuracy: 0.9589\nEpoch 41/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1317 - accuracy: 0.9574\nEpoch 42/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1303 - accuracy: 0.9576\nEpoch 43/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1314 - accuracy: 0.9579\nEpoch 44/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1284 - accuracy: 0.9586\nEpoch 45/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1255 - accuracy: 0.9601\nEpoch 46/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1299 - accuracy: 0.9586\nEpoch 47/48\n469/469 [==============================] - 1s 3ms/step - loss: 0.1273 - accuracy: 0.9591\nEpoch 48/48\n469/469 [==============================] - 2s 4ms/step - loss: 0.1266 - accuracy: 0.9583\n313/313 [==============================] - 1s 3ms/step - loss: 0.1475 - accuracy: 0.9569\nEpoch 1/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.5987 - accuracy: 0.8426 - val_loss: 0.2958 - val_accuracy: 0.9201\nEpoch 2/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.3405 - accuracy: 0.9019 - val_loss: 0.2486 - val_accuracy: 0.9301\nEpoch 3/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.3002 - accuracy: 0.9136 - val_loss: 0.2239 - val_accuracy: 0.9345\nEpoch 4/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.2793 - accuracy: 0.9193 - val_loss: 0.2085 - val_accuracy: 0.9403\nEpoch 5/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.2616 - accuracy: 0.9235 - val_loss: 0.1980 - val_accuracy: 0.9445\nEpoch 6/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.2541 - accuracy: 0.9255 - val_loss: 0.1880 - val_accuracy: 0.9456\nEpoch 7/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.2427 - accuracy: 0.9283 - val_loss: 0.1822 - val_accuracy: 0.9481\nEpoch 8/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.2367 - accuracy: 0.9301 - val_loss: 0.1794 - val_accuracy: 0.9491\nEpoch 9/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.2301 - accuracy: 0.9317 - val_loss: 0.1697 - val_accuracy: 0.9511\nEpoch 10/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.2242 - accuracy: 0.9317 - val_loss: 0.1670 - val_accuracy: 0.9507\nEpoch 11/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.2195 - accuracy: 0.9353 - val_loss: 0.1637 - val_accuracy: 0.9514\nEpoch 12/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.2153 - accuracy: 0.9359 - val_loss: 0.1613 - val_accuracy: 0.9541\nEpoch 13/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.2090 - accuracy: 0.9374 - val_loss: 0.1577 - val_accuracy: 0.9539\nEpoch 14/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.2094 - accuracy: 0.9368 - val_loss: 0.1544 - val_accuracy: 0.9557\nEpoch 15/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.2035 - accuracy: 0.9385 - val_loss: 0.1533 - val_accuracy: 0.9556\nEpoch 16/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.2023 - accuracy: 0.9396 - val_loss: 0.1479 - val_accuracy: 0.9571\nEpoch 17/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.1954 - accuracy: 0.9416 - val_loss: 0.1474 - val_accuracy: 0.9575\nEpoch 18/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.1979 - accuracy: 0.9399 - val_loss: 0.1500 - val_accuracy: 0.9555\nEpoch 19/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1922 - accuracy: 0.9421 - val_loss: 0.1497 - val_accuracy: 0.9562\nEpoch 20/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1919 - accuracy: 0.9413 - val_loss: 0.1442 - val_accuracy: 0.9578\nEpoch 21/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1882 - accuracy: 0.9425 - val_loss: 0.1439 - val_accuracy: 0.9586\nEpoch 22/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1896 - accuracy: 0.9416 - val_loss: 0.1458 - val_accuracy: 0.9575\nEpoch 23/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1858 - accuracy: 0.9432 - val_loss: 0.1410 - val_accuracy: 0.9580\nEpoch 24/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1838 - accuracy: 0.9439 - val_loss: 0.1396 - val_accuracy: 0.9591\nEpoch 25/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1807 - accuracy: 0.9447 - val_loss: 0.1391 - val_accuracy: 0.9592\nEpoch 26/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.1814 - accuracy: 0.9444 - val_loss: 0.1402 - val_accuracy: 0.9594\nEpoch 27/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.1790 - accuracy: 0.9450 - val_loss: 0.1397 - val_accuracy: 0.9596\nEpoch 28/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1744 - accuracy: 0.9473 - val_loss: 0.1368 - val_accuracy: 0.9612\nEpoch 29/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1757 - accuracy: 0.9456 - val_loss: 0.1369 - val_accuracy: 0.9605\nEpoch 30/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1763 - accuracy: 0.9449 - val_loss: 0.1375 - val_accuracy: 0.9594\nEpoch 31/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1748 - accuracy: 0.9462 - val_loss: 0.1392 - val_accuracy: 0.9600\nEpoch 32/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1719 - accuracy: 0.9469 - val_loss: 0.1389 - val_accuracy: 0.9593\nEpoch 33/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1741 - accuracy: 0.9466 - val_loss: 0.1366 - val_accuracy: 0.9605\nEpoch 34/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1718 - accuracy: 0.9462 - val_loss: 0.1338 - val_accuracy: 0.9613\nEpoch 35/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.1679 - accuracy: 0.9477 - val_loss: 0.1342 - val_accuracy: 0.9601\nEpoch 36/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.1669 - accuracy: 0.9478 - val_loss: 0.1345 - val_accuracy: 0.9622\nEpoch 37/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1653 - accuracy: 0.9483 - val_loss: 0.1349 - val_accuracy: 0.9598\nEpoch 38/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1666 - accuracy: 0.9482 - val_loss: 0.1308 - val_accuracy: 0.9617\nEpoch 39/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1651 - accuracy: 0.9501 - val_loss: 0.1339 - val_accuracy: 0.9616\nEpoch 40/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1637 - accuracy: 0.9498 - val_loss: 0.1336 - val_accuracy: 0.9609\nEpoch 41/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1650 - accuracy: 0.9490 - val_loss: 0.1356 - val_accuracy: 0.9610\nEpoch 42/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1608 - accuracy: 0.9503 - val_loss: 0.1338 - val_accuracy: 0.9604\nEpoch 43/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1604 - accuracy: 0.9508 - val_loss: 0.1325 - val_accuracy: 0.9622\nEpoch 44/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1590 - accuracy: 0.9506 - val_loss: 0.1319 - val_accuracy: 0.9622\nEpoch 45/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.1601 - accuracy: 0.9500 - val_loss: 0.1325 - val_accuracy: 0.9624\nEpoch 46/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.1582 - accuracy: 0.9512 - val_loss: 0.1326 - val_accuracy: 0.9610\nEpoch 47/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.1589 - accuracy: 0.9515 - val_loss: 0.1325 - val_accuracy: 0.9610\nEpoch 48/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.1568 - accuracy: 0.9510 - val_loss: 0.1322 - val_accuracy: 0.9602\nBest epoch: 38\nEpoch 1/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1642 - accuracy: 0.9503\nEpoch 2/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1640 - accuracy: 0.9500\nEpoch 3/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1586 - accuracy: 0.9511\nEpoch 4/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1616 - accuracy: 0.9499\nEpoch 5/45\n469/469 [==============================] - 2s 3ms/step - loss: 0.1610 - accuracy: 0.9504\nEpoch 6/45\n469/469 [==============================] - 2s 4ms/step - loss: 0.1594 - accuracy: 0.9506\nEpoch 7/45\n469/469 [==============================] - 2s 3ms/step - loss: 0.1578 - accuracy: 0.9513\nEpoch 8/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1581 - accuracy: 0.9513\nEpoch 9/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1582 - accuracy: 0.9517\nEpoch 10/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1554 - accuracy: 0.9523\nEpoch 11/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1568 - accuracy: 0.9522\nEpoch 12/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1540 - accuracy: 0.9521\nEpoch 13/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1540 - accuracy: 0.9527\nEpoch 14/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1514 - accuracy: 0.9531\nEpoch 15/45\n469/469 [==============================] - 2s 4ms/step - loss: 0.1527 - accuracy: 0.9530\nEpoch 16/45\n469/469 [==============================] - 2s 4ms/step - loss: 0.1519 - accuracy: 0.9528\nEpoch 17/45\n469/469 [==============================] - 3s 6ms/step - loss: 0.1536 - accuracy: 0.9525\nEpoch 18/45\n469/469 [==============================] - 3s 5ms/step - loss: 0.1496 - accuracy: 0.9533\nEpoch 19/45\n469/469 [==============================] - 2s 5ms/step - loss: 0.1519 - accuracy: 0.9526\nEpoch 20/45\n469/469 [==============================] - 2s 5ms/step - loss: 0.1499 - accuracy: 0.9534\nEpoch 21/45\n469/469 [==============================] - 3s 7ms/step - loss: 0.1506 - accuracy: 0.9529\nEpoch 22/45\n469/469 [==============================] - 2s 4ms/step - loss: 0.1488 - accuracy: 0.9528\nEpoch 23/45\n469/469 [==============================] - 2s 4ms/step - loss: 0.1481 - accuracy: 0.9540\nEpoch 24/45\n469/469 [==============================] - 2s 5ms/step - loss: 0.1485 - accuracy: 0.9530\nEpoch 25/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1489 - accuracy: 0.9541\nEpoch 26/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1468 - accuracy: 0.9546\nEpoch 27/45\n469/469 [==============================] - 2s 3ms/step - loss: 0.1474 - accuracy: 0.9538\nEpoch 28/45\n469/469 [==============================] - 2s 4ms/step - loss: 0.1477 - accuracy: 0.9546\nEpoch 29/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1447 - accuracy: 0.9550\nEpoch 30/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1474 - accuracy: 0.9539\nEpoch 31/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1453 - accuracy: 0.9546\nEpoch 32/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1447 - accuracy: 0.9547\nEpoch 33/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1426 - accuracy: 0.9553\nEpoch 34/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1454 - accuracy: 0.9538\nEpoch 35/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1421 - accuracy: 0.9552\nEpoch 36/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1446 - accuracy: 0.9546\nEpoch 37/45\n469/469 [==============================] - 2s 4ms/step - loss: 0.1420 - accuracy: 0.9546\nEpoch 38/45\n469/469 [==============================] - 2s 4ms/step - loss: 0.1437 - accuracy: 0.9551\nEpoch 39/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1423 - accuracy: 0.9555\nEpoch 40/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1423 - accuracy: 0.9551\nEpoch 41/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1423 - accuracy: 0.9552\nEpoch 42/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1411 - accuracy: 0.9564\nEpoch 43/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1421 - accuracy: 0.9555\nEpoch 44/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1414 - accuracy: 0.9555\nEpoch 45/45\n469/469 [==============================] - 1s 3ms/step - loss: 0.1395 - accuracy: 0.9573\n313/313 [==============================] - 1s 2ms/step - loss: 0.1281 - accuracy: 0.9594\nEpoch 1/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.9668 - accuracy: 0.6927 - val_loss: 0.3765 - val_accuracy: 0.9067\nEpoch 2/100\n391/391 [==============================] - 4s 10ms/step - loss: 0.5680 - accuracy: 0.8247 - val_loss: 0.3022 - val_accuracy: 0.9171\nEpoch 3/100\n391/391 [==============================] - 2s 6ms/step - loss: 0.5119 - accuracy: 0.8404 - val_loss: 0.2724 - val_accuracy: 0.9255\nEpoch 4/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.4760 - accuracy: 0.8503 - val_loss: 0.2577 - val_accuracy: 0.9304\nEpoch 5/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.4552 - accuracy: 0.8557 - val_loss: 0.2427 - val_accuracy: 0.9340\nEpoch 6/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.4440 - accuracy: 0.8607 - val_loss: 0.2324 - val_accuracy: 0.9361\nEpoch 7/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.4304 - accuracy: 0.8633 - val_loss: 0.2303 - val_accuracy: 0.9371\nEpoch 8/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.4181 - accuracy: 0.8684 - val_loss: 0.2285 - val_accuracy: 0.9380\nEpoch 9/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.4156 - accuracy: 0.8691 - val_loss: 0.2250 - val_accuracy: 0.9390\nEpoch 10/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.4088 - accuracy: 0.8696 - val_loss: 0.2186 - val_accuracy: 0.9414\nEpoch 11/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.4025 - accuracy: 0.8719 - val_loss: 0.2168 - val_accuracy: 0.9407\nEpoch 12/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.3985 - accuracy: 0.8732 - val_loss: 0.2130 - val_accuracy: 0.9407\nEpoch 13/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.3949 - accuracy: 0.8763 - val_loss: 0.2119 - val_accuracy: 0.9418\nEpoch 14/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.3909 - accuracy: 0.8754 - val_loss: 0.2130 - val_accuracy: 0.9429\nEpoch 15/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.3841 - accuracy: 0.8780 - val_loss: 0.2139 - val_accuracy: 0.9428\nEpoch 16/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.3844 - accuracy: 0.8792 - val_loss: 0.2105 - val_accuracy: 0.9433\nEpoch 17/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.3867 - accuracy: 0.8774 - val_loss: 0.2131 - val_accuracy: 0.9430\nEpoch 18/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.3810 - accuracy: 0.8808 - val_loss: 0.2114 - val_accuracy: 0.9436\nEpoch 19/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.3804 - accuracy: 0.8785 - val_loss: 0.2075 - val_accuracy: 0.9431\nEpoch 20/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.3764 - accuracy: 0.8807 - val_loss: 0.2045 - val_accuracy: 0.9447\nEpoch 21/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.3738 - accuracy: 0.8813 - val_loss: 0.2027 - val_accuracy: 0.9455\nEpoch 22/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.3747 - accuracy: 0.8812 - val_loss: 0.2018 - val_accuracy: 0.9444\nEpoch 23/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.3711 - accuracy: 0.8818 - val_loss: 0.2028 - val_accuracy: 0.9450\nEpoch 24/100\n391/391 [==============================] - 2s 6ms/step - loss: 0.3699 - accuracy: 0.8825 - val_loss: 0.2039 - val_accuracy: 0.9464\nEpoch 25/100\n391/391 [==============================] - 3s 7ms/step - loss: 0.3688 - accuracy: 0.8828 - val_loss: 0.2038 - val_accuracy: 0.9449\nEpoch 26/100\n391/391 [==============================] - 4s 10ms/step - loss: 0.3629 - accuracy: 0.8850 - val_loss: 0.2029 - val_accuracy: 0.9457\nEpoch 27/100\n391/391 [==============================] - 3s 7ms/step - loss: 0.3596 - accuracy: 0.8863 - val_loss: 0.2025 - val_accuracy: 0.9453\nEpoch 28/100\n391/391 [==============================] - 2s 6ms/step - loss: 0.3636 - accuracy: 0.8846 - val_loss: 0.2018 - val_accuracy: 0.9465\nEpoch 29/100\n391/391 [==============================] - 2s 6ms/step - loss: 0.3596 - accuracy: 0.8861 - val_loss: 0.1979 - val_accuracy: 0.9451\nEpoch 30/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.3580 - accuracy: 0.8878 - val_loss: 0.1990 - val_accuracy: 0.9440\nEpoch 31/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.3609 - accuracy: 0.8866 - val_loss: 0.2016 - val_accuracy: 0.9455\nEpoch 32/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.3580 - accuracy: 0.8853 - val_loss: 0.1999 - val_accuracy: 0.9450\nEpoch 33/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.3528 - accuracy: 0.8889 - val_loss: 0.1961 - val_accuracy: 0.9473\nEpoch 34/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.3539 - accuracy: 0.8882 - val_loss: 0.1992 - val_accuracy: 0.9457\nEpoch 35/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.3568 - accuracy: 0.8880 - val_loss: 0.1989 - val_accuracy: 0.9458\nEpoch 36/100\n391/391 [==============================] - 1s 4ms/step - loss: 0.3536 - accuracy: 0.8885 - val_loss: 0.1987 - val_accuracy: 0.9462\nEpoch 37/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.3496 - accuracy: 0.8892 - val_loss: 0.1992 - val_accuracy: 0.9473\nEpoch 38/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.3506 - accuracy: 0.8897 - val_loss: 0.2051 - val_accuracy: 0.9445\nEpoch 39/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.3524 - accuracy: 0.8884 - val_loss: 0.2024 - val_accuracy: 0.9446\nEpoch 40/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.3520 - accuracy: 0.8884 - val_loss: 0.2007 - val_accuracy: 0.9441\nEpoch 41/100\n391/391 [==============================] - 2s 4ms/step - loss: 0.3472 - accuracy: 0.8901 - val_loss: 0.2028 - val_accuracy: 0.9448\nEpoch 42/100\n391/391 [==============================] - 2s 5ms/step - loss: 0.3464 - accuracy: 0.8916 - val_loss: 0.2012 - val_accuracy: 0.9452\nEpoch 43/100\n391/391 [==============================] - 1s 3ms/step - loss: 0.3501 - accuracy: 0.8882 - val_loss: 0.2014 - val_accuracy: 0.9444\nBest epoch: 33\nEpoch 1/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3469 - accuracy: 0.8903\nEpoch 2/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3503 - accuracy: 0.8903\nEpoch 3/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3488 - accuracy: 0.8901\nEpoch 4/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3462 - accuracy: 0.8920\nEpoch 5/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3457 - accuracy: 0.8909\nEpoch 6/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3459 - accuracy: 0.8919\nEpoch 7/39\n469/469 [==============================] - 2s 4ms/step - loss: 0.3437 - accuracy: 0.8907\nEpoch 8/39\n469/469 [==============================] - 2s 5ms/step - loss: 0.3464 - accuracy: 0.8907\nEpoch 9/39\n469/469 [==============================] - 2s 4ms/step - loss: 0.3449 - accuracy: 0.8918\nEpoch 10/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3425 - accuracy: 0.8927\nEpoch 11/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3403 - accuracy: 0.8929\nEpoch 12/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3420 - accuracy: 0.8928\nEpoch 13/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3413 - accuracy: 0.8935\nEpoch 14/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3396 - accuracy: 0.8939\nEpoch 15/39\n469/469 [==============================] - 2s 3ms/step - loss: 0.3423 - accuracy: 0.8936\nEpoch 16/39\n469/469 [==============================] - 2s 4ms/step - loss: 0.3424 - accuracy: 0.8935\nEpoch 17/39\n469/469 [==============================] - 2s 3ms/step - loss: 0.3353 - accuracy: 0.8944\nEpoch 18/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3357 - accuracy: 0.8960\nEpoch 19/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3383 - accuracy: 0.8929\nEpoch 20/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3345 - accuracy: 0.8960\nEpoch 21/39\n469/469 [==============================] - 2s 4ms/step - loss: 0.3353 - accuracy: 0.8957\nEpoch 22/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3386 - accuracy: 0.8944\nEpoch 23/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3362 - accuracy: 0.8950\nEpoch 24/39\n469/469 [==============================] - 2s 4ms/step - loss: 0.3317 - accuracy: 0.8962\nEpoch 25/39\n469/469 [==============================] - 2s 4ms/step - loss: 0.3327 - accuracy: 0.8965\nEpoch 26/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3333 - accuracy: 0.8953\nEpoch 27/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3316 - accuracy: 0.8959\nEpoch 28/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3320 - accuracy: 0.8957\nEpoch 29/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3320 - accuracy: 0.8953\nEpoch 30/39\n469/469 [==============================] - 2s 3ms/step - loss: 0.3351 - accuracy: 0.8941\nEpoch 31/39\n469/469 [==============================] - 2s 5ms/step - loss: 0.3298 - accuracy: 0.8970\nEpoch 32/39\n469/469 [==============================] - 2s 4ms/step - loss: 0.3259 - accuracy: 0.8986\nEpoch 33/39\n469/469 [==============================] - 3s 6ms/step - loss: 0.3326 - accuracy: 0.8967\nEpoch 34/39\n469/469 [==============================] - 3s 6ms/step - loss: 0.3291 - accuracy: 0.8978\nEpoch 35/39\n469/469 [==============================] - 2s 4ms/step - loss: 0.3340 - accuracy: 0.8955\nEpoch 36/39\n469/469 [==============================] - 2s 5ms/step - loss: 0.3332 - accuracy: 0.8972\nEpoch 37/39\n469/469 [==============================] - 2s 4ms/step - loss: 0.3310 - accuracy: 0.8975\nEpoch 38/39\n469/469 [==============================] - 1s 3ms/step - loss: 0.3261 - accuracy: 0.8982\nEpoch 39/39\n469/469 [==============================] - 2s 4ms/step - loss: 0.3292 - accuracy: 0.8987\n313/313 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9439\n\n\nNote that if you’re not worried about slightly underperforming, there’s a shortcut you can take: just use the tuner to reload the top-performing models with the best weights saved during the hyperparameter search, without retraining new models from scratch:\n\nmodels = tuner.get_best_models(top_n)\nbest_model = models[0]\n# Build the model.\n# Needed for `Sequential` without specified `input_shape`.\nbest_model.build(input_shape=(None, 28, 28))\nbest_model.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten (Flatten)           (None, 784)               0         \n                                                                 \n dense (Dense)               (None, 32)                25120     \n                                                                 \n dropout (Dropout)           (None, 32)                0         \n                                                                 \n dense_1 (Dense)             (None, 10)                330       \n                                                                 \n=================================================================\nTotal params: 25,450\nTrainable params: 25,450\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nbest_model.evaluate(x_test, y_test)\n\n313/313 [==============================] - 2s 4ms/step - loss: 0.1319 - accuracy: 0.9616\n\n\n[0.1318952739238739, 0.9616000056266785]\n\n\nYou can also print a summary of the search results.\n\ntuner.results_summary()\n\nResults summary\nResults in mnist_kt_test/untitled_project\nShowing 10 best trials\nObjective(name=\"val_accuracy\", direction=\"max\")\n\nTrial 0 summary\nHyperparameters:\nnum_layers: 1\nunits: 32\nactivation: tanh\ndropout: True\nlr: 0.001023588238883239\noptimizer: adam\nScore: 0.9647500216960907\n\nTrial 2 summary\nHyperparameters:\nnum_layers: 1\nunits: 32\nactivation: tanh\ndropout: True\nlr: 0.00020952911266579243\noptimizer: rmsprop\nScore: 0.9639500081539154\n\nTrial 1 summary\nHyperparameters:\nnum_layers: 1\nunits: 16\nactivation: relu\ndropout: True\nlr: 0.004306213731281972\noptimizer: rmsprop\nScore: 0.9480000138282776\n\n\n\n\n\n13.3.2 Tune model training\nTo tune the model building process, we need to subclass the HyperModel class, which also makes it easy to share and reuse hypermodels.\nWe need to override HyperModel.build() and HyperModel.fit() to tune the model building and training process respectively. A HyperModel.build() method is the same as the model-building function, which creates a Keras model using the hyperparameters and returns it.\n\nclass MyHyperModel(kt.HyperModel):\n    def build(self, hp):\n        model = tf.keras.Sequential()\n        model.add(tf.keras.layers.Flatten())\n        model.add(\n            tf.keras.layers.Dense(\n                units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n                activation=\"relu\",\n            )\n        )\n        model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n        model.compile(\n            optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"],\n        )\n        return model\n\n    def fit(self, hp, model, *args, **kwargs):\n        return model.fit(\n            *args,\n            # Tune whether to shuffle the data in each epoch!\n            shuffle=hp.Boolean(\"shuffle\"),\n            **kwargs,\n        )\n\nWe can do a quick check to see if the code works correctly by using a small amount of random data.\n\nhp = kt.HyperParameters()\nhypermodel = MyHyperModel()\nmodel = hypermodel.build(hp)\nhypermodel.fit(hp, model, np.random.rand(100, 28, 28), np.random.rand(100, 10))\n\n4/4 [==============================] - 1s 7ms/step - loss: 12.5933 - accuracy: 0.1000\n\n\n&lt;keras.callbacks.History at 0x7f8cb6183be0&gt;\n\n\n\n\n13.3.3 Tune data preprocessing\nTo tune data preprocessing, we just add an additional step in HyperModel.fit(), where we can access the dataset from the arguments. In the following code, we tune whether to normalize the data before training the model. This time we explicitly put x and y in the function signature because we need to use them.\n\nclass MyHyperModel(kt.HyperModel):\n    def build(self, hp):\n        model = tf.keras.Sequential()\n        model.add(tf.keras.layers.Flatten())\n        model.add(\n            tf.keras.layers.Dense(\n                units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n                activation=\"relu\",\n            )\n        )\n        model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n        model.compile(\n            optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"],\n        )\n        return model\n\n    def fit(self, hp, model, x, y, **kwargs):\n        # Tune whether to add normalize layer!\n        if hp.Boolean(\"normalize\"):\n            x = tf.keras.layers.Normalization()(x)\n        return model.fit(\n            x,\n            y,\n            shuffle=hp.Boolean(\"shuffle\"),\n            **kwargs,\n        )\n\n\nhp = kt.HyperParameters()\nhypermodel = MyHyperModel()\nmodel = hypermodel.build(hp)\nhypermodel.fit(hp, model, np.random.rand(100, 28, 28), np.random.rand(100, 10))\n\n4/4 [==============================] - 1s 5ms/step - loss: 12.3237 - accuracy: 0.0900\n\n\n&lt;keras.callbacks.History at 0x7f8cb619d270&gt;\n\n\nFor more information, please refer to https://keras.io/keras_tuner/"
  },
  {
    "objectID": "13_Hyperparameter.html#network-architecture-search-with-autokeras",
    "href": "13_Hyperparameter.html#network-architecture-search-with-autokeras",
    "title": "13  Hyperparameter Tuning",
    "section": "13.4 Network architecture search with AutoKeras",
    "text": "13.4 Network architecture search with AutoKeras\nLet’s now build up an AutoML pipeline to improve the CNN structure and achieve better classification accuracy.\nCreating an AutoML pipeline with the AutoKeras functional API is quite similar to building up a neural network with the Keras functional API. The only difference is that the Keras layers are replaced with AutoKeras’s built-in AutoML blocks. Each block contains one or more deep learning models (or preprocessing methods) and a default search space for their hyperparameters. You can also modify the search space for each hyperparameter. To build up a network, we stack multiple Keras layers by wiring together their inputs and outputs sequentially. Specifically, it often contyains the following blocks:\n\nInput node: which is a placeholder for the tensor input of the pipeline, such as ImageInput, TextInput, or StructuredDataInput. You can also define a general tensor input with the Input class in AutoKeras. The input node accepts data in multiple formats, such as Numpy arrays, Pandas DataFrames, and TensorFlow Datasets. It will also conduct certain preprocessing operations automatically, such as extending the dimensions of images if they do not have a channel dimension. The input node does not have any hyperparameters that can be set or tuned.\nPreprocessor: which is block defines additional preprocessing operations to perform on the inputs, such as image normalization, text embedding, and so on. Depending on the operation, there may be hyperparameters to tune, such as the maximum size of the vocabulary table to use to convert text documents to their vector representations if text embedding is performed. In this block, there are no weights to be trained through backpropagation.\nNetwork: which is the most important type of AutoML block in AutoKeras. Each block represents a set of neural network models of the same structure. For example, a ConvBlock. The number and types of layers are treated as hyperparameters. You can select one or more network blocks to create the pipelines based on the task at hand, and specify the search space of their hyperparameters based on your requirements. Unlike the preprocessor block, there are weights to be trained through backpropagation after specifying the hyperparameters in the network block.\nHead: which is a task-specific component used to generate the final outputs, such as the ClassificationHead and RegressionHead. It reshapes each instance’s representation to a vector and applies a dense layer to transform it to the size of the target output. For example, if the head is a ClassificationHead and the problem is a binary classification problem, the output of each instance from the dense layer will be a vector of length two corresponding to the two labels. Each head also specifies the loss function and metrics to help compile each deep learning pipeline selected from the search space for training.\n\n\n13.4.1 Tuning CNNs for image classification\nWe leverage a ConvBlock in AutoKeras to tune the three main hyperparameters of the CNN: the number of filters, the number of convolutional layers, and the kernel size of the convolutional layers. A ConvBlock sequentially stacks multiple convolutional blocks (or convolutional cells). Each convolutional block sequentially stacks multiple convolutional layers, a max pooling layer, and a dropout layer.\nAll the convolutional blocks have the same number of convolutional layers, but there can be a different number of filters in each layer. There are seven hyperparameters in the search space of a ConvBlock:\n\nNumber of convolutional blocks.\nNumber of convolutional layers in each block. This is the same in all the convolutional blocks.\nType of the convolutional layer. Each convolutional layer can be one of two types: it can be a regular 2D convolutional layer or a separable convolutional layer, which contains fewer weights than a normal convolutional layer but may achieve comparable performance.\nNumber of filters in the convolutional layer. This can be different for each layer of each block.\nKernel size of the convolutional layer. The kernel size of the max pooling layers is set to be the kernel size minus one. Once the kernel size is selected by the tuning algorithm for a ConvBlock in a trial, it will be applied for every pooling layer and convolutional layer in all the cells of that ConvBlock.\nWhether to apply the max pooling layer in each cell. Once this is selected for a trial, it’s applied for every cell in the ConvBlock.\nWhether to apply the dropout layer in each cell. Once this is selected for a trial, it’s applied for every cell in the ConvBlock.\n\nTo keep this example simple, we’ll constrain the search space by fixing the number of blocks as two. We do not apply the dropout layer or use separable convolutional layers. The hyperparameters to be tuned are the number of layers, the kernel size, and the number of filters in each layer in the blocks. By default, they are selected from the lists [1, 2], [3, 5, 7], and [16, 32, 64, 128, 256, 512], respectively.\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n# The model\ninput_node = ak.ImageInput()\noutput_node = ak.Normalization()(input_node)\noutput_node = ak.ConvBlock(\n    # do not specify if we want to let it to search automatically\n    num_blocks=2, max_pooling=True, separable=False, dropout=0.0\n)(output_node)\noutput_node = ak.ClassificationHead(dropout=0.0)(output_node)\nauto_model = ak.AutoModel(\n    inputs=input_node, outputs=output_node, max_trials=10, overwrite=True, seed=42\n)\n\n# You may run with the full dataset, but expect a longer training time.\nauto_model.fit(x_train, y_train, epochs=3)\n\nTrial 10 Complete [00h 00m 54s]\nval_loss: 0.03022073395550251\n\nBest val_loss So Far: 0.03022073395550251\nTotal elapsed time: 00h 07m 25s\nEpoch 1/3\n1875/1875 [==============================] - 20s 9ms/step - loss: 0.1914 - accuracy: 0.9415\nEpoch 2/3\n1875/1875 [==============================] - 14s 7ms/step - loss: 0.0371 - accuracy: 0.9890\nEpoch 3/3\n1875/1875 [==============================] - 15s 8ms/step - loss: 0.0143 - accuracy: 0.9958\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n\n\n&lt;keras.callbacks.History at 0x7f8cb1aaa7d0&gt;\n\n\n\ntest_loss, test_acc = auto_model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test accuracy: \", test_acc)\n\nTest accuracy:  0.9929999709129333\n\n\nThe best CNN achieves 99.3% accuracy on the test set. To discover smaller architectures, we can limit the number of layers and filters in the search space. It is possible to find a smaller architecture with comparableb performance to the CNN we constructed here.\n\nbest_model = auto_model.export_model()\nbest_model.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 28, 28)]          0         \n                                                                 \n cast_to_float32 (CastToFloa  (None, 28, 28)           0         \n t32)                                                            \n                                                                 \n expand_last_dim (ExpandLast  (None, 28, 28, 1)        0         \n Dim)                                                            \n                                                                 \n normalization (Normalizatio  (None, 28, 28, 1)        3         \n n)                                                              \n                                                                 \n conv2d (Conv2D)             (None, 24, 24, 256)       6656      \n                                                                 \n conv2d_1 (Conv2D)           (None, 20, 20, 16)        102416    \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 5, 5, 16)         0         \n )                                                               \n                                                                 \n conv2d_2 (Conv2D)           (None, 5, 5, 32)          12832     \n                                                                 \n conv2d_3 (Conv2D)           (None, 5, 5, 128)         102528    \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 2, 2, 128)        0         \n 2D)                                                             \n                                                                 \n flatten (Flatten)           (None, 512)               0         \n                                                                 \n dense (Dense)               (None, 10)                5130      \n                                                                 \n classification_head_1 (Soft  (None, 10)               0         \n max)                                                            \n                                                                 \n=================================================================\nTotal params: 229,565\nTrainable params: 229,562\nNon-trainable params: 3\n_________________________________________________________________\n\n\n\n\n13.4.2 Automated pipeline search with AutoKeras\nIt is also possible to use autoKeras in AutoML applications: selecting the best types of components (models or preprocessors) to use in the deep learning pipeline. This is a more complex scenario than only tuning the hyperparameters of a specific type of model, as introduced in the previous section, because different models and preprocessors may compose different operations and have unique hyperparameters. It requires us to jointly select the combination of preprocessors and models and their coupled hyperparameters.\nFor example, in image classification, there are a lot of advanced models proposed beyond the naive CNN we used previously, such as ResNet, Xception, and so on. You’ll also need to decide on suitable preprocessing methods, such as choosing whether to use normalization or not. We’ll work through some image classification examples here to show you how to automatically select models and preprocessing methods.\n\n13.4.2.1 Automated selection of image preprocessing methods\nIn fact, it is also straightforward to extend the AutoML pipeline to tune and select a suitable data augmentation method — that is, to use an AutoML block to select and evaluate various data augmentation methods. The ImageBlock also allows us to select among multiple data preprocessing methods, such as deciding whether to use normalization and/or data augmentation methods to prepare the data.\nLet’s use an image classification example to illustrate how to automatically select preprocessing methods for a ResNet model. We decide whether to use data augmentation and normalization methods or not. The dataset we use here is a subset of the CIFAR-10 dataset. To make things easier, we’ll only use images from two classes, “airplane” and “automobile.”\n\n#ssl._create_default_https_context = ssl._create_unverified_context\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nprint(\"Training image shape:\", x_train.shape)  # (60000, 28, 28)\nprint(\"Training label shape:\", y_train.shape)  # (60000,)\nprint(\"First five training labels:\", y_train[:5])  # array([5 0 4 1 9], dtype=uint8)\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 [==============================] - 2s 0us/step\nTraining image shape: (50000, 32, 32, 3)\nTraining label shape: (50000, 1)\nFirst five training labels: [[6]\n [9]\n [9]\n [4]\n [1]]\n\n\n\nairplane_automobile_indices_train = (y_train[:, 0] == 0) | (y_train[:, 0] == 1)\nairplane_automobile_indices_test = (y_test[:, 0] == 0) | (y_test[:, 0] == 1)\nx_train, y_train = (\n    x_train[airplane_automobile_indices_train],\n    y_train[airplane_automobile_indices_train],\n)\nx_test, y_test = (\n    x_test[airplane_automobile_indices_test],\n    y_test[airplane_automobile_indices_test],\n)\nprint(\"Training image shape:\", x_train.shape)  # (60000, 28, 28)\nprint(\"Training label shape:\", y_train.shape)  # (60000,)\nprint(\"First five training labels:\", y_train[:5])  # array([5 0 4 1 9], dtype=uint8)\n\nTraining image shape: (10000, 32, 32, 3)\nTraining label shape: (10000, 1)\nFirst five training labels: [[1]\n [1]\n [0]\n [0]\n [1]]\n\n\n\n# plot first few images\nfor i in range(9):\n    # define subplot\n    plt.subplot(330 + 1 + i)\n    # plot raw pixel data\n    plt.imshow(x_train[i])\n\n\n\n\nLet’s first create an AutoML pipeline to select the data augmentation method for the ResNet models. The pipeline has the same structure as the sequential AutoML pipelines we built in the previous section for tuning a single ResNet model. The only difference is that we add the image hyperblock (ImageBlock) in AutoKeras also contains preprocessing methods. The augmentation methods are selected along with the structure and other hyperparameters, such as the optimization method and learning rate.\n\ninput_node = ak.ImageInput()\noutput_node = ak.ImageBlock(\n    # do not specify if we want to use normalization and let it to search automatically\n    normalize=None,\n    # do not specify if we want to use adata ugmentation method and let it to search automatically\n    augment=None,\n    # Only search resnet architectures.\n    block_type=\"resnet\",\n)(input_node)\noutput_node = ak.ClassificationHead(dropout=0.0)(output_node)\n\nauto_model = ak.AutoModel(\n    inputs=input_node, outputs=output_node, max_trials=10, overwrite=True, seed=42\n)\n\nauto_model.fit(x_train, y_train, epochs=10, batch_size=64)\n\nTrial 10 Complete [00h 02m 05s]\nval_loss: 0.2805823087692261\n\nBest val_loss So Far: 0.19601556658744812\nTotal elapsed time: 00h 19m 12s\nEpoch 1/10\n157/157 [==============================] - 39s 52ms/step - loss: 0.4771 - accuracy: 0.7922\nEpoch 2/10\n157/157 [==============================] - 7s 42ms/step - loss: 0.3570 - accuracy: 0.8460\nEpoch 3/10\n157/157 [==============================] - 7s 46ms/step - loss: 0.3102 - accuracy: 0.8693\nEpoch 4/10\n157/157 [==============================] - 7s 43ms/step - loss: 0.2904 - accuracy: 0.8785\nEpoch 5/10\n157/157 [==============================] - 7s 46ms/step - loss: 0.2678 - accuracy: 0.8889\nEpoch 6/10\n157/157 [==============================] - 7s 47ms/step - loss: 0.2402 - accuracy: 0.9012\nEpoch 7/10\n157/157 [==============================] - 7s 45ms/step - loss: 0.2284 - accuracy: 0.9058\nEpoch 8/10\n157/157 [==============================] - 7s 44ms/step - loss: 0.2169 - accuracy: 0.9135\nEpoch 9/10\n157/157 [==============================] - 7s 46ms/step - loss: 0.2171 - accuracy: 0.9109\nEpoch 10/10\n157/157 [==============================] - 7s 42ms/step - loss: 0.2014 - accuracy: 0.9168\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n\n\n&lt;keras.callbacks.History at 0x7f8c996e84f0&gt;\n\n\n\nauto_model.tuner.results_summary()\n\nResults summary\nResults in ./auto_model\nShowing 10 best trials\nObjective(name=\"val_loss\", direction=\"min\")\n\nTrial 07 summary\nHyperparameters:\nimage_block_1/normalize: False\nimage_block_1/augment: True\nimage_block_1/res_net_block_1/pretrained: False\nimage_block_1/res_net_block_1/version: resnet50_v2\nimage_block_1/res_net_block_1/imagenet_size: False\nclassification_head_1/spatial_reduction_1/reduction_type: global_max\noptimizer: adam\nlearning_rate: 0.001\nimage_block_1/image_augmentation_1/translation_factor: 0.1\nimage_block_1/image_augmentation_1/horizontal_flip: True\nimage_block_1/image_augmentation_1/vertical_flip: False\nimage_block_1/image_augmentation_1/rotation_factor: 0.1\nimage_block_1/image_augmentation_1/zoom_factor: 0.0\nimage_block_1/image_augmentation_1/contrast_factor: 0.1\nScore: 0.19601556658744812\n\nTrial 05 summary\nHyperparameters:\nimage_block_1/normalize: False\nimage_block_1/augment: True\nimage_block_1/res_net_block_1/pretrained: False\nimage_block_1/res_net_block_1/version: resnet50_v2\nimage_block_1/res_net_block_1/imagenet_size: False\nclassification_head_1/spatial_reduction_1/reduction_type: flatten\noptimizer: adam\nlearning_rate: 0.001\nimage_block_1/image_augmentation_1/translation_factor: 0.1\nimage_block_1/image_augmentation_1/horizontal_flip: True\nimage_block_1/image_augmentation_1/vertical_flip: False\nimage_block_1/image_augmentation_1/rotation_factor: 0.1\nimage_block_1/image_augmentation_1/zoom_factor: 0.0\nimage_block_1/image_augmentation_1/contrast_factor: 0.1\nScore: 0.2053665965795517\n\nTrial 00 summary\nHyperparameters:\nimage_block_1/normalize: False\nimage_block_1/augment: False\nimage_block_1/res_net_block_1/pretrained: False\nimage_block_1/res_net_block_1/version: resnet50_v2\nimage_block_1/res_net_block_1/imagenet_size: False\nclassification_head_1/spatial_reduction_1/reduction_type: flatten\noptimizer: adam\nlearning_rate: 0.001\nScore: 0.25839027762413025\n\nTrial 01 summary\nHyperparameters:\nimage_block_1/normalize: False\nimage_block_1/augment: False\nimage_block_1/res_net_block_1/pretrained: False\nimage_block_1/res_net_block_1/version: resnet50_v2\nimage_block_1/res_net_block_1/imagenet_size: False\nclassification_head_1/spatial_reduction_1/reduction_type: flatten\noptimizer: adam\nlearning_rate: 0.1\nScore: 0.2603760361671448\n\nTrial 08 summary\nHyperparameters:\nimage_block_1/normalize: False\nimage_block_1/augment: True\nimage_block_1/res_net_block_1/pretrained: False\nimage_block_1/res_net_block_1/version: resnet50_v2\nimage_block_1/res_net_block_1/imagenet_size: False\nclassification_head_1/spatial_reduction_1/reduction_type: global_max\noptimizer: adam\nlearning_rate: 0.001\nimage_block_1/image_augmentation_1/translation_factor: 0.1\nimage_block_1/image_augmentation_1/horizontal_flip: True\nimage_block_1/image_augmentation_1/vertical_flip: False\nimage_block_1/image_augmentation_1/rotation_factor: 0.1\nimage_block_1/image_augmentation_1/zoom_factor: 0.1\nimage_block_1/image_augmentation_1/contrast_factor: 0.1\nScore: 0.2673555910587311\n\nTrial 09 summary\nHyperparameters:\nimage_block_1/normalize: False\nimage_block_1/augment: True\nimage_block_1/res_net_block_1/pretrained: False\nimage_block_1/res_net_block_1/version: resnet50_v2\nimage_block_1/res_net_block_1/imagenet_size: False\nclassification_head_1/spatial_reduction_1/reduction_type: global_avg\noptimizer: adam\nlearning_rate: 0.001\nimage_block_1/image_augmentation_1/translation_factor: 0.1\nimage_block_1/image_augmentation_1/horizontal_flip: True\nimage_block_1/image_augmentation_1/vertical_flip: False\nimage_block_1/image_augmentation_1/rotation_factor: 0.1\nimage_block_1/image_augmentation_1/zoom_factor: 0.0\nimage_block_1/image_augmentation_1/contrast_factor: 0.1\nScore: 0.2805823087692261\n\nTrial 04 summary\nHyperparameters:\nimage_block_1/normalize: False\nimage_block_1/augment: True\nimage_block_1/res_net_block_1/pretrained: False\nimage_block_1/res_net_block_1/version: resnet50_v2\nimage_block_1/res_net_block_1/imagenet_size: False\nclassification_head_1/spatial_reduction_1/reduction_type: flatten\noptimizer: adam\nlearning_rate: 0.001\nimage_block_1/image_augmentation_1/translation_factor: 0.0\nimage_block_1/image_augmentation_1/horizontal_flip: True\nimage_block_1/image_augmentation_1/vertical_flip: True\nimage_block_1/image_augmentation_1/rotation_factor: 0.0\nimage_block_1/image_augmentation_1/zoom_factor: 0.0\nimage_block_1/image_augmentation_1/contrast_factor: 0.0\nScore: 0.28421175479888916\n\nTrial 03 summary\nHyperparameters:\nimage_block_1/normalize: False\nimage_block_1/augment: False\nimage_block_1/res_net_block_1/pretrained: False\nimage_block_1/res_net_block_1/version: resnet50_v2\nimage_block_1/res_net_block_1/imagenet_size: False\nclassification_head_1/spatial_reduction_1/reduction_type: global_avg\noptimizer: adam\nlearning_rate: 0.001\nScore: 0.29526445269584656\n\nTrial 02 summary\nHyperparameters:\nimage_block_1/normalize: False\nimage_block_1/augment: False\nimage_block_1/res_net_block_1/pretrained: False\nimage_block_1/res_net_block_1/version: resnet50_v2\nimage_block_1/res_net_block_1/imagenet_size: False\nclassification_head_1/spatial_reduction_1/reduction_type: global_max\noptimizer: adam\nlearning_rate: 0.001\nScore: 0.3357878029346466\n\nTrial 06 summary\nHyperparameters:\nimage_block_1/normalize: True\nimage_block_1/augment: True\nimage_block_1/res_net_block_1/pretrained: False\nimage_block_1/res_net_block_1/version: resnet50_v2\nimage_block_1/res_net_block_1/imagenet_size: False\nclassification_head_1/spatial_reduction_1/reduction_type: flatten\noptimizer: adam\nlearning_rate: 0.001\nimage_block_1/image_augmentation_1/translation_factor: 0.1\nimage_block_1/image_augmentation_1/horizontal_flip: True\nimage_block_1/image_augmentation_1/vertical_flip: False\nimage_block_1/image_augmentation_1/rotation_factor: 0.1\nimage_block_1/image_augmentation_1/zoom_factor: 0.0\nimage_block_1/image_augmentation_1/contrast_factor: 0.1\nScore: 0.5242509245872498\n\n\n\nbest_model = auto_model.export_model()\nbest_model.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n cast_to_float32 (CastToFloa  (None, 32, 32, 3)        0         \n t32)                                                            \n                                                                 \n random_translation (RandomT  (None, 32, 32, 3)        0         \n ranslation)                                                     \n                                                                 \n random_flip (RandomFlip)    (None, 32, 32, 3)         0         \n                                                                 \n random_rotation (RandomRota  (None, 32, 32, 3)        0         \n tion)                                                           \n                                                                 \n random_contrast (RandomCont  (None, 32, 32, 3)        0         \n rast)                                                           \n                                                                 \n resnet50v2 (Functional)     (None, 1, 1, 2048)        23564800  \n                                                                 \n global_max_pooling2d (Globa  (None, 2048)             0         \n lMaxPooling2D)                                                  \n                                                                 \n dense (Dense)               (None, 1)                 2049      \n                                                                 \n classification_head_1 (Acti  (None, 1)                0         \n vation)                                                         \n                                                                 \n=================================================================\nTotal params: 23,566,849\nTrainable params: 23,521,409\nNon-trainable params: 45,440\n_________________________________________________________________\n\n\n\ntest_loss, test_acc = auto_model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: {accuracy}%\".format(accuracy=round(test_acc * 100, 2)))\n\nAccuracy: 84.55%\n\n\nFor more information, please refer to https://autokeras.com/"
  },
  {
    "objectID": "13_Hyperparameter.html#w-b",
    "href": "13_Hyperparameter.html#w-b",
    "title": "13  Hyperparameter Tuning",
    "section": "13.5 W & B",
    "text": "13.5 W & B\nWe can use Weights & Biases for machine learning experiment tracking, model checkpointing, and collaboration with your team. See the full Weights & Biases Documentation here\nStart by logging in to your account. If this is your first time using W&B or you are not logged in, the link that appears after running wandb.login() will take you to sign-up/login page.\n\nwandb.login()\n\nwandb: Currently logged in as: phonchi. Use `wandb login --relogin` to force relogin\n\n\nTrue\n\n\n\n13.5.1 Normal logging flow\nLet us first import the cifar10 data and define the model.\n\n(x_train, y_train), (x_test, y_test) =  tf.keras.datasets.cifar10.load_data()\n\n# Subsetting train data and normalizing to [0., 1.]\nx_train, x_test = x_train[::5] / 255., x_test / 255.\ny_train = y_train[::5]\n\nCLASS_NAMES = [\"airplane\", \"automobile\", \"bird\", \"cat\",\n               \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n\nprint('Shape of x_train: ', x_train.shape)\nprint('Shape of y_train: ', y_train.shape)\nprint('Shape of x_test: ', x_test.shape)\nprint('Shape of y_test: ', y_test.shape)\n\nShape of x_train:  (10000, 32, 32, 3)\nShape of y_train:  (10000, 1)\nShape of x_test:  (10000, 32, 32, 3)\nShape of y_test:  (10000, 1)\n\n\n\ndef Model():\n    inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n    \n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(inputs)\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=2)(x)\n    \n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n    \n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    \n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    x = tf.keras.layers.Dense(32, activation='relu')(x)\n    \n    outputs = tf.keras.layers.Dense(len(CLASS_NAMES), activation='softmax')(x)\n    \n    return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n\n\n13.5.1.1 Give wandb.init() your config\nWe first initialize our wandb run, letting W&B know some training is about to happen. Check the official documentation for .init here\nThat’s when we need to set our hyperparameters. They’re passed in as a dictionary via the config argument, and then become available as the config attribute of wandb. Learn more about config in this Colab Notebook\n\n# Initialize wandb with your project name\nrun = wandb.init(project='my-keras-project',\n                 config={  # and include hyperparameters and metadata\n                     \"learning_rate\": 0.005,\n                     \"epochs\": 5,\n                     \"batch_size\": 1024,\n                     \"loss_function\": \"sparse_categorical_crossentropy\",\n                     \"architecture\": \"CNN\",\n                     \"dataset\": \"CIFAR-10\"\n                 })\nconfig = wandb.config  # We'll use this to configure our experiment\n\n# Initialize model like you usually do.\ntf.keras.backend.clear_session()\nmodel = Model()\nmodel.summary()\n\n# Compile model like you usually do.\n# Notice that we use config, so our metadata matches what gets executed\noptimizer = tf.keras.optimizers.Adam(config.learning_rate) \nmodel.compile(optimizer, config.loss_function, metrics=['acc'])\n\nwandb: Currently logged in as: phonchi. Use `wandb login --relogin` to force relogin\n\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230524_065507-y7peyypu\n\n\nSyncing run cool-spaceship-1 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/phonchi/my-keras-project\n\n\n View run at https://wandb.ai/phonchi/my-keras-project/runs/y7peyypu\n\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 30, 30, 32)        896       \n                                                                 \n conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n )                                                               \n                                                                 \n conv2d_2 (Conv2D)           (None, 12, 12, 32)        9248      \n                                                                 \n conv2d_3 (Conv2D)           (None, 10, 10, 32)        9248      \n                                                                 \n global_average_pooling2d (G  (None, 32)               0         \n lobalAveragePooling2D)                                          \n                                                                 \n dense (Dense)               (None, 128)               4224      \n                                                                 \n dense_1 (Dense)             (None, 32)                4128      \n                                                                 \n dense_2 (Dense)             (None, 10)                330       \n                                                                 \n=================================================================\nTotal params: 37,322\nTrainable params: 37,322\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n13.5.1.2 Pass WandbMetricsLogger and WandbModelCheckpoint to model.fit()\nKeras has a robust callbacks system that allows users to separate model definition and the core training logic from other behaviors that occur during training and testing.\n\n# Add WandbMetricsLogger to log metrics and WandbModelCheckpoint to log model checkpoints\nwandb_callbacks = [\n    WandbMetricsLogger(),\n    WandbModelCheckpoint(filepath=\"my_model_{epoch:02d}\"),\n]\n\nmodel.fit(x_train, y_train,\n          epochs=config.epochs, \n          batch_size=config.batch_size,\n          validation_data=(x_test, y_test),\n          callbacks=wandb_callbacks)\n\nEpoch 1/5\n10/10 [==============================] - ETA: 0s - loss: 2.2830 - acc: 0.1079\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_01)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 8s 429ms/step - loss: 2.2830 - acc: 0.1079 - val_loss: 2.2013 - val_acc: 0.1579\nEpoch 2/5\n10/10 [==============================] - ETA: 0s - loss: 2.2821 - acc: 0.1239\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_02)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 2s 201ms/step - loss: 2.2821 - acc: 0.1239 - val_loss: 2.3022 - val_acc: 0.1023\nEpoch 3/5\n 9/10 [==========================&gt;...] - ETA: 0s - loss: 2.2968 - acc: 0.1159\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_03)... Done. 0.1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 4s 425ms/step - loss: 2.2951 - acc: 0.1151 - val_loss: 2.2644 - val_acc: 0.1002\nEpoch 4/5\n 9/10 [==========================&gt;...] - ETA: 0s - loss: 2.2085 - acc: 0.1336\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_04)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 4s 450ms/step - loss: 2.2064 - acc: 0.1388 - val_loss: 2.1812 - val_acc: 0.1477\nEpoch 5/5\n 9/10 [==========================&gt;...] - ETA: 0s - loss: 2.1471 - acc: 0.1798\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_05)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 3s 274ms/step - loss: 2.1420 - acc: 0.1820 - val_loss: 2.0732 - val_acc: 0.2106\n\n\n&lt;keras.callbacks.History at 0x7f8d4034b310&gt;\n\n\nWe can use wandb.log() to add custom metrics. Here, we log the error rate on the test set.\n\nloss, accuracy = model.evaluate(x_test, y_test)\nprint('Test Error Rate: ', round((1 - accuracy) * 100, 2))\n\n# With wandb.log, we can easily pass in metrics as key-value pairs.\nwandb.log({'Test Error Rate': round((1 - accuracy) * 100, 2)})\n\nrun.finish()\n\n313/313 [==============================] - 1s 3ms/step - loss: 2.0732 - acc: 0.2106\nTest Error Rate:  78.94\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\nRun history:\n\n\n\nTest Error Rate\n▁\n\n\nepoch/acc\n▁▃▂▄█\n\n\nepoch/epoch\n▁▃▅▆█\n\n\nepoch/learning_rate\n▁▁▁▁▁\n\n\nepoch/loss\n▇▇█▄▁\n\n\nepoch/val_acc\n▅▁▁▄█\n\n\nepoch/val_loss\n▅█▇▄▁\n\n\n\n\n\n\n13.5.2 Run summary:\n\n\n\n\n\nTest Error Rate\n78.94\n\n\nepoch/acc\n0.182\n\n\nepoch/epoch\n4\n\n\nepoch/learning_rate\n0.005\n\n\nepoch/loss\n2.14203\n\n\nepoch/val_acc\n0.2106\n\n\nepoch/val_loss\n2.0732\n\n\n\n\n\n\n\n View run cool-spaceship-1 at: https://wandb.ai/phonchi/my-keras-project/runs/y7peyypuSynced 5 W&B file(s), 0 media file(s), 25 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230524_065507-y7peyypu/logs\n\n\n\n\n\n13.5.3 Log predictions on test data using WandbEvalCallback\nThe WandbEvalCallback is an abstract base class to build Keras callbacks for primarily model prediction visualization and secondarily dataset visualization.\nThis is a dataset and task agnostic abstract callback. To use this, inherit from this base callback class and implement the add_ground_truth and add_model_prediction methods. The WandbEvalCallback is a utility class that provides helpful methods to:\n\ncreate data and prediction wandb.Table instances,\nlog data and prediction Tables as wandb.Artifact,\nlogs the data table on_train_begin,\nlogs the prediction table on_epoch_end.\n\nAs an example, we have implemented WandbClsEvalCallback below for an image classification task. This example callback: - logs the validation data (data_table) to W&B, - performs inference and logs the prediction (pred_table) to W&B on every epoch end.\nWe log the data_table to W&B when the on_train_begin method is ivoked. Once it’s uploaded as a W&B Artifact, we get a reference to this table which can be accessed using data_table_ref class variable. The data_table_ref is a 2D list that can be indexed like self.data_table_ref[idx][n] where idx is the row number while n is the column number. Let’s see the usage in the example below.\n\nclass WandbClsEvalCallback(WandbEvalCallback):\n    def __init__(\n        self, validloader, data_table_columns, pred_table_columns, num_samples=100\n    ):\n        super().__init__(data_table_columns, pred_table_columns)\n\n        self.val_data = validloader.unbatch().take(num_samples)\n\n    def add_ground_truth(self, logs=None):\n        for idx, (image, label) in enumerate(self.val_data):\n            self.data_table.add_data(\n                idx,\n                wandb.Image(image),\n                np.argmax(label, axis=-1)\n            )\n\n    def add_model_predictions(self, epoch, logs=None):\n        # Get predictions\n        preds = self._inference()\n        table_idxs = self.data_table_ref.get_index()\n\n        for idx in table_idxs:\n            pred = preds[idx]\n            self.pred_table.add_data(\n                epoch,\n                self.data_table_ref.data[idx][0],\n                self.data_table_ref.data[idx][1],\n                self.data_table_ref.data[idx][2],\n                pred\n            )\n\n    def _inference(self):\n        preds = []\n        for image, label in self.val_data:\n            pred = self.model(tf.expand_dims(image, axis=0))\n            argmax_pred = tf.argmax(pred, axis=-1).numpy()[0]\n            preds.append(argmax_pred)\n          \n        return preds\n\nCreate Dataset processing, Dataloaders functions and the model\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ndef parse_data(example):\n    # Get image\n    image = example[\"image\"]\n    # Get label\n    label = example[\"label\"]\n    label = tf.one_hot(label, depth=configs[\"num_classes\"])\n\n    return image, label\n\n\ndef get_dataloader(ds, configs, dataloader_type=\"train\"):\n    dataloader = ds.map(parse_data, num_parallel_calls=AUTOTUNE)\n\n    if dataloader_type==\"train\":\n        dataloader = dataloader.shuffle(configs[\"shuffle_buffer\"])\n      \n    dataloader = (\n        dataloader\n        .batch(configs[\"batch_size\"])\n        .prefetch(AUTOTUNE)\n    )\n\n    return dataloader\n\n\ndef get_model(configs):\n    backbone = tf.keras.applications.mobilenet_v2.MobileNetV2(weights='imagenet', include_top=False)\n    backbone.trainable = False\n\n    inputs = tf.keras.layers.Input(shape=(configs[\"image_size\"], configs[\"image_size\"], configs[\"image_channels\"]))\n    resize = tf.keras.layers.Resizing(32, 32)(inputs)\n    neck = tf.keras.layers.Conv2D(3, (3,3), padding=\"same\")(resize)\n    preprocess_input = tf.keras.applications.mobilenet.preprocess_input(neck)\n    x = backbone(preprocess_input)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    outputs = tf.keras.layers.Dense(configs[\"num_classes\"], activation=\"softmax\")(x)\n\n    return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n\nSet the config for the fashion MNIST dataset.\n\ntrain_ds, valid_ds = tfds.load('fashion_mnist', split=['train', 'test'])\n\nDownloading and preparing dataset 29.45 MiB (download: 29.45 MiB, generated: 36.42 MiB, total: 65.87 MiB) to /root/tensorflow_datasets/fashion_mnist/3.0.1...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset fashion_mnist downloaded and prepared to /root/tensorflow_datasets/fashion_mnist/3.0.1. Subsequent calls will reuse this data.\n\n\n\nconfigs = dict(\n    num_classes = 10,\n    shuffle_buffer = 1024,\n    batch_size = 64,\n    image_size = 28,\n    image_channels = 1,\n    earlystopping_patience = 3,\n    learning_rate = 1e-3,\n    epochs = 10\n)\n\n\ntrainloader = get_dataloader(train_ds, configs)\nvalidloader = get_dataloader(valid_ds, configs, dataloader_type=\"valid\")\n\n\ntf.keras.backend.clear_session()\nmodel = get_model(configs)\nmodel.summary()\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n9406464/9406464 [==============================] - 0s 0us/step\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n                                                                 \n resizing (Resizing)         (None, 32, 32, 1)         0         \n                                                                 \n conv2d (Conv2D)             (None, 32, 32, 3)         30        \n                                                                 \n tf.math.truediv (TFOpLambda  (None, 32, 32, 3)        0         \n )                                                               \n                                                                 \n tf.math.subtract (TFOpLambd  (None, 32, 32, 3)        0         \n a)                                                              \n                                                                 \n mobilenetv2_1.00_224 (Funct  (None, None, None, 1280)  2257984  \n ional)                                                          \n                                                                 \n global_average_pooling2d (G  (None, 1280)             0         \n lobalAveragePooling2D)                                          \n                                                                 \n dense (Dense)               (None, 10)                12810     \n                                                                 \n=================================================================\nTotal params: 2,270,824\nTrainable params: 12,840\nNon-trainable params: 2,257,984\n_________________________________________________________________\n\n\n\nmodel.compile(\n    optimizer = \"adam\",\n    loss = \"categorical_crossentropy\",\n    metrics = [\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top@5_accuracy')]\n)\n\n\n13.5.3.1 Train the model and log the predictions to a W&B Table\n\n# Initialize a W&B run\nrun = wandb.init(\n    project = \"my-keras-project\",\n    config = configs\n)\n\nwandb_callbacks = [\n        WandbMetricsLogger(log_freq=10),\n        WandbModelCheckpoint(filepath=\"my_model_{epoch:02d}\"),\n        WandbClsEvalCallback(\n            validloader,\n            data_table_columns=[\"idx\", \"image\", \"ground_truth\"],\n            pred_table_columns=[\"epoch\", \"idx\", \"image\", \"ground_truth\", \"prediction\"]\n        ) \n    ]\n\n# Train your model\nmodel.fit(\n    trainloader,\n    epochs = configs[\"epochs\"],\n    validation_data = validloader,\n    callbacks = wandb_callbacks\n)\n\n# Close the W&B run\nrun.finish()\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230524_070743-oy7pezpd\n\n\nSyncing run faithful-yogurt-2 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/phonchi/my-keras-project\n\n\n View run at https://wandb.ai/phonchi/my-keras-project/runs/oy7pezpd\n\n\nwandb:   101 of 101 files downloaded.  \n\n\nEpoch 1/10\n938/938 [==============================] - ETA: 0s - loss: 1.4748 - accuracy: 0.5377 - top@5_accuracy: 0.9332\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_01)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b938/938 [==============================] - 63s 62ms/step - loss: 1.4748 - accuracy: 0.5377 - top@5_accuracy: 0.9332 - val_loss: 1.1581 - val_accuracy: 0.6125 - val_top@5_accuracy: 0.9697\nEpoch 2/10\n935/938 [============================&gt;.] - ETA: 0s - loss: 1.0646 - accuracy: 0.6415 - top@5_accuracy: 0.9733\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_02)... Done. 0.1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b938/938 [==============================] - 48s 51ms/step - loss: 1.0645 - accuracy: 0.6415 - top@5_accuracy: 0.9733 - val_loss: 1.0253 - val_accuracy: 0.6412 - val_top@5_accuracy: 0.9737\nEpoch 3/10\n936/938 [============================&gt;.] - ETA: 0s - loss: 0.9811 - accuracy: 0.6626 - top@5_accuracy: 0.9757\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_03)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b938/938 [==============================] - 50s 53ms/step - loss: 0.9809 - accuracy: 0.6627 - top@5_accuracy: 0.9757 - val_loss: 0.9748 - val_accuracy: 0.6609 - val_top@5_accuracy: 0.9755\nEpoch 4/10\n935/938 [============================&gt;.] - ETA: 0s - loss: 0.9414 - accuracy: 0.6719 - top@5_accuracy: 0.9777\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_04)... Done. 0.1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b938/938 [==============================] - 49s 52ms/step - loss: 0.9414 - accuracy: 0.6720 - top@5_accuracy: 0.9777 - val_loss: 0.9463 - val_accuracy: 0.6672 - val_top@5_accuracy: 0.9782\nEpoch 5/10\n937/938 [============================&gt;.] - ETA: 0s - loss: 0.9164 - accuracy: 0.6802 - top@5_accuracy: 0.9794\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_05)... Done. 0.1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b938/938 [==============================] - 53s 56ms/step - loss: 0.9164 - accuracy: 0.6802 - top@5_accuracy: 0.9794 - val_loss: 0.9290 - val_accuracy: 0.6730 - val_top@5_accuracy: 0.9796\nEpoch 6/10\n938/938 [==============================] - ETA: 0s - loss: 0.8988 - accuracy: 0.6862 - top@5_accuracy: 0.9799\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_06)... Done. 0.1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b938/938 [==============================] - 49s 52ms/step - loss: 0.8988 - accuracy: 0.6862 - top@5_accuracy: 0.9799 - val_loss: 0.9072 - val_accuracy: 0.6822 - val_top@5_accuracy: 0.9799\nEpoch 7/10\n938/938 [==============================] - ETA: 0s - loss: 0.8845 - accuracy: 0.6912 - top@5_accuracy: 0.9803\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_07)... Done. 0.1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b938/938 [==============================] - 50s 53ms/step - loss: 0.8845 - accuracy: 0.6912 - top@5_accuracy: 0.9803 - val_loss: 0.9122 - val_accuracy: 0.6801 - val_top@5_accuracy: 0.9792\nEpoch 8/10\n937/938 [============================&gt;.] - ETA: 0s - loss: 0.8766 - accuracy: 0.6923 - top@5_accuracy: 0.9814\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_08)... Done. 0.1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b938/938 [==============================] - 51s 54ms/step - loss: 0.8766 - accuracy: 0.6923 - top@5_accuracy: 0.9814 - val_loss: 0.8987 - val_accuracy: 0.6842 - val_top@5_accuracy: 0.9795\nEpoch 9/10\n938/938 [==============================] - ETA: 0s - loss: 0.8687 - accuracy: 0.6953 - top@5_accuracy: 0.9815\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_09)... Done. 0.1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b938/938 [==============================] - 51s 55ms/step - loss: 0.8687 - accuracy: 0.6953 - top@5_accuracy: 0.9815 - val_loss: 0.8967 - val_accuracy: 0.6876 - val_top@5_accuracy: 0.9802\nEpoch 10/10\n936/938 [============================&gt;.] - ETA: 0s - loss: 0.8632 - accuracy: 0.6976 - top@5_accuracy: 0.9819\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_10)... Done. 0.1s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b938/938 [==============================] - 49s 52ms/step - loss: 0.8632 - accuracy: 0.6976 - top@5_accuracy: 0.9819 - val_loss: 0.8845 - val_accuracy: 0.6908 - val_top@5_accuracy: 0.9799\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\nbatch/accuracy\n▁▃▄▅▇▇▇▇█▇▇▇████████████████████████████\n\n\nbatch/batch_step\n▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n\n\nbatch/learning_rate\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nbatch/loss\n█▇▆▅▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nbatch/top@5_accuracy\n▁▄▆▆████████████████████████████████████\n\n\nepoch/accuracy\n▁▆▆▇▇█████\n\n\nepoch/epoch\n▁▂▃▃▄▅▆▆▇█\n\n\nepoch/learning_rate\n▁▁▁▁▁▁▁▁▁▁\n\n\nepoch/loss\n█▃▂▂▂▁▁▁▁▁\n\n\nepoch/top@5_accuracy\n▁▇▇▇██████\n\n\nepoch/val_accuracy\n▁▄▅▆▆▇▇▇██\n\n\nepoch/val_loss\n█▅▃▃▂▂▂▁▁▁\n\n\nepoch/val_top@5_accuracy\n▁▄▅▇██▇███\n\n\n\n\n\n\n13.5.4 Run summary:\n\n\n\n\n\nbatch/accuracy\n0.69742\n\n\nbatch/batch_step\n9390\n\n\nbatch/learning_rate\n0.001\n\n\nbatch/loss\n0.8639\n\n\nbatch/top@5_accuracy\n0.98187\n\n\nepoch/accuracy\n0.69758\n\n\nepoch/epoch\n9\n\n\nepoch/learning_rate\n0.001\n\n\nepoch/loss\n0.86324\n\n\nepoch/top@5_accuracy\n0.98192\n\n\nepoch/val_accuracy\n0.6908\n\n\nepoch/val_loss\n0.88455\n\n\nepoch/val_top@5_accuracy\n0.9799\n\n\n\n\n\n\n\n View run faithful-yogurt-2 at: https://wandb.ai/phonchi/my-keras-project/runs/oy7pezpdSynced 5 W&B file(s), 0 media file(s), 161 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230524_070743-oy7pezpd/logs\n\n\nWe can see a table from our database as follows:\n\n\n\n\n\n\n13.5.5 Introduction to Hyperparameter Sweeps using W&B and Keras\nSearching through high dimensional hyperparameter spaces to find the most performant model can get unwieldy very fast. Hyperparameter sweeps provide an organized and efficient way to conduct a battle royale of models and pick the most accurate model. They enable this by automatically searching through combinations of hyperparameter values (e.g. learning rate, batch size, number of hidden layers, optimizer type) to find the most optimal values.\n\n13.5.5.1 Sweeps: An Overview\nRunning a hyperparameter sweep with Weights & Biases is very easy. There are just 3 simple steps:\n\nDefine the sweep: we do this by creating a dictionary or a YAML file that specifies the parameters to search through, the search strategy, the optimization metric et. al.\nInitialize the sweep: with one line of code we initialize the sweep and pass in the dictionary of sweep configurations: sweep_id = wandb.sweep(sweep_config)\nRun the sweep agent: also accomplished with one line of code, we call wandb.agent() and pass the sweep_id to run, along with a function that defines your model architecture and trains it: wandb.agent(sweep_id, function=train)\n\nAnd voila! That’s all there is to running a hyperparameter sweep! In the notebook below, we’ll walk through these 3 steps in more detail.\nWe will use MNIST directly from tf.keras.datasets\n\n# Get the dataset\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nnum_classes = len(np.unique(y_train))\ninput_shape = x_train.shape[-2:] + (1,)\n\n# Scale\nx_train = x_train / 255.\nx_test = x_test / 255.\n\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\n\n# convert class vectors to binary class matrices\ny_train = tf.keras.utils.to_categorical(y_train, num_classes)\ny_test = tf.keras.utils.to_categorical(y_test, num_classes)\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11490434/11490434 [==============================] - 0s 0us/step\n\n\nDefine a simple model\n\ndef ConvNet(dropout=0.2):\n    return tf.keras.Sequential(\n    [\n        tf.keras.Input(shape=input_shape),\n        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n\nmodel = ConvNet()\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 26, 26, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n 2D)                                                             \n                                                                 \n flatten (Flatten)           (None, 1600)              0         \n                                                                 \n dropout (Dropout)           (None, 1600)              0         \n                                                                 \n dense (Dense)               (None, 10)                16010     \n                                                                 \n=================================================================\nTotal params: 34,826\nTrainable params: 34,826\nNon-trainable params: 0\n_________________________________________________________________\n\n\nThe training script is as follows:\n\ndef get_optimizer(lr=1e-3, optimizer=\"adam\"):\n    \"Select optmizer between adam and sgd with momentum\"\n    if optimizer.lower() == \"adam\":\n        return tf.keras.optimizers.Adam(learning_rate=lr)\n    if optimizer.lower() == \"sgd\":\n        return tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.1)\n\ndef train(model, batch_size=64, epochs=10, lr=1e-3, optimizer='adam', log_freq=10):  \n    \n    # Compile model like you usually do.\n    tf.keras.backend.clear_session()\n    model.compile(loss=\"categorical_crossentropy\", \n                  optimizer=get_optimizer(lr, optimizer), \n                  metrics=[\"accuracy\"])\n\n    # callback setup\n    wandb_callbacks = [\n        WandbMetricsLogger(log_freq=log_freq),\n        WandbModelCheckpoint(filepath=\"my_model_{epoch:02d}\")\n    ]\n\n    model.fit(x_train, \n              y_train, \n              batch_size=batch_size, \n              epochs=epochs, \n              validation_data=(x_test, y_test), \n              callbacks=wandb_callbacks)\n\n\n\n13.5.5.2 Define the Sweep\nFundamentally, a Sweep combines a strategy for trying out a bunch of hyperparameter values with the code that evalutes them. Whether that strategy is as simple as trying every option or as complex as BOHB, Weights & Biases Sweeps have you covered. You just need to define your strategy in the form of a configuration.\nWhen you’re setting up a Sweep in a notebook like this, that config object is a nested dictionary. When you run a Sweep via the command line, the config object is a YAML file. Let’s walk through the definition of a Sweep config together. We’ll do it slowly, so we get a chance to explain each component. In a typical Sweep pipeline, this step would be done in a single assignment.\nThe first thing we need to define is the method for choosing new parameter values.\nWe provide the following search methods: * Grid Search – Iterate over every combination of hyperparameter values. Very effective, but can be computationally costly. * Random Search – Select each new combination at random according to provided distributions. Surprisingly effective! * Bayesian Search – Create a probabilistic model of metric score as a function of the hyperparameters, and choose parameters with high probability of improving the metric. Works well for small numbers of continuous parameters but scales poorly.\nWe’ll stick with bayes here.\n\nsweep_config = {\n    'method': 'bayes'\n    }\n\nFor bayes Sweeps, you also need to tell it a bit about your metric. W&B need to know its name, so it can find it in the model outputs and it need to know whether your goal is to minimize it (e.g. if it’s the squared error) or to maximize it (e.g. if it’s the accuracy).\n\nmetric = {\n    'name': 'val_loss',\n    'goal': 'minimize'   \n    }\n\nsweep_config['metric'] = metric\n\nIf you’re not running a bayes Sweep, you don’t have to, but it’s not a bad idea to include this in your sweep_config anyway, in case you change your mind later. It’s also good reproducibility practice to keep note of things like this, in case you, or someone else, come back to your Sweep in 6 months and don’t know whether val_G_batch is supposed to be high or low.\nOnce you’ve picked a method to try out new values of the hyperparameters, you need to define what those parameters are. Most of the time, this step is straightforward: you just give the parameter a name and specify a list of legal values of the parameter.\nFor example, when we choose the optimizer for our network, there’s only a finite number of options. Here we stick with the two most popular choices, adam and sgd. Even for hyperparameters that have potentially infinite options, it usually only makes sense to try out a few select values, as we do here with dropout:\n\nparameters_dict = {\n    'optimizer': {\n        'values': ['adam', 'sgd']\n        },\n    'dropout': {\n          'values': [0.1, 0.3, 0.5]\n        },\n    }\n\nsweep_config['parameters'] = parameters_dict\n\nIt’s often the case that there are hyperparameters that we don’t want to vary in this Sweep, but which we still want to set in our sweep_config. In that case, we just set the value directly:\n\nparameters_dict.update({\n    'epochs': {\n        'value': 1}\n    })\n\nFor a grid search, that’s all you ever need. For a random search, all the values of a parameter are equally likely to be chosen on a given run.\nIf that just won’t do, you can instead specify a named distribution, plus its parameters, like the mean mu and standard deviation sigma of a normal distribution. See more on how to set the distributions of your random variables here.\n\nparameters_dict.update({\n    'learning_rate': {\n        # a flat distribution between 0 and 0.1\n        'distribution': 'uniform',\n        'min': 0.001,\n        'max': 0.1\n      },\n    'batch_size': {\n        'values': [64, 128]\n      }\n    })\n\nWhen we’re finished, sweep_config is a nested dictionary that specifies exactly which parameters we’re interested in trying and what method we’re going to use to try them.\n\npprint.pprint(sweep_config)\n\n{'method': 'bayes',\n 'metric': {'goal': 'minimize', 'name': 'val_loss'},\n 'parameters': {'batch_size': {'values': [64, 128]},\n                'dropout': {'values': [0.1, 0.3, 0.5]},\n                'epochs': {'value': 1},\n                'learning_rate': {'distribution': 'uniform',\n                                  'max': 0.1,\n                                  'min': 0.001},\n                'optimizer': {'values': ['adam', 'sgd']}}}\n\n\nBut that’s not all of the configuration options! For example, we also offer the option to early_terminate your runs with the HyperBand scheduling algorithm. See more here. You can find a list of all configuration options here and a big collection of examples in YAML format here.\n\n\n13.5.5.3 Wrap the Training Loop\nYou’ll need a function, like sweep_train() below, that uses wandb.config to set the hyperparameters before train gets called.\n\ndef sweep_train(config_defaults=None):\n    # Initialize wandb with a sample project name\n    with wandb.init(config=config_defaults):  # this gets over-written in the Sweep\n\n        # Specify the other hyperparameters to the configuration, if any\n        wandb.config.architecture_name = \"ConvNet\"\n        wandb.config.dataset_name = \"MNIST\"\n\n        # initialize model\n        model = ConvNet(wandb.config.dropout)\n\n        train(model, \n              wandb.config.batch_size, \n              wandb.config.epochs,\n              wandb.config.learning_rate,\n              wandb.config.optimizer)\n\n\nsweep_id = wandb.sweep(sweep_config, project=\"sweeps-keras-test\")\n\n\n\n\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\n\nCreate sweep with ID: 3jg0srrt\nSweep URL: https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\nYou can limit the number of total runs with the count parameter, we will limit a 10 to make the script run fast, feel free to increase the number of runs and see what happens.\n\nwandb.agent(sweep_id, function=sweep_train, count=10)\n\nwandb: Agent Starting Run: w3yo9n5b with config:\nwandb:  batch_size: 128\nwandb:  dropout: 0.1\nwandb:  epochs: 1\nwandb:  learning_rate: 0.06930189510244192\nwandb:  optimizer: adam\nwandb: Currently logged in as: phonchi. Use `wandb login --relogin` to force relogin\n\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230528_044309-w3yo9n5b\n\n\nSyncing run spring-sweep-1 to Weights & Biases (docs)Sweep page: https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View project at https://wandb.ai/phonchi/sweeps-keras-test\n\n\n View sweep at https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View run at https://wandb.ai/phonchi/sweeps-keras-test/runs/w3yo9n5b\n\n\n469/469 [==============================] - ETA: 0s - loss: 2.4057 - accuracy: 0.1047\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_01)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b469/469 [==============================] - 14s 9ms/step - loss: 2.4057 - accuracy: 0.1047 - val_loss: 2.3041 - val_accuracy: 0.1028\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\nbatch/accuracy\n▇▁█▆▄▅▄▃▄▂▂▂▃▃▃▄▅▅▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄\n\n\nbatch/batch_step\n▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n\n\nbatch/learning_rate\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nbatch/loss\n▁█▅▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nepoch/accuracy\n▁\n\n\nepoch/epoch\n▁\n\n\nepoch/learning_rate\n▁\n\n\nepoch/loss\n▁\n\n\nepoch/val_accuracy\n▁\n\n\nepoch/val_loss\n▁\n\n\n\n\n\n\n13.5.6 Run summary:\n\n\n\n\n\nbatch/accuracy\n0.10461\n\n\nbatch/batch_step\n460\n\n\nbatch/learning_rate\n0.0693\n\n\nbatch/loss\n2.40738\n\n\nepoch/accuracy\n0.1047\n\n\nepoch/epoch\n0\n\n\nepoch/learning_rate\n0.0693\n\n\nepoch/loss\n2.40567\n\n\nepoch/val_accuracy\n0.1028\n\n\nepoch/val_loss\n2.30411\n\n\n\n\n\n\n\n View run spring-sweep-1 at: https://wandb.ai/phonchi/sweeps-keras-test/runs/w3yo9n5bSynced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230528_044309-w3yo9n5b/logs\n\n\nwandb: Agent Starting Run: avxmaq9k with config:\nwandb:  batch_size: 64\nwandb:  dropout: 0.3\nwandb:  epochs: 1\nwandb:  learning_rate: 0.004034650060468464\nwandb:  optimizer: adam\n\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230528_044339-avxmaq9k\n\n\nSyncing run wild-sweep-2 to Weights & Biases (docs)Sweep page: https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View project at https://wandb.ai/phonchi/sweeps-keras-test\n\n\n View sweep at https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View run at https://wandb.ai/phonchi/sweeps-keras-test/runs/avxmaq9k\n\n\n938/938 [==============================] - ETA: 0s - loss: 0.1364 - accuracy: 0.9578\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_01)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b938/938 [==============================] - 8s 8ms/step - loss: 0.1364 - accuracy: 0.9578 - val_loss: 0.0560 - val_accuracy: 0.9808\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\nbatch/accuracy\n▁▅▆▇▇▇▇▇▇███████████████████████████████\n\n\nbatch/batch_step\n▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n\n\nbatch/learning_rate\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nbatch/loss\n█▅▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nepoch/accuracy\n▁\n\n\nepoch/epoch\n▁\n\n\nepoch/learning_rate\n▁\n\n\nepoch/loss\n▁\n\n\nepoch/val_accuracy\n▁\n\n\nepoch/val_loss\n▁\n\n\n\n\n\n\n13.5.7 Run summary:\n\n\n\n\n\nbatch/accuracy\n0.95762\n\n\nbatch/batch_step\n930\n\n\nbatch/learning_rate\n0.00403\n\n\nbatch/loss\n0.13691\n\n\nepoch/accuracy\n0.95778\n\n\nepoch/epoch\n0\n\n\nepoch/learning_rate\n0.00403\n\n\nepoch/loss\n0.13635\n\n\nepoch/val_accuracy\n0.9808\n\n\nepoch/val_loss\n0.05604\n\n\n\n\n\n\n\n View run wild-sweep-2 at: https://wandb.ai/phonchi/sweeps-keras-test/runs/avxmaq9kSynced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230528_044339-avxmaq9k/logs\n\n\nwandb: Agent Starting Run: jsucsgpn with config:\nwandb:  batch_size: 64\nwandb:  dropout: 0.1\nwandb:  epochs: 1\nwandb:  learning_rate: 0.06346304009072952\nwandb:  optimizer: sgd\n\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230528_044405-jsucsgpn\n\n\nSyncing run happy-sweep-3 to Weights & Biases (docs)Sweep page: https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View project at https://wandb.ai/phonchi/sweeps-keras-test\n\n\n View sweep at https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View run at https://wandb.ai/phonchi/sweeps-keras-test/runs/jsucsgpn\n\n\n938/938 [==============================] - ETA: 0s - loss: 0.2785 - accuracy: 0.9157\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_01)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b938/938 [==============================] - 7s 7ms/step - loss: 0.2785 - accuracy: 0.9157 - val_loss: 0.0945 - val_accuracy: 0.9715\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\nbatch/accuracy\n▁▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████████████████\n\n\nbatch/batch_step\n▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n\n\nbatch/learning_rate\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nbatch/loss\n█▇▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nepoch/accuracy\n▁\n\n\nepoch/epoch\n▁\n\n\nepoch/learning_rate\n▁\n\n\nepoch/loss\n▁\n\n\nepoch/val_accuracy\n▁\n\n\nepoch/val_loss\n▁\n\n\n\n\n\n\n13.5.8 Run summary:\n\n\n\n\n\nbatch/accuracy\n0.9154\n\n\nbatch/batch_step\n930\n\n\nbatch/learning_rate\n0.06346\n\n\nbatch/loss\n0.27971\n\n\nepoch/accuracy\n0.91573\n\n\nepoch/epoch\n0\n\n\nepoch/learning_rate\n0.06346\n\n\nepoch/loss\n0.27853\n\n\nepoch/val_accuracy\n0.9715\n\n\nepoch/val_loss\n0.09455\n\n\n\n\n\n\n\n View run happy-sweep-3 at: https://wandb.ai/phonchi/sweeps-keras-test/runs/jsucsgpnSynced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230528_044405-jsucsgpn/logs\n\n\nwandb: Sweep Agent: Waiting for job.\nwandb: Job received.\nwandb: Agent Starting Run: jiiph0o0 with config:\nwandb:  batch_size: 128\nwandb:  dropout: 0.5\nwandb:  epochs: 1\nwandb:  learning_rate: 0.04159232096092625\nwandb:  optimizer: adam\n\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230528_044438-jiiph0o0\n\n\nSyncing run silver-sweep-4 to Weights & Biases (docs)Sweep page: https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View project at https://wandb.ai/phonchi/sweeps-keras-test\n\n\n View sweep at https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View run at https://wandb.ai/phonchi/sweeps-keras-test/runs/jiiph0o0\n\n\n464/469 [============================&gt;.] - ETA: 0s - loss: 0.3718 - accuracy: 0.8901\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_01)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b469/469 [==============================] - 5s 8ms/step - loss: 0.3705 - accuracy: 0.8906 - val_loss: 0.1096 - val_accuracy: 0.9697\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\nbatch/accuracy\n▁▂▄▅▅▆▆▇▇▇▇▇▇▇▇▇▇███████████████████████\n\n\nbatch/batch_step\n▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n\n\nbatch/learning_rate\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nbatch/loss\n▆█▅▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nepoch/accuracy\n▁\n\n\nepoch/epoch\n▁\n\n\nepoch/learning_rate\n▁\n\n\nepoch/loss\n▁\n\n\nepoch/val_accuracy\n▁\n\n\nepoch/val_loss\n▁\n\n\n\n\n\n\n13.5.9 Run summary:\n\n\n\n\n\nbatch/accuracy\n0.88976\n\n\nbatch/batch_step\n460\n\n\nbatch/learning_rate\n0.04159\n\n\nbatch/loss\n0.37278\n\n\nepoch/accuracy\n0.89058\n\n\nepoch/epoch\n0\n\n\nepoch/learning_rate\n0.04159\n\n\nepoch/loss\n0.3705\n\n\nepoch/val_accuracy\n0.9697\n\n\nepoch/val_loss\n0.10964\n\n\n\n\n\n\n\n View run silver-sweep-4 at: https://wandb.ai/phonchi/sweeps-keras-test/runs/jiiph0o0Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230528_044438-jiiph0o0/logs\n\n\nwandb: Agent Starting Run: nmat1qcr with config:\nwandb:  batch_size: 128\nwandb:  dropout: 0.5\nwandb:  epochs: 1\nwandb:  learning_rate: 0.05519859429843507\nwandb:  optimizer: sgd\n\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230528_044501-nmat1qcr\n\n\nSyncing run glamorous-sweep-5 to Weights & Biases (docs)Sweep page: https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View project at https://wandb.ai/phonchi/sweeps-keras-test\n\n\n View sweep at https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View run at https://wandb.ai/phonchi/sweeps-keras-test/runs/nmat1qcr\n\n\n469/469 [==============================] - ETA: 0s - loss: 0.5343 - accuracy: 0.8305\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_01)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b469/469 [==============================] - 5s 10ms/step - loss: 0.5343 - accuracy: 0.8305 - val_loss: 0.1431 - val_accuracy: 0.9573\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\nbatch/accuracy\n▁▂▂▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████████████\n\n\nbatch/batch_step\n▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n\n\nbatch/learning_rate\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nbatch/loss\n███▇▇▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nepoch/accuracy\n▁\n\n\nepoch/epoch\n▁\n\n\nepoch/learning_rate\n▁\n\n\nepoch/loss\n▁\n\n\nepoch/val_accuracy\n▁\n\n\nepoch/val_loss\n▁\n\n\n\n\n\n\n13.5.10 Run summary:\n\n\n\n\n\nbatch/accuracy\n0.82862\n\n\nbatch/batch_step\n460\n\n\nbatch/learning_rate\n0.0552\n\n\nbatch/loss\n0.54003\n\n\nepoch/accuracy\n0.8305\n\n\nepoch/epoch\n0\n\n\nepoch/learning_rate\n0.0552\n\n\nepoch/loss\n0.5343\n\n\nepoch/val_accuracy\n0.9573\n\n\nepoch/val_loss\n0.14307\n\n\n\n\n\n\n\n View run glamorous-sweep-5 at: https://wandb.ai/phonchi/sweeps-keras-test/runs/nmat1qcrSynced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230528_044501-nmat1qcr/logs\n\n\nwandb: Sweep Agent: Waiting for job.\nwandb: Job received.\nwandb: Agent Starting Run: 8qocqbjb with config:\nwandb:  batch_size: 128\nwandb:  dropout: 0.1\nwandb:  epochs: 1\nwandb:  learning_rate: 0.042115826914488744\nwandb:  optimizer: sgd\n\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230528_044546-8qocqbjb\n\n\nSyncing run fearless-sweep-6 to Weights & Biases (docs)Sweep page: https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View project at https://wandb.ai/phonchi/sweeps-keras-test\n\n\n View sweep at https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View run at https://wandb.ai/phonchi/sweeps-keras-test/runs/8qocqbjb\n\n\n458/469 [============================&gt;.] - ETA: 0s - loss: 0.5663 - accuracy: 0.8294\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_01)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b469/469 [==============================] - 4s 8ms/step - loss: 0.5578 - accuracy: 0.8319 - val_loss: 0.1772 - val_accuracy: 0.9485\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\nRun history:\n\n\n\nbatch/accuracy\n▁▂▃▃▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████████\n\n\nbatch/batch_step\n▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n\n\nbatch/learning_rate\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nbatch/loss\n███▇▇▇▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n\n\nepoch/accuracy\n▁\n\n\nepoch/epoch\n▁\n\n\nepoch/learning_rate\n▁\n\n\nepoch/loss\n▁\n\n\nepoch/val_accuracy\n▁\n\n\nepoch/val_loss\n▁\n\n\n\n\n\n\n13.5.11 Run summary:\n\n\n\n\n\nbatch/accuracy\n0.82994\n\n\nbatch/batch_step\n460\n\n\nbatch/learning_rate\n0.04212\n\n\nbatch/loss\n0.56409\n\n\nepoch/accuracy\n0.83188\n\n\nepoch/epoch\n0\n\n\nepoch/learning_rate\n0.04212\n\n\nepoch/loss\n0.55776\n\n\nepoch/val_accuracy\n0.9485\n\n\nepoch/val_loss\n0.17717\n\n\n\n\n\n\n\n View run fearless-sweep-6 at: https://wandb.ai/phonchi/sweeps-keras-test/runs/8qocqbjbSynced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230528_044546-8qocqbjb/logs\n\n\nwandb: Agent Starting Run: fzf3w5ev with config:\nwandb:  batch_size: 128\nwandb:  dropout: 0.3\nwandb:  epochs: 1\nwandb:  learning_rate: 0.08608979253191129\nwandb:  optimizer: sgd\n\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230528_044619-fzf3w5ev\n\n\nSyncing run rosy-sweep-7 to Weights & Biases (docs)Sweep page: https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View project at https://wandb.ai/phonchi/sweeps-keras-test\n\n\n View sweep at https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View run at https://wandb.ai/phonchi/sweeps-keras-test/runs/fzf3w5ev\n\n\n460/469 [============================&gt;.] - ETA: 0s - loss: 0.4030 - accuracy: 0.8767\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_01)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b469/469 [==============================] - 8s 15ms/step - loss: 0.3984 - accuracy: 0.8782 - val_loss: 0.1189 - val_accuracy: 0.9655\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\nRun history:\n\n\n\nbatch/accuracy\n▁▂▃▄▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████████████████\n\n\nbatch/batch_step\n▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n\n\nbatch/learning_rate\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nbatch/loss\n██▇▆▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nepoch/accuracy\n▁\n\n\nepoch/epoch\n▁\n\n\nepoch/learning_rate\n▁\n\n\nepoch/loss\n▁\n\n\nepoch/val_accuracy\n▁\n\n\nepoch/val_loss\n▁\n\n\n\n\n\n\n13.5.12 Run summary:\n\n\n\n\n\nbatch/accuracy\n0.8769\n\n\nbatch/batch_step\n460\n\n\nbatch/learning_rate\n0.08609\n\n\nbatch/loss\n0.4025\n\n\nepoch/accuracy\n0.87817\n\n\nepoch/epoch\n0\n\n\nepoch/learning_rate\n0.08609\n\n\nepoch/loss\n0.39843\n\n\nepoch/val_accuracy\n0.9655\n\n\nepoch/val_loss\n0.1189\n\n\n\n\n\n\n\n View run rosy-sweep-7 at: https://wandb.ai/phonchi/sweeps-keras-test/runs/fzf3w5evSynced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230528_044619-fzf3w5ev/logs\n\n\nwandb: Sweep Agent: Waiting for job.\nwandb: Job received.\nwandb: Agent Starting Run: h9nyuncw with config:\nwandb:  batch_size: 128\nwandb:  dropout: 0.5\nwandb:  epochs: 1\nwandb:  learning_rate: 0.05812571512662621\nwandb:  optimizer: sgd\n\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230528_044656-h9nyuncw\n\n\nSyncing run dashing-sweep-8 to Weights & Biases (docs)Sweep page: https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View project at https://wandb.ai/phonchi/sweeps-keras-test\n\n\n View sweep at https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View run at https://wandb.ai/phonchi/sweeps-keras-test/runs/h9nyuncw\n\n\n461/469 [============================&gt;.] - ETA: 0s - loss: 0.5099 - accuracy: 0.8407\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_01)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b469/469 [==============================] - 5s 10ms/step - loss: 0.5048 - accuracy: 0.8424 - val_loss: 0.1392 - val_accuracy: 0.9599\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\nbatch/accuracy\n▁▂▂▃▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇████████████████\n\n\nbatch/batch_step\n▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n\n\nbatch/learning_rate\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nbatch/loss\n███▇▇▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nepoch/accuracy\n▁\n\n\nepoch/epoch\n▁\n\n\nepoch/learning_rate\n▁\n\n\nepoch/loss\n▁\n\n\nepoch/val_accuracy\n▁\n\n\nepoch/val_loss\n▁\n\n\n\n\n\n\n13.5.13 Run summary:\n\n\n\n\n\nbatch/accuracy\n0.8407\n\n\nbatch/batch_step\n460\n\n\nbatch/learning_rate\n0.05813\n\n\nbatch/loss\n0.5099\n\n\nepoch/accuracy\n0.84237\n\n\nepoch/epoch\n0\n\n\nepoch/learning_rate\n0.05813\n\n\nepoch/loss\n0.50481\n\n\nepoch/val_accuracy\n0.9599\n\n\nepoch/val_loss\n0.13917\n\n\n\n\n\n\n\n View run dashing-sweep-8 at: https://wandb.ai/phonchi/sweeps-keras-test/runs/h9nyuncwSynced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230528_044656-h9nyuncw/logs\n\n\nwandb: Agent Starting Run: q6w4eo98 with config:\nwandb:  batch_size: 64\nwandb:  dropout: 0.5\nwandb:  epochs: 1\nwandb:  learning_rate: 0.0840398624399258\nwandb:  optimizer: sgd\n\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230528_044722-q6w4eo98\n\n\nSyncing run giddy-sweep-9 to Weights & Biases (docs)Sweep page: https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View project at https://wandb.ai/phonchi/sweeps-keras-test\n\n\n View sweep at https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View run at https://wandb.ai/phonchi/sweeps-keras-test/runs/q6w4eo98\n\n\n933/938 [============================&gt;.] - ETA: 0s - loss: 0.3203 - accuracy: 0.9004\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_01)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b938/938 [==============================] - 7s 7ms/step - loss: 0.3192 - accuracy: 0.9007 - val_loss: 0.0900 - val_accuracy: 0.9729\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\nbatch/accuracy\n▁▂▃▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████████████\n\n\nbatch/batch_step\n▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n\n\nbatch/learning_rate\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nbatch/loss\n██▇▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nepoch/accuracy\n▁\n\n\nepoch/epoch\n▁\n\n\nepoch/learning_rate\n▁\n\n\nepoch/loss\n▁\n\n\nepoch/val_accuracy\n▁\n\n\nepoch/val_loss\n▁\n\n\n\n\n\n\n13.5.14 Run summary:\n\n\n\n\n\nbatch/accuracy\n0.90024\n\n\nbatch/batch_step\n930\n\n\nbatch/learning_rate\n0.08404\n\n\nbatch/loss\n0.32056\n\n\nepoch/accuracy\n0.90068\n\n\nepoch/epoch\n0\n\n\nepoch/learning_rate\n0.08404\n\n\nepoch/loss\n0.31923\n\n\nepoch/val_accuracy\n0.9729\n\n\nepoch/val_loss\n0.09002\n\n\n\n\n\n\n\n View run giddy-sweep-9 at: https://wandb.ai/phonchi/sweeps-keras-test/runs/q6w4eo98Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230528_044722-q6w4eo98/logs\n\n\nwandb: Agent Starting Run: 0u2eenmd with config:\nwandb:  batch_size: 64\nwandb:  dropout: 0.5\nwandb:  epochs: 1\nwandb:  learning_rate: 0.06016121976975345\nwandb:  optimizer: adam\n\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230528_044747-0u2eenmd\n\n\nSyncing run toasty-sweep-10 to Weights & Biases (docs)Sweep page: https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View project at https://wandb.ai/phonchi/sweeps-keras-test\n\n\n View sweep at https://wandb.ai/phonchi/sweeps-keras-test/sweeps/3jg0srrt\n\n\n View run at https://wandb.ai/phonchi/sweeps-keras-test/runs/0u2eenmd\n\n\n935/938 [============================&gt;.] - ETA: 0s - loss: 0.4499 - accuracy: 0.8718\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./my_model_01)... Done. 0.0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b938/938 [==============================] - 8s 7ms/step - loss: 0.4496 - accuracy: 0.8718 - val_loss: 0.1438 - val_accuracy: 0.9590\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\nRun history:\n\n\n\nbatch/accuracy\n▁▄▅▆▇▇▇▇▇▇██████████████████████████████\n\n\nbatch/batch_step\n▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n\n\nbatch/learning_rate\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nbatch/loss\n▇█▅▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nepoch/accuracy\n▁\n\n\nepoch/epoch\n▁\n\n\nepoch/learning_rate\n▁\n\n\nepoch/loss\n▁\n\n\nepoch/val_accuracy\n▁\n\n\nepoch/val_loss\n▁\n\n\n\n\n\n\n13.5.15 Run summary:\n\n\n\n\n\nbatch/accuracy\n0.87161\n\n\nbatch/batch_step\n930\n\n\nbatch/learning_rate\n0.06016\n\n\nbatch/loss\n0.45045\n\n\nepoch/accuracy\n0.87183\n\n\nepoch/epoch\n0\n\n\nepoch/learning_rate\n0.06016\n\n\nepoch/loss\n0.44964\n\n\nepoch/val_accuracy\n0.959\n\n\nepoch/val_loss\n0.14376\n\n\n\n\n\n\n\n View run toasty-sweep-10 at: https://wandb.ai/phonchi/sweeps-keras-test/runs/0u2eenmdSynced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230528_044747-0u2eenmd/logs\n\n\n\n\n\n13.5.16 Model and data versioning\nChoose one of the three dataset size options below to run the rest of the demo. With fewer images, you’ll run through the demo much faster and use less storage space. With more images, you’ll get more realistic model training and more interesting results and examples to explore.\nNote: for the largest dataset, this stage might take a few minutes. If you end up needing to rerun a cell, comment out the first capture line (change %%capture to #%%capture ) so you can respond to the prompt about re-downloading the dataset (and see the progress bar). Each zipped directory contains randomly sampled images from the iNaturalist dataset, evenly distributed across 10 classes of living things like birds, insects, plants, and mammals (names given in Latin—so Aves, Insecta, Plantae, etc :).\n\n# set SIZE to \"TINY\", \"MEDIUM\", or \"LARGE\"\n# to select one of these three datasets\n# TINY dataset: 100 images, 30MB\n# MEDIUM dataset: 1000 images, 312MB\n# LARGE datast: 12,000 images, 3.6GB\n\nSIZE = \"TINY\"\n\n\nif SIZE == \"TINY\":\n    src_url = \"https://storage.googleapis.com/wandb_datasets/nature_100.zip\"\n    src_zip = \"nature_100.zip\"\n    DATA_SRC = \"nature_100\"\n    IMAGES_PER_LABEL = 10\n    BALANCED_SPLITS = {\"train\" : 8, \"val\" : 1, \"test\": 1}\nelif SIZE == \"MEDIUM\":\n    src_url = \"https://storage.googleapis.com/wandb_datasets/nature_1K.zip\"\n    src_zip = \"nature_1K.zip\"\n    DATA_SRC = \"nature_1K\"\n    IMAGES_PER_LABEL = 100\n    BALANCED_SPLITS = {\"train\" : 80, \"val\" : 10, \"test\": 10}\nelif SIZE == \"LARGE\":\n    src_url = \"https://storage.googleapis.com/wandb_datasets/nature_12K.zip\"\n    src_zip = \"nature_12K.zip\"\n    DATA_SRC = \"inaturalist_12K/train\" # (technically a subset of only 10K images)\n    IMAGES_PER_LABEL = 1000\n    BALANCED_SPLITS = {\"train\" : 800, \"val\" : 100, \"test\": 100}\n\n\n%%capture\n!curl -SL $src_url &gt; $src_zip\n!unzip $src_zip\n\n\n# source directory for all raw data\nSRC = DATA_SRC\n# number of images per class label\n# the total number of images is 10X this (10 classes)\nTOTAL_IMAGES = IMAGES_PER_LABEL * 10\nPROJECT_NAME = \"artifacts_demo\"\nPREFIX = \"inat\" # convenient for tracking local data\n\n\n13.5.16.1 Upload raw data\nYou can use log_artifact() to upload the data to the database.\n\nRAW_DATA_AT = \"_\".join([PREFIX, \"raw_data\", str(TOTAL_IMAGES)])\nrun = wandb.init(project=PROJECT_NAME, job_type=\"upload\")\n\n\n\n\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230524_091424-n767hv0i\n\n\nSyncing run iconic-serenity-1 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/phonchi/artifacts_demo\n\n\n View run at https://wandb.ai/phonchi/artifacts_demo/runs/n767hv0i\n\n\n\n# create an artifact for all the raw data\nraw_data_at = wandb.Artifact(RAW_DATA_AT, type=\"raw_data\")\n\n# SRC_DIR contains 10 folders, one for each of 10 class labels\n# each folder contains images of the corresponding class\nlabels = os.listdir(SRC)\nfor l in labels:\n    imgs_per_label = os.path.join(SRC, l)\n    if os.path.isdir(imgs_per_label):\n        imgs = os.listdir(imgs_per_label)\n        # randomize the order\n        shuffle(imgs)\n        img_file_ids = imgs[:IMAGES_PER_LABEL]\n        for f in img_file_ids:\n            file_path = os.path.join(SRC, l, f)\n            # add file to artifact by full path\n            raw_data_at.add_file(file_path, name=l + \"/\" + f)\n\n# save artifact to W&B\nrun.log_artifact(raw_data_at)\nrun.finish()\n\nWaiting for W&B process to finish... (success).\n\n\n View run iconic-serenity-1 at: https://wandb.ai/phonchi/artifacts_demo/runs/n767hv0iSynced 4 W&B file(s), 0 media file(s), 100 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230524_091424-n767hv0i/logs\n\n\nNow we prepare a data split:\n\nrun = wandb.init(project=PROJECT_NAME, job_type=\"data_split\")\n\n# find the most recent (\"latest\") version of the full raw data\n# you can of course pass around programmatic aliases and not string literals\ndata_at = run.use_artifact(RAW_DATA_AT + \":latest\")\n# download it locally (for illustration purposes/across hardware; you can\n# also sync/version artifacts by reference)\ndata_dir = data_at.download()\n\n# create balanced train, val, test splits\n# each count is the number of images per label\nDATA_SPLITS = BALANCED_SPLITS\n\nats = {}\n# wrap artifacts in dictionary for convenience\nfor split, count in DATA_SPLITS.items():\n    ats[split] = wandb.Artifact(\"_\".join([PREFIX, split, \"data\", str(count*10)]), \n                                \"_\".join([split, \"data\"]))\n\nlabels = os.listdir(data_dir)\nfor l in labels:\n    if l.startswith(\".\"): # skip non-label file\n        continue\n    imgs_per_label = os.listdir(os.path.join(data_dir, l))\n    shuffle(imgs_per_label)\n    start_id = 0\n    for split, count in DATA_SPLITS.items():\n        # take a subset\n        split_imgs = imgs_per_label[start_id:start_id+count]\n        for img_file in split_imgs:\n            full_path = os.path.join(data_dir, l, img_file)\n            # add file to artifact by full path\n            # note: pass the label to the name parameter to retain it in\n            # the data structure \n            ats[split].add_file(full_path, name = os.path.join(l, img_file))\n        start_id += count\n\n# save all three artifacts to W&B\n# note: yes, in this example, we are cheating and have labels for the \"test\" data ;)\nfor split, artifact in ats.items():\n    run.log_artifact(artifact)\n\nrun.finish()\n\nwandb: Currently logged in as: phonchi. Use `wandb login --relogin` to force relogin\n\n\n\n\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230524_091509-ppnqommz\n\n\nSyncing run pretty-puddle-2 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/phonchi/artifacts_demo\n\n\n View run at https://wandb.ai/phonchi/artifacts_demo/runs/ppnqommz\n\n\nwandb:   100 of 100 files downloaded.  \n\n\nWaiting for W&B process to finish... (success).\n\n\n View run pretty-puddle-2 at: https://wandb.ai/phonchi/artifacts_demo/runs/ppnqommzSynced 5 W&B file(s), 0 media file(s), 100 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230524_091509-ppnqommz/logs\n\n\n\n\n13.5.16.2 Train with artifacts and save model\nYou can use use_artifact() to download and use stored artifact.\n\n# EXPERIMENT CONFIG\n#---------------------------\n# if you modify these, make sure the total count is less than or equal to\n# the number of files uploaded for that split in the train/val data artifact\nNUM_TRAIN = BALANCED_SPLITS[\"train\"] * 10\nNUM_VAL = BALANCED_SPLITS[\"val\"] * 10\nNUM_EPOCHS = 1 # set low for demo purposes; try 3, 5, or as many as you like\n\n# model name\n# if you want to train a sufficiently different model, give this a new name\n# to start a new lineage for the model, instead of just incrementing the\n# version of the old model\nMODEL_NAME = \"iv3_trained\"\n\n# folder in which to save initial, untrained model\nINIT_MODEL_DIR = \"init_model_keras_iv3\"\n\n# folder in which to save the final, trained model\n# if you want to train a sufficiently different model, give this a new name\n# to start a new lineage for the model, instead of just incrementing the\n# version of the old model\nFINAL_MODEL_DIR = \"trained_keras_model_iv3\"\n\n# experiment configuration saved to W&B\nconfig_defaults = {\n    \"num_train\" : NUM_TRAIN,\n    \"num_val\" : NUM_VAL,\n    \"epochs\" : NUM_EPOCHS,\n    \"num_classes\" : 10,\n    \"fc_size\" : 1024,\n    # inceptionV3 settings\n    \"img_width\" : 299,\n    \"img_height\": 299,\n    \"batch_size\" : 32\n}\n\ndef finetune_inception_model(fc_size, num_classes):\n    \"\"\"Load InceptionV3 with ImageNet weights, freeze it,\n    and attach a finetuning top for this classification task\"\"\"\n    # load InceptionV3 as base\n    base = tf.keras.applications.inception_v3.InceptionV3(weights=\"imagenet\", include_top=\"False\")\n    # freeze base layers\n    for layer in base.layers:\n      layer.trainable = False\n    x = base.get_layer('mixed10').output \n    \n    # attach a fine-tuning layer\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(fc_size, activation='relu')(x)\n    guesses = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = tf.keras.models.Model(inputs=base.input, outputs=guesses)\n    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\ndef train():\n    \"\"\" Main training loop. This is called pretrain because it freezes\n    the InceptionV3 layers of the model and only trains the new top layers\n    on the new data.   subsequent training phase would unfreeze all the layers\n    and finetune the whole model on the new data\"\"\" \n    # track this experiment with wandb: all runs will be sent\n    # to the given project name\n    run = wandb.init(project=PROJECT_NAME, job_type=\"train\", config=config_defaults)\n    cfg = wandb.config\n    \n    # artifact names\n    train_at = os.path.join(PROJECT_NAME, PREFIX + \"_train_data_\" + str(NUM_TRAIN)) + \":latest\"\n    val_at = os.path.join(PROJECT_NAME, PREFIX + \"_val_data_\" + str(NUM_VAL)) + \":latest\"\n    \n    train_data = run.use_artifact(train_at, type='train_data')\n    train_dir = train_data.download()\n    val_data = run.use_artifact(val_at, type='val_data')\n    val_dir = val_data.download()\n    \n    # create train and validation data generators\n    train_datagen = ImageDataGenerator(\n        rescale=1. / 255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n    val_datagen = ImageDataGenerator(rescale=1. / 255)\n    \n    train_generator = train_datagen.flow_from_directory(\n      train_dir,\n      target_size=(cfg.img_width, cfg.img_height),\n      batch_size=cfg.batch_size,\n      class_mode='categorical')\n    \n    val_generator = val_datagen.flow_from_directory(\n      val_dir,\n      target_size=(cfg.img_width, cfg.img_height),\n      batch_size=cfg.batch_size,\n      class_mode='categorical')\n    \n    # instantiate model and callbacks\n    model = finetune_inception_model(cfg.fc_size, cfg.num_classes)\n    \n    # log initial model before training\n    model_artifact = wandb.Artifact(\n              \"iv3\", type=\"model\",\n              description=\"unmodified inception v3\",\n              metadata=dict(cfg))\n    \n    model.save(INIT_MODEL_DIR)\n    model_artifact.add_dir(INIT_MODEL_DIR)\n    run.log_artifact(model_artifact)\n    callbacks = [WandbCallback()]\n    \n    # train!\n    model.fit(\n        train_generator,\n        steps_per_epoch = cfg.num_train // cfg.batch_size,\n        epochs=cfg.epochs,\n        validation_data=val_generator,\n        callbacks = callbacks,\n        validation_steps=cfg.num_val // cfg.batch_size)\n    \n    # save trained model as artifact\n    trained_model_artifact = wandb.Artifact(\n              MODEL_NAME, type=\"model\",\n              description=\"trained inception v3\",\n              metadata=dict(cfg))\n    \n    model.save(FINAL_MODEL_DIR)\n    trained_model_artifact.add_dir(FINAL_MODEL_DIR)\n    run.log_artifact(trained_model_artifact)\n    run.finish()\n\n\ntrain()\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230524_091548-er1z6hnx\n\n\nSyncing run giddy-smoke-3 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/phonchi/artifacts_demo\n\n\n View run at https://wandb.ai/phonchi/artifacts_demo/runs/er1z6hnx\n\n\nwandb:   80 of 80 files downloaded.  \nwandb:   10 of 10 files downloaded.  \n\n\nFound 80 images belonging to 10 classes.\nFound 10 images belonging to 10 classes.\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n96112376/96112376 [==============================] - 5s 0us/step\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 95). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./init_model_keras_iv3)... Done. 0.3s\nwandb: WARNING The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n\n\n2/2 [==============================] - 21s 5s/step - loss: 5.9535 - accuracy: 0.1250\n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\nwandb: Adding directory to artifact (./trained_keras_model_iv3)... Done. 0.3s\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\nRun history:\n\n\n\naccuracy\n▁\n\n\nepoch\n▁\n\n\nloss\n▁\n\n\n\n\n\n\n13.5.17 Run summary:\n\n\n\n\n\naccuracy\n0.125\n\n\nepoch\n0\n\n\nloss\n5.95346\n\n\n\n\n\n\n\n View run giddy-smoke-3 at: https://wandb.ai/phonchi/artifacts_demo/runs/er1z6hnxSynced 5 W&B file(s), 1 media file(s), 10 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230524_091548-er1z6hnx/logs\n\n\n\n\n13.5.17.1 Load model for inference\nYou can use use_artifact() to download and use stored artifact.\n\nrun = wandb.init(project=PROJECT_NAME, job_type=\"inference\")\n# use the latest version of the model\nmodel_at = run.use_artifact(MODEL_NAME + \":latest\")\n# download the directory in which the model is saved\nmodel_dir= model_at.download()\nprint(\"model: \", model_dir)\nmodel = tf.keras.models.load_model(model_dir)\n\nTEST_DATA_AT = PREFIX + \"_test_data_\" + str(BALANCED_SPLITS[\"test\"]*10) + \":latest\"\ntest_data_at = run.use_artifact(TEST_DATA_AT)\ntest_dir = test_data_at.download()\n\nimgs = []\nclass_labels = os.listdir(test_dir)\nfor l in class_labels:\n    if l.startswith(\".\"):\n      continue\n    imgs_per_class = os.listdir(os.path.join(test_dir, l))\n    for img in imgs_per_class:\n      img_path = os.path.join(test_dir, l, img)\n      img = tf.keras.preprocessing.image.load_img(img_path, target_size=(299, 299))\n      img = tf.keras.preprocessing.image.img_to_array(img)\n      # don't forget to rescale test images to match the range of inputs\n      # to the network\n      img = np.expand_dims(img/255.0, axis=0)\n      imgs.append(img)\n\npreds = {}\nimgs = np.vstack(imgs)\nclasses = model.predict(imgs, batch_size=32)\nfor c in classes:\n    class_id = np.argmax(c)\n    if class_id in preds:\n      preds[class_id] += 1\n    else:\n      preds[class_id] = 1\n\n# print the counts of predicted labels as a quick sanity check\n# note that for tiny/medium datasets, this won't be very meaningful\nprint(preds)\nrun.finish()\n\nFinishing last run (ID:fo7u8bx6) before initializing another...\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n View run quiet-grass-4 at: https://wandb.ai/phonchi/artifacts_demo/runs/fo7u8bx6Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230524_091836-fo7u8bx6/logs\n\n\nSuccessfully finished last run (ID:fo7u8bx6). Initializing new run:\n\n\nTracking run with wandb version 0.15.3\n\n\nRun data is saved locally in /content/wandb/run-20230524_091913-7y1vt7qq\n\n\nSyncing run genial-wind-5 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/phonchi/artifacts_demo\n\n\n View run at https://wandb.ai/phonchi/artifacts_demo/runs/7y1vt7qq\n\n\nwandb: Downloading large artifact iv3_trained:latest, 104.85MB. 5 files... \nwandb:   5 of 5 files downloaded.  \nDone. 0:0:0.2\n\n\nmodel:  ./artifacts/iv3_trained:v0\n\n\nwandb:   10 of 10 files downloaded.  \n\n\n1/1 [==============================] - 6s 6s/step\n{7: 10}\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n View run genial-wind-5 at: https://wandb.ai/phonchi/artifacts_demo/runs/7y1vt7qqSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230524_091913-7y1vt7qq/logs"
  },
  {
    "objectID": "13_Hyperparameter.html#references",
    "href": "13_Hyperparameter.html#references",
    "title": "13  Hyperparameter Tuning",
    "section": "13.6 References",
    "text": "13.6 References\n\nhttps://github.com/fchollet/deep-learning-with-python-notebooks_\nhttps://keras.io/guides/keras_tuner/getting_started/\nhttps://github.com/datamllab/automl-in-action-notebooks/blob/master/5.2.2-Tuning-CNNs-Image-Classification.ipynb\nhttps://towardsdatascience.com/grid-search-vs-random-search-vs-bayesian-optimization-2e68f57c3c46\nhttps://github.com/wandb/examples"
  },
  {
    "objectID": "NumPy_tutorial.html#introduction",
    "href": "NumPy_tutorial.html#introduction",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.1 Introduction",
    "text": "A.1 Introduction\nPython objects:\n\nHigh-level objects: integers, floating-point\nContainers: lists (costless append), dictionaries (fast lookup)\nPython lists are very general. They can contain any kind of object and are dynamically typed\nHowever, they do not support mathematical functions such as matrix and dot multiplications. Implementing such functions for Python lists would not be very efficient because of the dynamic typing\n\nNumPy provides:\n\nExtension package to Python for multi-dimensional arrays\nNumpy arrays are statically typed and homogeneous. The type of the elements is determined when the array is created\nBecause of the static typing, fast implementation of mathematical functions such as multiplication and addition of numpy arrays can be implemented in a compiled language (C and Fortran is used). Moreover, Numpy arrays are memory efficient\n\nThe numpy package (module) is used in almost all numerical computation using Python. It is a package that provides high-performance vector, matrix and higher-dimensional data structures for Python. It is implemented in C and Fortran so when calculations are vectorized (formulated with vectors and matrices) which provides good performance\nTo use numpy you need to import the module, using for example:\n\nimport numpy as np\n\n\na = range(1000)\n\n\n%%timeit\na1 = []\nfor i in range(1000):\n    a1.append(a[i]**2)\n\n426 µs ± 19.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%%timeit \nglobal a2\na2 = [i**2 for i in a]\n\n255 µs ± 14.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\nb = np.arange(1000)\n\n\n%%timeit \nb1 = b**2\n\n1.68 µs ± 96.2 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nIn the numpy package the terminology used for vectors, matrices and higher-dimensional data sets is array."
  },
  {
    "objectID": "NumPy_tutorial.html#documentation",
    "href": "NumPy_tutorial.html#documentation",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.2 Documentation",
    "text": "A.2 Documentation\n\nhttps://scipy-lectures.org/intro/numpy/index.html\nhttps://numpy.org/doc/\n\n\nnp.array?\n\nDocstring:\narray(object, dtype=None, copy=True, order='K', subok=False, ndmin=0)\n\nCreate an array.\n\nParameters\n----------\nobject : array_like\n    An array, any object exposing the array interface, an object whose\n    __array__ method returns an array, or any (nested) sequence.\ndtype : data-type, optional\n    The desired data-type for the array.  If not given, then the type will\n    be determined as the minimum type required to hold the objects in the\n    sequence.\ncopy : bool, optional\n    If true (default), then the object is copied.  Otherwise, a copy will\n    only be made if __array__ returns a copy, if obj is a nested sequence,\n    or if a copy is needed to satisfy any of the other requirements\n    (`dtype`, `order`, etc.).\norder : {'K', 'A', 'C', 'F'}, optional\n    Specify the memory layout of the array. If object is not an array, the\n    newly created array will be in C order (row major) unless 'F' is\n    specified, in which case it will be in Fortran order (column major).\n    If object is an array the following holds.\n\n    ===== ========= ===================================================\n    order  no copy                     copy=True\n    ===== ========= ===================================================\n    'K'   unchanged F & C order preserved, otherwise most similar order\n    'A'   unchanged F order if input is F and not C, otherwise C order\n    'C'   C order   C order\n    'F'   F order   F order\n    ===== ========= ===================================================\n\n    When ``copy=False`` and a copy is made for other reasons, the result is\n    the same as if ``copy=True``, with some exceptions for `A`, see the\n    Notes section. The default order is 'K'.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise\n    the returned array will be forced to be a base-class array (default).\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting\n    array should have.  Ones will be pre-pended to the shape as\n    needed to meet this requirement.\n\nReturns\n-------\nout : ndarray\n    An array object satisfying the specified requirements.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\nWhen order is 'A' and `object` is an array in neither 'C' nor 'F' order,\nand a copy is forced by a change in dtype, then the order of the result is\nnot necessarily 'C' as expected. This is likely a bug.\n\nExamples\n--------\n&gt;&gt;&gt; np.array([1, 2, 3])\narray([1, 2, 3])\n\nUpcasting:\n\n&gt;&gt;&gt; np.array([1, 2, 3.0])\narray([ 1.,  2.,  3.])\n\nMore than one dimension:\n\n&gt;&gt;&gt; np.array([[1, 2], [3, 4]])\narray([[1, 2],\n       [3, 4]])\n\nMinimum dimensions 2:\n\n&gt;&gt;&gt; np.array([1, 2, 3], ndmin=2)\narray([[1, 2, 3]])\n\nType provided:\n\n&gt;&gt;&gt; np.array([1, 2, 3], dtype=complex)\narray([ 1.+0.j,  2.+0.j,  3.+0.j])\n\nData-type consisting of more than one element:\n\n&gt;&gt;&gt; x = np.array([(1,2),(3,4)],dtype=[('a','&lt;i4'),('b','&lt;i4')])\n&gt;&gt;&gt; x['a']\narray([1, 3])\n\nCreating an array from sub-classes:\n\n&gt;&gt;&gt; np.array(np.mat('1 2; 3 4'))\narray([[1, 2],\n       [3, 4]])\n\n&gt;&gt;&gt; np.array(np.mat('1 2; 3 4'), subok=True)\nmatrix([[1, 2],\n        [3, 4]])\nType:      builtin_function_or_method"
  },
  {
    "objectID": "NumPy_tutorial.html#creating-numpy-arrays",
    "href": "NumPy_tutorial.html#creating-numpy-arrays",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.3 Creating numpy arrays",
    "text": "A.3 Creating numpy arrays\nThere are a number of ways to initialize new numpy arrays, for example, from\n\nA Python list or tuples\nUsing functions that are dedicated to generating numpy arrays, such as arange, linspace, etc.\nReading data from files (npy)\n\n\nA.3.1 From Python list\nFor example, to create new vector and matrix arrays from Python lists, we can use the numpy.array function.\n\n# a vector: the argument to the array function is a Python list\nv = np.array([1,2,3,4])\nv, type(v), v.dtype, v.shape\n\n(array([1, 2, 3, 4]), numpy.ndarray, dtype('int32'), (4,))\n\n\n\n# a matrix: the argument to the array function is a nested Python list\nM = np.array([[1, 2], [3, 4]])\nM, type(M), M.dtype, M.shape\n\n(array([[1, 2],\n        [3, 4]]),\n numpy.ndarray,\n dtype('int32'),\n (2, 2))\n\n\nNote that the v and M objects are both of the type ndarray that the numpy module provides. The difference between the v and M arrays is only their shapes. We can get information about the shape of an array by using the ndarray.shape property.\nSince it is statically typing, we can explicitly define the type of the array data when we create it, using the dtype keyword argument:\n\nM = np.array([[1, 2], [3, 4]], dtype=complex)\nM\n\narray([[1.+0.j, 2.+0.j],\n       [3.+0.j, 4.+0.j]])\n\n\nCommon data types that can be used with dtype are: int, float, complex, bool, etc.\nWe can also explicitly define the bit size of the data types, for example: int64, int16, float128, complex128.\n\n\nA.3.2 Using array-generating functions\nFor larger arrays, it is impractical to initialize the data manually using explicit python lists. Instead, we can use one of the many functions in numpy that generate arrays of different forms. Some of the more common are:\n\nnp.arange(-1, 1, 0.1) # arguments: start, stop, step\n\narray([-1.00000000e+00, -9.00000000e-01, -8.00000000e-01, -7.00000000e-01,\n       -6.00000000e-01, -5.00000000e-01, -4.00000000e-01, -3.00000000e-01,\n       -2.00000000e-01, -1.00000000e-01, -2.22044605e-16,  1.00000000e-01,\n        2.00000000e-01,  3.00000000e-01,  4.00000000e-01,  5.00000000e-01,\n        6.00000000e-01,  7.00000000e-01,  8.00000000e-01,  9.00000000e-01])\n\n\n\n# using linspace, both end points ARE included\nnp.linspace(0, 10, 25) # arguments: start, end, number of samples\n\narray([ 0.        ,  0.41666667,  0.83333333,  1.25      ,  1.66666667,\n        2.08333333,  2.5       ,  2.91666667,  3.33333333,  3.75      ,\n        4.16666667,  4.58333333,  5.        ,  5.41666667,  5.83333333,\n        6.25      ,  6.66666667,  7.08333333,  7.5       ,  7.91666667,\n        8.33333333,  8.75      ,  9.16666667,  9.58333333, 10.        ])\n\n\n\nfrom numpy import random\n\n\n# uniform random numbers in [0,1]\nnp.random.rand(5,5) #argument: shape\n\narray([[0.95856122, 0.46008766, 0.18125959, 0.29118265, 0.12936857],\n       [0.66136799, 0.31069994, 0.02396709, 0.19487356, 0.2781103 ],\n       [0.95491478, 0.39030392, 0.98749426, 0.11391192, 0.71392245],\n       [0.45548694, 0.26654714, 0.39209578, 0.09068336, 0.1440259 ],\n       [0.65795932, 0.07484714, 0.33585994, 0.38683142, 0.25092455]])\n\n\n\n# standard normal distributed random numbers\nnp.random.randn(5,5)\n\narray([[-0.1083494 , -1.4625737 , -1.52901998, -0.19867851, -0.69311333],\n       [ 0.22918277, -0.54191491, -0.11518915, -0.39199225, -0.46892591],\n       [-1.74171355,  0.04522399, -1.30233269, -0.56877774, -0.96248809],\n       [ 0.47210184, -0.67675756,  0.25428361,  0.42873618,  0.94328066],\n       [ 1.04585954, -1.53339424,  1.22914079,  0.83127729, -0.45995271]])\n\n\n\nA.3.2.1 diag\n\n# a diagonal matrix\nnp.diag([1,2,3])\n\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])\n\n\n\n\nA.3.2.2 zeros and ones\n\nnp.zeros((3,3))\n\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\nnp.ones((3,3))\n\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n\n\n\nnp.random.seed(89)\nnp.random.rand(5)\n\narray([0.49969432, 0.25593713, 0.25810063, 0.09692171, 0.56418511])\n\n\n\nnp.random.rand(5)\n\narray([0.01599007, 0.15259523, 0.48024773, 0.09987276, 0.41696389])"
  },
  {
    "objectID": "NumPy_tutorial.html#manipulating-arrays",
    "href": "NumPy_tutorial.html#manipulating-arrays",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.4 Manipulating arrays",
    "text": "A.4 Manipulating arrays\n\nA.4.1 Indexing and slicing\n\nNote that the indices begin at 0, like other Python sequences (and C/C++). In contrast, in Fortran or Matlab, indices start at 1.\nIn 2D, the first dimension corresponds to rows, the second to columns.\n\nWe can index elements in an array using square brackets and indices:\n\n# v is a vector, and has only one dimension, taking one index\nv = np.random.rand(5) #Note it starts with zero \nv, v[3]\n\n(array([0.91365081, 0.35071951, 0.11460437, 0.71260839, 0.10188615]),\n 0.7126083905021839)\n\n\n\n# M is a matrix, or a 2 dimensional array, taking two indices \nM = np.random.rand(5,5)\nM, M[2,3]\n\n(array([[0.40570044, 0.66548144, 0.13835937, 0.83043309, 0.12319969],\n        [0.58779155, 0.06309849, 0.49710274, 0.92839462, 0.80603084],\n        [0.19839124, 0.34528354, 0.53473647, 0.97858347, 0.5030445 ],\n        [0.3474475 , 0.21278653, 0.17745402, 0.1040286 , 0.18745545],\n        [0.04031375, 0.23991727, 0.5462427 , 0.20778317, 0.99270398]]),\n 0.9785834687356999)\n\n\nWe can get rows and columns as follows\n\nM[1,:] # row 1\n\narray([0.58779155, 0.06309849, 0.49710274, 0.92839462, 0.80603084])\n\n\n\nM[:,1] # column 1\n\narray([0.66548144, 0.06309849, 0.34528354, 0.21278653, 0.23991727])\n\n\nIndex slicing is the technical name for the syntax M[lower:upper:step] to extract part of an array:\n\nA = np.array([1,2,3,4,5])\nA\n\narray([1, 2, 3, 4, 5])\n\n\n\nA[1:3]\n\narray([2, 3])\n\n\nWe can omit any of the three parameters in M[lower:upper:step]:\n\nA[::2] # step is 2, lower and upper defaults to the beginning and end of the array\n\narray([1, 3, 5])\n\n\n\nA[:3] # first three elements\n\narray([1, 2, 3])\n\n\n\nA[3:] # elements from index 3\n\narray([4, 5])\n\n\nNegative indices counts from the end of the array (positive index from the begining):\n\nA = array([1,2,3,4,5])\n\nNameError: name 'array' is not defined\n\n\n\nA[-1] # the last element in the array\n\n5\n\n\n\nA[-3:] # the last three elements\n\narray([3, 4, 5])\n\n\nIndex slicing works exactly the same way for multidimensional arrays:\n\nA = np.array([[n+m*10 for n in range(5)] for m in range(5)])\nA\n\narray([[ 0,  1,  2,  3,  4],\n       [10, 11, 12, 13, 14],\n       [20, 21, 22, 23, 24],\n       [30, 31, 32, 33, 34],\n       [40, 41, 42, 43, 44]])\n\n\n\n# a block from the original array\nA[1:4, 1:4]\n\narray([[11, 12, 13],\n       [21, 22, 23],\n       [31, 32, 33]])\n\n\n\n\nChcek “Fancy indexing” at https://scipy-lectures.org/intro/numpy/array_object.html#fancy-indexing"
  },
  {
    "objectID": "NumPy_tutorial.html#linear-algebra-on-array",
    "href": "NumPy_tutorial.html#linear-algebra-on-array",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.5 Linear algebra on array",
    "text": "A.5 Linear algebra on array\nVectorizing code is the key to writing efficient numerical calculations with Python/Numpy. That means that as much as possible of a program should be formulated in terms of matrix and vector operations, like matrix-matrix multiplication.\n\nA.5.1 Scalar and array operations\nWe can use the usual arithmetic operators to multiply, add, subtract, and divide arrays with scalar numbers.\n\nv = np.arange(0, 5)\n\n\nv * 2, v + 2\n\n(array([0, 2, 4, 6, 8]), array([2, 3, 4, 5, 6]))\n\n\n\nprint(A * 2)\nprint(A + 2)\n\n[[ 0  2  4  6  8]\n [20 22 24 26 28]\n [40 42 44 46 48]\n [60 62 64 66 68]\n [80 82 84 86 88]]\n[[ 2  3  4  5  6]\n [12 13 14 15 16]\n [22 23 24 25 26]\n [32 33 34 35 36]\n [42 43 44 45 46]]\n\n\n\na = np.arange(10000)\n\n\n%timeit a + 1  \n\n6.25 µs ± 186 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nl = range(10000)\n\n\n%timeit [i+1 for i in l] \n\n813 µs ± 65.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\nA.5.1.1 Element-wise array-array operations\nWhen we add, subtract, multiply and divide arrays with each other, the default behavior is element-wise operations:\n\nA * A # element-wise multiplication\n\narray([[   0,    1,    4,    9,   16],\n       [ 100,  121,  144,  169,  196],\n       [ 400,  441,  484,  529,  576],\n       [ 900,  961, 1024, 1089, 1156],\n       [1600, 1681, 1764, 1849, 1936]])\n\n\n\nv * v\n\narray([ 0,  1,  4,  9, 16])\n\n\nIf we multiply arrays with compatible shapes, we get an element-wise multiplication of each row:\n\nA, v\n\n(array([[ 0,  1,  2,  3,  4],\n        [10, 11, 12, 13, 14],\n        [20, 21, 22, 23, 24],\n        [30, 31, 32, 33, 34],\n        [40, 41, 42, 43, 44]]),\n array([0, 1, 2, 3, 4]))\n\n\n\nA.shape, v.shape\n\n((5, 5), (5,))\n\n\n\nA * v    \n\narray([[  0,   1,   4,   9,  16],\n       [  0,  11,  24,  39,  56],\n       [  0,  21,  44,  69,  96],\n       [  0,  31,  64,  99, 136],\n       [  0,  41,  84, 129, 176]])\n\n\n\n\nSee Broadcasting at https://scipy-lectures.org/intro/numpy/operations.html#broadcasting and https://cs231n.github.io/python-numpy-tutorial/#broadcasting\n\n\n\n\n\nA.5.2 Matrix algebra\nWhat about matrix multiplication? There are two ways. We can either use the dot function, which applies a matrix-matrix, matrix-vector, or inner vector multiplication to its two arguments:\n\nnp.dot(A, A)\n\narray([[ 300,  310,  320,  330,  340],\n       [1300, 1360, 1420, 1480, 1540],\n       [2300, 2410, 2520, 2630, 2740],\n       [3300, 3460, 3620, 3780, 3940],\n       [4300, 4510, 4720, 4930, 5140]])\n\n\n\nnp.dot(A, v)\n\narray([ 30, 130, 230, 330, 430])\n\n\n\nnp.dot(v, v)\n\n30\n\n\n\nA.T #transpose\n\narray([[ 0, 10, 20, 30, 40],\n       [ 1, 11, 21, 31, 41],\n       [ 2, 12, 22, 32, 42],\n       [ 3, 13, 23, 33, 43],\n       [ 4, 14, 24, 34, 44]])\n\n\nAlternatively, we can cast the array objects to the type matrix. This changes the behavior of the standard arithmetic operators +, -, * to use matrix algebra. (Become matrix operation!)\n\nhelp(np.matrix)\n\nHelp on class matrix in module numpy:\n\nclass matrix(ndarray)\n |  matrix(data, dtype=None, copy=True)\n |  \n |  matrix(data, dtype=None, copy=True)\n |  \n |  .. note:: It is no longer recommended to use this class, even for linear\n |            algebra. Instead use regular arrays. The class may be removed\n |            in the future.\n |  \n |  Returns a matrix from an array-like object, or from a string of data.\n |  A matrix is a specialized 2-D array that retains its 2-D nature\n |  through operations.  It has certain special operators, such as ``*``\n |  (matrix multiplication) and ``**`` (matrix power).\n |  \n |  Parameters\n |  ----------\n |  data : array_like or string\n |     If `data` is a string, it is interpreted as a matrix with commas\n |     or spaces separating columns, and semicolons separating rows.\n |  dtype : data-type\n |     Data-type of the output matrix.\n |  copy : bool\n |     If `data` is already an `ndarray`, then this flag determines\n |     whether the data is copied (the default), or whether a view is\n |     constructed.\n |  \n |  See Also\n |  --------\n |  array\n |  \n |  Examples\n |  --------\n |  &gt;&gt;&gt; a = np.matrix('1 2; 3 4')\n |  &gt;&gt;&gt; a\n |  matrix([[1, 2],\n |          [3, 4]])\n |  \n |  &gt;&gt;&gt; np.matrix([[1, 2], [3, 4]])\n |  matrix([[1, 2],\n |          [3, 4]])\n |  \n |  Method resolution order:\n |      matrix\n |      ndarray\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __array_finalize__(self, obj)\n |      None.\n |  \n |  __getitem__(self, index)\n |      Return self[key].\n |  \n |  __imul__(self, other)\n |      Return self*=value.\n |  \n |  __ipow__(self, other)\n |      Return self**=value.\n |  \n |  __mul__(self, other)\n |      Return self*value.\n |  \n |  __pow__(self, other)\n |      Return pow(self, value, mod).\n |  \n |  __rmul__(self, other)\n |      Return value*self.\n |  \n |  __rpow__(self, other)\n |      Return pow(value, self, mod).\n |  \n |  all(self, axis=None, out=None)\n |      Test whether all matrix elements along a given axis evaluate to True.\n |      \n |      Parameters\n |      ----------\n |      See `numpy.all` for complete descriptions\n |      \n |      See Also\n |      --------\n |      numpy.all\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.all`, but it returns a `matrix` object.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; y = x[0]; y\n |      matrix([[0, 1, 2, 3]])\n |      &gt;&gt;&gt; (x == y)\n |      matrix([[ True,  True,  True,  True],\n |              [False, False, False, False],\n |              [False, False, False, False]])\n |      &gt;&gt;&gt; (x == y).all()\n |      False\n |      &gt;&gt;&gt; (x == y).all(0)\n |      matrix([[False, False, False, False]])\n |      &gt;&gt;&gt; (x == y).all(1)\n |      matrix([[ True],\n |              [False],\n |              [False]])\n |  \n |  any(self, axis=None, out=None)\n |      Test whether any array element along a given axis evaluates to True.\n |      \n |      Refer to `numpy.any` for full documentation.\n |      \n |      Parameters\n |      ----------\n |      axis : int, optional\n |          Axis along which logical OR is performed\n |      out : ndarray, optional\n |          Output to existing array instead of creating new one, must have\n |          same shape as expected output\n |      \n |      Returns\n |      -------\n |          any : bool, ndarray\n |              Returns a single bool if `axis` is ``None``; otherwise,\n |              returns `ndarray`\n |  \n |  argmax(self, axis=None, out=None)\n |      Indexes of the maximum values along an axis.\n |      \n |      Return the indexes of the first occurrences of the maximum values\n |      along the specified axis.  If axis is None, the index is for the\n |      flattened matrix.\n |      \n |      Parameters\n |      ----------\n |      See `numpy.argmax` for complete descriptions\n |      \n |      See Also\n |      --------\n |      numpy.argmax\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.argmax`, but returns a `matrix` object\n |      where `ndarray.argmax` would return an `ndarray`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.argmax()\n |      11\n |      &gt;&gt;&gt; x.argmax(0)\n |      matrix([[2, 2, 2, 2]])\n |      &gt;&gt;&gt; x.argmax(1)\n |      matrix([[3],\n |              [3],\n |              [3]])\n |  \n |  argmin(self, axis=None, out=None)\n |      Indexes of the minimum values along an axis.\n |      \n |      Return the indexes of the first occurrences of the minimum values\n |      along the specified axis.  If axis is None, the index is for the\n |      flattened matrix.\n |      \n |      Parameters\n |      ----------\n |      See `numpy.argmin` for complete descriptions.\n |      \n |      See Also\n |      --------\n |      numpy.argmin\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.argmin`, but returns a `matrix` object\n |      where `ndarray.argmin` would return an `ndarray`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = -np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[  0,  -1,  -2,  -3],\n |              [ -4,  -5,  -6,  -7],\n |              [ -8,  -9, -10, -11]])\n |      &gt;&gt;&gt; x.argmin()\n |      11\n |      &gt;&gt;&gt; x.argmin(0)\n |      matrix([[2, 2, 2, 2]])\n |      &gt;&gt;&gt; x.argmin(1)\n |      matrix([[3],\n |              [3],\n |              [3]])\n |  \n |  flatten(self, order='C')\n |      Return a flattened copy of the matrix.\n |      \n |      All `N` elements of the matrix are placed into a single row.\n |      \n |      Parameters\n |      ----------\n |      order : {'C', 'F', 'A', 'K'}, optional\n |          'C' means to flatten in row-major (C-style) order. 'F' means to\n |          flatten in column-major (Fortran-style) order. 'A' means to\n |          flatten in column-major order if `m` is Fortran *contiguous* in\n |          memory, row-major order otherwise. 'K' means to flatten `m` in\n |          the order the elements occur in memory. The default is 'C'.\n |      \n |      Returns\n |      -------\n |      y : matrix\n |          A copy of the matrix, flattened to a `(1, N)` matrix where `N`\n |          is the number of elements in the original matrix.\n |      \n |      See Also\n |      --------\n |      ravel : Return a flattened array.\n |      flat : A 1-D flat iterator over the matrix.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; m = np.matrix([[1,2], [3,4]])\n |      &gt;&gt;&gt; m.flatten()\n |      matrix([[1, 2, 3, 4]])\n |      &gt;&gt;&gt; m.flatten('F')\n |      matrix([[1, 3, 2, 4]])\n |  \n |  getA = A(self)\n |      Return `self` as an `ndarray` object.\n |      \n |      Equivalent to ``np.asarray(self)``.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : ndarray\n |          `self` as an `ndarray`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.getA()\n |      array([[ 0,  1,  2,  3],\n |             [ 4,  5,  6,  7],\n |             [ 8,  9, 10, 11]])\n |  \n |  getA1 = A1(self)\n |      Return `self` as a flattened `ndarray`.\n |      \n |      Equivalent to ``np.asarray(x).ravel()``\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : ndarray\n |          `self`, 1-D, as an `ndarray`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.getA1()\n |      array([ 0,  1,  2, ...,  9, 10, 11])\n |  \n |  getH = H(self)\n |      Returns the (complex) conjugate transpose of `self`.\n |      \n |      Equivalent to ``np.transpose(self)`` if `self` is real-valued.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : matrix object\n |          complex conjugate transpose of `self`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4)))\n |      &gt;&gt;&gt; z = x - 1j*x; z\n |      matrix([[  0. +0.j,   1. -1.j,   2. -2.j,   3. -3.j],\n |              [  4. -4.j,   5. -5.j,   6. -6.j,   7. -7.j],\n |              [  8. -8.j,   9. -9.j,  10.-10.j,  11.-11.j]])\n |      &gt;&gt;&gt; z.getH()\n |      matrix([[ 0. -0.j,  4. +4.j,  8. +8.j],\n |              [ 1. +1.j,  5. +5.j,  9. +9.j],\n |              [ 2. +2.j,  6. +6.j, 10.+10.j],\n |              [ 3. +3.j,  7. +7.j, 11.+11.j]])\n |  \n |  getI = I(self)\n |      Returns the (multiplicative) inverse of invertible `self`.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : matrix object\n |          If `self` is non-singular, `ret` is such that ``ret * self`` ==\n |          ``self * ret`` == ``np.matrix(np.eye(self[0,:].size)`` all return\n |          ``True``.\n |      \n |      Raises\n |      ------\n |      numpy.linalg.LinAlgError: Singular matrix\n |          If `self` is singular.\n |      \n |      See Also\n |      --------\n |      linalg.inv\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; m = np.matrix('[1, 2; 3, 4]'); m\n |      matrix([[1, 2],\n |              [3, 4]])\n |      &gt;&gt;&gt; m.getI()\n |      matrix([[-2. ,  1. ],\n |              [ 1.5, -0.5]])\n |      &gt;&gt;&gt; m.getI() * m\n |      matrix([[ 1.,  0.], # may vary\n |              [ 0.,  1.]])\n |  \n |  getT = T(self)\n |      Returns the transpose of the matrix.\n |      \n |      Does *not* conjugate!  For the complex conjugate transpose, use ``.H``.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : matrix object\n |          The (non-conjugated) transpose of the matrix.\n |      \n |      See Also\n |      --------\n |      transpose, getH\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; m = np.matrix('[1, 2; 3, 4]')\n |      &gt;&gt;&gt; m\n |      matrix([[1, 2],\n |              [3, 4]])\n |      &gt;&gt;&gt; m.getT()\n |      matrix([[1, 3],\n |              [2, 4]])\n |  \n |  max(self, axis=None, out=None)\n |      Return the maximum value along an axis.\n |      \n |      Parameters\n |      ----------\n |      See `amax` for complete descriptions\n |      \n |      See Also\n |      --------\n |      amax, ndarray.max\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.max`, but returns a `matrix` object\n |      where `ndarray.max` would return an ndarray.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.max()\n |      11\n |      &gt;&gt;&gt; x.max(0)\n |      matrix([[ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.max(1)\n |      matrix([[ 3],\n |              [ 7],\n |              [11]])\n |  \n |  mean(self, axis=None, dtype=None, out=None)\n |      Returns the average of the matrix elements along the given axis.\n |      \n |      Refer to `numpy.mean` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.mean\n |      \n |      Notes\n |      -----\n |      Same as `ndarray.mean` except that, where that returns an `ndarray`,\n |      this returns a `matrix` object.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3, 4)))\n |      &gt;&gt;&gt; x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.mean()\n |      5.5\n |      &gt;&gt;&gt; x.mean(0)\n |      matrix([[4., 5., 6., 7.]])\n |      &gt;&gt;&gt; x.mean(1)\n |      matrix([[ 1.5],\n |              [ 5.5],\n |              [ 9.5]])\n |  \n |  min(self, axis=None, out=None)\n |      Return the minimum value along an axis.\n |      \n |      Parameters\n |      ----------\n |      See `amin` for complete descriptions.\n |      \n |      See Also\n |      --------\n |      amin, ndarray.min\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.min`, but returns a `matrix` object\n |      where `ndarray.min` would return an ndarray.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = -np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[  0,  -1,  -2,  -3],\n |              [ -4,  -5,  -6,  -7],\n |              [ -8,  -9, -10, -11]])\n |      &gt;&gt;&gt; x.min()\n |      -11\n |      &gt;&gt;&gt; x.min(0)\n |      matrix([[ -8,  -9, -10, -11]])\n |      &gt;&gt;&gt; x.min(1)\n |      matrix([[ -3],\n |              [ -7],\n |              [-11]])\n |  \n |  prod(self, axis=None, dtype=None, out=None)\n |      Return the product of the array elements over the given axis.\n |      \n |      Refer to `prod` for full documentation.\n |      \n |      See Also\n |      --------\n |      prod, ndarray.prod\n |      \n |      Notes\n |      -----\n |      Same as `ndarray.prod`, except, where that returns an `ndarray`, this\n |      returns a `matrix` object instead.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.prod()\n |      0\n |      &gt;&gt;&gt; x.prod(0)\n |      matrix([[  0,  45, 120, 231]])\n |      &gt;&gt;&gt; x.prod(1)\n |      matrix([[   0],\n |              [ 840],\n |              [7920]])\n |  \n |  ptp(self, axis=None, out=None)\n |      Peak-to-peak (maximum - minimum) value along the given axis.\n |      \n |      Refer to `numpy.ptp` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.ptp\n |      \n |      Notes\n |      -----\n |      Same as `ndarray.ptp`, except, where that would return an `ndarray` object,\n |      this returns a `matrix` object.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.ptp()\n |      11\n |      &gt;&gt;&gt; x.ptp(0)\n |      matrix([[8, 8, 8, 8]])\n |      &gt;&gt;&gt; x.ptp(1)\n |      matrix([[3],\n |              [3],\n |              [3]])\n |  \n |  ravel(self, order='C')\n |      Return a flattened matrix.\n |      \n |      Refer to `numpy.ravel` for more documentation.\n |      \n |      Parameters\n |      ----------\n |      order : {'C', 'F', 'A', 'K'}, optional\n |          The elements of `m` are read using this index order. 'C' means to\n |          index the elements in C-like order, with the last axis index\n |          changing fastest, back to the first axis index changing slowest.\n |          'F' means to index the elements in Fortran-like index order, with\n |          the first index changing fastest, and the last index changing\n |          slowest. Note that the 'C' and 'F' options take no account of the\n |          memory layout of the underlying array, and only refer to the order\n |          of axis indexing.  'A' means to read the elements in Fortran-like\n |          index order if `m` is Fortran *contiguous* in memory, C-like order\n |          otherwise.  'K' means to read the elements in the order they occur\n |          in memory, except for reversing the data when strides are negative.\n |          By default, 'C' index order is used.\n |      \n |      Returns\n |      -------\n |      ret : matrix\n |          Return the matrix flattened to shape `(1, N)` where `N`\n |          is the number of elements in the original matrix.\n |          A copy is made only if necessary.\n |      \n |      See Also\n |      --------\n |      matrix.flatten : returns a similar output matrix but always a copy\n |      matrix.flat : a flat iterator on the array.\n |      numpy.ravel : related function which returns an ndarray\n |  \n |  squeeze(self, axis=None)\n |      Return a possibly reshaped matrix.\n |      \n |      Refer to `numpy.squeeze` for more documentation.\n |      \n |      Parameters\n |      ----------\n |      axis : None or int or tuple of ints, optional\n |          Selects a subset of the single-dimensional entries in the shape.\n |          If an axis is selected with shape entry greater than one,\n |          an error is raised.\n |      \n |      Returns\n |      -------\n |      squeezed : matrix\n |          The matrix, but as a (1, N) matrix if it had shape (N, 1).\n |      \n |      See Also\n |      --------\n |      numpy.squeeze : related function\n |      \n |      Notes\n |      -----\n |      If `m` has a single column then that column is returned\n |      as the single row of a matrix.  Otherwise `m` is returned.\n |      The returned matrix is always either `m` itself or a view into `m`.\n |      Supplying an axis keyword argument will not affect the returned matrix\n |      but it may cause an error to be raised.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; c = np.matrix([[1], [2]])\n |      &gt;&gt;&gt; c\n |      matrix([[1],\n |              [2]])\n |      &gt;&gt;&gt; c.squeeze()\n |      matrix([[1, 2]])\n |      &gt;&gt;&gt; r = c.T\n |      &gt;&gt;&gt; r\n |      matrix([[1, 2]])\n |      &gt;&gt;&gt; r.squeeze()\n |      matrix([[1, 2]])\n |      &gt;&gt;&gt; m = np.matrix([[1, 2], [3, 4]])\n |      &gt;&gt;&gt; m.squeeze()\n |      matrix([[1, 2],\n |              [3, 4]])\n |  \n |  std(self, axis=None, dtype=None, out=None, ddof=0)\n |      Return the standard deviation of the array elements along the given axis.\n |      \n |      Refer to `numpy.std` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.std\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.std`, except that where an `ndarray` would\n |      be returned, a `matrix` object is returned instead.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3, 4)))\n |      &gt;&gt;&gt; x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.std()\n |      3.4520525295346629 # may vary\n |      &gt;&gt;&gt; x.std(0)\n |      matrix([[ 3.26598632,  3.26598632,  3.26598632,  3.26598632]]) # may vary\n |      &gt;&gt;&gt; x.std(1)\n |      matrix([[ 1.11803399],\n |              [ 1.11803399],\n |              [ 1.11803399]])\n |  \n |  sum(self, axis=None, dtype=None, out=None)\n |      Returns the sum of the matrix elements, along the given axis.\n |      \n |      Refer to `numpy.sum` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.sum\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.sum`, except that where an `ndarray` would\n |      be returned, a `matrix` object is returned instead.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix([[1, 2], [4, 3]])\n |      &gt;&gt;&gt; x.sum()\n |      10\n |      &gt;&gt;&gt; x.sum(axis=1)\n |      matrix([[3],\n |              [7]])\n |      &gt;&gt;&gt; x.sum(axis=1, dtype='float')\n |      matrix([[3.],\n |              [7.]])\n |      &gt;&gt;&gt; out = np.zeros((2, 1), dtype='float')\n |      &gt;&gt;&gt; x.sum(axis=1, dtype='float', out=np.asmatrix(out))\n |      matrix([[3.],\n |              [7.]])\n |  \n |  tolist(self)\n |      Return the matrix as a (possibly nested) list.\n |      \n |      See `ndarray.tolist` for full documentation.\n |      \n |      See Also\n |      --------\n |      ndarray.tolist\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.tolist()\n |      [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]\n |  \n |  var(self, axis=None, dtype=None, out=None, ddof=0)\n |      Returns the variance of the matrix elements, along the given axis.\n |      \n |      Refer to `numpy.var` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.var\n |      \n |      Notes\n |      -----\n |      This is the same as `ndarray.var`, except that where an `ndarray` would\n |      be returned, a `matrix` object is returned instead.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3, 4)))\n |      &gt;&gt;&gt; x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.var()\n |      11.916666666666666\n |      &gt;&gt;&gt; x.var(0)\n |      matrix([[ 10.66666667,  10.66666667,  10.66666667,  10.66666667]]) # may vary\n |      &gt;&gt;&gt; x.var(1)\n |      matrix([[1.25],\n |              [1.25],\n |              [1.25]])\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(subtype, data, dtype=None, copy=True)\n |      Create and return a new object.  See help(type) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  A\n |      Return `self` as an `ndarray` object.\n |      \n |      Equivalent to ``np.asarray(self)``.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : ndarray\n |          `self` as an `ndarray`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.getA()\n |      array([[ 0,  1,  2,  3],\n |             [ 4,  5,  6,  7],\n |             [ 8,  9, 10, 11]])\n |  \n |  A1\n |      Return `self` as a flattened `ndarray`.\n |      \n |      Equivalent to ``np.asarray(x).ravel()``\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : ndarray\n |          `self`, 1-D, as an `ndarray`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4))); x\n |      matrix([[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]])\n |      &gt;&gt;&gt; x.getA1()\n |      array([ 0,  1,  2, ...,  9, 10, 11])\n |  \n |  H\n |      Returns the (complex) conjugate transpose of `self`.\n |      \n |      Equivalent to ``np.transpose(self)`` if `self` is real-valued.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : matrix object\n |          complex conjugate transpose of `self`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.matrix(np.arange(12).reshape((3,4)))\n |      &gt;&gt;&gt; z = x - 1j*x; z\n |      matrix([[  0. +0.j,   1. -1.j,   2. -2.j,   3. -3.j],\n |              [  4. -4.j,   5. -5.j,   6. -6.j,   7. -7.j],\n |              [  8. -8.j,   9. -9.j,  10.-10.j,  11.-11.j]])\n |      &gt;&gt;&gt; z.getH()\n |      matrix([[ 0. -0.j,  4. +4.j,  8. +8.j],\n |              [ 1. +1.j,  5. +5.j,  9. +9.j],\n |              [ 2. +2.j,  6. +6.j, 10.+10.j],\n |              [ 3. +3.j,  7. +7.j, 11.+11.j]])\n |  \n |  I\n |      Returns the (multiplicative) inverse of invertible `self`.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : matrix object\n |          If `self` is non-singular, `ret` is such that ``ret * self`` ==\n |          ``self * ret`` == ``np.matrix(np.eye(self[0,:].size)`` all return\n |          ``True``.\n |      \n |      Raises\n |      ------\n |      numpy.linalg.LinAlgError: Singular matrix\n |          If `self` is singular.\n |      \n |      See Also\n |      --------\n |      linalg.inv\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; m = np.matrix('[1, 2; 3, 4]'); m\n |      matrix([[1, 2],\n |              [3, 4]])\n |      &gt;&gt;&gt; m.getI()\n |      matrix([[-2. ,  1. ],\n |              [ 1.5, -0.5]])\n |      &gt;&gt;&gt; m.getI() * m\n |      matrix([[ 1.,  0.], # may vary\n |              [ 0.,  1.]])\n |  \n |  T\n |      Returns the transpose of the matrix.\n |      \n |      Does *not* conjugate!  For the complex conjugate transpose, use ``.H``.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      ret : matrix object\n |          The (non-conjugated) transpose of the matrix.\n |      \n |      See Also\n |      --------\n |      transpose, getH\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; m = np.matrix('[1, 2; 3, 4]')\n |      &gt;&gt;&gt; m\n |      matrix([[1, 2],\n |              [3, 4]])\n |      &gt;&gt;&gt; m.getT()\n |      matrix([[1, 3],\n |              [2, 4]])\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __array_priority__ = 10.0\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from ndarray:\n |  \n |  __abs__(self, /)\n |      abs(self)\n |  \n |  __add__(self, value, /)\n |      Return self+value.\n |  \n |  __and__(self, value, /)\n |      Return self&value.\n |  \n |  __array__(...)\n |      a.__array__(|dtype) -&gt; reference if type unchanged, copy otherwise.\n |      \n |      Returns either a new reference to self if dtype is not given or a new array\n |      of provided data type if dtype is different from the current dtype of the\n |      array.\n |  \n |  __array_function__(...)\n |  \n |  __array_prepare__(...)\n |      a.__array_prepare__(obj) -&gt; Object of same type as ndarray object obj.\n |  \n |  __array_ufunc__(...)\n |  \n |  __array_wrap__(...)\n |      a.__array_wrap__(obj) -&gt; Object of same type as ndarray object a.\n |  \n |  __bool__(self, /)\n |      self != 0\n |  \n |  __complex__(...)\n |  \n |  __contains__(self, key, /)\n |      Return key in self.\n |  \n |  __copy__(...)\n |      a.__copy__()\n |      \n |      Used if :func:`copy.copy` is called on an array. Returns a copy of the array.\n |      \n |      Equivalent to ``a.copy(order='K')``.\n |  \n |  __deepcopy__(...)\n |      a.__deepcopy__(memo, /) -&gt; Deep copy of array.\n |      \n |      Used if :func:`copy.deepcopy` is called on an array.\n |  \n |  __delitem__(self, key, /)\n |      Delete self[key].\n |  \n |  __divmod__(self, value, /)\n |      Return divmod(self, value).\n |  \n |  __eq__(self, value, /)\n |      Return self==value.\n |  \n |  __float__(self, /)\n |      float(self)\n |  \n |  __floordiv__(self, value, /)\n |      Return self//value.\n |  \n |  __format__(...)\n |      Default object formatter.\n |  \n |  __ge__(self, value, /)\n |      Return self&gt;=value.\n |  \n |  __gt__(self, value, /)\n |      Return self&gt;value.\n |  \n |  __iadd__(self, value, /)\n |      Return self+=value.\n |  \n |  __iand__(self, value, /)\n |      Return self&=value.\n |  \n |  __ifloordiv__(self, value, /)\n |      Return self//=value.\n |  \n |  __ilshift__(self, value, /)\n |      Return self&lt;&lt;=value.\n |  \n |  __imatmul__(self, value, /)\n |      Return self@=value.\n |  \n |  __imod__(self, value, /)\n |      Return self%=value.\n |  \n |  __index__(self, /)\n |      Return self converted to an integer, if self is suitable for use as an index into a list.\n |  \n |  __int__(self, /)\n |      int(self)\n |  \n |  __invert__(self, /)\n |      ~self\n |  \n |  __ior__(self, value, /)\n |      Return self|=value.\n |  \n |  __irshift__(self, value, /)\n |      Return self&gt;&gt;=value.\n |  \n |  __isub__(self, value, /)\n |      Return self-=value.\n |  \n |  __iter__(self, /)\n |      Implement iter(self).\n |  \n |  __itruediv__(self, value, /)\n |      Return self/=value.\n |  \n |  __ixor__(self, value, /)\n |      Return self^=value.\n |  \n |  __le__(self, value, /)\n |      Return self&lt;=value.\n |  \n |  __len__(self, /)\n |      Return len(self).\n |  \n |  __lshift__(self, value, /)\n |      Return self&lt;&lt;value.\n |  \n |  __lt__(self, value, /)\n |      Return self&lt;value.\n |  \n |  __matmul__(self, value, /)\n |      Return self@value.\n |  \n |  __mod__(self, value, /)\n |      Return self%value.\n |  \n |  __ne__(self, value, /)\n |      Return self!=value.\n |  \n |  __neg__(self, /)\n |      -self\n |  \n |  __or__(self, value, /)\n |      Return self|value.\n |  \n |  __pos__(self, /)\n |      +self\n |  \n |  __radd__(self, value, /)\n |      Return value+self.\n |  \n |  __rand__(self, value, /)\n |      Return value&self.\n |  \n |  __rdivmod__(self, value, /)\n |      Return divmod(value, self).\n |  \n |  __reduce__(...)\n |      a.__reduce__()\n |      \n |      For pickling.\n |  \n |  __reduce_ex__(...)\n |      Helper for pickle.\n |  \n |  __repr__(self, /)\n |      Return repr(self).\n |  \n |  __rfloordiv__(self, value, /)\n |      Return value//self.\n |  \n |  __rlshift__(self, value, /)\n |      Return value&lt;&lt;self.\n |  \n |  __rmatmul__(self, value, /)\n |      Return value@self.\n |  \n |  __rmod__(self, value, /)\n |      Return value%self.\n |  \n |  __ror__(self, value, /)\n |      Return value|self.\n |  \n |  __rrshift__(self, value, /)\n |      Return value&gt;&gt;self.\n |  \n |  __rshift__(self, value, /)\n |      Return self&gt;&gt;value.\n |  \n |  __rsub__(self, value, /)\n |      Return value-self.\n |  \n |  __rtruediv__(self, value, /)\n |      Return value/self.\n |  \n |  __rxor__(self, value, /)\n |      Return value^self.\n |  \n |  __setitem__(self, key, value, /)\n |      Set self[key] to value.\n |  \n |  __setstate__(...)\n |      a.__setstate__(state, /)\n |      \n |      For unpickling.\n |      \n |      The `state` argument must be a sequence that contains the following\n |      elements:\n |      \n |      Parameters\n |      ----------\n |      version : int\n |          optional pickle version. If omitted defaults to 0.\n |      shape : tuple\n |      dtype : data-type\n |      isFortran : bool\n |      rawdata : string or list\n |          a binary string with the data (or a list if 'a' is an object array)\n |  \n |  __sizeof__(...)\n |      Size of object in memory, in bytes.\n |  \n |  __str__(self, /)\n |      Return str(self).\n |  \n |  __sub__(self, value, /)\n |      Return self-value.\n |  \n |  __truediv__(self, value, /)\n |      Return self/value.\n |  \n |  __xor__(self, value, /)\n |      Return self^value.\n |  \n |  argpartition(...)\n |      a.argpartition(kth, axis=-1, kind='introselect', order=None)\n |      \n |      Returns the indices that would partition this array.\n |      \n |      Refer to `numpy.argpartition` for full documentation.\n |      \n |      .. versionadded:: 1.8.0\n |      \n |      See Also\n |      --------\n |      numpy.argpartition : equivalent function\n |  \n |  argsort(...)\n |      a.argsort(axis=-1, kind=None, order=None)\n |      \n |      Returns the indices that would sort this array.\n |      \n |      Refer to `numpy.argsort` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.argsort : equivalent function\n |  \n |  astype(...)\n |      a.astype(dtype, order='K', casting='unsafe', subok=True, copy=True)\n |      \n |      Copy of the array, cast to a specified type.\n |      \n |      Parameters\n |      ----------\n |      dtype : str or dtype\n |          Typecode or data-type to which the array is cast.\n |      order : {'C', 'F', 'A', 'K'}, optional\n |          Controls the memory layout order of the result.\n |          'C' means C order, 'F' means Fortran order, 'A'\n |          means 'F' order if all the arrays are Fortran contiguous,\n |          'C' order otherwise, and 'K' means as close to the\n |          order the array elements appear in memory as possible.\n |          Default is 'K'.\n |      casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n |          Controls what kind of data casting may occur. Defaults to 'unsafe'\n |          for backwards compatibility.\n |      \n |            * 'no' means the data types should not be cast at all.\n |            * 'equiv' means only byte-order changes are allowed.\n |            * 'safe' means only casts which can preserve values are allowed.\n |            * 'same_kind' means only safe casts or casts within a kind,\n |              like float64 to float32, are allowed.\n |            * 'unsafe' means any data conversions may be done.\n |      subok : bool, optional\n |          If True, then sub-classes will be passed-through (default), otherwise\n |          the returned array will be forced to be a base-class array.\n |      copy : bool, optional\n |          By default, astype always returns a newly allocated array. If this\n |          is set to false, and the `dtype`, `order`, and `subok`\n |          requirements are satisfied, the input array is returned instead\n |          of a copy.\n |      \n |      Returns\n |      -------\n |      arr_t : ndarray\n |          Unless `copy` is False and the other conditions for returning the input\n |          array are satisfied (see description for `copy` input parameter), `arr_t`\n |          is a new array of the same shape as the input array, with dtype, order\n |          given by `dtype`, `order`.\n |      \n |      Notes\n |      -----\n |      .. versionchanged:: 1.17.0\n |         Casting between a simple data type and a structured one is possible only\n |         for \"unsafe\" casting.  Casting to multiple fields is allowed, but\n |         casting from multiple fields is not.\n |      \n |      .. versionchanged:: 1.9.0\n |         Casting from numeric to string types in 'safe' casting mode requires\n |         that the string dtype length is long enough to store the max\n |         integer/float value converted.\n |      \n |      Raises\n |      ------\n |      ComplexWarning\n |          When casting from complex to float or int. To avoid this,\n |          one should use ``a.real.astype(t)``.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([1, 2, 2.5])\n |      &gt;&gt;&gt; x\n |      array([1. ,  2. ,  2.5])\n |      \n |      &gt;&gt;&gt; x.astype(int)\n |      array([1, 2, 2])\n |  \n |  byteswap(...)\n |      a.byteswap(inplace=False)\n |      \n |      Swap the bytes of the array elements\n |      \n |      Toggle between low-endian and big-endian data representation by\n |      returning a byteswapped array, optionally swapped in-place.\n |      Arrays of byte-strings are not swapped. The real and imaginary\n |      parts of a complex number are swapped individually.\n |      \n |      Parameters\n |      ----------\n |      inplace : bool, optional\n |          If ``True``, swap bytes in-place, default is ``False``.\n |      \n |      Returns\n |      -------\n |      out : ndarray\n |          The byteswapped array. If `inplace` is ``True``, this is\n |          a view to self.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; A = np.array([1, 256, 8755], dtype=np.int16)\n |      &gt;&gt;&gt; list(map(hex, A))\n |      ['0x1', '0x100', '0x2233']\n |      &gt;&gt;&gt; A.byteswap(inplace=True)\n |      array([  256,     1, 13090], dtype=int16)\n |      &gt;&gt;&gt; list(map(hex, A))\n |      ['0x100', '0x1', '0x3322']\n |      \n |      Arrays of byte-strings are not swapped\n |      \n |      &gt;&gt;&gt; A = np.array([b'ceg', b'fac'])\n |      &gt;&gt;&gt; A.byteswap()\n |      array([b'ceg', b'fac'], dtype='|S3')\n |      \n |      ``A.newbyteorder().byteswap()`` produces an array with the same values\n |        but different representation in memory\n |      \n |      &gt;&gt;&gt; A = np.array([1, 2, 3])\n |      &gt;&gt;&gt; A.view(np.uint8)\n |      array([1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0,\n |             0, 0], dtype=uint8)\n |      &gt;&gt;&gt; A.newbyteorder().byteswap(inplace=True)\n |      array([1, 2, 3])\n |      &gt;&gt;&gt; A.view(np.uint8)\n |      array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n |             0, 3], dtype=uint8)\n |  \n |  choose(...)\n |      a.choose(choices, out=None, mode='raise')\n |      \n |      Use an index array to construct a new array from a set of choices.\n |      \n |      Refer to `numpy.choose` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.choose : equivalent function\n |  \n |  clip(...)\n |      a.clip(min=None, max=None, out=None, **kwargs)\n |      \n |      Return an array whose values are limited to ``[min, max]``.\n |      One of max or min must be given.\n |      \n |      Refer to `numpy.clip` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.clip : equivalent function\n |  \n |  compress(...)\n |      a.compress(condition, axis=None, out=None)\n |      \n |      Return selected slices of this array along given axis.\n |      \n |      Refer to `numpy.compress` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.compress : equivalent function\n |  \n |  conj(...)\n |      a.conj()\n |      \n |      Complex-conjugate all elements.\n |      \n |      Refer to `numpy.conjugate` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.conjugate : equivalent function\n |  \n |  conjugate(...)\n |      a.conjugate()\n |      \n |      Return the complex conjugate, element-wise.\n |      \n |      Refer to `numpy.conjugate` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.conjugate : equivalent function\n |  \n |  copy(...)\n |      a.copy(order='C')\n |      \n |      Return a copy of the array.\n |      \n |      Parameters\n |      ----------\n |      order : {'C', 'F', 'A', 'K'}, optional\n |          Controls the memory layout of the copy. 'C' means C-order,\n |          'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n |          'C' otherwise. 'K' means match the layout of `a` as closely\n |          as possible. (Note that this function and :func:`numpy.copy` are very\n |          similar, but have different default values for their order=\n |          arguments.)\n |      \n |      See also\n |      --------\n |      numpy.copy\n |      numpy.copyto\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([[1,2,3],[4,5,6]], order='F')\n |      \n |      &gt;&gt;&gt; y = x.copy()\n |      \n |      &gt;&gt;&gt; x.fill(0)\n |      \n |      &gt;&gt;&gt; x\n |      array([[0, 0, 0],\n |             [0, 0, 0]])\n |      \n |      &gt;&gt;&gt; y\n |      array([[1, 2, 3],\n |             [4, 5, 6]])\n |      \n |      &gt;&gt;&gt; y.flags['C_CONTIGUOUS']\n |      True\n |  \n |  cumprod(...)\n |      a.cumprod(axis=None, dtype=None, out=None)\n |      \n |      Return the cumulative product of the elements along the given axis.\n |      \n |      Refer to `numpy.cumprod` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.cumprod : equivalent function\n |  \n |  cumsum(...)\n |      a.cumsum(axis=None, dtype=None, out=None)\n |      \n |      Return the cumulative sum of the elements along the given axis.\n |      \n |      Refer to `numpy.cumsum` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.cumsum : equivalent function\n |  \n |  diagonal(...)\n |      a.diagonal(offset=0, axis1=0, axis2=1)\n |      \n |      Return specified diagonals. In NumPy 1.9 the returned array is a\n |      read-only view instead of a copy as in previous NumPy versions.  In\n |      a future version the read-only restriction will be removed.\n |      \n |      Refer to :func:`numpy.diagonal` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.diagonal : equivalent function\n |  \n |  dot(...)\n |      a.dot(b, out=None)\n |      \n |      Dot product of two arrays.\n |      \n |      Refer to `numpy.dot` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.dot : equivalent function\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = np.eye(2)\n |      &gt;&gt;&gt; b = np.ones((2, 2)) * 2\n |      &gt;&gt;&gt; a.dot(b)\n |      array([[2.,  2.],\n |             [2.,  2.]])\n |      \n |      This array method can be conveniently chained:\n |      \n |      &gt;&gt;&gt; a.dot(b).dot(b)\n |      array([[8.,  8.],\n |             [8.,  8.]])\n |  \n |  dump(...)\n |      a.dump(file)\n |      \n |      Dump a pickle of the array to the specified file.\n |      The array can be read back with pickle.load or numpy.load.\n |      \n |      Parameters\n |      ----------\n |      file : str or Path\n |          A string naming the dump file.\n |      \n |          .. versionchanged:: 1.17.0\n |              `pathlib.Path` objects are now accepted.\n |  \n |  dumps(...)\n |      a.dumps()\n |      \n |      Returns the pickle of the array as a string.\n |      pickle.loads or numpy.loads will convert the string back to an array.\n |      \n |      Parameters\n |      ----------\n |      None\n |  \n |  fill(...)\n |      a.fill(value)\n |      \n |      Fill the array with a scalar value.\n |      \n |      Parameters\n |      ----------\n |      value : scalar\n |          All elements of `a` will be assigned this value.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = np.array([1, 2])\n |      &gt;&gt;&gt; a.fill(0)\n |      &gt;&gt;&gt; a\n |      array([0, 0])\n |      &gt;&gt;&gt; a = np.empty(2)\n |      &gt;&gt;&gt; a.fill(1)\n |      &gt;&gt;&gt; a\n |      array([1.,  1.])\n |  \n |  getfield(...)\n |      a.getfield(dtype, offset=0)\n |      \n |      Returns a field of the given array as a certain type.\n |      \n |      A field is a view of the array data with a given data-type. The values in\n |      the view are determined by the given type and the offset into the current\n |      array in bytes. The offset needs to be such that the view dtype fits in the\n |      array dtype; for example an array of dtype complex128 has 16-byte elements.\n |      If taking a view with a 32-bit integer (4 bytes), the offset needs to be\n |      between 0 and 12 bytes.\n |      \n |      Parameters\n |      ----------\n |      dtype : str or dtype\n |          The data type of the view. The dtype size of the view can not be larger\n |          than that of the array itself.\n |      offset : int\n |          Number of bytes to skip before beginning the element view.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.diag([1.+1.j]*2)\n |      &gt;&gt;&gt; x[1, 1] = 2 + 4.j\n |      &gt;&gt;&gt; x\n |      array([[1.+1.j,  0.+0.j],\n |             [0.+0.j,  2.+4.j]])\n |      &gt;&gt;&gt; x.getfield(np.float64)\n |      array([[1.,  0.],\n |             [0.,  2.]])\n |      \n |      By choosing an offset of 8 bytes we can select the complex part of the\n |      array for our view:\n |      \n |      &gt;&gt;&gt; x.getfield(np.float64, offset=8)\n |      array([[1.,  0.],\n |             [0.,  4.]])\n |  \n |  item(...)\n |      a.item(*args)\n |      \n |      Copy an element of an array to a standard Python scalar and return it.\n |      \n |      Parameters\n |      ----------\n |      \\*args : Arguments (variable number and type)\n |      \n |          * none: in this case, the method only works for arrays\n |            with one element (`a.size == 1`), which element is\n |            copied into a standard Python scalar object and returned.\n |      \n |          * int_type: this argument is interpreted as a flat index into\n |            the array, specifying which element to copy and return.\n |      \n |          * tuple of int_types: functions as does a single int_type argument,\n |            except that the argument is interpreted as an nd-index into the\n |            array.\n |      \n |      Returns\n |      -------\n |      z : Standard Python scalar object\n |          A copy of the specified element of the array as a suitable\n |          Python scalar\n |      \n |      Notes\n |      -----\n |      When the data type of `a` is longdouble or clongdouble, item() returns\n |      a scalar array object because there is no available Python scalar that\n |      would not lose information. Void arrays return a buffer object for item(),\n |      unless fields are defined, in which case a tuple is returned.\n |      \n |      `item` is very similar to a[args], except, instead of an array scalar,\n |      a standard Python scalar is returned. This can be useful for speeding up\n |      access to elements of the array and doing arithmetic on elements of the\n |      array using Python's optimized math.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; np.random.seed(123)\n |      &gt;&gt;&gt; x = np.random.randint(9, size=(3, 3))\n |      &gt;&gt;&gt; x\n |      array([[2, 2, 6],\n |             [1, 3, 6],\n |             [1, 0, 1]])\n |      &gt;&gt;&gt; x.item(3)\n |      1\n |      &gt;&gt;&gt; x.item(7)\n |      0\n |      &gt;&gt;&gt; x.item((0, 1))\n |      2\n |      &gt;&gt;&gt; x.item((2, 2))\n |      1\n |  \n |  itemset(...)\n |      a.itemset(*args)\n |      \n |      Insert scalar into an array (scalar is cast to array's dtype, if possible)\n |      \n |      There must be at least 1 argument, and define the last argument\n |      as *item*.  Then, ``a.itemset(*args)`` is equivalent to but faster\n |      than ``a[args] = item``.  The item should be a scalar value and `args`\n |      must select a single item in the array `a`.\n |      \n |      Parameters\n |      ----------\n |      \\*args : Arguments\n |          If one argument: a scalar, only used in case `a` is of size 1.\n |          If two arguments: the last argument is the value to be set\n |          and must be a scalar, the first argument specifies a single array\n |          element location. It is either an int or a tuple.\n |      \n |      Notes\n |      -----\n |      Compared to indexing syntax, `itemset` provides some speed increase\n |      for placing a scalar into a particular location in an `ndarray`,\n |      if you must do this.  However, generally this is discouraged:\n |      among other problems, it complicates the appearance of the code.\n |      Also, when using `itemset` (and `item`) inside a loop, be sure\n |      to assign the methods to a local variable to avoid the attribute\n |      look-up at each loop iteration.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; np.random.seed(123)\n |      &gt;&gt;&gt; x = np.random.randint(9, size=(3, 3))\n |      &gt;&gt;&gt; x\n |      array([[2, 2, 6],\n |             [1, 3, 6],\n |             [1, 0, 1]])\n |      &gt;&gt;&gt; x.itemset(4, 0)\n |      &gt;&gt;&gt; x.itemset((2, 2), 9)\n |      &gt;&gt;&gt; x\n |      array([[2, 2, 6],\n |             [1, 0, 6],\n |             [1, 0, 9]])\n |  \n |  newbyteorder(...)\n |      arr.newbyteorder(new_order='S')\n |      \n |      Return the array with the same data viewed with a different byte order.\n |      \n |      Equivalent to::\n |      \n |          arr.view(arr.dtype.newbytorder(new_order))\n |      \n |      Changes are also made in all fields and sub-arrays of the array data\n |      type.\n |      \n |      \n |      \n |      Parameters\n |      ----------\n |      new_order : string, optional\n |          Byte order to force; a value from the byte order specifications\n |          below. `new_order` codes can be any of:\n |      \n |          * 'S' - swap dtype from current to opposite endian\n |          * {'&lt;', 'L'} - little endian\n |          * {'&gt;', 'B'} - big endian\n |          * {'=', 'N'} - native order\n |          * {'|', 'I'} - ignore (no change to byte order)\n |      \n |          The default value ('S') results in swapping the current\n |          byte order. The code does a case-insensitive check on the first\n |          letter of `new_order` for the alternatives above.  For example,\n |          any of 'B' or 'b' or 'biggish' are valid to specify big-endian.\n |      \n |      \n |      Returns\n |      -------\n |      new_arr : array\n |          New array object with the dtype reflecting given change to the\n |          byte order.\n |  \n |  nonzero(...)\n |      a.nonzero()\n |      \n |      Return the indices of the elements that are non-zero.\n |      \n |      Refer to `numpy.nonzero` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.nonzero : equivalent function\n |  \n |  partition(...)\n |      a.partition(kth, axis=-1, kind='introselect', order=None)\n |      \n |      Rearranges the elements in the array in such a way that the value of the\n |      element in kth position is in the position it would be in a sorted array.\n |      All elements smaller than the kth element are moved before this element and\n |      all equal or greater are moved behind it. The ordering of the elements in\n |      the two partitions is undefined.\n |      \n |      .. versionadded:: 1.8.0\n |      \n |      Parameters\n |      ----------\n |      kth : int or sequence of ints\n |          Element index to partition by. The kth element value will be in its\n |          final sorted position and all smaller elements will be moved before it\n |          and all equal or greater elements behind it.\n |          The order of all elements in the partitions is undefined.\n |          If provided with a sequence of kth it will partition all elements\n |          indexed by kth of them into their sorted position at once.\n |      axis : int, optional\n |          Axis along which to sort. Default is -1, which means sort along the\n |          last axis.\n |      kind : {'introselect'}, optional\n |          Selection algorithm. Default is 'introselect'.\n |      order : str or list of str, optional\n |          When `a` is an array with fields defined, this argument specifies\n |          which fields to compare first, second, etc. A single field can\n |          be specified as a string, and not all fields need to be specified,\n |          but unspecified fields will still be used, in the order in which\n |          they come up in the dtype, to break ties.\n |      \n |      See Also\n |      --------\n |      numpy.partition : Return a parititioned copy of an array.\n |      argpartition : Indirect partition.\n |      sort : Full sort.\n |      \n |      Notes\n |      -----\n |      See ``np.partition`` for notes on the different algorithms.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = np.array([3, 4, 2, 1])\n |      &gt;&gt;&gt; a.partition(3)\n |      &gt;&gt;&gt; a\n |      array([2, 1, 3, 4])\n |      \n |      &gt;&gt;&gt; a.partition((1, 3))\n |      &gt;&gt;&gt; a\n |      array([1, 2, 3, 4])\n |  \n |  put(...)\n |      a.put(indices, values, mode='raise')\n |      \n |      Set ``a.flat[n] = values[n]`` for all `n` in indices.\n |      \n |      Refer to `numpy.put` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.put : equivalent function\n |  \n |  repeat(...)\n |      a.repeat(repeats, axis=None)\n |      \n |      Repeat elements of an array.\n |      \n |      Refer to `numpy.repeat` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.repeat : equivalent function\n |  \n |  reshape(...)\n |      a.reshape(shape, order='C')\n |      \n |      Returns an array containing the same data with a new shape.\n |      \n |      Refer to `numpy.reshape` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.reshape : equivalent function\n |      \n |      Notes\n |      -----\n |      Unlike the free function `numpy.reshape`, this method on `ndarray` allows\n |      the elements of the shape parameter to be passed in as separate arguments.\n |      For example, ``a.reshape(10, 11)`` is equivalent to\n |      ``a.reshape((10, 11))``.\n |  \n |  resize(...)\n |      a.resize(new_shape, refcheck=True)\n |      \n |      Change shape and size of array in-place.\n |      \n |      Parameters\n |      ----------\n |      new_shape : tuple of ints, or `n` ints\n |          Shape of resized array.\n |      refcheck : bool, optional\n |          If False, reference count will not be checked. Default is True.\n |      \n |      Returns\n |      -------\n |      None\n |      \n |      Raises\n |      ------\n |      ValueError\n |          If `a` does not own its own data or references or views to it exist,\n |          and the data memory must be changed.\n |          PyPy only: will always raise if the data memory must be changed, since\n |          there is no reliable way to determine if references or views to it\n |          exist.\n |      \n |      SystemError\n |          If the `order` keyword argument is specified. This behaviour is a\n |          bug in NumPy.\n |      \n |      See Also\n |      --------\n |      resize : Return a new array with the specified shape.\n |      \n |      Notes\n |      -----\n |      This reallocates space for the data area if necessary.\n |      \n |      Only contiguous arrays (data elements consecutive in memory) can be\n |      resized.\n |      \n |      The purpose of the reference count check is to make sure you\n |      do not use this array as a buffer for another Python object and then\n |      reallocate the memory. However, reference counts can increase in\n |      other ways so if you are sure that you have not shared the memory\n |      for this array with another Python object, then you may safely set\n |      `refcheck` to False.\n |      \n |      Examples\n |      --------\n |      Shrinking an array: array is flattened (in the order that the data are\n |      stored in memory), resized, and reshaped:\n |      \n |      &gt;&gt;&gt; a = np.array([[0, 1], [2, 3]], order='C')\n |      &gt;&gt;&gt; a.resize((2, 1))\n |      &gt;&gt;&gt; a\n |      array([[0],\n |             [1]])\n |      \n |      &gt;&gt;&gt; a = np.array([[0, 1], [2, 3]], order='F')\n |      &gt;&gt;&gt; a.resize((2, 1))\n |      &gt;&gt;&gt; a\n |      array([[0],\n |             [2]])\n |      \n |      Enlarging an array: as above, but missing entries are filled with zeros:\n |      \n |      &gt;&gt;&gt; b = np.array([[0, 1], [2, 3]])\n |      &gt;&gt;&gt; b.resize(2, 3) # new_shape parameter doesn't have to be a tuple\n |      &gt;&gt;&gt; b\n |      array([[0, 1, 2],\n |             [3, 0, 0]])\n |      \n |      Referencing an array prevents resizing...\n |      \n |      &gt;&gt;&gt; c = a\n |      &gt;&gt;&gt; a.resize((1, 1))\n |      Traceback (most recent call last):\n |      ...\n |      ValueError: cannot resize an array that references or is referenced ...\n |      \n |      Unless `refcheck` is False:\n |      \n |      &gt;&gt;&gt; a.resize((1, 1), refcheck=False)\n |      &gt;&gt;&gt; a\n |      array([[0]])\n |      &gt;&gt;&gt; c\n |      array([[0]])\n |  \n |  round(...)\n |      a.round(decimals=0, out=None)\n |      \n |      Return `a` with each element rounded to the given number of decimals.\n |      \n |      Refer to `numpy.around` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.around : equivalent function\n |  \n |  searchsorted(...)\n |      a.searchsorted(v, side='left', sorter=None)\n |      \n |      Find indices where elements of v should be inserted in a to maintain order.\n |      \n |      For full documentation, see `numpy.searchsorted`\n |      \n |      See Also\n |      --------\n |      numpy.searchsorted : equivalent function\n |  \n |  setfield(...)\n |      a.setfield(val, dtype, offset=0)\n |      \n |      Put a value into a specified place in a field defined by a data-type.\n |      \n |      Place `val` into `a`'s field defined by `dtype` and beginning `offset`\n |      bytes into the field.\n |      \n |      Parameters\n |      ----------\n |      val : object\n |          Value to be placed in field.\n |      dtype : dtype object\n |          Data-type of the field in which to place `val`.\n |      offset : int, optional\n |          The number of bytes into the field at which to place `val`.\n |      \n |      Returns\n |      -------\n |      None\n |      \n |      See Also\n |      --------\n |      getfield\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.eye(3)\n |      &gt;&gt;&gt; x.getfield(np.float64)\n |      array([[1.,  0.,  0.],\n |             [0.,  1.,  0.],\n |             [0.,  0.,  1.]])\n |      &gt;&gt;&gt; x.setfield(3, np.int32)\n |      &gt;&gt;&gt; x.getfield(np.int32)\n |      array([[3, 3, 3],\n |             [3, 3, 3],\n |             [3, 3, 3]], dtype=int32)\n |      &gt;&gt;&gt; x\n |      array([[1.0e+000, 1.5e-323, 1.5e-323],\n |             [1.5e-323, 1.0e+000, 1.5e-323],\n |             [1.5e-323, 1.5e-323, 1.0e+000]])\n |      &gt;&gt;&gt; x.setfield(np.eye(3), np.int32)\n |      &gt;&gt;&gt; x\n |      array([[1.,  0.,  0.],\n |             [0.,  1.,  0.],\n |             [0.,  0.,  1.]])\n |  \n |  setflags(...)\n |      a.setflags(write=None, align=None, uic=None)\n |      \n |      Set array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY),\n |      respectively.\n |      \n |      These Boolean-valued flags affect how numpy interprets the memory\n |      area used by `a` (see Notes below). The ALIGNED flag can only\n |      be set to True if the data is actually aligned according to the type.\n |      The WRITEBACKIFCOPY and (deprecated) UPDATEIFCOPY flags can never be set\n |      to True. The flag WRITEABLE can only be set to True if the array owns its\n |      own memory, or the ultimate owner of the memory exposes a writeable buffer\n |      interface, or is a string. (The exception for string is made so that\n |      unpickling can be done without copying memory.)\n |      \n |      Parameters\n |      ----------\n |      write : bool, optional\n |          Describes whether or not `a` can be written to.\n |      align : bool, optional\n |          Describes whether or not `a` is aligned properly for its type.\n |      uic : bool, optional\n |          Describes whether or not `a` is a copy of another \"base\" array.\n |      \n |      Notes\n |      -----\n |      Array flags provide information about how the memory area used\n |      for the array is to be interpreted. There are 7 Boolean flags\n |      in use, only four of which can be changed by the user:\n |      WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED.\n |      \n |      WRITEABLE (W) the data area can be written to;\n |      \n |      ALIGNED (A) the data and strides are aligned appropriately for the hardware\n |      (as determined by the compiler);\n |      \n |      UPDATEIFCOPY (U) (deprecated), replaced by WRITEBACKIFCOPY;\n |      \n |      WRITEBACKIFCOPY (X) this array is a copy of some other array (referenced\n |      by .base). When the C-API function PyArray_ResolveWritebackIfCopy is\n |      called, the base array will be updated with the contents of this array.\n |      \n |      All flags can be accessed using the single (upper case) letter as well\n |      as the full name.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; y = np.array([[3, 1, 7],\n |      ...               [2, 0, 0],\n |      ...               [8, 5, 9]])\n |      &gt;&gt;&gt; y\n |      array([[3, 1, 7],\n |             [2, 0, 0],\n |             [8, 5, 9]])\n |      &gt;&gt;&gt; y.flags\n |        C_CONTIGUOUS : True\n |        F_CONTIGUOUS : False\n |        OWNDATA : True\n |        WRITEABLE : True\n |        ALIGNED : True\n |        WRITEBACKIFCOPY : False\n |        UPDATEIFCOPY : False\n |      &gt;&gt;&gt; y.setflags(write=0, align=0)\n |      &gt;&gt;&gt; y.flags\n |        C_CONTIGUOUS : True\n |        F_CONTIGUOUS : False\n |        OWNDATA : True\n |        WRITEABLE : False\n |        ALIGNED : False\n |        WRITEBACKIFCOPY : False\n |        UPDATEIFCOPY : False\n |      &gt;&gt;&gt; y.setflags(uic=1)\n |      Traceback (most recent call last):\n |        File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n |      ValueError: cannot set WRITEBACKIFCOPY flag to True\n |  \n |  sort(...)\n |      a.sort(axis=-1, kind=None, order=None)\n |      \n |      Sort an array in-place. Refer to `numpy.sort` for full documentation.\n |      \n |      Parameters\n |      ----------\n |      axis : int, optional\n |          Axis along which to sort. Default is -1, which means sort along the\n |          last axis.\n |      kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n |          Sorting algorithm. The default is 'quicksort'. Note that both 'stable'\n |          and 'mergesort' use timsort under the covers and, in general, the\n |          actual implementation will vary with datatype. The 'mergesort' option\n |          is retained for backwards compatibility.\n |      \n |          .. versionchanged:: 1.15.0.\n |             The 'stable' option was added.\n |      \n |      order : str or list of str, optional\n |          When `a` is an array with fields defined, this argument specifies\n |          which fields to compare first, second, etc.  A single field can\n |          be specified as a string, and not all fields need be specified,\n |          but unspecified fields will still be used, in the order in which\n |          they come up in the dtype, to break ties.\n |      \n |      See Also\n |      --------\n |      numpy.sort : Return a sorted copy of an array.\n |      numpy.argsort : Indirect sort.\n |      numpy.lexsort : Indirect stable sort on multiple keys.\n |      numpy.searchsorted : Find elements in sorted array.\n |      numpy.partition: Partial sort.\n |      \n |      Notes\n |      -----\n |      See `numpy.sort` for notes on the different sorting algorithms.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = np.array([[1,4], [3,1]])\n |      &gt;&gt;&gt; a.sort(axis=1)\n |      &gt;&gt;&gt; a\n |      array([[1, 4],\n |             [1, 3]])\n |      &gt;&gt;&gt; a.sort(axis=0)\n |      &gt;&gt;&gt; a\n |      array([[1, 3],\n |             [1, 4]])\n |      \n |      Use the `order` keyword to specify a field to use when sorting a\n |      structured array:\n |      \n |      &gt;&gt;&gt; a = np.array([('a', 2), ('c', 1)], dtype=[('x', 'S1'), ('y', int)])\n |      &gt;&gt;&gt; a.sort(order='y')\n |      &gt;&gt;&gt; a\n |      array([(b'c', 1), (b'a', 2)],\n |            dtype=[('x', 'S1'), ('y', '&lt;i8')])\n |  \n |  swapaxes(...)\n |      a.swapaxes(axis1, axis2)\n |      \n |      Return a view of the array with `axis1` and `axis2` interchanged.\n |      \n |      Refer to `numpy.swapaxes` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.swapaxes : equivalent function\n |  \n |  take(...)\n |      a.take(indices, axis=None, out=None, mode='raise')\n |      \n |      Return an array formed from the elements of `a` at the given indices.\n |      \n |      Refer to `numpy.take` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.take : equivalent function\n |  \n |  tobytes(...)\n |      a.tobytes(order='C')\n |      \n |      Construct Python bytes containing the raw data bytes in the array.\n |      \n |      Constructs Python bytes showing a copy of the raw contents of\n |      data memory. The bytes object can be produced in either 'C' or 'Fortran',\n |      or 'Any' order (the default is 'C'-order). 'Any' order means C-order\n |      unless the F_CONTIGUOUS flag in the array is set, in which case it\n |      means 'Fortran' order.\n |      \n |      .. versionadded:: 1.9.0\n |      \n |      Parameters\n |      ----------\n |      order : {'C', 'F', None}, optional\n |          Order of the data for multidimensional arrays:\n |          C, Fortran, or the same as for the original array.\n |      \n |      Returns\n |      -------\n |      s : bytes\n |          Python bytes exhibiting a copy of `a`'s raw data.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([[0, 1], [2, 3]], dtype='&lt;u2')\n |      &gt;&gt;&gt; x.tobytes()\n |      b'\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00'\n |      &gt;&gt;&gt; x.tobytes('C') == x.tobytes()\n |      True\n |      &gt;&gt;&gt; x.tobytes('F')\n |      b'\\x00\\x00\\x02\\x00\\x01\\x00\\x03\\x00'\n |  \n |  tofile(...)\n |      a.tofile(fid, sep=\"\", format=\"%s\")\n |      \n |      Write array to a file as text or binary (default).\n |      \n |      Data is always written in 'C' order, independent of the order of `a`.\n |      The data produced by this method can be recovered using the function\n |      fromfile().\n |      \n |      Parameters\n |      ----------\n |      fid : file or str or Path\n |          An open file object, or a string containing a filename.\n |      \n |          .. versionchanged:: 1.17.0\n |              `pathlib.Path` objects are now accepted.\n |      \n |      sep : str\n |          Separator between array items for text output.\n |          If \"\" (empty), a binary file is written, equivalent to\n |          ``file.write(a.tobytes())``.\n |      format : str\n |          Format string for text file output.\n |          Each entry in the array is formatted to text by first converting\n |          it to the closest Python type, and then using \"format\" % item.\n |      \n |      Notes\n |      -----\n |      This is a convenience function for quick storage of array data.\n |      Information on endianness and precision is lost, so this method is not a\n |      good choice for files intended to archive data or transport data between\n |      machines with different endianness. Some of these problems can be overcome\n |      by outputting the data as text files, at the expense of speed and file\n |      size.\n |      \n |      When fid is a file object, array contents are directly written to the\n |      file, bypassing the file object's ``write`` method. As a result, tofile\n |      cannot be used with files objects supporting compression (e.g., GzipFile)\n |      or file-like objects that do not support ``fileno()`` (e.g., BytesIO).\n |  \n |  tostring(...)\n |      a.tostring(order='C')\n |      \n |      Construct Python bytes containing the raw data bytes in the array.\n |      \n |      Constructs Python bytes showing a copy of the raw contents of\n |      data memory. The bytes object can be produced in either 'C' or 'Fortran',\n |      or 'Any' order (the default is 'C'-order). 'Any' order means C-order\n |      unless the F_CONTIGUOUS flag in the array is set, in which case it\n |      means 'Fortran' order.\n |      \n |      This function is a compatibility alias for tobytes. Despite its name it returns bytes not strings.\n |      \n |      Parameters\n |      ----------\n |      order : {'C', 'F', None}, optional\n |          Order of the data for multidimensional arrays:\n |          C, Fortran, or the same as for the original array.\n |      \n |      Returns\n |      -------\n |      s : bytes\n |          Python bytes exhibiting a copy of `a`'s raw data.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([[0, 1], [2, 3]], dtype='&lt;u2')\n |      &gt;&gt;&gt; x.tobytes()\n |      b'\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00'\n |      &gt;&gt;&gt; x.tobytes('C') == x.tobytes()\n |      True\n |      &gt;&gt;&gt; x.tobytes('F')\n |      b'\\x00\\x00\\x02\\x00\\x01\\x00\\x03\\x00'\n |  \n |  trace(...)\n |      a.trace(offset=0, axis1=0, axis2=1, dtype=None, out=None)\n |      \n |      Return the sum along diagonals of the array.\n |      \n |      Refer to `numpy.trace` for full documentation.\n |      \n |      See Also\n |      --------\n |      numpy.trace : equivalent function\n |  \n |  transpose(...)\n |      a.transpose(*axes)\n |      \n |      Returns a view of the array with axes transposed.\n |      \n |      For a 1-D array this has no effect, as a transposed vector is simply the\n |      same vector. To convert a 1-D array into a 2D column vector, an additional\n |      dimension must be added. `np.atleast2d(a).T` achieves this, as does\n |      `a[:, np.newaxis]`.\n |      For a 2-D array, this is a standard matrix transpose.\n |      For an n-D array, if axes are given, their order indicates how the\n |      axes are permuted (see Examples). If axes are not provided and\n |      ``a.shape = (i[0], i[1], ... i[n-2], i[n-1])``, then\n |      ``a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0])``.\n |      \n |      Parameters\n |      ----------\n |      axes : None, tuple of ints, or `n` ints\n |      \n |       * None or no argument: reverses the order of the axes.\n |      \n |       * tuple of ints: `i` in the `j`-th place in the tuple means `a`'s\n |         `i`-th axis becomes `a.transpose()`'s `j`-th axis.\n |      \n |       * `n` ints: same as an n-tuple of the same ints (this form is\n |         intended simply as a \"convenience\" alternative to the tuple form)\n |      \n |      Returns\n |      -------\n |      out : ndarray\n |          View of `a`, with axes suitably permuted.\n |      \n |      See Also\n |      --------\n |      ndarray.T : Array property returning the array transposed.\n |      ndarray.reshape : Give a new shape to an array without changing its data.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; a = np.array([[1, 2], [3, 4]])\n |      &gt;&gt;&gt; a\n |      array([[1, 2],\n |             [3, 4]])\n |      &gt;&gt;&gt; a.transpose()\n |      array([[1, 3],\n |             [2, 4]])\n |      &gt;&gt;&gt; a.transpose((1, 0))\n |      array([[1, 3],\n |             [2, 4]])\n |      &gt;&gt;&gt; a.transpose(1, 0)\n |      array([[1, 3],\n |             [2, 4]])\n |  \n |  view(...)\n |      a.view(dtype=None, type=None)\n |      \n |      New view of array with the same data.\n |      \n |      Parameters\n |      ----------\n |      dtype : data-type or ndarray sub-class, optional\n |          Data-type descriptor of the returned view, e.g., float32 or int16. The\n |          default, None, results in the view having the same data-type as `a`.\n |          This argument can also be specified as an ndarray sub-class, which\n |          then specifies the type of the returned object (this is equivalent to\n |          setting the ``type`` parameter).\n |      type : Python type, optional\n |          Type of the returned view, e.g., ndarray or matrix.  Again, the\n |          default None results in type preservation.\n |      \n |      Notes\n |      -----\n |      ``a.view()`` is used two different ways:\n |      \n |      ``a.view(some_dtype)`` or ``a.view(dtype=some_dtype)`` constructs a view\n |      of the array's memory with a different data-type.  This can cause a\n |      reinterpretation of the bytes of memory.\n |      \n |      ``a.view(ndarray_subclass)`` or ``a.view(type=ndarray_subclass)`` just\n |      returns an instance of `ndarray_subclass` that looks at the same array\n |      (same shape, dtype, etc.)  This does not cause a reinterpretation of the\n |      memory.\n |      \n |      For ``a.view(some_dtype)``, if ``some_dtype`` has a different number of\n |      bytes per entry than the previous dtype (for example, converting a\n |      regular array to a structured array), then the behavior of the view\n |      cannot be predicted just from the superficial appearance of ``a`` (shown\n |      by ``print(a)``). It also depends on exactly how ``a`` is stored in\n |      memory. Therefore if ``a`` is C-ordered versus fortran-ordered, versus\n |      defined as a slice or transpose, etc., the view may give different\n |      results.\n |      \n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([(1, 2)], dtype=[('a', np.int8), ('b', np.int8)])\n |      \n |      Viewing array data using a different type and dtype:\n |      \n |      &gt;&gt;&gt; y = x.view(dtype=np.int16, type=np.matrix)\n |      &gt;&gt;&gt; y\n |      matrix([[513]], dtype=int16)\n |      &gt;&gt;&gt; print(type(y))\n |      &lt;class 'numpy.matrix'&gt;\n |      \n |      Creating a view on a structured array so it can be used in calculations\n |      \n |      &gt;&gt;&gt; x = np.array([(1, 2),(3,4)], dtype=[('a', np.int8), ('b', np.int8)])\n |      &gt;&gt;&gt; xv = x.view(dtype=np.int8).reshape(-1,2)\n |      &gt;&gt;&gt; xv\n |      array([[1, 2],\n |             [3, 4]], dtype=int8)\n |      &gt;&gt;&gt; xv.mean(0)\n |      array([2.,  3.])\n |      \n |      Making changes to the view changes the underlying array\n |      \n |      &gt;&gt;&gt; xv[0,1] = 20\n |      &gt;&gt;&gt; x\n |      array([(1, 20), (3,  4)], dtype=[('a', 'i1'), ('b', 'i1')])\n |      \n |      Using a view to convert an array to a recarray:\n |      \n |      &gt;&gt;&gt; z = x.view(np.recarray)\n |      &gt;&gt;&gt; z.a\n |      array([1, 3], dtype=int8)\n |      \n |      Views share data:\n |      \n |      &gt;&gt;&gt; x[0] = (9, 10)\n |      &gt;&gt;&gt; z[0]\n |      (9, 10)\n |      \n |      Views that change the dtype size (bytes per entry) should normally be\n |      avoided on arrays defined by slices, transposes, fortran-ordering, etc.:\n |      \n |      &gt;&gt;&gt; x = np.array([[1,2,3],[4,5,6]], dtype=np.int16)\n |      &gt;&gt;&gt; y = x[:, 0:2]\n |      &gt;&gt;&gt; y\n |      array([[1, 2],\n |             [4, 5]], dtype=int16)\n |      &gt;&gt;&gt; y.view(dtype=[('width', np.int16), ('length', np.int16)])\n |      Traceback (most recent call last):\n |          ...\n |      ValueError: To change to a dtype of a different size, the array must be C-contiguous\n |      &gt;&gt;&gt; z = y.copy()\n |      &gt;&gt;&gt; z.view(dtype=[('width', np.int16), ('length', np.int16)])\n |      array([[(1, 2)],\n |             [(4, 5)]], dtype=[('width', '&lt;i2'), ('length', '&lt;i2')])\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from ndarray:\n |  \n |  __array_interface__\n |      Array protocol: Python side.\n |  \n |  __array_struct__\n |      Array protocol: C-struct side.\n |  \n |  base\n |      Base object if memory is from some other object.\n |      \n |      Examples\n |      --------\n |      The base of an array that owns its memory is None:\n |      \n |      &gt;&gt;&gt; x = np.array([1,2,3,4])\n |      &gt;&gt;&gt; x.base is None\n |      True\n |      \n |      Slicing creates a view, whose memory is shared with x:\n |      \n |      &gt;&gt;&gt; y = x[2:]\n |      &gt;&gt;&gt; y.base is x\n |      True\n |  \n |  ctypes\n |      An object to simplify the interaction of the array with the ctypes\n |      module.\n |      \n |      This attribute creates an object that makes it easier to use arrays\n |      when calling shared libraries with the ctypes module. The returned\n |      object has, among others, data, shape, and strides attributes (see\n |      Notes below) which themselves return ctypes objects that can be used\n |      as arguments to a shared library.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      c : Python object\n |          Possessing attributes data, shape, strides, etc.\n |      \n |      See Also\n |      --------\n |      numpy.ctypeslib\n |      \n |      Notes\n |      -----\n |      Below are the public attributes of this object which were documented\n |      in \"Guide to NumPy\" (we have omitted undocumented public attributes,\n |      as well as documented private attributes):\n |      \n |      .. autoattribute:: numpy.core._internal._ctypes.data\n |          :noindex:\n |      \n |      .. autoattribute:: numpy.core._internal._ctypes.shape\n |          :noindex:\n |      \n |      .. autoattribute:: numpy.core._internal._ctypes.strides\n |          :noindex:\n |      \n |      .. automethod:: numpy.core._internal._ctypes.data_as\n |          :noindex:\n |      \n |      .. automethod:: numpy.core._internal._ctypes.shape_as\n |          :noindex:\n |      \n |      .. automethod:: numpy.core._internal._ctypes.strides_as\n |          :noindex:\n |      \n |      If the ctypes module is not available, then the ctypes attribute\n |      of array objects still returns something useful, but ctypes objects\n |      are not returned and errors may be raised instead. In particular,\n |      the object will still have the ``as_parameter`` attribute which will\n |      return an integer equal to the data attribute.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; import ctypes\n |      &gt;&gt;&gt; x\n |      array([[0, 1],\n |             [2, 3]])\n |      &gt;&gt;&gt; x.ctypes.data\n |      30439712\n |      &gt;&gt;&gt; x.ctypes.data_as(ctypes.POINTER(ctypes.c_long))\n |      &lt;ctypes.LP_c_long object at 0x01F01300&gt;\n |      &gt;&gt;&gt; x.ctypes.data_as(ctypes.POINTER(ctypes.c_long)).contents\n |      c_long(0)\n |      &gt;&gt;&gt; x.ctypes.data_as(ctypes.POINTER(ctypes.c_longlong)).contents\n |      c_longlong(4294967296L)\n |      &gt;&gt;&gt; x.ctypes.shape\n |      &lt;numpy.core._internal.c_long_Array_2 object at 0x01FFD580&gt;\n |      &gt;&gt;&gt; x.ctypes.shape_as(ctypes.c_long)\n |      &lt;numpy.core._internal.c_long_Array_2 object at 0x01FCE620&gt;\n |      &gt;&gt;&gt; x.ctypes.strides\n |      &lt;numpy.core._internal.c_long_Array_2 object at 0x01FCE620&gt;\n |      &gt;&gt;&gt; x.ctypes.strides_as(ctypes.c_longlong)\n |      &lt;numpy.core._internal.c_longlong_Array_2 object at 0x01F01300&gt;\n |  \n |  data\n |      Python buffer object pointing to the start of the array's data.\n |  \n |  dtype\n |      Data-type of the array's elements.\n |      \n |      Parameters\n |      ----------\n |      None\n |      \n |      Returns\n |      -------\n |      d : numpy dtype object\n |      \n |      See Also\n |      --------\n |      numpy.dtype\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x\n |      array([[0, 1],\n |             [2, 3]])\n |      &gt;&gt;&gt; x.dtype\n |      dtype('int32')\n |      &gt;&gt;&gt; type(x.dtype)\n |      &lt;type 'numpy.dtype'&gt;\n |  \n |  flags\n |      Information about the memory layout of the array.\n |      \n |      Attributes\n |      ----------\n |      C_CONTIGUOUS (C)\n |          The data is in a single, C-style contiguous segment.\n |      F_CONTIGUOUS (F)\n |          The data is in a single, Fortran-style contiguous segment.\n |      OWNDATA (O)\n |          The array owns the memory it uses or borrows it from another object.\n |      WRITEABLE (W)\n |          The data area can be written to.  Setting this to False locks\n |          the data, making it read-only.  A view (slice, etc.) inherits WRITEABLE\n |          from its base array at creation time, but a view of a writeable\n |          array may be subsequently locked while the base array remains writeable.\n |          (The opposite is not true, in that a view of a locked array may not\n |          be made writeable.  However, currently, locking a base object does not\n |          lock any views that already reference it, so under that circumstance it\n |          is possible to alter the contents of a locked array via a previously\n |          created writeable view onto it.)  Attempting to change a non-writeable\n |          array raises a RuntimeError exception.\n |      ALIGNED (A)\n |          The data and all elements are aligned appropriately for the hardware.\n |      WRITEBACKIFCOPY (X)\n |          This array is a copy of some other array. The C-API function\n |          PyArray_ResolveWritebackIfCopy must be called before deallocating\n |          to the base array will be updated with the contents of this array.\n |      UPDATEIFCOPY (U)\n |          (Deprecated, use WRITEBACKIFCOPY) This array is a copy of some other array.\n |          When this array is\n |          deallocated, the base array will be updated with the contents of\n |          this array.\n |      FNC\n |          F_CONTIGUOUS and not C_CONTIGUOUS.\n |      FORC\n |          F_CONTIGUOUS or C_CONTIGUOUS (one-segment test).\n |      BEHAVED (B)\n |          ALIGNED and WRITEABLE.\n |      CARRAY (CA)\n |          BEHAVED and C_CONTIGUOUS.\n |      FARRAY (FA)\n |          BEHAVED and F_CONTIGUOUS and not C_CONTIGUOUS.\n |      \n |      Notes\n |      -----\n |      The `flags` object can be accessed dictionary-like (as in ``a.flags['WRITEABLE']``),\n |      or by using lowercased attribute names (as in ``a.flags.writeable``). Short flag\n |      names are only supported in dictionary access.\n |      \n |      Only the WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED flags can be\n |      changed by the user, via direct assignment to the attribute or dictionary\n |      entry, or by calling `ndarray.setflags`.\n |      \n |      The array flags cannot be set arbitrarily:\n |      \n |      - UPDATEIFCOPY can only be set ``False``.\n |      - WRITEBACKIFCOPY can only be set ``False``.\n |      - ALIGNED can only be set ``True`` if the data is truly aligned.\n |      - WRITEABLE can only be set ``True`` if the array owns its own memory\n |        or the ultimate owner of the memory exposes a writeable buffer\n |        interface or is a string.\n |      \n |      Arrays can be both C-style and Fortran-style contiguous simultaneously.\n |      This is clear for 1-dimensional arrays, but can also be true for higher\n |      dimensional arrays.\n |      \n |      Even for contiguous arrays a stride for a given dimension\n |      ``arr.strides[dim]`` may be *arbitrary* if ``arr.shape[dim] == 1``\n |      or the array has no elements.\n |      It does *not* generally hold that ``self.strides[-1] == self.itemsize``\n |      for C-style contiguous arrays or ``self.strides[0] == self.itemsize`` for\n |      Fortran-style contiguous arrays is true.\n |  \n |  flat\n |      A 1-D iterator over the array.\n |      \n |      This is a `numpy.flatiter` instance, which acts similarly to, but is not\n |      a subclass of, Python's built-in iterator object.\n |      \n |      See Also\n |      --------\n |      flatten : Return a copy of the array collapsed into one dimension.\n |      \n |      flatiter\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.arange(1, 7).reshape(2, 3)\n |      &gt;&gt;&gt; x\n |      array([[1, 2, 3],\n |             [4, 5, 6]])\n |      &gt;&gt;&gt; x.flat[3]\n |      4\n |      &gt;&gt;&gt; x.T\n |      array([[1, 4],\n |             [2, 5],\n |             [3, 6]])\n |      &gt;&gt;&gt; x.T.flat[3]\n |      5\n |      &gt;&gt;&gt; type(x.flat)\n |      &lt;class 'numpy.flatiter'&gt;\n |      \n |      An assignment example:\n |      \n |      &gt;&gt;&gt; x.flat = 3; x\n |      array([[3, 3, 3],\n |             [3, 3, 3]])\n |      &gt;&gt;&gt; x.flat[[1,4]] = 1; x\n |      array([[3, 1, 3],\n |             [3, 1, 3]])\n |  \n |  imag\n |      The imaginary part of the array.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.sqrt([1+0j, 0+1j])\n |      &gt;&gt;&gt; x.imag\n |      array([ 0.        ,  0.70710678])\n |      &gt;&gt;&gt; x.imag.dtype\n |      dtype('float64')\n |  \n |  itemsize\n |      Length of one array element in bytes.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([1,2,3], dtype=np.float64)\n |      &gt;&gt;&gt; x.itemsize\n |      8\n |      &gt;&gt;&gt; x = np.array([1,2,3], dtype=np.complex128)\n |      &gt;&gt;&gt; x.itemsize\n |      16\n |  \n |  nbytes\n |      Total bytes consumed by the elements of the array.\n |      \n |      Notes\n |      -----\n |      Does not include memory consumed by non-element attributes of the\n |      array object.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.zeros((3,5,2), dtype=np.complex128)\n |      &gt;&gt;&gt; x.nbytes\n |      480\n |      &gt;&gt;&gt; np.prod(x.shape) * x.itemsize\n |      480\n |  \n |  ndim\n |      Number of array dimensions.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([1, 2, 3])\n |      &gt;&gt;&gt; x.ndim\n |      1\n |      &gt;&gt;&gt; y = np.zeros((2, 3, 4))\n |      &gt;&gt;&gt; y.ndim\n |      3\n |  \n |  real\n |      The real part of the array.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.sqrt([1+0j, 0+1j])\n |      &gt;&gt;&gt; x.real\n |      array([ 1.        ,  0.70710678])\n |      &gt;&gt;&gt; x.real.dtype\n |      dtype('float64')\n |      \n |      See Also\n |      --------\n |      numpy.real : equivalent function\n |  \n |  shape\n |      Tuple of array dimensions.\n |      \n |      The shape property is usually used to get the current shape of an array,\n |      but may also be used to reshape the array in-place by assigning a tuple of\n |      array dimensions to it.  As with `numpy.reshape`, one of the new shape\n |      dimensions can be -1, in which case its value is inferred from the size of\n |      the array and the remaining dimensions. Reshaping an array in-place will\n |      fail if a copy is required.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.array([1, 2, 3, 4])\n |      &gt;&gt;&gt; x.shape\n |      (4,)\n |      &gt;&gt;&gt; y = np.zeros((2, 3, 4))\n |      &gt;&gt;&gt; y.shape\n |      (2, 3, 4)\n |      &gt;&gt;&gt; y.shape = (3, 8)\n |      &gt;&gt;&gt; y\n |      array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n |             [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n |             [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n |      &gt;&gt;&gt; y.shape = (3, 6)\n |      Traceback (most recent call last):\n |        File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n |      ValueError: total size of new array must be unchanged\n |      &gt;&gt;&gt; np.zeros((4,2))[::2].shape = (-1,)\n |      Traceback (most recent call last):\n |        File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n |      AttributeError: incompatible shape for a non-contiguous array\n |      \n |      See Also\n |      --------\n |      numpy.reshape : similar function\n |      ndarray.reshape : similar method\n |  \n |  size\n |      Number of elements in the array.\n |      \n |      Equal to ``np.prod(a.shape)``, i.e., the product of the array's\n |      dimensions.\n |      \n |      Notes\n |      -----\n |      `a.size` returns a standard arbitrary precision Python integer. This\n |      may not be the case with other methods of obtaining the same value\n |      (like the suggested ``np.prod(a.shape)``, which returns an instance\n |      of ``np.int_``), and may be relevant if the value is used further in\n |      calculations that may overflow a fixed size integer type.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = np.zeros((3, 5, 2), dtype=np.complex128)\n |      &gt;&gt;&gt; x.size\n |      30\n |      &gt;&gt;&gt; np.prod(x.shape)\n |      30\n |  \n |  strides\n |      Tuple of bytes to step in each dimension when traversing an array.\n |      \n |      The byte offset of element ``(i[0], i[1], ..., i[n])`` in an array `a`\n |      is::\n |      \n |          offset = sum(np.array(i) * a.strides)\n |      \n |      A more detailed explanation of strides can be found in the\n |      \"ndarray.rst\" file in the NumPy reference guide.\n |      \n |      Notes\n |      -----\n |      Imagine an array of 32-bit integers (each 4 bytes)::\n |      \n |        x = np.array([[0, 1, 2, 3, 4],\n |                      [5, 6, 7, 8, 9]], dtype=np.int32)\n |      \n |      This array is stored in memory as 40 bytes, one after the other\n |      (known as a contiguous block of memory).  The strides of an array tell\n |      us how many bytes we have to skip in memory to move to the next position\n |      along a certain axis.  For example, we have to skip 4 bytes (1 value) to\n |      move to the next column, but 20 bytes (5 values) to get to the same\n |      position in the next row.  As such, the strides for the array `x` will be\n |      ``(20, 4)``.\n |      \n |      See Also\n |      --------\n |      numpy.lib.stride_tricks.as_strided\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; y = np.reshape(np.arange(2*3*4), (2,3,4))\n |      &gt;&gt;&gt; y\n |      array([[[ 0,  1,  2,  3],\n |              [ 4,  5,  6,  7],\n |              [ 8,  9, 10, 11]],\n |             [[12, 13, 14, 15],\n |              [16, 17, 18, 19],\n |              [20, 21, 22, 23]]])\n |      &gt;&gt;&gt; y.strides\n |      (48, 16, 4)\n |      &gt;&gt;&gt; y[1,1,1]\n |      17\n |      &gt;&gt;&gt; offset=sum(y.strides * np.array((1,1,1)))\n |      &gt;&gt;&gt; offset/y.itemsize\n |      17\n |      \n |      &gt;&gt;&gt; x = np.reshape(np.arange(5*6*7*8), (5,6,7,8)).transpose(2,3,1,0)\n |      &gt;&gt;&gt; x.strides\n |      (32, 4, 224, 1344)\n |      &gt;&gt;&gt; i = np.array([3,5,2,2])\n |      &gt;&gt;&gt; offset = sum(i * x.strides)\n |      &gt;&gt;&gt; x[3,5,2,2]\n |      813\n |      &gt;&gt;&gt; offset / x.itemsize\n |      813\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from ndarray:\n |  \n |  __hash__ = None\n\n\n\n\n\nA.5.3 Matrix computations\n\nThe sub-module numpy.linalg implements basic linear algebra, such as solving linear systems, singular value decomposition, etc.\n\n\nA.5.3.1 Inverse\n\nnp.linalg.det(A)\n\n0.0\n\n\n\n\n\nA.5.4 Data processing and reshaping\nOften it is useful to store datasets in Numpy arrays. Numpy provides a number of functions to calculate the statistics of datasets in arrays.\n\n# column 4\nnp.mean(A[:,3]), np.std(A[:,3])\n\n(23.0, 14.142135623730951)\n\n\n\na = range(10000)\nb = np.arange(10000)\n\n\n%%timeit\nsum(a)\n\n199 µs ± 9.13 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n%%timeit\nnp.sum(b)\n\n7.99 µs ± 202 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nWhen functions such as min, max, etc. are applied to a multidimensional arrays, it is sometimes useful to apply the calculation to the entire array, and sometimes only on a row or column basis. Using the axis argument we can specify how these functions should behave:\n\nm = np.random.rand(3,3)\nm\n\narray([[0.31751608, 0.41545447, 0.94062331],\n       [0.17379774, 0.57561705, 0.31818086],\n       [0.40848656, 0.62145644, 0.79010869]])\n\n\n\n# global max\nm.max()\n\n0.9406233060883264\n\n\n\n\n\n\nsource: https://scipy-lectures.org/intro/numpy/operations.html\n\n\n# max in each column\nm.max(axis=0)\n\narray([0.40848656, 0.62145644, 0.94062331])\n\n\n\n# max in each row\nm.max(axis=1)\n\narray([0.94062331, 0.57561705, 0.79010869])\n\n\nMany other functions and methods in the array and matrix classes accept the same (optional) axis keyword argument.\nThe shape of a Numpy array can be modified without copying the underlying data, which makes it a fast operation even for large arrays.\n\nA\n\narray([[ 0,  1,  2,  3,  4],\n       [10, 11, 12, 13, 14],\n       [20, 21, 22, 23, 24],\n       [30, 31, 32, 33, 34],\n       [40, 41, 42, 43, 44]])\n\n\n\nn, m = A.shape\n\n\nB = A.reshape((1,n*m))\nB\n\narray([[ 0,  1,  2,  3,  4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 30,\n        31, 32, 33, 34, 40, 41, 42, 43, 44]])\n\n\nWith newaxis, we can insert new dimensions in an array, for example converting a vector to a column or row matrix:\n\nv = np.array([1,2,3])\n\n\nnp.shape(v)\n\n(3,)\n\n\n\n# make a column matrix of the vector v\nv[:, np.newaxis]\n\narray([[1],\n       [2],\n       [3]])\n\n\n\n# column matrix\nv[:,np.newaxis].shape\n\n(3, 1)\n\n\n\nA.5.4.1 Stacking and repeating arrays\nUsing function repeat, tile, vstack, hstack, and concatenate we can create larger vectors and matrices from smaller ones:"
  },
  {
    "objectID": "NumPy_tutorial.html#copy-and-deep-copy",
    "href": "NumPy_tutorial.html#copy-and-deep-copy",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.6 Copy and “deep copy”",
    "text": "A.6 Copy and “deep copy”\n\nTo achieve high performance, assignments in Python usually do not copy the underlying objects. This is important for example when objects are passed between functions, to avoid an excessive amount of memory copying when it is not necessary (technical term: pass by reference).\nA slicing operation creates a view on the original array, which is just a way of accessing array data. Thus the original array is not copied in memory.\n\n\nA = np.array([[1, 2], [3, 4]])\nA\n\narray([[1, 2],\n       [3, 4]])\n\n\n\n# now B is referring to the same array data as A \nB = A \n\n\n# changing B affects A\nB[0,0] = 10\nprint(B)\nprint(A)\n\n[[10  2]\n [ 3  4]]\n[[10  2]\n [ 3  4]]\n\n\n\nnp.may_share_memory(A, B)\n\nTrue\n\n\nIf we want to avoid this behavior, so that when we get a new completely independent object B copied from A, then we need to do a so-called “deep copy” using the function copy:\n\nB = np.copy(A)\n\n\n# now, if we modify B, A is not affected\nB[0,0] = -5\nprint(B)\nprint(A)\n\n[[-5  2]\n [ 3  4]]\n[[10  2]\n [ 3  4]]\n\n\n\nA.6.1 Memory layout matters!\n\n\n\n\nsource: https://scipy-lectures.org/advanced/advanced_numpy/index.html\n\n\nx = np.zeros((20000,))\n\n\ny = np.zeros((20000*67,))[::67]\n\n\n%timeit x.sum()\n\n9.98 µs ± 241 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n%timeit y.sum()\n\n59.6 µs ± 3.38 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\nx.strides, y.strides\n\n((8,), (536,))"
  },
  {
    "objectID": "NumPy_tutorial.html#iterating-over-array-elements",
    "href": "NumPy_tutorial.html#iterating-over-array-elements",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.7 Iterating over array elements",
    "text": "A.7 Iterating over array elements\nGenerally, we want to avoid iterating over the elements of arrays whenever we can (at all costs). The reason is that in an interpreted language like Python (or MATLAB), iterations are really slow compared to vectorized operations.\n\nimport math\n\n\n%%timeit\n# itersative sum\ntotal = 0\nfor item in range(0, 10000):\n    total += item\n\n585 µs ± 24.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%%timeit\n# vectorized sum\nnp.sum(np.arange(10000))\n\n15.2 µs ± 892 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n%%timeit\n# iterative  operation\n[math.exp(item) for item in range(100)]\n\n17.9 µs ± 454 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n%%timeit\n# vectorized operation\nnp.exp(np.arange(100)) \n\n2.86 µs ± 171 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nA.7.1 Create Your Own Vectorizing functions\nTo get good performance we should try to avoid looping over elements in our vectors and matrices, and instead use vectorized algorithms. The first step in converting a scalar algorithm to a vectorized algorithm is to make sure that the functions we write work with vector inputs.\n\ndef Theta(x):\n    \"\"\"\n    Scalar implemenation of the Heaviside step function.\n    \"\"\"\n    if x &gt;= 0:\n        return 1\n    else:\n        return 0\n\nTo get a vectorized version of Theta we can use the Numpy function vectorize. In many cases it can automatically vectorize a function:\n\nTheta_vec = np.vectorize(Theta)\n\n\nTheta_vec(np.array([-3,-2,-1,0,1,2,3]))\n\narray([0, 0, 0, 1, 1, 1, 1])\n\n\n\n\nA.7.2 Type casting\nSince Numpy arrays are statically typed, the type of an array does not change once created. But we can explicitly cast an array of some type to another using the astype functions (see also the similar asarray function). This always creates a new array of a new type:\n\nM.dtype\n\ndtype('float64')\n\n\n\nM2 = M.astype(float)\nM2\n\narray([[0.40570044, 0.66548144, 0.13835937, 0.83043309, 0.12319969],\n       [0.58779155, 0.06309849, 0.49710274, 0.92839462, 0.80603084],\n       [0.19839124, 0.34528354, 0.53473647, 0.97858347, 0.5030445 ],\n       [0.3474475 , 0.21278653, 0.17745402, 0.1040286 , 0.18745545],\n       [0.04031375, 0.23991727, 0.5462427 , 0.20778317, 0.99270398]])\n\n\n\nM2.dtype\n\ndtype('float64')\n\n\n\nM3 = M.astype(bool)\nM3\n\narray([[ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True]])\n\n\n\n\nSee casting at https://scipy-lectures.org/intro/numpy/elaborate_arrays.html"
  },
  {
    "objectID": "NumPy_tutorial.html#file-io",
    "href": "NumPy_tutorial.html#file-io",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.8 File I/O",
    "text": "A.8 File I/O\n\nNumPy has its own binary format, not portable but with efficient I/O\nUseful when storing and reading back numpy array data. Use the functions numpy.save and numpy.load\nMatlab: scipy.io.loadmat, scipy.io.savemat\n\n\nnp.save(\"random-matrix.npy\", M)\n\n\nnp.load(\"random-matrix.npy\")\n\narray([[0.40570044, 0.66548144, 0.13835937, 0.83043309, 0.12319969],\n       [0.58779155, 0.06309849, 0.49710274, 0.92839462, 0.80603084],\n       [0.19839124, 0.34528354, 0.53473647, 0.97858347, 0.5030445 ],\n       [0.3474475 , 0.21278653, 0.17745402, 0.1040286 , 0.18745545],\n       [0.04031375, 0.23991727, 0.5462427 , 0.20778317, 0.99270398]])"
  },
  {
    "objectID": "NumPy_tutorial.html#conclusion",
    "href": "NumPy_tutorial.html#conclusion",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.9 Conclusion",
    "text": "A.9 Conclusion\nTo make the code faster using NumPy and - Vectorizing for loops: Find tricks to avoid for loops using numpy arrays.\n\nIn place operations: a *= 3 instead of a = 3*a\nUse views instead of copies whenever possible\nMemory arrangement is important. Keep strides small as possible for coalescing memory access\nBroadcasting: Use broadcasting to do operations on arrays as small as possible before combining them.\nUse compiled code (The following session)"
  },
  {
    "objectID": "NumPy_tutorial.html#references",
    "href": "NumPy_tutorial.html#references",
    "title": "Appendix A — Numpy - multidimensional data arrays for python",
    "section": "A.10 References",
    "text": "A.10 References\n\nhttps://scipy-lectures.org/intro/numpy/index.html - A good introduction to pydata stack\nhttps://github.com/jrjohansson/scientific-python-lectures/blob/master/Lecture-2-Numpy.ipynb - A good introduction for NumPy though a bit of outdated\nhttp://cs229.stanford.edu/section/cs229_python_tutorial/Spring_2020_Notebook.html - Another good introduction to NumPy\nhttps://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/Broadcasting.html - A great reference for broadcasting and distance calculation\nhttps://eli.thegreenplace.net/2015/memory-layout-of-multi-dimensional-arrays- A great article for memory layout behind NumPy\nhttps://numpy.org/doc/stable/user/numpy-for-matlab-users.html - A Numpy guide for MATLAB users\nhttp://mathesaurus.sourceforge.net/r-numpy.html - A Numpy guide for R users"
  },
  {
    "objectID": "Colab_tutorial.html#setup",
    "href": "Colab_tutorial.html#setup",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.1 Setup",
    "text": "B.1 Setup\nYou can lookup the resources first:\n\nimport multiprocessing\ncores = multiprocessing.cpu_count() # Count the number of cores in a computer\ncores\n\n\nfrom psutil import virtual_memory\nram_gb = virtual_memory().total / 1e9\nprint('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n\n\nimport sys\n# Is this notebook running on Colab or Kaggle?\nIS_COLAB = \"google.colab\" in sys.modules\nIS_KAGGLE = \"kaggle_secrets\" in sys.modules\n\ngpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') &gt;= 0:\n  if IS_COLAB:\n    print(\"Go to Runtime &gt; Change runtime and select a GPU hardware accelerator.\")\n  if IS_KAGGLE:\n    print(\"Go to Settings &gt; Accelerator and select GPU.\")\nelse:\n  from tensorflow.python.client import device_lib \n  print(device_lib.list_local_devices())\n\n\n!nvidia-smi -L\n\nV100 &gt; P100 &gt; T4 &gt; K80 (but most of the time you get K80 or T4 using the free Colab)"
  },
  {
    "objectID": "Colab_tutorial.html#cells",
    "href": "Colab_tutorial.html#cells",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.2 Cells",
    "text": "B.2 Cells\nA notebook is a list of cells. Cells contain either explanatory text or executable code and its output. Click a cell to select it.\n\nB.2.1 Code cells\nBelow is a code cell. Once the toolbar button indicates CONNECTED, click in the cell to select it and execute the contents in the following ways:\n\nClick the Play icon in the left gutter of the cell;\nType Cmd/Ctrl+Enter to run the cell in place;\nType Shift+Enter to run the cell and move focus to the next cell (adding one if none exists); or\nType Alt+Enter to run the cell and insert a new code cell immediately below it.\n\nThere are additional options for running some or all cells in the Runtime menu.\n\na = 10\na\n\n\n\nB.2.2 Text cells\nThis is a text cell. You can double-click to edit this cell. Text cells use markdown syntax. To learn more, see our markdown guide.\nYou can also add math to text cells using LaTeX to be rendered by MathJax. Just place the statement within a pair of $ signs. For example $\\sqrt{3x-1}+(1+x)^2$ becomes \\(\\sqrt{3x-1}+(1+x)^2.\\)\nTable generator also works here.\n\n\nB.2.3 Adding and moving cells\nYou can add new cells by using the + CODE and + TEXT buttons that show when you hover between cells. These buttons are also in the toolbar above the notebook where they can be used to add a cell below the currently selected cell.\nYou can move a cell by selecting it and clicking Cell Up or Cell Down in the top toolbar.\nConsecutive cells can be selected by “lasso selection” by dragging from outside one cell and through the group. Non-adjacent cells can be selected concurrently by clicking one and then holding down Ctrl while clicking another. Similarly, using Shift instead of Ctrl will select all intermediate cells."
  },
  {
    "objectID": "Colab_tutorial.html#working-with-bash",
    "href": "Colab_tutorial.html#working-with-bash",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.3 Working with Bash",
    "text": "B.3 Working with Bash\n\n!pip install colab-xterm -qq\n%load_ext colabxterm\n\n\n%xterm"
  },
  {
    "objectID": "Colab_tutorial.html#working-with-python",
    "href": "Colab_tutorial.html#working-with-python",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.4 Working with python",
    "text": "B.4 Working with python\nColaboratory is built on top of Jupyter Notebook. Below are some examples of convenience functions provided.\nLong running python processes can be interrupted. Run the following cell and select Runtime -&gt; Interrupt execution (hotkey: Cmd/Ctrl-M I) to stop execution.\n\nimport time\nprint(\"Sleeping\")\ntime.sleep(90) # sleep for a while; interrupt me!\nprint(\"Done Sleeping\")\n\n\nB.4.1 System aliases\nJupyter includes shortcuts for common operations, such as ls:\n\n%ls /bin\n\n! calls out to a shell (in a new process), while % affects the process associated with the notebook\n\n\n!cd sample_data\n\n\n%cd sample_data\n\nThat !ls probably generated a large output. You can select the cell and clear the output by either:\n\nClicking on the clear output button (x) in the toolbar above the cell; or\nRight clicking the left gutter of the output area and selecting “Clear output” from the context menu.\n\nExecute any other process using ! with string interpolation from python variables, and note the result can be assigned to a variable:\n\nmessage = 'Colaboratory is great!'\n\n\n!echo -e '{message}\\n'\n\n\nfoo = !echo -e '{message}\\n'\nfoo\n\n\n!mkdir test\n\n\nOUT_DIR = './test'\n!rm -rf {OUT_DIR}\n\n\n!apt-get -qq install htop\n\n\n\nB.4.2 Magics\nColaboratory shares the notion of magics from Jupyter. There are shorthand annotations that change how a cell’s text is executed. To learn more, see Jupyter’s magics page.\n\n%load_ext autoreload\n%autoreload 2\n\n\n\nB.4.3 Automatic completions and exploring code\nColab provides automatic completions to explore attributes of Python objects, as well as to quickly view documentation strings. As an example, first run the following cell to import the numpy module.\n\nimport numpy as np\n\n\nfrom numpy import arccos\n\nIf you now insert your cursor after np and press Period(.), you will see the list of available completions within the np module.\n\nnp\n\nIf you type an open parenthesis after any function or class in the module, you will see a pop-up of its documentation string:\n\nnp.ndarray()\n\n\nnp.min??\n\n\nhelp(np.min)\n\nWhen hovering over the method name the Open in tab link will open the documentation in a persistent pane. The View source link will navigate to the source code for the method."
  },
  {
    "objectID": "Colab_tutorial.html#integration-with-drive",
    "href": "Colab_tutorial.html#integration-with-drive",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.5 Integration with Drive",
    "text": "B.5 Integration with Drive\nColaboratory is integrated with Google Drive. It allows you to share, comment, and collaborate on the same document with multiple people:\n\nThe SHARE button (top-right of the toolbar) allows you to share the notebook and control permissions set on it.\nFile-&gt;Make a Copy creates a copy of the notebook in Drive.\nFile-&gt;Save saves the File to Drive. File-&gt;Save and checkpoint pins the version so it doesn’t get deleted from the revision history.\nFile-&gt;Revision history shows the notebook’s revision history.\n\n\nB.5.1 Uploading files from your local file system\nfiles.upload returns a dictionary of the files which were uploaded. The dictionary is keyed by the file name and values are the data which were uploaded.\n\nfrom google.colab import files\n\nuploaded = files.upload()\n\nfor fn in uploaded.keys():\n  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n      name=fn, length=len(uploaded[fn])))\n\nFiles are temporarily stored, and will be removed once you end your session.\n\n\nB.5.2 Downloading files to your local file system\nfiles.download will invoke a browser download of the file to your local computer.\n\nfrom google.colab import files\n\nwith open('example.txt', 'w') as f:\n  f.write('some content')\n\nfiles.download('example.txt')\n\n\n\nB.5.3 Mounting Google Drive locally\nThe example below shows how to mount your Google Drive on your runtime using an authorization code, and how to write and read files there. Once executed, you will be able to see the new file (foo.txt) at https://drive.google.com/.\nThis only supports reading, writing, and moving files; to programmatically modify sharing settings or other metadata, use one of the other options below.\nNote: When using the ‘Mount Drive’ button in the file browser, no authentication codes are necessary for notebooks that have only been edited by the current user.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n!ls /content/drive\n\n\nwith open('/content/drive/My Drive/foo.txt', 'w') as f:\n  f.write('Hello Google Drive!')\n!cat /content/drive/My\\ Drive/foo.txt\n\n\n#drive.flush_and_unmount()\n#print('All changes made in this colab session should now be visible in Drive.')\n\n\n!gdown --fuzzy https://drive.google.com/file/d/1KE8dUFWUM389SdDhGj-UJgyGtXpnsqpl/view?usp=sharing\n\n\nfrom nsysu import hello\n\n\nhello()\n\n\nimport sys\nsys.path.append('/content/drive/MyDrive/colab_test/')\nfrom nsysu_math import hello_math\n# file available at https://drive.google.com/file/d/1KAu1yxGmR_oAcCLk4aEltWWM1eMp3FJj/view?usp=sharing\n\n\nhello_math()\n\n\nRemember DO NOT store input data in your drive and load from there. The input/output is very slow (store at ./ instead). Your output data should be stored in your google drive so that it can be accessed next time.\n\n\n\nB.5.4 Loading Public Notebooks Directly from GitHub\nColab can load public github notebooks directly, with no required authorization step.\nFor example, consider the notebook at this address: https://github.com/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb.\nThe direct colab link to this notebook is: https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb.\nTo generate such links in one click, you can use the Open in Colab Chrome extension."
  },
  {
    "objectID": "Colab_tutorial.html#run-flask-or-other-web-app",
    "href": "Colab_tutorial.html#run-flask-or-other-web-app",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.6 Run Flask or other web app",
    "text": "B.6 Run Flask or other web app\n\n!pip install flask -qq\n!pip install pyngrok -qq\n\n\nfrom pyngrok import ngrok, conf\nimport getpass\n\n\nprint(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\nconf.get_default().auth_token = getpass.getpass()\n\n\n# Setup a tunnel to the streamlit port 8050\npublic_url = ngrok.connect(8050)\npublic_url\n\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello():\n    return 'Hello NSYSU!'\n\nif __name__ == '__main__':\n    app.run(port=8050)"
  },
  {
    "objectID": "Colab_tutorial.html#use-different-version-of-pythonenvironment-or-lanaguage",
    "href": "Colab_tutorial.html#use-different-version-of-pythonenvironment-or-lanaguage",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.7 Use different version of python/environment or lanaguage",
    "text": "B.7 Use different version of python/environment or lanaguage\nRefer to here and condacolab for more information.\n\nimport sys\nprint(sys.version)\n\nJupyter is named after Julia, Python and R. You can change the kernel to R or Julia."
  },
  {
    "objectID": "Colab_tutorial.html#downloading-data-from-kaggle",
    "href": "Colab_tutorial.html#downloading-data-from-kaggle",
    "title": "Appendix B — Introduction to Colab",
    "section": "B.8 Downloading data from Kaggle",
    "text": "B.8 Downloading data from Kaggle\nThe Dogs vs. Cats dataset that we will use isn’t packaged with Keras. It was made available by Kaggle as part of a computer vision competition in late 2013, back when convnets weren’t mainstream. You can download the original dataset from www.kaggle.com/c/dogs-vs-cats/data.\nBut you can also use Kaggle API. First, you need to create a Kaggle API key and download it to your local machine. Just navigate to the Kaggle website in a web browser, log in, and go to the My Account page. In your account settings, you’ll find an API section. Clicking the Create New API Token button will generate a kaggle.json key file and will download it to your machine.\n\n# Upload the API’s key JSON file to your Colab\n# session by running the following code in a notebook cell:\nfrom google.colab import files\nfiles.upload()\n\nFinally, create a ~/.kaggle folder, and copy the key file to it. As a security best practice, you should also make sure that the file is only readable by the current user, yourself:\n\n!mkdir ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n\n\n# You can now download the data we’re about to use:\n!kaggle competitions download -c dogs-vs-cats\n\nThe first time you try to download the data, you may get a “403 Forbidden” error. That’s because you need to accept the terms associated with the dataset before you download it—you’ll have to go to www.kaggle.com/c/dogs-vs-cats/rules (while logged into your Kaggle account) and click the I Understand and Accept button. You only need to do this once.\nMore information about Kaggle API"
  },
  {
    "objectID": "kaggle-explore.html#setup",
    "href": "kaggle-explore.html#setup",
    "title": "Appendix C — Introduction to Kaggle",
    "section": "C.1 Setup",
    "text": "C.1 Setup\nYou can lookup the resources first:\n\nimport multiprocessing\ncores = multiprocessing.cpu_count() # Count the number of cores in a computer\ncores\n\n\nfrom psutil import virtual_memory\nram_gb = virtual_memory().total / 1e9\nprint('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n\n\nimport sys\n# Is this notebook running on Colab or Kaggle?\nIS_COLAB = \"google.colab\" in sys.modules\nIS_KAGGLE = \"kaggle_secrets\" in sys.modules\n\ngpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') &gt;= 0:\n  if IS_COLAB:\n    print(\"Go to Runtime &gt; Change runtime and select a GPU hardware accelerator.\")\n  if IS_KAGGLE:\n    print(\"Go to Settings &gt; Accelerator and select GPU.\")\nelse:\n  from tensorflow.python.client import device_lib \n  print(device_lib.list_local_devices())\n\n\n!nvidia-smi -L\n\nV100 &gt; P100 &gt; T4 &gt; K80 (but most of the time you get K80 or T4 using the free Colab)"
  },
  {
    "objectID": "kaggle-explore.html#working-with-bash",
    "href": "kaggle-explore.html#working-with-bash",
    "title": "Appendix C — Introduction to Kaggle",
    "section": "C.2 Working with Bash",
    "text": "C.2 Working with Bash\nKaggle provides simple bash (console) below"
  },
  {
    "objectID": "kaggle-explore.html#working-with-python",
    "href": "kaggle-explore.html#working-with-python",
    "title": "Appendix C — Introduction to Kaggle",
    "section": "C.3 Working with python",
    "text": "C.3 Working with python\nKaggle kernel is built on top of Jupyter Notebook. Below are some examples of convenience functions provided.\n\nC.3.1 System aliases\nJupyter includes shortcuts for common operations, such as ls:\n\n%ls /kaggle/\n\n! calls out to a shell (in a new process), while % affects the process associated with the notebook\nExecute any other process using ! with string interpolation from python variables, and note the result can be assigned to a variable:\n\nmessage = 'Kaggle is great!'\n!echo {message}\n\nKaggle is great!\n\n\n\nfoo = !echo {message}\nfoo\n\n['Kaggle is great!']\n\n\n\n!mkdir test\n\n\nOUT_DIR = './test'\n!rm -rf {OUT_DIR}\n\n\n!apt-get  install htop\n\n\n\nC.3.2 Magics\nKaggle shares the notion of magics from Jupyter. There are shorthand annotations that change how a cell’s text is executed. To learn more, see Jupyter’s magics page.\n\n%load_ext autoreload\n%autoreload 2\n\n\n\nC.3.3 Automatic completions and exploring code\nColab provides automatic completions to explore attributes of Python objects, as well as to quickly view documentation strings. As an example, first run the following cell to import the numpy module.\n\nimport numpy as np\n\n\nfrom numpy import arccos\n\nIf you now insert your cursor after np and press Period(.), you will see the list of available completions within the np module.\n\nnp.\n\nIf you type an open parenthesis after any function or class in the module, you will see a pop-up of its documentation string:\n\n??np.ndarray\n\n\nhelp(np.ndarray)"
  },
  {
    "objectID": "kaggle-explore.html#adding-data-sources",
    "href": "kaggle-explore.html#adding-data-sources",
    "title": "Appendix C — Introduction to Kaggle",
    "section": "C.4 Adding Data Sources",
    "text": "C.4 Adding Data Sources\nOne of the advantages to using Notebooks as your data science workbench is that you can easily add data sources from thousands of publicly available Datasets or even upload your own. You can also use output files from another Notebook as a data source. You can add multiple data sources to your Notebook’s environment, allowing you to join together interesting datasets.\nNavigate to the “Data” pane in a Notebook editor and click the “Add Data” button. This will open a modal that lets you select Datasets to add to your Notebook. The input data will be stored in the /kaggle/input/ directory. The output data will be stored in the /kaggle/working/ directory. You can also use the kaggle datasets download command to download a dataset to your Notebook’s environment. For more information, see the Kaggle Datasets documentation.\nYou will notice that there is a third option in the “Add Data” modal: Notebook Output Files. Up to 20 GBs of output from a Notebook may be saved to disk in /kaggle/working. This data is saved automatically and you can then reuse that data in any future Notebook: just navigate to the “Data” pane in a Notebook editor, click on “Add Data”, click on the “Notebook Output Files” tab, find a Notebook of interest, and then click to add it to your current Notebook. By chaining Notebooks as data sources in this way, it’s possible to build pipelines and generate more and better content than you could in a single notebook alone. If you need additional temporary scratch space, consider saving to /kaggle/tmp/ where the limits are much more generous. Note that the /kaggle/tmp/ directory is not guaranteed to be persisted between Notebook runs. For more information, see the Kaggle Notebooks documentation.\nFinally, you can add your own dataset by uploading your files.\n\n!pip install --upgrade --no-cache-dir gdown -qq\n\n\n!gdown --fuzzy https://drive.google.com/file/d/1KE8dUFWUM389SdDhGj-UJgyGtXpnsqpl/view?usp=sharing\n\n\nfrom nsysu import hello\n\n\nhello()\n\n\n!ls /kaggle/input/\n\n\nC.4.1 Resoruces\nThe Resources of Kaggle GPU: - Kaggle GPU: 16G NVIDIA TESLA P100 - Limited to 30+ hrs/week depending on usage. - Limited to 12hrs/run\n\n\nC.4.2 Save version\nYou can run the code in the background with Kaggle. Firstly, make sure your code is bug-free, as any error in any code block would result in early stopping. Click the “Save Version” button. The concept of “Versions” is a collection consisting of a Notebook version, the output it generates, and the associated metadata about the environment. Two options are available:\n\nQuick Save: Skips the top-to-bottom notebook execution and just takes a snapshot of your notebook exactly as it’s displayed in the editor. This is a great option for taking a bunch of versions while you’re still actively experimenting. You can choose to reserve the output\nSave & Run All: Creates a new session with a completely clean state and runs your notebook from top to bottom. In order to save successfully, the entire notebook must execute within 12 hours (9 hours for TPU notebooks). Save & Run All is identical to the “Commit” behavior."
  },
  {
    "objectID": "08_neural_nets_with_pytorch.html#setup",
    "href": "08_neural_nets_with_pytorch.html#setup",
    "title": "Appendix D — Introduction to Artificial Neural Networks - Pytorch",
    "section": "D.1 Setup",
    "text": "D.1 Setup\n\n!pip install torchinfo -qq\n\n\n# Python ≥3.7 is recommended\nimport sys\nassert sys.version_info &gt;= (3, 7)\nimport os\nfrom pathlib import Path\nfrom time import strftime\nimport gc\n\n# Scikit-Learn ≥1.01 is recommended\nfrom packaging import version\nimport sklearn\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\n\n# Pytorch related\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim \nfrom torchvision import datasets, transforms\nimport torch.nn.functional as F\nfrom torchinfo import summary\nfrom fastai.vision.all import *\n\n# Common imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator at 0x7f1b310fe9b0&gt;\n\n\n\nif \"google.colab\" in sys.modules:  # extra code\n    %pip install -q -U tensorboard-plugin-profile\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 48.8 MB/s eta 0:00:00\n\n\n\nif not torch.cuda.device_count():\n    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n    if \"google.colab\" in sys.modules:\n        print(\"Go to Runtime &gt; Change runtime and select a GPU hardware \"\n              \"accelerator.\")\n    if \"kaggle_secrets\" in sys.modules:\n        print(\"Go to Settings &gt; Accelerator and select GPU.\")"
  },
  {
    "objectID": "08_neural_nets_with_pytorch.html#perceptrons",
    "href": "08_neural_nets_with_pytorch.html#perceptrons",
    "title": "Appendix D — Introduction to Artificial Neural Networks - Pytorch",
    "section": "D.2 Perceptrons",
    "text": "D.2 Perceptrons\nLet’s use the iris dataset from openml. This is a famous dataset that contains the sepal and petal length and width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica\n\n\n\nYou can find more information about the dataset here.\n\niris = load_iris(as_frame=True)\nprint(iris.data.shape)\niris.data.head()\n\n(150, 4)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nFor simplicity, here we perform binary classification based on two features.\n\n# Choose two features and setup a binary classification problem\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].to_numpy()\ny = (iris.target == 0)  # Iris setosa\n\n# Build Perceptron model\nper_clf = Perceptron(random_state=42)\nper_clf.fit(X, y)\n\n# Test on two new instances\nX_new = [[2, 0.5], [3, 1]]\ny_pred = per_clf.predict(X_new)  # predicts True and False for these 2 flowers\ny_pred\n\narray([ True, False])\n\n\n\n# Plot the decision boundary\n\na = -per_clf.coef_[0, 0] / per_clf.coef_[0, 1]\nb = -per_clf.intercept_ / per_clf.coef_[0, 1]\naxes = [0, 5, 0, 2]\nx0, x1 = np.meshgrid(\n    np.linspace(axes[0], axes[1], 500).reshape(-1, 1),\n    np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n)\nX_new = np.c_[x0.ravel(), x1.ravel()]\ny_predict = per_clf.predict(X_new)\nzz = y_predict.reshape(x0.shape)\ncustom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n\nplt.figure(figsize=(7, 3))\nplt.plot(X[y == 0, 0], X[y == 0, 1], \"bs\", label=\"Not Iris setosa\")\nplt.plot(X[y == 1, 0], X[y == 1, 1], \"yo\", label=\"Iris setosa\")\nplt.plot([axes[0], axes[1]], [a * axes[0] + b, a * axes[1] + b], \"k-\",\n         linewidth=3)\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"lower right\")\nplt.axis(axes)\nplt.show()"
  },
  {
    "objectID": "08_neural_nets_with_pytorch.html#tensorflow-playground",
    "href": "08_neural_nets_with_pytorch.html#tensorflow-playground",
    "title": "Appendix D — Introduction to Artificial Neural Networks - Pytorch",
    "section": "D.3 Tensorflow Playground",
    "text": "D.3 Tensorflow Playground\nhttp://playground.tensorflow.org/\n\nD.3.1 Introduction\nThe Playground provides mainly 6 different types of datasets. 1. Classification: Circle, Exclusive or, Gaussian, spiral. 2. Regression: Plane, Multi Gaussian.\nSmall circle points are represented as data points that correspond to Positive (+) and Negative (-). Positive represented by blue, Negative represented by orange. These same colours are used in representing Data, Neuron, Weight values.\nThe datasets all have 2 input features and 1 output label. The 2 input features, X1 and X2, are represented by the coordinates.\n\nThe data points (represented by small circles) are initially colored orange or blue, which correspond to positive one and negative one.\nIn the hidden layers, the lines are colored by the weights of the connections between neurons. Blue shows a positive weight, which means the network is using that output of the neuron as given. An orange line shows that the network is assiging a negative weight.\nIn the output layer, the dots are colored orange or blue depending on their original values. The background color shows what the network is predicting for a particular area. The intensity of the color shows how confident that prediction is\n\n\n\nD.3.2 Try it\n\nLayers and patterns: try training the default neural network by clicking the “Run” button (top left). Notice how it quickly finds a good solution for the classification task. Notice that the neurons in the first hidden layer have learned simple patterns, while the neurons in the second hidden layer have learned to combine the simple patterns of the first hidden layer into more complex patterns). In general, the more layers, the more complex the patterns can be.\nActivation function: try replacing the Tanh activation function with the ReLU activation function, and train the network again. Notice that it finds a solution even faster.\nLocal minima: modify the network architecture to have just one hidden layer with three neurons. Train it multiple times (to reset the network weights, just add and remove a neuron). Notice that the training time varies a lot, and sometimes it even gets stuck in a local minimum.\nToo small: now remove one neuron to keep just 2. Notice that the neural network is now incapable of finding a good solution, even if you try multiple times. The model has too few parameters and it systematically underfits the training set.\nLarge enough: next, set the number of neurons to 8 and train the network several times. Notice that it is now consistently fast and never gets stuck. This highlights an important finding in neural network theory: large neural networks almost never get stuck in local minima, and even when they do these local optima are almost as good as the global optimum. However, they can still get stuck on long plateaus for a long time.\nDeep net and vanishing gradients: now change the dataset to be the spiral (bottom right dataset under “DATA”). Change the network architecture to have 4 hidden layers with 4 neurons each. Notice that training takes much longer, and often gets stuck on plateaus for long periods of time. Also notice that the neurons in the highest layers (i.e. on the right) tend to evolve faster than the neurons in the lowest layers (i.e. on the left). This problem, called the “vanishing gradients” problem, can be alleviated using better weight initialization and other techniques, better optimizers (such as AdaGrad or Adam), or using Batch Normalization."
  },
  {
    "objectID": "08_neural_nets_with_pytorch.html#building-an-image-classifier-using-the-sequential-api",
    "href": "08_neural_nets_with_pytorch.html#building-an-image-classifier-using-the-sequential-api",
    "title": "Appendix D — Introduction to Artificial Neural Networks - Pytorch",
    "section": "D.4 Building an Image Classifier Using the Sequential API",
    "text": "D.4 Building an Image Classifier Using the Sequential API\nFirst let’s import pytorch.\n\ntorch.__version__\n\n'2.0.0+cu118'\n\n\nFirst, we need to load a dataset. We will tackle Fashion MNIST, which is a drop-in replacement of MNIST. It has the exact same format as MNIST (70,000 grayscale images of \\(28 \\times 28\\) pixels each, with 10 classes), but the images represent fashion items rather than handwritten digits, so each class is more diverse and the problem turns out to be significantly more challenging than MNIST. For example, a simple linear model reaches about 92% accuracy on MNIST, but only about 83% on Fashion MNIST.\nLet’s start by loading the fashion MNIST dataset. Pytorch has a number of functions to load popular datasets in datasets. The dataset is already split for you between a training set and a test set, but it can be useful to split the training set further to have a validation set:\n\ntransform = transforms.Compose([transforms.ToTensor(), lambda x: x/255])\n\ntrainset = datasets.FashionMNIST(\n    root=\"data\",            \n    train=True,             \n    download=True,         \n    transform=transform,  \n)\n\ntestset = datasets.FashionMNIST(\n    root=\"data\",           \n    train=False,           \n    download=True,         \n    transform=transform,\n)\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|██████████| 26421880/26421880 [00:01&lt;00:00, 15937675.15it/s]\n\n\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n\n100%|██████████| 29515/29515 [00:00&lt;00:00, 271715.97it/s]\n\n\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n100%|██████████| 4422102/4422102 [00:00&lt;00:00, 5055744.32it/s]\n\n\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n100%|██████████| 5148/5148 [00:00&lt;00:00, 20841966.21it/s]\n\n\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\n\n\n\n\n\nLet’s split the full training set into a validation set and a (smaller) training set. Now the validation set contains 5,000 images, and the test set contains 10,000 images.\n\n# Preparing for validaion test\n\ntrainset, validset = torch.utils.data.random_split(trainset, [55000, 5000])\n\n# Data Loader\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\nvalidloader = torch.utils.data.DataLoader(validset, batch_size=32, shuffle=False, num_workers=2)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n\nNotice that the order of Pytorch is [batch, channel, height, width].\n\nfor X, y in trainloader:\n    print(\"Shape of X [N, C, H, W]: \", X.shape)\n    print(\"Shape of y: \", y.shape, y.dtype)\n    print(y)\n    break\n\nShape of X [N, C, H, W]:  torch.Size([32, 1, 28, 28])\nShape of y:  torch.Size([32]) torch.int64\ntensor([3, 4, 5, 7, 7, 0, 6, 2, 0, 1, 1, 5, 2, 8, 8, 0, 5, 1, 8, 7, 7, 3, 4, 1,\n        5, 1, 0, 9, 6, 7, 3, 0])\n\n\nThe labels are the class IDs from 0 to 9.\nHere are the corresponding class names:\n\nclass_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n\nLet’s take a look at a sample of the images in the dataset:\n\ndataiter = iter(trainloader)\nprint(dataiter)\nimages, labels = next(dataiter)\n\n\nfig = plt.figure(figsize=(15,5))\nfor idx in np.arange(20):\n  # xticks=[], yticks=[] is empty to print the images without any ticks around them\n  #np.sqeeze : Remove single-dimensional entries from the shape of an array.\n  ax = fig.add_subplot(4, int(20/4), idx+1, xticks=[], yticks=[])\n  ax.imshow(np.squeeze(images[idx]), cmap='binary')\n   # .item() gets the value contained in a Tensor\n  ax.set_title(class_names[labels[idx].item()])\n  fig.tight_layout()\n\n&lt;torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1ac0892dc0&gt;\n\n\n\n\n\n\nD.4.1 Creating the Model Using the Sequential API\nNow let’s build the neural network! Here is a classification MLP with two hidden layers:\n\nmodel = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(28 * 28, 300),\n    torch.nn.ReLU(),\n    torch.nn.Linear(300, 100),\n    torch.nn.ReLU(),\n    torch.nn.Linear(100, 10)\n)\n\n\nWe build the first layer and add it to the model. It is a Flatten layer whose role is simply to convert each input image into a 1D array: if it receives input data X, it computes X.reshape(-1, 1). This layer does not have any parameters, it is just there to do some simple preprocessing.\nNext we add a Linear hidden layer with 300 neurons. It will use the ReLU activation function. Each Linear layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of weight bias terms (one per neuron in the next layer).\nNext we add a second Linear hidden layer with 100 neurons, also using the ReLU activation function.\nFinally, we add a Linear output layer with 10 neurons (one per class)\n\nThe model’s summary() method displays all the model’s layers, including each layer’s name (which is automatically generated unless you set it when creating the layer), its output shape (None means the batch size can be anything), and its number of parameters. The summary ends with the total number of parameters, including trainable and non-trainable parameters. Here we only have trainable parameters. Since we have not specified the shape of input in the model definition, you should specify the input shape.\n\nsummary(model, input_size=(32, 1, 28, 28))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nSequential                               [32, 10]                  --\n├─Flatten: 1-1                           [32, 784]                 --\n├─Linear: 1-2                            [32, 300]                 235,500\n├─ReLU: 1-3                              [32, 300]                 --\n├─Linear: 1-4                            [32, 100]                 30,100\n├─ReLU: 1-5                              [32, 100]                 --\n├─Linear: 1-6                            [32, 10]                  1,010\n==========================================================================================\nTotal params: 266,610\nTrainable params: 266,610\nNon-trainable params: 0\nTotal mult-adds (M): 8.53\n==========================================================================================\nInput size (MB): 0.10\nForward/backward pass size (MB): 0.10\nParams size (MB): 1.07\nEstimated Total Size (MB): 1.27\n==========================================================================================\n\n\nNote that Linear layers often have a lot of parameters. For example, the first hidden layer has 784×300 connection weights, plus 300 bias terms, which adds up to 235,500 parameters! This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data!\nAll the parameters of a layer can be accessed using parameters or named_parameters. For a Linear layer, this includes both the connection weights and the bias terms:\n\nmodel\n\nSequential(\n  (0): Flatten(start_dim=1, end_dim=-1)\n  (1): Linear(in_features=784, out_features=300, bias=True)\n  (2): ReLU()\n  (3): Linear(in_features=300, out_features=100, bias=True)\n  (4): ReLU()\n  (5): Linear(in_features=100, out_features=10, bias=True)\n)\n\n\n\nfor name, param in model[1].named_parameters():\n    print(name, param)\n    print(param.shape)\n\nweight Parameter containing:\ntensor([[-0.0008,  0.0200,  0.0067,  ..., -0.0131, -0.0300,  0.0034],\n        [-0.0015, -0.0090,  0.0235,  ..., -0.0063,  0.0129,  0.0004],\n        [-0.0313,  0.0174, -0.0309,  ..., -0.0151, -0.0003, -0.0060],\n        ...,\n        [-0.0189,  0.0302,  0.0253,  ...,  0.0066, -0.0163,  0.0019],\n        [-0.0316,  0.0122,  0.0078,  ...,  0.0180, -0.0269,  0.0266],\n        [-0.0120, -0.0033, -0.0263,  ...,  0.0092, -0.0053, -0.0234]],\n       device='cuda:0', requires_grad=True)\ntorch.Size([300, 784])\nbias Parameter containing:\ntensor([-9.2447e-03, -2.8974e-02,  7.2220e-03, -3.3954e-02,  2.2371e-02,\n         1.8663e-02, -4.9257e-03, -2.1786e-02, -6.3649e-06,  1.6387e-02,\n        -2.2864e-02,  5.9067e-03,  7.7610e-03, -6.7648e-03,  2.9585e-04,\n        -2.6493e-02, -5.4183e-03,  7.9403e-03, -8.6405e-03,  2.9657e-02,\n         1.5868e-02, -9.9445e-03,  1.7332e-02, -2.3654e-02,  2.2842e-02,\n         2.5317e-02,  2.4360e-02, -8.2912e-03,  3.1216e-02,  1.5463e-02,\n        -1.0169e-02,  2.5816e-02, -1.3469e-02, -3.2832e-02,  1.2529e-02,\n        -2.7177e-02,  6.2815e-03, -1.4683e-02, -2.1896e-02, -1.7237e-02,\n         1.0154e-02,  2.1535e-02,  3.2067e-02,  2.8216e-03, -3.1141e-02,\n         9.2012e-03,  3.5173e-02,  6.1674e-03, -6.3851e-03, -1.4740e-02,\n        -1.6019e-02, -2.6387e-02, -1.7757e-02, -3.9408e-03,  1.1568e-02,\n         4.2389e-03, -2.8631e-02, -1.3997e-03,  3.1402e-02, -3.5590e-02,\n         2.4734e-02,  2.8789e-02,  6.8656e-03,  5.9841e-03, -2.5894e-02,\n         1.3375e-02,  2.5252e-02,  2.3386e-02,  3.0545e-02,  8.2353e-03,\n         3.0568e-02, -3.4779e-02, -3.4079e-02,  3.0365e-02, -2.4662e-02,\n        -1.9887e-03, -6.9185e-03,  5.1128e-04, -5.2750e-03, -2.6210e-02,\n        -2.9502e-02,  1.0410e-02,  2.8435e-02, -3.0872e-02, -3.4853e-02,\n        -2.9207e-02, -1.3540e-02, -2.6068e-02,  2.6272e-03,  3.4383e-02,\n         3.5367e-03,  2.9826e-02, -3.0363e-02,  2.8289e-02, -1.5520e-02,\n         2.8845e-02,  8.3951e-03, -2.3115e-02,  3.1465e-02, -7.2679e-03,\n         1.8229e-02,  9.3152e-03, -3.5144e-02,  1.6315e-02, -3.3828e-02,\n         2.3353e-02,  2.8609e-02, -2.6712e-02,  3.1670e-02,  1.7699e-02,\n         2.0659e-03,  2.8378e-02, -1.6027e-02, -1.8193e-02,  2.7328e-02,\n        -3.4533e-02,  8.5889e-04,  9.4282e-03, -2.6854e-02, -2.1584e-02,\n         2.6076e-02,  3.2076e-02,  8.5227e-03,  1.3048e-02,  1.1044e-02,\n         6.0950e-03, -3.3592e-02,  2.8679e-02, -5.2404e-04,  1.6140e-02,\n         2.4623e-02, -2.4162e-03,  3.2872e-02, -1.0452e-02,  5.5769e-03,\n        -1.4323e-02, -2.3068e-02, -1.5116e-02,  2.4937e-02, -1.9900e-03,\n         1.8617e-02,  1.9982e-02,  1.1068e-02,  3.1604e-02, -3.0483e-02,\n         5.3171e-03, -1.6588e-02, -1.3809e-03,  2.7469e-02, -1.0831e-03,\n        -1.2490e-02,  2.1345e-02,  1.4646e-02, -1.6319e-02, -3.3509e-02,\n         3.1080e-02, -2.3143e-02, -2.2578e-02,  1.4178e-02,  3.0439e-03,\n         1.6791e-02,  1.4951e-02,  2.1536e-02,  1.1985e-02, -2.3818e-02,\n        -2.7063e-02, -3.2855e-02,  2.3895e-02, -1.0812e-02, -3.1468e-03,\n        -1.0324e-02,  1.1700e-02, -4.4395e-03,  2.9190e-02,  6.4556e-03,\n         2.4577e-02, -6.9744e-03, -9.4140e-03,  3.2622e-02,  1.9515e-03,\n         3.0799e-02, -2.5119e-02,  3.0590e-02, -2.7039e-02, -2.1676e-02,\n         2.0446e-02,  2.6810e-02, -1.5904e-03, -2.1362e-02,  2.1427e-02,\n        -4.6989e-03, -1.5359e-03, -2.8713e-02, -2.7395e-02, -9.5997e-03,\n        -6.3844e-03,  3.3899e-02,  1.2112e-02, -3.4466e-02,  2.6060e-02,\n         2.5547e-02,  1.0490e-02, -1.6767e-02,  1.6235e-02, -1.4759e-02,\n         1.0598e-02,  2.3993e-02,  2.7616e-02, -2.2009e-02,  1.3510e-02,\n         1.7211e-02, -8.0213e-03,  1.5486e-02,  2.6799e-02,  2.4796e-02,\n        -2.4025e-02,  2.6916e-02,  2.0536e-02,  1.5900e-02, -1.0640e-02,\n        -3.2833e-02,  7.8252e-03,  3.0454e-02,  9.9070e-03, -1.7561e-02,\n        -5.8674e-03, -2.1999e-02, -2.6726e-02,  1.0378e-02,  2.7650e-02,\n        -2.7787e-02, -2.0327e-02, -2.2503e-02,  1.7165e-02, -3.4127e-02,\n        -6.2338e-03,  2.8526e-02, -2.6287e-02, -3.4710e-02, -3.1144e-02,\n         2.3019e-02, -3.2435e-03,  7.6013e-03,  2.3533e-02, -1.0957e-02,\n        -2.9534e-02,  1.4351e-04, -7.0979e-03,  2.6821e-02, -3.0888e-02,\n        -1.4907e-03,  1.0754e-02,  2.7180e-02,  2.9236e-02,  1.4429e-02,\n         2.9763e-02,  8.7761e-03, -2.8120e-02, -3.7655e-03, -1.0074e-02,\n         3.1215e-02, -1.8262e-02,  1.7809e-02,  2.7542e-02,  6.4198e-03,\n         1.5621e-02, -1.6357e-02, -1.7261e-02, -2.9069e-02, -1.8037e-02,\n        -5.9544e-03,  3.0685e-03, -2.6223e-02, -3.2612e-02,  1.9598e-02,\n        -2.3883e-02, -3.5633e-02, -2.6732e-02, -2.9994e-02,  1.2320e-02,\n         2.0810e-02, -4.9483e-03,  9.6650e-03, -2.6957e-02, -1.4085e-02,\n         2.4534e-02, -1.3710e-02, -3.0980e-02,  2.2327e-02, -8.2447e-04,\n        -3.0320e-02, -1.4513e-02, -2.4652e-02,  1.2461e-03, -1.8602e-02,\n        -4.4680e-03, -2.9267e-02,  3.1477e-02, -8.2771e-03,  3.2391e-02],\n       device='cuda:0', requires_grad=True)\ntorch.Size([300])\n\n\nNotice that the Linear layer initialized the connection weights randomly (which is needed to break symmetry).\n\n\nD.4.2 Compiling the Model\nHere, we use the high-level API fastai for training, but you may also use Lighting instead.\n\ndata = DataLoaders(trainloader, validloader)\n\n\nlearn = Learner(data, model, loss_func=F.cross_entropy, opt_func=Adam, metrics=[accuracy])\n\n\n\nD.4.3 Training and Evaluating the Model\n\nlearn.fit(30, 0.001)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.790450\n0.780884\n0.720000\n00:20\n\n\n1\n0.686098\n0.695901\n0.753200\n00:17\n\n\n2\n0.553506\n0.608219\n0.786600\n00:19\n\n\n3\n0.513209\n0.545953\n0.813400\n00:20\n\n\n4\n0.479568\n0.516947\n0.819800\n00:18\n\n\n5\n0.488435\n0.491999\n0.831600\n00:17\n\n\n6\n0.440884\n0.468577\n0.836600\n00:22\n\n\n7\n0.458791\n0.459762\n0.840800\n00:22\n\n\n8\n0.409206\n0.452688\n0.839600\n00:19\n\n\n9\n0.386557\n0.436145\n0.847200\n00:21\n\n\n10\n0.386161\n0.424733\n0.848000\n00:20\n\n\n11\n0.372232\n0.428308\n0.848800\n00:26\n\n\n12\n0.396859\n0.417189\n0.848800\n00:20\n\n\n13\n0.334558\n0.404274\n0.854200\n00:19\n\n\n14\n0.355626\n0.399699\n0.859600\n00:19\n\n\n15\n0.354593\n0.401432\n0.858800\n00:24\n\n\n16\n0.354426\n0.388033\n0.863000\n00:20\n\n\n17\n0.330213\n0.370720\n0.863200\n00:18\n\n\n18\n0.321238\n0.378462\n0.865600\n00:19\n\n\n19\n0.314699\n0.377342\n0.861600\n00:18\n\n\n20\n0.299304\n0.372751\n0.867400\n00:18\n\n\n21\n0.322843\n0.356741\n0.871400\n00:18\n\n\n22\n0.328895\n0.357033\n0.874000\n00:19\n\n\n23\n0.301488\n0.362713\n0.870000\n00:18\n\n\n24\n0.306989\n0.354016\n0.873400\n00:23\n\n\n25\n0.318756\n0.348814\n0.874800\n00:18\n\n\n26\n0.276582\n0.345385\n0.876800\n00:19\n\n\n27\n0.268662\n0.335636\n0.879200\n00:17\n\n\n28\n0.280722\n0.341237\n0.878400\n00:19\n\n\n29\n0.304366\n0.336023\n0.878200\n00:19\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\nfastai_loss, fastai_accuracy = learn.validate(dl=testloader)\n\n\n\n\n\n\n\n\n\nfastai_accuracy\n\n0.8761000037193298\n\n\nIt is common to get slightly lower performance on the test set than on the validation set, because the hyperparameters are tuned on the validation set, not the test set (however, in this example, we did not do any hyperparameter tuning, so the lower accuracy is just bad luck).\n\n\nD.4.4 Using the Model to Make Predictions\n\n# Use the model to make predictions on the new data\npredictions, labels = learn.get_preds(dl=testloader)\n\n# Print the predicted classes and their corresponding true labels\nprint('Predicted probs:', predictions)\nprint('Predicted labels:', labels)\n\n\n\n\n\n\n\n\nPredicted probs: tensor([[-10.3676, -10.8953, -11.4818,  ...,   2.2426,  -1.8765,   4.7194],\n        [ -2.0989,  -9.2065,   7.2844,  ..., -51.7720,  -8.7795, -46.2127],\n        [ -2.0484,   6.5507,  -6.7518,  ..., -23.1491, -10.7383, -26.7502],\n        ...,\n        [ -5.0137, -16.9200, -10.1650,  ..., -22.1841,   0.7203, -21.2843],\n        [ -5.0028,   5.4822,  -7.8417,  ..., -11.9855, -12.1367, -15.4872],\n        [ -7.9864,  -6.1703,  -5.9303,  ...,  -2.6586,  -2.9111,  -7.0408]])\nPredicted labels: tensor([9, 2, 1,  ..., 8, 1, 5])\n\n\nYou can also use plain Pytorch to do the inference:\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.eval()\ntest_loss = 0\ncorrect = 0\nwith torch.no_grad():\n    for data, target in testloader:\n        data, target = data.to(device), target.to(device)\n        output = model(data)      \n        # prediction\n        pred = output.argmax(dim=1, keepdim=True)  \n        correct += pred.eq(target.view_as(pred)).sum().item()\n\ndata_count = len(testloader.dataset)\npercentage = 100. * correct / data_count\nprint(percentage)\n\n87.61\n\n\n\n\nD.4.5 Try different network architecture and hyperparameters\n\nlearn = None\nmodel = None\ngc.collect()\ntorch.cuda.empty_cache()\n\n\n# Sometimes applying BN before the activation function works better (there's a debate on this topic)\nmodel = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(28 * 28, 300),\n    torch.nn.BatchNorm1d(300, momentum=0.99, eps=0.001),\n    torch.nn.PReLU(300),\n    torch.nn.Dropout(0.5),\n    torch.nn.Linear(300, 100),\n    torch.nn.BatchNorm1d(100, momentum=0.99, eps=0.001),\n    torch.nn.PReLU(100),\n    torch.nn.Dropout(0.5),\n    torch.nn.Linear(100, 10),\n    nn.LogSoftmax(dim=1)\n)\n\n\nsummary(model, input_size=(32, 1, 28, 28))\n\n/usr/local/lib/python3.9/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  action_fn=lambda data: sys.getsizeof(data.storage()),\n/usr/local/lib/python3.9/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return super().__sizeof__() + self.nbytes()\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nSequential                               [32, 10]                  --\n├─Flatten: 1-1                           [32, 784]                 --\n├─Linear: 1-2                            [32, 300]                 235,500\n├─BatchNorm1d: 1-3                       [32, 300]                 600\n├─PReLU: 1-4                             [32, 300]                 300\n├─Dropout: 1-5                           [32, 300]                 --\n├─Linear: 1-6                            [32, 100]                 30,100\n├─BatchNorm1d: 1-7                       [32, 100]                 200\n├─PReLU: 1-8                             [32, 100]                 100\n├─Dropout: 1-9                           [32, 100]                 --\n├─Linear: 1-10                           [32, 10]                  1,010\n├─LogSoftmax: 1-11                       [32, 10]                  --\n==========================================================================================\nTotal params: 267,810\nTrainable params: 267,810\nNon-trainable params: 0\nTotal mult-adds (M): 8.57\n==========================================================================================\nInput size (MB): 0.10\nForward/backward pass size (MB): 0.31\nParams size (MB): 1.07\nEstimated Total Size (MB): 1.48\n==========================================================================================\n\n\n\ndata = DataLoaders(trainloader, validloader)\n\n\nlearn = Learner(data, model, loss_func=F.cross_entropy, opt_func=Adam, metrics=[accuracy])\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0008317637839354575)\n\n\n\n\n\n\nlearn.fit_one_cycle(30, 0.01)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.536114\n0.492205\n0.831200\n00:22\n\n\n1\n0.500279\n0.457570\n0.833200\n00:24\n\n\n2\n0.503207\n0.490403\n0.826600\n00:29\n\n\n3\n0.502929\n0.447084\n0.839400\n00:29\n\n\n4\n0.469937\n0.420875\n0.853600\n00:24\n\n\n5\n0.481606\n0.436684\n0.848200\n00:22\n\n\n6\n0.485776\n0.417646\n0.853000\n00:22\n\n\n7\n0.468681\n0.394094\n0.863200\n00:22\n\n\n8\n0.453838\n0.423227\n0.844600\n00:22\n\n\n9\n0.446407\n0.413163\n0.850400\n00:22\n\n\n10\n0.448527\n0.397825\n0.855600\n00:22\n\n\n11\n0.448806\n0.389306\n0.860800\n00:21\n\n\n12\n0.438929\n0.405902\n0.858200\n00:22\n\n\n13\n0.433270\n0.388356\n0.860200\n00:22\n\n\n14\n0.405702\n0.350491\n0.872400\n00:21\n\n\n15\n0.394444\n0.364743\n0.874800\n00:21\n\n\n16\n0.407924\n0.393063\n0.856200\n00:21\n\n\n17\n0.376187\n0.345115\n0.872800\n00:21\n\n\n18\n0.371495\n0.371342\n0.870000\n00:21\n\n\n19\n0.343097\n0.346637\n0.874600\n00:21\n\n\n20\n0.324037\n0.338525\n0.878600\n00:22\n\n\n21\n0.333618\n0.330014\n0.884600\n00:22\n\n\n22\n0.315893\n0.337856\n0.883400\n00:20\n\n\n23\n0.334329\n0.433380\n0.859200\n00:22\n\n\n24\n0.279653\n0.335189\n0.881800\n00:22\n\n\n25\n0.307796\n0.337096\n0.884800\n00:22\n\n\n26\n0.275426\n0.323011\n0.892400\n00:21\n\n\n27\n0.250883\n0.320636\n0.891600\n00:22\n\n\n28\n0.266205\n0.323404\n0.888200\n00:22\n\n\n29\n0.276716\n0.335915\n0.887200\n00:21"
  },
  {
    "objectID": "08_neural_nets_with_pytorch.html#saving-and-restoring",
    "href": "08_neural_nets_with_pytorch.html#saving-and-restoring",
    "title": "Appendix D — Introduction to Artificial Neural Networks - Pytorch",
    "section": "D.5 Saving and Restoring",
    "text": "D.5 Saving and Restoring\nSee https://jonathan-sands.com/deep%20learning/fastai/pytorch/vision/classifier/2020/11/15/MNIST.html and https://benjaminwarner.dev/2021/10/01/inference-with-fastai\n\nlearn.save(\"fastai\")\n\nPath('models/fastai.pth')\n\n\n\nlearn.load('fastai')\n\n&lt;fastai.learner.Learner at 0x7f1a740e3cd0&gt;\n\n\n\nfastai_loss, fastai_accuracy = learn.validate(dl=testloader)\n\n\n\n\n\n\n\n\n\nfastai_accuracy\n\n0.8823999762535095\n\n\nYou can also save Pytorch model only:\n\ntorch.save(model, \"my_torch_model\")\n\n\nmodel = torch.load(\"my_torch_model\")\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.eval()\ntest_loss = 0\ncorrect = 0\nwith torch.no_grad():\n    for data, target in testloader:\n        data, target = data.to(device), target.to(device)\n        output = model(data)      \n        # prediction\n        pred = output.argmax(dim=1, keepdim=True)  \n        correct += pred.eq(target.view_as(pred)).sum().item()\n\ndata_count = len(testloader.dataset)\npercentage = 100. * correct / data_count\nprint(percentage)\n\n88.24"
  },
  {
    "objectID": "08_neural_nets_with_pytorch.html#using-callbacks-during-training",
    "href": "08_neural_nets_with_pytorch.html#using-callbacks-during-training",
    "title": "Appendix D — Introduction to Artificial Neural Networks - Pytorch",
    "section": "D.6 Using Callbacks during Training",
    "text": "D.6 Using Callbacks during Training\n\nlearn = None\nmodel = None\ngc.collect()\ntorch.cuda.empty_cache()\n\n\n# Sometimes applying BN before the activation function works better (there's a debate on this topic)\nmodel = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(28 * 28, 300),\n    torch.nn.BatchNorm1d(300, momentum=0.99, eps=0.001),\n    torch.nn.PReLU(300),\n    torch.nn.Dropout(0.5),\n    torch.nn.Linear(300, 100),\n    torch.nn.BatchNorm1d(100, momentum=0.99, eps=0.001),\n    torch.nn.PReLU(100),\n    torch.nn.Dropout(0.5),\n    torch.nn.Linear(100, 10),\n    nn.LogSoftmax(dim=1)\n)\n\nlearn = Learner(data, model, loss_func=F.cross_entropy, opt_func=Adam, metrics=[accuracy])\n\n\nlearn.fit_one_cycle(30, 0.01, cbs=[SaveModelCallback(monitor='valid_loss', fname='model', at_end=True), ShowGraphCallback()])\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.502928\n0.479331\n0.827400\n00:24\n\n\n1\n0.533826\n0.466613\n0.828200\n00:24\n\n\n2\n0.518706\n0.473331\n0.827200\n00:21\n\n\n3\n0.480978\n0.529183\n0.822000\n00:20\n\n\n4\n0.506808\n0.405815\n0.853000\n00:22\n\n\n5\n0.477145\n0.471803\n0.843000\n00:23\n\n\n6\n0.474596\n0.410654\n0.855800\n00:23\n\n\n7\n0.457896\n0.422041\n0.853000\n00:21\n\n\n8\n0.450831\n0.419836\n0.852800\n00:22\n\n\n9\n0.425248\n0.401135\n0.855000\n00:22\n\n\n10\n0.449351\n0.379731\n0.866400\n00:22\n\n\n11\n0.422142\n0.408070\n0.859000\n00:21\n\n\n12\n0.409592\n0.390603\n0.866000\n00:21\n\n\n13\n0.397809\n0.363653\n0.874000\n00:22\n\n\n14\n0.384585\n0.361813\n0.874800\n00:23\n\n\n15\n0.396216\n0.373937\n0.866600\n00:22\n\n\n16\n0.400895\n0.385886\n0.859600\n00:21\n\n\n17\n0.392388\n0.394157\n0.864600\n00:22\n\n\n18\n0.369430\n0.358140\n0.876800\n00:22\n\n\n19\n0.366111\n0.348118\n0.878800\n00:22\n\n\n20\n0.333376\n0.389643\n0.865600\n00:21\n\n\n21\n0.348145\n0.336223\n0.875800\n00:21\n\n\n22\n0.302654\n0.380208\n0.876800\n00:22\n\n\n23\n0.308771\n0.314582\n0.887800\n00:21\n\n\n24\n0.302305\n0.321902\n0.889600\n00:21\n\n\n25\n0.275600\n0.332582\n0.882000\n00:21\n\n\n26\n0.292592\n0.333376\n0.876200\n00:22\n\n\n27\n0.277050\n0.322500\n0.892600\n00:22\n\n\n28\n0.269607\n0.353560\n0.887400\n00:22\n\n\n29\n0.264075\n0.333345\n0.885800\n00:21\n\n\n\n\n\nBetter model found at epoch 0 with valid_loss value: 0.47933071851730347.\n\n\n\n\n\nBetter model found at epoch 1 with valid_loss value: 0.46661290526390076.\nBetter model found at epoch 4 with valid_loss value: 0.4058149755001068.\nBetter model found at epoch 9 with valid_loss value: 0.40113452076911926.\nBetter model found at epoch 10 with valid_loss value: 0.3797314167022705.\nBetter model found at epoch 13 with valid_loss value: 0.3636534810066223.\nBetter model found at epoch 14 with valid_loss value: 0.3618128001689911.\nBetter model found at epoch 18 with valid_loss value: 0.35813993215560913.\nBetter model found at epoch 19 with valid_loss value: 0.34811756014823914.\nBetter model found at epoch 21 with valid_loss value: 0.336223304271698.\nBetter model found at epoch 23 with valid_loss value: 0.31458228826522827."
  },
  {
    "objectID": "08_neural_nets_with_pytorch.html#pytorch-references",
    "href": "08_neural_nets_with_pytorch.html#pytorch-references",
    "title": "Appendix D — Introduction to Artificial Neural Networks - Pytorch",
    "section": "D.7 Pytorch references",
    "text": "D.7 Pytorch references\n\nhttps://ithelp.ithome.com.tw/articles/10285392 - Pytorch also has functional and subclassing API!\nhttps://github.com/lanpa/tensorboardX - TensorBoard for Pytorch\nhttps://docs.fast.ai/\nhttps://www.pytorchlightning.ai/"
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_pytorch.html#setup",
    "href": "09_Convolutional_NeuralNetworks_pytorch.html#setup",
    "title": "Appendix E — Image processing with Convolutional Neural Networks - Pytorch",
    "section": "E.1 Setup",
    "text": "E.1 Setup\n\n!apt-get install tree -qq\n!pip install torchinfo -qq\n!pip install pyyaml==5.1 -qq\nimport distutils.core\n# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.\n# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n!git clone 'https://github.com/facebookresearch/detectron2'\ndist = distutils.core.run_setup(\"./detectron2/setup.py\")\n!pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])} -qq\n\nSelecting previously unselected package tree.\n(Reading database ... 122518 files and directories currently installed.)\nPreparing to unpack .../tree_1.8.0-1_amd64.deb ...\nUnpacking tree (1.8.0-1) ...\nSetting up tree (1.8.0-1) ...\nProcessing triggers for man-db (2.9.1-1) ...\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 274.2/274.2 kB 5.6 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n  Building wheel for pyyaml (setup.py) ... done\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nflax 0.6.9 requires PyYAML&gt;=5.4.1, but you have pyyaml 5.1 which is incompatible.\ndask 2022.12.1 requires pyyaml&gt;=5.3.1, but you have pyyaml 5.1 which is incompatible.\nCloning into 'detectron2'...\nremote: Enumerating objects: 15022, done.\nremote: Counting objects: 100% (47/47), done.\nremote: Compressing objects: 100% (34/34), done.\nremote: Total 15022 (delta 23), reused 31 (delta 13), pack-reused 14975\nReceiving objects: 100% (15022/15022), 6.10 MiB | 26.49 MiB/s, done.\nResolving deltas: 100% (10886/10886), done.\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.2/50.2 kB 2.7 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 9.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.5/154.5 kB 16.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 35.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 16.1 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n  Building wheel for fvcore (setup.py) ... done\n  Building wheel for antlr4-python3-runtime (setup.py) ... done\n\n\n\n!pip install git+https://github.com/cleanlab/cleanvision.git -qq\n!pip install cleanlab -qq\n!pip install skorch -qq\n\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 296.5/296.5 kB 6.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 31.2 MB/s eta 0:00:00\n  Building wheel for cleanvision (pyproject.toml) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175.8/175.8 kB 2.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.7/193.7 kB 5.0 MB/s eta 0:00:00\n\n\nWe need to restart the environment after installing.\n\n# Python ≥3.7 is recommended\nimport sys\nassert sys.version_info &gt;= (3, 7)\nimport os\nsys.path.insert(0, os.path.abspath('./detectron2'))\nimport gc\n\n# Scikit-Learn ≥1.01 is recommended\nfrom packaging import version\nimport sklearn\nfrom sklearn.datasets import load_sample_image\nfrom sklearn.datasets import load_sample_images\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import accuracy_score\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\n# Pytorch related\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim \nfrom torchvision import datasets, transforms\nfrom torchvision.transforms.functional import to_pil_image\nimport torch.nn.functional as F\nfrom torchinfo import summary\nfrom fastai.vision.all import *\n\n# Object detection\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.structures import BoxMode\nfrom google.colab.patches import cv2_imshow\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.utils.visualizer import ColorMode\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\n\n# Image augmentation\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# Data centric AI\nfrom cleanvision.imagelab import Imagelab\nfrom cleanlab.filter import find_label_issues\nfrom skorch import NeuralNetClassifier\n\n# Common imports\nimport numpy as np\nimport shutil\nimport pathlib\nimport resource\nimport tqdm\nimport copy\nimport json\nimport cv2\nimport random\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator at 0x7f94d710f890&gt;\n\n\n\nif not torch.cuda.device_count():\n    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n    if \"google.colab\" in sys.modules:\n        print(\"Go to Runtime &gt; Change runtime and select a GPU hardware \"\n              \"accelerator.\")\n    if \"kaggle_secrets\" in sys.modules:\n        print(\"Go to Settings &gt; Accelerator and select GPU.\")\n\nA couple utility functions to plot grayscale and RGB images:\n\ndef plot_image(image):\n    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n    plt.axis(\"off\")\n\ndef plot_color_image(image):\n    plt.imshow(image, interpolation=\"nearest\")\n    plt.axis(\"off\")\n\ndef plot_examples(id_iter, nrows=1, ncols=1):\n    for count, id in enumerate(id_iter):\n        plt.subplot(nrows, ncols, count + 1)\n        plt.imshow(X[id].reshape(28, 28), cmap=\"gray\")\n        plt.title(f\"id: {id} \\n label: {labels[id]}\")\n        plt.axis(\"off\")\n\n    plt.tight_layout(h_pad=2.0)\n\nYou can find useful kernels here https://setosa.io/ev/image-kernels/"
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_pytorch.html#tackling-fashion-mnist-with-a-cnn",
    "href": "09_Convolutional_NeuralNetworks_pytorch.html#tackling-fashion-mnist-with-a-cnn",
    "title": "Appendix E — Image processing with Convolutional Neural Networks - Pytorch",
    "section": "E.2 Tackling Fashion MNIST With a CNN",
    "text": "E.2 Tackling Fashion MNIST With a CNN\nBefore delving into the code, you can go through https://poloclub.github.io/cnn-explainer/ to make sure you understand every piece of CNN.\nTypical CNN architectures stack a few convolutional layers (each one generally followed by a ReLU layer), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on. The image gets smaller and smaller as it progresses through the network, but it also typically gets deeper and deeper (i.e.,with more feature maps) thanks to the convolutional layers. At the top of the stack, a regular feedforward neural network is added, composed of a few fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a softmax layer that outputs estimated class probabilities).\n\n\n\nHere is how you can implement a simple CNN to tackle the fashion MNIST dataset\n\nlearn = None\nmodel = None\ngc.collect()\ntorch.cuda.empty_cache()\n\n\ntransform = transforms.Compose([transforms.ToTensor(), lambda x: x/255])\n\ntrainset = datasets.FashionMNIST(\n    root=\"data\",            \n    train=True,             \n    download=True,         \n    transform=transform,  \n)\n\ntestset = datasets.FashionMNIST(\n    root=\"data\",           \n    train=False,           \n    download=True,         \n    transform=transform,\n)\n\n# Preparing for validaion test\n\ntrainset, validset = torch.utils.data.random_split(trainset, [55000, 5000])\n\n# Data Loader\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\nvalidloader = torch.utils.data.DataLoader(validset, batch_size=32, shuffle=False, num_workers=2)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n\nlen(trainset), len(validset), len(testset)\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|██████████| 26421880/26421880 [00:03&lt;00:00, 8415136.41it/s] \n\n\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n\n100%|██████████| 29515/29515 [00:00&lt;00:00, 143845.01it/s]\n\n\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n100%|██████████| 4422102/4422102 [00:01&lt;00:00, 2692092.56it/s]\n\n\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n100%|██████████| 5148/5148 [00:00&lt;00:00, 18957223.00it/s]\n\n\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\n\n\n(55000, 5000, 10000)\n\n\n\nmodel = torch.nn.Sequential(\n    torch.nn.Conv2d(1, 32, 7, padding='same'),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(2),\n    torch.nn.Conv2d(32, 64, 3, padding='same'),\n    torch.nn.ReLU(),\n    torch.nn.Conv2d(64, 64, 3, padding='same'),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(2),\n    torch.nn.Conv2d(64, 128, 3, padding='same'),\n    torch.nn.ReLU(),\n    torch.nn.Conv2d(128, 128, 3, padding='same'),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(2),\n    torch.nn.Flatten(),\n    torch.nn.Linear(3*3*128, 64),\n    torch.nn.ReLU(),\n    torch.nn.Dropout(0.5),\n    torch.nn.Linear(64, 32),\n    torch.nn.ReLU(),\n    torch.nn.Dropout(0.5),\n    torch.nn.Linear(32, 10)\n)\n\n\n# Use He initialization for the convolutional layers and the linear layer\nfor m in model.modules():\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        nn.init.kaiming_uniform_(m.weight)\n\n\nsummary(model, input_size=(32, 1, 28, 28))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nSequential                               [32, 10]                  --\n├─Conv2d: 1-1                            [32, 32, 28, 28]          1,600\n├─ReLU: 1-2                              [32, 32, 28, 28]          --\n├─MaxPool2d: 1-3                         [32, 32, 14, 14]          --\n├─Conv2d: 1-4                            [32, 64, 14, 14]          18,496\n├─ReLU: 1-5                              [32, 64, 14, 14]          --\n├─Conv2d: 1-6                            [32, 64, 14, 14]          36,928\n├─ReLU: 1-7                              [32, 64, 14, 14]          --\n├─MaxPool2d: 1-8                         [32, 64, 7, 7]            --\n├─Conv2d: 1-9                            [32, 128, 7, 7]           73,856\n├─ReLU: 1-10                             [32, 128, 7, 7]           --\n├─Conv2d: 1-11                           [32, 128, 7, 7]           147,584\n├─ReLU: 1-12                             [32, 128, 7, 7]           --\n├─MaxPool2d: 1-13                        [32, 128, 3, 3]           --\n├─Flatten: 1-14                          [32, 1152]                --\n├─Linear: 1-15                           [32, 64]                  73,792\n├─ReLU: 1-16                             [32, 64]                  --\n├─Dropout: 1-17                          [32, 64]                  --\n├─Linear: 1-18                           [32, 32]                  2,080\n├─ReLU: 1-19                             [32, 32]                  --\n├─Dropout: 1-20                          [32, 32]                  --\n├─Linear: 1-21                           [32, 10]                  330\n==========================================================================================\nTotal params: 354,666\nTrainable params: 354,666\nNon-trainable params: 0\nTotal mult-adds (M): 737.42\n==========================================================================================\nInput size (MB): 0.10\nForward/backward pass size (MB): 16.08\nParams size (MB): 1.42\nEstimated Total Size (MB): 17.60\n==========================================================================================\n\n\n\ndata = DataLoaders(trainloader, validloader)\nlearn = Learner(data, model, loss_func=F.cross_entropy, opt_func=RMSProp, metrics=[accuracy])\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.2089296132326126)\n\n\n\n\n\n\nlearn.fit_one_cycle(30, 0.001)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.910867\n1.753917\n0.486200\n00:21\n\n\n1\n1.303495\n1.115846\n0.652000\n00:21\n\n\n2\n0.969552\n0.697844\n0.740800\n00:23\n\n\n3\n0.735478\n0.566994\n0.796200\n00:22\n\n\n4\n0.651329\n0.497390\n0.816000\n00:21\n\n\n5\n0.575974\n0.416111\n0.851800\n00:21\n\n\n6\n0.541723\n0.407052\n0.855200\n00:22\n\n\n7\n0.485088\n0.381411\n0.866400\n00:22\n\n\n8\n0.476140\n0.380247\n0.870800\n00:21\n\n\n9\n0.435762\n0.370856\n0.875000\n00:21\n\n\n10\n0.390848\n0.378833\n0.880400\n00:22\n\n\n11\n0.371974\n0.356836\n0.883000\n00:21\n\n\n12\n0.395236\n0.327657\n0.886200\n00:21\n\n\n13\n0.363806\n0.330452\n0.890800\n00:21\n\n\n14\n0.339303\n0.357805\n0.895600\n00:22\n\n\n15\n0.348606\n0.336609\n0.893800\n00:22\n\n\n16\n0.340379\n0.316998\n0.894600\n00:21\n\n\n17\n0.363921\n0.306676\n0.895000\n00:21\n\n\n18\n0.296538\n0.350968\n0.893000\n00:22\n\n\n19\n0.295195\n0.309254\n0.902600\n00:22\n\n\n20\n0.293125\n0.312394\n0.905400\n00:22\n\n\n21\n0.278338\n0.328402\n0.907000\n00:21\n\n\n22\n0.260811\n0.313633\n0.909600\n00:22\n\n\n23\n0.254771\n0.315433\n0.908000\n00:22\n\n\n24\n0.263927\n0.326123\n0.913400\n00:22\n\n\n25\n0.219861\n0.322429\n0.909200\n00:21\n\n\n26\n0.218416\n0.326497\n0.913800\n00:22\n\n\n27\n0.199811\n0.323420\n0.912400\n00:22\n\n\n28\n0.192666\n0.327092\n0.913200\n00:22\n\n\n29\n0.191411\n0.323766\n0.913000\n00:22\n\n\n\n\n\n\nfastai_loss, fastai_accuracy = learn.validate(dl=testloader)\nfastai_accuracy\n\n\n\n\n\n\n\n\n0.9077000021934509"
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_pytorch.html#training-a-convnet-from-scratch-on-a-small-dataset",
    "href": "09_Convolutional_NeuralNetworks_pytorch.html#training-a-convnet-from-scratch-on-a-small-dataset",
    "title": "Appendix E — Image processing with Convolutional Neural Networks - Pytorch",
    "section": "E.3 Training a convnet from scratch on a small dataset",
    "text": "E.3 Training a convnet from scratch on a small dataset\nHaving to train an image-classification model using very little data is a common situation, which you’ll likely encounter in practice if you ever do computer vision in a professional context. A “few” samples can mean anywhere from a few hundred to a few tens of thousands of images. As a practical example, we’ll focus on classifying images as dogs or cats in a dataset containing 5,000 pictures of cats and dogs (2,500 cats, 2,500 dogs). We’ll use 2,000 pictures for training, 1,000 for validation, and 2,000 for testing.\nIn this section, we’ll review one basic strategy to tackle this problem: training a new model from scratch using what little data you have. We’ll start by naively training a small convnet on the 2,000 training samples, without any regularization, to set a baseline for what can be achieved. This will get us to a classification accuracy of about 70%. At that point, the main issue will be overfitting. Then we’ll introduce data augmentation, a powerful technique for mitigating overfitting in computer vision. By using data augmentation, we’ll improve the model to reach an accuracy of 80–85%.\n\nE.3.1 The relevance of deep learning for small-data problems\nWhat qualifies as “enough samples” to train a model is relative— relative to the size and depth of the model you’re trying to train, for starters. It isn’t possible to train a convnet to solve a complex problem with just a few tens of samples, but a few hundred can potentially suffice if the model is small and well regularized and the task is simple.\nBecause convnets learn local, translation-invariant features, they’re highly data-efficient on perceptual problems. Training a convnet from scratch on a very small image dataset will yield reasonable results despite a relative lack of data, without the need for any custom feature engineering. You’ll see this in action in this section.\n\n\nE.3.2 Downloading the data\nThe Dogs vs. Cats dataset that we will use isn’t packaged with Keras. It was made available by Kaggle as part of a computer vision competition in late 2013, back when convnets weren’t mainstream. You can download the original dataset from www.kaggle.com/c/dogs-vs-cats/data.\nBut you can also use Kaggle API. First, you need to create a Kaggle API key and download it to your local machine. Just navigate to the Kaggle website in a web browser, log in, and go to the My Account page. In your account settings, you’ll find an API section. Clicking the Create New API Token button will generate a kaggle.json key file and will download it to your machine.\n\n\n\n\n# Upload the API’s key JSON file to your Colab\n# session by running the following code in a notebook cell:\nfrom google.colab import files\nfiles.upload()\n\nFinally, create a ~/.kaggle folder, and copy the key file to it. As a security best practice, you should also make sure that the file is only readable by the current user, yourself:\n\n!mkdir ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n\n\n# You can now download the data we’re about to use:\n!kaggle competitions download -c dogs-vs-cats\n\nDownloading dogs-vs-cats.zip to /content\n 99% 805M/812M [00:04&lt;00:00, 223MB/s]\n100% 812M/812M [00:04&lt;00:00, 177MB/s]\n\n\nThe first time you try to download the data, you may get a “403 Forbidden” error. That’s because you need to accept the terms associated with the dataset before you download it—you’ll have to go to www.kaggle.com/c/dogs-vs-cats/rules (while logged into your Kaggle account) and click the I Understand and Accept button. You only need to do this once.\n\n!unzip -qq dogs-vs-cats.zip\n\n\n!unzip -qq train.zip\n\nThe pictures in our dataset are medium-resolution color JPEGs. Unsurprisingly, the original dogs-versus-cats Kaggle competition, all the way back in 2013, was won by entrants who used convnets. The best entries achieved up to 95% accuracy. Even though we will train our models on less than 10% of the data that was available to the competitors, we will still get a resonable well performance.\nThis dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed). After downloading and uncompressing the data, we’ll create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 1,000 samples of each class. Why do this? Because many of the image datasets you’ll encounter in your career only contain a few thousand samples, not tens of thousands. Having more data available would make the problem easier, so it’s good practice to learn with a small dataset.\n\n!tree train\n\n串流輸出內容已截斷至最後 5000 行。\n├── dog.5502.jpg\n├── dog.5503.jpg\n├── dog.5504.jpg\n├── dog.5505.jpg\n├── dog.5506.jpg\n├── dog.5507.jpg\n├── dog.5508.jpg\n├── dog.5509.jpg\n├── dog.550.jpg\n├── dog.5510.jpg\n├── dog.5511.jpg\n├── dog.5512.jpg\n├── dog.5513.jpg\n├── dog.5514.jpg\n├── dog.5515.jpg\n├── dog.5516.jpg\n├── dog.5517.jpg\n├── dog.5518.jpg\n├── dog.5519.jpg\n├── dog.551.jpg\n├── dog.5520.jpg\n├── dog.5521.jpg\n├── dog.5522.jpg\n├── dog.5523.jpg\n├── dog.5524.jpg\n├── dog.5525.jpg\n├── dog.5526.jpg\n├── dog.5527.jpg\n├── dog.5528.jpg\n├── dog.5529.jpg\n├── dog.552.jpg\n├── dog.5530.jpg\n├── dog.5531.jpg\n├── dog.5532.jpg\n├── dog.5533.jpg\n├── dog.5534.jpg\n├── dog.5535.jpg\n├── dog.5536.jpg\n├── dog.5537.jpg\n├── dog.5538.jpg\n├── dog.5539.jpg\n├── dog.553.jpg\n├── dog.5540.jpg\n├── dog.5541.jpg\n├── dog.5542.jpg\n├── dog.5543.jpg\n├── dog.5544.jpg\n├── dog.5545.jpg\n├── dog.5546.jpg\n├── dog.5547.jpg\n├── dog.5548.jpg\n├── dog.5549.jpg\n├── dog.554.jpg\n├── dog.5550.jpg\n├── dog.5551.jpg\n├── dog.5552.jpg\n├── dog.5553.jpg\n├── dog.5554.jpg\n├── dog.5555.jpg\n├── dog.5556.jpg\n├── dog.5557.jpg\n├── dog.5558.jpg\n├── dog.5559.jpg\n├── dog.555.jpg\n├── dog.5560.jpg\n├── dog.5561.jpg\n├── dog.5562.jpg\n├── dog.5563.jpg\n├── dog.5564.jpg\n├── dog.5565.jpg\n├── dog.5566.jpg\n├── dog.5567.jpg\n├── dog.5568.jpg\n├── dog.5569.jpg\n├── dog.556.jpg\n├── dog.5570.jpg\n├── dog.5571.jpg\n├── dog.5572.jpg\n├── dog.5573.jpg\n├── dog.5574.jpg\n├── dog.5575.jpg\n├── dog.5576.jpg\n├── dog.5577.jpg\n├── dog.5578.jpg\n├── dog.5579.jpg\n├── dog.557.jpg\n├── dog.5580.jpg\n├── dog.5581.jpg\n├── dog.5582.jpg\n├── dog.5583.jpg\n├── dog.5584.jpg\n├── dog.5585.jpg\n├── dog.5586.jpg\n├── dog.5587.jpg\n├── dog.5588.jpg\n├── dog.5589.jpg\n├── dog.558.jpg\n├── dog.5590.jpg\n├── dog.5591.jpg\n├── dog.5592.jpg\n├── dog.5593.jpg\n├── dog.5594.jpg\n├── dog.5595.jpg\n├── dog.5596.jpg\n├── dog.5597.jpg\n├── dog.5598.jpg\n├── dog.5599.jpg\n├── dog.559.jpg\n├── dog.55.jpg\n├── dog.5600.jpg\n├── dog.5601.jpg\n├── dog.5602.jpg\n├── dog.5603.jpg\n├── dog.5604.jpg\n├── dog.5605.jpg\n├── dog.5606.jpg\n├── dog.5607.jpg\n├── dog.5608.jpg\n├── dog.5609.jpg\n├── dog.560.jpg\n├── dog.5610.jpg\n├── dog.5611.jpg\n├── dog.5612.jpg\n├── dog.5613.jpg\n├── dog.5614.jpg\n├── dog.5615.jpg\n├── dog.5616.jpg\n├── dog.5617.jpg\n├── dog.5618.jpg\n├── dog.5619.jpg\n├── dog.561.jpg\n├── dog.5620.jpg\n├── dog.5621.jpg\n├── dog.5622.jpg\n├── dog.5623.jpg\n├── dog.5624.jpg\n├── dog.5625.jpg\n├── dog.5626.jpg\n├── dog.5627.jpg\n├── dog.5628.jpg\n├── dog.5629.jpg\n├── dog.562.jpg\n├── dog.5630.jpg\n├── dog.5631.jpg\n├── dog.5632.jpg\n├── dog.5633.jpg\n├── dog.5634.jpg\n├── dog.5635.jpg\n├── dog.5636.jpg\n├── dog.5637.jpg\n├── dog.5638.jpg\n├── dog.5639.jpg\n├── dog.563.jpg\n├── dog.5640.jpg\n├── dog.5641.jpg\n├── dog.5642.jpg\n├── dog.5643.jpg\n├── dog.5644.jpg\n├── dog.5645.jpg\n├── dog.5646.jpg\n├── dog.5647.jpg\n├── dog.5648.jpg\n├── dog.5649.jpg\n├── dog.564.jpg\n├── dog.5650.jpg\n├── dog.5651.jpg\n├── dog.5652.jpg\n├── dog.5653.jpg\n├── dog.5654.jpg\n├── dog.5655.jpg\n├── dog.5656.jpg\n├── dog.5657.jpg\n├── dog.5658.jpg\n├── dog.5659.jpg\n├── dog.565.jpg\n├── dog.5660.jpg\n├── dog.5661.jpg\n├── dog.5662.jpg\n├── dog.5663.jpg\n├── dog.5664.jpg\n├── dog.5665.jpg\n├── dog.5666.jpg\n├── dog.5667.jpg\n├── dog.5668.jpg\n├── dog.5669.jpg\n├── dog.566.jpg\n├── dog.5670.jpg\n├── dog.5671.jpg\n├── dog.5672.jpg\n├── dog.5673.jpg\n├── dog.5674.jpg\n├── dog.5675.jpg\n├── dog.5676.jpg\n├── dog.5677.jpg\n├── dog.5678.jpg\n├── dog.5679.jpg\n├── dog.567.jpg\n├── dog.5680.jpg\n├── dog.5681.jpg\n├── dog.5682.jpg\n├── dog.5683.jpg\n├── dog.5684.jpg\n├── dog.5685.jpg\n├── dog.5686.jpg\n├── dog.5687.jpg\n├── dog.5688.jpg\n├── dog.5689.jpg\n├── dog.568.jpg\n├── dog.5690.jpg\n├── dog.5691.jpg\n├── dog.5692.jpg\n├── dog.5693.jpg\n├── dog.5694.jpg\n├── dog.5695.jpg\n├── dog.5696.jpg\n├── dog.5697.jpg\n├── dog.5698.jpg\n├── dog.5699.jpg\n├── dog.569.jpg\n├── dog.56.jpg\n├── dog.5700.jpg\n├── dog.5701.jpg\n├── dog.5702.jpg\n├── dog.5703.jpg\n├── dog.5704.jpg\n├── dog.5705.jpg\n├── dog.5706.jpg\n├── dog.5707.jpg\n├── dog.5708.jpg\n├── dog.5709.jpg\n├── dog.570.jpg\n├── dog.5710.jpg\n├── dog.5711.jpg\n├── dog.5712.jpg\n├── dog.5713.jpg\n├── dog.5714.jpg\n├── dog.5715.jpg\n├── dog.5716.jpg\n├── dog.5717.jpg\n├── dog.5718.jpg\n├── dog.5719.jpg\n├── dog.571.jpg\n├── dog.5720.jpg\n├── dog.5721.jpg\n├── dog.5722.jpg\n├── dog.5723.jpg\n├── dog.5724.jpg\n├── dog.5725.jpg\n├── dog.5726.jpg\n├── dog.5727.jpg\n├── dog.5728.jpg\n├── dog.5729.jpg\n├── dog.572.jpg\n├── dog.5730.jpg\n├── dog.5731.jpg\n├── dog.5732.jpg\n├── dog.5733.jpg\n├── dog.5734.jpg\n├── dog.5735.jpg\n├── dog.5736.jpg\n├── dog.5737.jpg\n├── dog.5738.jpg\n├── dog.5739.jpg\n├── dog.573.jpg\n├── dog.5740.jpg\n├── dog.5741.jpg\n├── dog.5742.jpg\n├── dog.5743.jpg\n├── dog.5744.jpg\n├── dog.5745.jpg\n├── dog.5746.jpg\n├── dog.5747.jpg\n├── dog.5748.jpg\n├── dog.5749.jpg\n├── dog.574.jpg\n├── dog.5750.jpg\n├── dog.5751.jpg\n├── dog.5752.jpg\n├── dog.5753.jpg\n├── dog.5754.jpg\n├── dog.5755.jpg\n├── dog.5756.jpg\n├── dog.5757.jpg\n├── dog.5758.jpg\n├── dog.5759.jpg\n├── dog.575.jpg\n├── dog.5760.jpg\n├── dog.5761.jpg\n├── dog.5762.jpg\n├── dog.5763.jpg\n├── dog.5764.jpg\n├── dog.5765.jpg\n├── dog.5766.jpg\n├── dog.5767.jpg\n├── dog.5768.jpg\n├── dog.5769.jpg\n├── dog.576.jpg\n├── dog.5770.jpg\n├── dog.5771.jpg\n├── dog.5772.jpg\n├── dog.5773.jpg\n├── dog.5774.jpg\n├── dog.5775.jpg\n├── dog.5776.jpg\n├── dog.5777.jpg\n├── dog.5778.jpg\n├── dog.5779.jpg\n├── dog.577.jpg\n├── dog.5780.jpg\n├── dog.5781.jpg\n├── dog.5782.jpg\n├── dog.5783.jpg\n├── dog.5784.jpg\n├── dog.5785.jpg\n├── dog.5786.jpg\n├── dog.5787.jpg\n├── dog.5788.jpg\n├── dog.5789.jpg\n├── dog.578.jpg\n├── dog.5790.jpg\n├── dog.5791.jpg\n├── dog.5792.jpg\n├── dog.5793.jpg\n├── dog.5794.jpg\n├── dog.5795.jpg\n├── dog.5796.jpg\n├── dog.5797.jpg\n├── dog.5798.jpg\n├── dog.5799.jpg\n├── dog.579.jpg\n├── dog.57.jpg\n├── dog.5800.jpg\n├── dog.5801.jpg\n├── dog.5802.jpg\n├── dog.5803.jpg\n├── dog.5804.jpg\n├── dog.5805.jpg\n├── dog.5806.jpg\n├── dog.5807.jpg\n├── dog.5808.jpg\n├── dog.5809.jpg\n├── dog.580.jpg\n├── dog.5810.jpg\n├── dog.5811.jpg\n├── dog.5812.jpg\n├── dog.5813.jpg\n├── dog.5814.jpg\n├── dog.5815.jpg\n├── dog.5816.jpg\n├── dog.5817.jpg\n├── dog.5818.jpg\n├── dog.5819.jpg\n├── dog.581.jpg\n├── dog.5820.jpg\n├── dog.5821.jpg\n├── dog.5822.jpg\n├── dog.5823.jpg\n├── dog.5824.jpg\n├── dog.5825.jpg\n├── dog.5826.jpg\n├── dog.5827.jpg\n├── dog.5828.jpg\n├── dog.5829.jpg\n├── dog.582.jpg\n├── dog.5830.jpg\n├── dog.5831.jpg\n├── dog.5832.jpg\n├── dog.5833.jpg\n├── dog.5834.jpg\n├── dog.5835.jpg\n├── dog.5836.jpg\n├── dog.5837.jpg\n├── dog.5838.jpg\n├── dog.5839.jpg\n├── dog.583.jpg\n├── dog.5840.jpg\n├── dog.5841.jpg\n├── dog.5842.jpg\n├── dog.5843.jpg\n├── dog.5844.jpg\n├── dog.5845.jpg\n├── dog.5846.jpg\n├── dog.5847.jpg\n├── dog.5848.jpg\n├── dog.5849.jpg\n├── dog.584.jpg\n├── dog.5850.jpg\n├── dog.5851.jpg\n├── dog.5852.jpg\n├── dog.5853.jpg\n├── dog.5854.jpg\n├── dog.5855.jpg\n├── dog.5856.jpg\n├── dog.5857.jpg\n├── dog.5858.jpg\n├── dog.5859.jpg\n├── dog.585.jpg\n├── dog.5860.jpg\n├── dog.5861.jpg\n├── dog.5862.jpg\n├── dog.5863.jpg\n├── dog.5864.jpg\n├── dog.5865.jpg\n├── dog.5866.jpg\n├── dog.5867.jpg\n├── dog.5868.jpg\n├── dog.5869.jpg\n├── dog.586.jpg\n├── dog.5870.jpg\n├── dog.5871.jpg\n├── dog.5872.jpg\n├── dog.5873.jpg\n├── dog.5874.jpg\n├── dog.5875.jpg\n├── dog.5876.jpg\n├── dog.5877.jpg\n├── dog.5878.jpg\n├── dog.5879.jpg\n├── dog.587.jpg\n├── dog.5880.jpg\n├── dog.5881.jpg\n├── dog.5882.jpg\n├── dog.5883.jpg\n├── dog.5884.jpg\n├── dog.5885.jpg\n├── dog.5886.jpg\n├── dog.5887.jpg\n├── dog.5888.jpg\n├── dog.5889.jpg\n├── dog.588.jpg\n├── dog.5890.jpg\n├── dog.5891.jpg\n├── dog.5892.jpg\n├── dog.5893.jpg\n├── dog.5894.jpg\n├── dog.5895.jpg\n├── dog.5896.jpg\n├── dog.5897.jpg\n├── dog.5898.jpg\n├── dog.5899.jpg\n├── dog.589.jpg\n├── dog.58.jpg\n├── dog.5900.jpg\n├── dog.5901.jpg\n├── dog.5902.jpg\n├── dog.5903.jpg\n├── dog.5904.jpg\n├── dog.5905.jpg\n├── dog.5906.jpg\n├── dog.5907.jpg\n├── dog.5908.jpg\n├── dog.5909.jpg\n├── dog.590.jpg\n├── dog.5910.jpg\n├── dog.5911.jpg\n├── dog.5912.jpg\n├── dog.5913.jpg\n├── dog.5914.jpg\n├── dog.5915.jpg\n├── dog.5916.jpg\n├── dog.5917.jpg\n├── dog.5918.jpg\n├── dog.5919.jpg\n├── dog.591.jpg\n├── dog.5920.jpg\n├── dog.5921.jpg\n├── dog.5922.jpg\n├── dog.5923.jpg\n├── dog.5924.jpg\n├── dog.5925.jpg\n├── dog.5926.jpg\n├── dog.5927.jpg\n├── dog.5928.jpg\n├── dog.5929.jpg\n├── dog.592.jpg\n├── dog.5930.jpg\n├── dog.5931.jpg\n├── dog.5932.jpg\n├── dog.5933.jpg\n├── dog.5934.jpg\n├── dog.5935.jpg\n├── dog.5936.jpg\n├── dog.5937.jpg\n├── dog.5938.jpg\n├── dog.5939.jpg\n├── dog.593.jpg\n├── dog.5940.jpg\n├── dog.5941.jpg\n├── dog.5942.jpg\n├── dog.5943.jpg\n├── dog.5944.jpg\n├── dog.5945.jpg\n├── dog.5946.jpg\n├── dog.5947.jpg\n├── dog.5948.jpg\n├── dog.5949.jpg\n├── dog.594.jpg\n├── dog.5950.jpg\n├── dog.5951.jpg\n├── dog.5952.jpg\n├── dog.5953.jpg\n├── dog.5954.jpg\n├── dog.5955.jpg\n├── dog.5956.jpg\n├── dog.5957.jpg\n├── dog.5958.jpg\n├── dog.5959.jpg\n├── dog.595.jpg\n├── dog.5960.jpg\n├── dog.5961.jpg\n├── dog.5962.jpg\n├── dog.5963.jpg\n├── dog.5964.jpg\n├── dog.5965.jpg\n├── dog.5966.jpg\n├── dog.5967.jpg\n├── dog.5968.jpg\n├── dog.5969.jpg\n├── dog.596.jpg\n├── dog.5970.jpg\n├── dog.5971.jpg\n├── dog.5972.jpg\n├── dog.5973.jpg\n├── dog.5974.jpg\n├── dog.5975.jpg\n├── dog.5976.jpg\n├── dog.5977.jpg\n├── dog.5978.jpg\n├── dog.5979.jpg\n├── dog.597.jpg\n├── dog.5980.jpg\n├── dog.5981.jpg\n├── dog.5982.jpg\n├── dog.5983.jpg\n├── dog.5984.jpg\n├── dog.5985.jpg\n├── dog.5986.jpg\n├── dog.5987.jpg\n├── dog.5988.jpg\n├── dog.5989.jpg\n├── dog.598.jpg\n├── dog.5990.jpg\n├── dog.5991.jpg\n├── dog.5992.jpg\n├── dog.5993.jpg\n├── dog.5994.jpg\n├── dog.5995.jpg\n├── dog.5996.jpg\n├── dog.5997.jpg\n├── dog.5998.jpg\n├── dog.5999.jpg\n├── dog.599.jpg\n├── dog.59.jpg\n├── dog.5.jpg\n├── dog.6000.jpg\n├── dog.6001.jpg\n├── dog.6002.jpg\n├── dog.6003.jpg\n├── dog.6004.jpg\n├── dog.6005.jpg\n├── dog.6006.jpg\n├── dog.6007.jpg\n├── dog.6008.jpg\n├── dog.6009.jpg\n├── dog.600.jpg\n├── dog.6010.jpg\n├── dog.6011.jpg\n├── dog.6012.jpg\n├── dog.6013.jpg\n├── dog.6014.jpg\n├── dog.6015.jpg\n├── dog.6016.jpg\n├── dog.6017.jpg\n├── dog.6018.jpg\n├── dog.6019.jpg\n├── dog.601.jpg\n├── dog.6020.jpg\n├── dog.6021.jpg\n├── dog.6022.jpg\n├── dog.6023.jpg\n├── dog.6024.jpg\n├── dog.6025.jpg\n├── dog.6026.jpg\n├── dog.6027.jpg\n├── dog.6028.jpg\n├── dog.6029.jpg\n├── dog.602.jpg\n├── dog.6030.jpg\n├── dog.6031.jpg\n├── dog.6032.jpg\n├── dog.6033.jpg\n├── dog.6034.jpg\n├── dog.6035.jpg\n├── dog.6036.jpg\n├── dog.6037.jpg\n├── dog.6038.jpg\n├── dog.6039.jpg\n├── dog.603.jpg\n├── dog.6040.jpg\n├── dog.6041.jpg\n├── dog.6042.jpg\n├── dog.6043.jpg\n├── dog.6044.jpg\n├── dog.6045.jpg\n├── dog.6046.jpg\n├── dog.6047.jpg\n├── dog.6048.jpg\n├── dog.6049.jpg\n├── dog.604.jpg\n├── dog.6050.jpg\n├── dog.6051.jpg\n├── dog.6052.jpg\n├── dog.6053.jpg\n├── dog.6054.jpg\n├── dog.6055.jpg\n├── dog.6056.jpg\n├── dog.6057.jpg\n├── dog.6058.jpg\n├── dog.6059.jpg\n├── dog.605.jpg\n├── dog.6060.jpg\n├── dog.6061.jpg\n├── dog.6062.jpg\n├── dog.6063.jpg\n├── dog.6064.jpg\n├── dog.6065.jpg\n├── dog.6066.jpg\n├── dog.6067.jpg\n├── dog.6068.jpg\n├── dog.6069.jpg\n├── dog.606.jpg\n├── dog.6070.jpg\n├── dog.6071.jpg\n├── dog.6072.jpg\n├── dog.6073.jpg\n├── dog.6074.jpg\n├── dog.6075.jpg\n├── dog.6076.jpg\n├── dog.6077.jpg\n├── dog.6078.jpg\n├── dog.6079.jpg\n├── dog.607.jpg\n├── dog.6080.jpg\n├── dog.6081.jpg\n├── dog.6082.jpg\n├── dog.6083.jpg\n├── dog.6084.jpg\n├── dog.6085.jpg\n├── dog.6086.jpg\n├── dog.6087.jpg\n├── dog.6088.jpg\n├── dog.6089.jpg\n├── dog.608.jpg\n├── dog.6090.jpg\n├── dog.6091.jpg\n├── dog.6092.jpg\n├── dog.6093.jpg\n├── dog.6094.jpg\n├── dog.6095.jpg\n├── dog.6096.jpg\n├── dog.6097.jpg\n├── dog.6098.jpg\n├── dog.6099.jpg\n├── dog.609.jpg\n├── dog.60.jpg\n├── dog.6100.jpg\n├── dog.6101.jpg\n├── dog.6102.jpg\n├── dog.6103.jpg\n├── dog.6104.jpg\n├── dog.6105.jpg\n├── dog.6106.jpg\n├── dog.6107.jpg\n├── dog.6108.jpg\n├── dog.6109.jpg\n├── dog.610.jpg\n├── dog.6110.jpg\n├── dog.6111.jpg\n├── dog.6112.jpg\n├── dog.6113.jpg\n├── dog.6114.jpg\n├── dog.6115.jpg\n├── dog.6116.jpg\n├── dog.6117.jpg\n├── dog.6118.jpg\n├── dog.6119.jpg\n├── dog.611.jpg\n├── dog.6120.jpg\n├── dog.6121.jpg\n├── dog.6122.jpg\n├── dog.6123.jpg\n├── dog.6124.jpg\n├── dog.6125.jpg\n├── dog.6126.jpg\n├── dog.6127.jpg\n├── dog.6128.jpg\n├── dog.6129.jpg\n├── dog.612.jpg\n├── dog.6130.jpg\n├── dog.6131.jpg\n├── dog.6132.jpg\n├── dog.6133.jpg\n├── dog.6134.jpg\n├── dog.6135.jpg\n├── dog.6136.jpg\n├── dog.6137.jpg\n├── dog.6138.jpg\n├── dog.6139.jpg\n├── dog.613.jpg\n├── dog.6140.jpg\n├── dog.6141.jpg\n├── dog.6142.jpg\n├── dog.6143.jpg\n├── dog.6144.jpg\n├── dog.6145.jpg\n├── dog.6146.jpg\n├── dog.6147.jpg\n├── dog.6148.jpg\n├── dog.6149.jpg\n├── dog.614.jpg\n├── dog.6150.jpg\n├── dog.6151.jpg\n├── dog.6152.jpg\n├── dog.6153.jpg\n├── dog.6154.jpg\n├── dog.6155.jpg\n├── dog.6156.jpg\n├── dog.6157.jpg\n├── dog.6158.jpg\n├── dog.6159.jpg\n├── dog.615.jpg\n├── dog.6160.jpg\n├── dog.6161.jpg\n├── dog.6162.jpg\n├── dog.6163.jpg\n├── dog.6164.jpg\n├── dog.6165.jpg\n├── dog.6166.jpg\n├── dog.6167.jpg\n├── dog.6168.jpg\n├── dog.6169.jpg\n├── dog.616.jpg\n├── dog.6170.jpg\n├── dog.6171.jpg\n├── dog.6172.jpg\n├── dog.6173.jpg\n├── dog.6174.jpg\n├── dog.6175.jpg\n├── dog.6176.jpg\n├── dog.6177.jpg\n├── dog.6178.jpg\n├── dog.6179.jpg\n├── dog.617.jpg\n├── dog.6180.jpg\n├── dog.6181.jpg\n├── dog.6182.jpg\n├── dog.6183.jpg\n├── dog.6184.jpg\n├── dog.6185.jpg\n├── dog.6186.jpg\n├── dog.6187.jpg\n├── dog.6188.jpg\n├── dog.6189.jpg\n├── dog.618.jpg\n├── dog.6190.jpg\n├── dog.6191.jpg\n├── dog.6192.jpg\n├── dog.6193.jpg\n├── dog.6194.jpg\n├── dog.6195.jpg\n├── dog.6196.jpg\n├── dog.6197.jpg\n├── dog.6198.jpg\n├── dog.6199.jpg\n├── dog.619.jpg\n├── dog.61.jpg\n├── dog.6200.jpg\n├── dog.6201.jpg\n├── dog.6202.jpg\n├── dog.6203.jpg\n├── dog.6204.jpg\n├── dog.6205.jpg\n├── dog.6206.jpg\n├── dog.6207.jpg\n├── dog.6208.jpg\n├── dog.6209.jpg\n├── dog.620.jpg\n├── dog.6210.jpg\n├── dog.6211.jpg\n├── dog.6212.jpg\n├── dog.6213.jpg\n├── dog.6214.jpg\n├── dog.6215.jpg\n├── dog.6216.jpg\n├── dog.6217.jpg\n├── dog.6218.jpg\n├── dog.6219.jpg\n├── dog.621.jpg\n├── dog.6220.jpg\n├── dog.6221.jpg\n├── dog.6222.jpg\n├── dog.6223.jpg\n├── dog.6224.jpg\n├── dog.6225.jpg\n├── dog.6226.jpg\n├── dog.6227.jpg\n├── dog.6228.jpg\n├── dog.6229.jpg\n├── dog.622.jpg\n├── dog.6230.jpg\n├── dog.6231.jpg\n├── dog.6232.jpg\n├── dog.6233.jpg\n├── dog.6234.jpg\n├── dog.6235.jpg\n├── dog.6236.jpg\n├── dog.6237.jpg\n├── dog.6238.jpg\n├── dog.6239.jpg\n├── dog.623.jpg\n├── dog.6240.jpg\n├── dog.6241.jpg\n├── dog.6242.jpg\n├── dog.6243.jpg\n├── dog.6244.jpg\n├── dog.6245.jpg\n├── dog.6246.jpg\n├── dog.6247.jpg\n├── dog.6248.jpg\n├── dog.6249.jpg\n├── dog.624.jpg\n├── dog.6250.jpg\n├── dog.6251.jpg\n├── dog.6252.jpg\n├── dog.6253.jpg\n├── dog.6254.jpg\n├── dog.6255.jpg\n├── dog.6256.jpg\n├── dog.6257.jpg\n├── dog.6258.jpg\n├── dog.6259.jpg\n├── dog.625.jpg\n├── dog.6260.jpg\n├── dog.6261.jpg\n├── dog.6262.jpg\n├── dog.6263.jpg\n├── dog.6264.jpg\n├── dog.6265.jpg\n├── dog.6266.jpg\n├── dog.6267.jpg\n├── dog.6268.jpg\n├── dog.6269.jpg\n├── dog.626.jpg\n├── dog.6270.jpg\n├── dog.6271.jpg\n├── dog.6272.jpg\n├── dog.6273.jpg\n├── dog.6274.jpg\n├── dog.6275.jpg\n├── dog.6276.jpg\n├── dog.6277.jpg\n├── dog.6278.jpg\n├── dog.6279.jpg\n├── dog.627.jpg\n├── dog.6280.jpg\n├── dog.6281.jpg\n├── dog.6282.jpg\n├── dog.6283.jpg\n├── dog.6284.jpg\n├── dog.6285.jpg\n├── dog.6286.jpg\n├── dog.6287.jpg\n├── dog.6288.jpg\n├── dog.6289.jpg\n├── dog.628.jpg\n├── dog.6290.jpg\n├── dog.6291.jpg\n├── dog.6292.jpg\n├── dog.6293.jpg\n├── dog.6294.jpg\n├── dog.6295.jpg\n├── dog.6296.jpg\n├── dog.6297.jpg\n├── dog.6298.jpg\n├── dog.6299.jpg\n├── dog.629.jpg\n├── dog.62.jpg\n├── dog.6300.jpg\n├── dog.6301.jpg\n├── dog.6302.jpg\n├── dog.6303.jpg\n├── dog.6304.jpg\n├── dog.6305.jpg\n├── dog.6306.jpg\n├── dog.6307.jpg\n├── dog.6308.jpg\n├── dog.6309.jpg\n├── dog.630.jpg\n├── dog.6310.jpg\n├── dog.6311.jpg\n├── dog.6312.jpg\n├── dog.6313.jpg\n├── dog.6314.jpg\n├── dog.6315.jpg\n├── dog.6316.jpg\n├── dog.6317.jpg\n├── dog.6318.jpg\n├── dog.6319.jpg\n├── dog.631.jpg\n├── dog.6320.jpg\n├── dog.6321.jpg\n├── dog.6322.jpg\n├── dog.6323.jpg\n├── dog.6324.jpg\n├── dog.6325.jpg\n├── dog.6326.jpg\n├── dog.6327.jpg\n├── dog.6328.jpg\n├── dog.6329.jpg\n├── dog.632.jpg\n├── dog.6330.jpg\n├── dog.6331.jpg\n├── dog.6332.jpg\n├── dog.6333.jpg\n├── dog.6334.jpg\n├── dog.6335.jpg\n├── dog.6336.jpg\n├── dog.6337.jpg\n├── dog.6338.jpg\n├── dog.6339.jpg\n├── dog.633.jpg\n├── dog.6340.jpg\n├── dog.6341.jpg\n├── dog.6342.jpg\n├── dog.6343.jpg\n├── dog.6344.jpg\n├── dog.6345.jpg\n├── dog.6346.jpg\n├── dog.6347.jpg\n├── dog.6348.jpg\n├── dog.6349.jpg\n├── dog.634.jpg\n├── dog.6350.jpg\n├── dog.6351.jpg\n├── dog.6352.jpg\n├── dog.6353.jpg\n├── dog.6354.jpg\n├── dog.6355.jpg\n├── dog.6356.jpg\n├── dog.6357.jpg\n├── dog.6358.jpg\n├── dog.6359.jpg\n├── dog.635.jpg\n├── dog.6360.jpg\n├── dog.6361.jpg\n├── dog.6362.jpg\n├── dog.6363.jpg\n├── dog.6364.jpg\n├── dog.6365.jpg\n├── dog.6366.jpg\n├── dog.6367.jpg\n├── dog.6368.jpg\n├── dog.6369.jpg\n├── dog.636.jpg\n├── dog.6370.jpg\n├── dog.6371.jpg\n├── dog.6372.jpg\n├── dog.6373.jpg\n├── dog.6374.jpg\n├── dog.6375.jpg\n├── dog.6376.jpg\n├── dog.6377.jpg\n├── dog.6378.jpg\n├── dog.6379.jpg\n├── dog.637.jpg\n├── dog.6380.jpg\n├── dog.6381.jpg\n├── dog.6382.jpg\n├── dog.6383.jpg\n├── dog.6384.jpg\n├── dog.6385.jpg\n├── dog.6386.jpg\n├── dog.6387.jpg\n├── dog.6388.jpg\n├── dog.6389.jpg\n├── dog.638.jpg\n├── dog.6390.jpg\n├── dog.6391.jpg\n├── dog.6392.jpg\n├── dog.6393.jpg\n├── dog.6394.jpg\n├── dog.6395.jpg\n├── dog.6396.jpg\n├── dog.6397.jpg\n├── dog.6398.jpg\n├── dog.6399.jpg\n├── dog.639.jpg\n├── dog.63.jpg\n├── dog.6400.jpg\n├── dog.6401.jpg\n├── dog.6402.jpg\n├── dog.6403.jpg\n├── dog.6404.jpg\n├── dog.6405.jpg\n├── dog.6406.jpg\n├── dog.6407.jpg\n├── dog.6408.jpg\n├── dog.6409.jpg\n├── dog.640.jpg\n├── dog.6410.jpg\n├── dog.6411.jpg\n├── dog.6412.jpg\n├── dog.6413.jpg\n├── dog.6414.jpg\n├── dog.6415.jpg\n├── dog.6416.jpg\n├── dog.6417.jpg\n├── dog.6418.jpg\n├── dog.6419.jpg\n├── dog.641.jpg\n├── dog.6420.jpg\n├── dog.6421.jpg\n├── dog.6422.jpg\n├── dog.6423.jpg\n├── dog.6424.jpg\n├── dog.6425.jpg\n├── dog.6426.jpg\n├── dog.6427.jpg\n├── dog.6428.jpg\n├── dog.6429.jpg\n├── dog.642.jpg\n├── dog.6430.jpg\n├── dog.6431.jpg\n├── dog.6432.jpg\n├── dog.6433.jpg\n├── dog.6434.jpg\n├── dog.6435.jpg\n├── dog.6436.jpg\n├── dog.6437.jpg\n├── dog.6438.jpg\n├── dog.6439.jpg\n├── dog.643.jpg\n├── dog.6440.jpg\n├── dog.6441.jpg\n├── dog.6442.jpg\n├── dog.6443.jpg\n├── dog.6444.jpg\n├── dog.6445.jpg\n├── dog.6446.jpg\n├── dog.6447.jpg\n├── dog.6448.jpg\n├── dog.6449.jpg\n├── dog.644.jpg\n├── dog.6450.jpg\n├── dog.6451.jpg\n├── dog.6452.jpg\n├── dog.6453.jpg\n├── dog.6454.jpg\n├── dog.6455.jpg\n├── dog.6456.jpg\n├── dog.6457.jpg\n├── dog.6458.jpg\n├── dog.6459.jpg\n├── dog.645.jpg\n├── dog.6460.jpg\n├── dog.6461.jpg\n├── dog.6462.jpg\n├── dog.6463.jpg\n├── dog.6464.jpg\n├── dog.6465.jpg\n├── dog.6466.jpg\n├── dog.6467.jpg\n├── dog.6468.jpg\n├── dog.6469.jpg\n├── dog.646.jpg\n├── dog.6470.jpg\n├── dog.6471.jpg\n├── dog.6472.jpg\n├── dog.6473.jpg\n├── dog.6474.jpg\n├── dog.6475.jpg\n├── dog.6476.jpg\n├── dog.6477.jpg\n├── dog.6478.jpg\n├── dog.6479.jpg\n├── dog.647.jpg\n├── dog.6480.jpg\n├── dog.6481.jpg\n├── dog.6482.jpg\n├── dog.6483.jpg\n├── dog.6484.jpg\n├── dog.6485.jpg\n├── dog.6486.jpg\n├── dog.6487.jpg\n├── dog.6488.jpg\n├── dog.6489.jpg\n├── dog.648.jpg\n├── dog.6490.jpg\n├── dog.6491.jpg\n├── dog.6492.jpg\n├── dog.6493.jpg\n├── dog.6494.jpg\n├── dog.6495.jpg\n├── dog.6496.jpg\n├── dog.6497.jpg\n├── dog.6498.jpg\n├── dog.6499.jpg\n├── dog.649.jpg\n├── dog.64.jpg\n├── dog.6500.jpg\n├── dog.6501.jpg\n├── dog.6502.jpg\n├── dog.6503.jpg\n├── dog.6504.jpg\n├── dog.6505.jpg\n├── dog.6506.jpg\n├── dog.6507.jpg\n├── dog.6508.jpg\n├── dog.6509.jpg\n├── dog.650.jpg\n├── dog.6510.jpg\n├── dog.6511.jpg\n├── dog.6512.jpg\n├── dog.6513.jpg\n├── dog.6514.jpg\n├── dog.6515.jpg\n├── dog.6516.jpg\n├── dog.6517.jpg\n├── dog.6518.jpg\n├── dog.6519.jpg\n├── dog.651.jpg\n├── dog.6520.jpg\n├── dog.6521.jpg\n├── dog.6522.jpg\n├── dog.6523.jpg\n├── dog.6524.jpg\n├── dog.6525.jpg\n├── dog.6526.jpg\n├── dog.6527.jpg\n├── dog.6528.jpg\n├── dog.6529.jpg\n├── dog.652.jpg\n├── dog.6530.jpg\n├── dog.6531.jpg\n├── dog.6532.jpg\n├── dog.6533.jpg\n├── dog.6534.jpg\n├── dog.6535.jpg\n├── dog.6536.jpg\n├── dog.6537.jpg\n├── dog.6538.jpg\n├── dog.6539.jpg\n├── dog.653.jpg\n├── dog.6540.jpg\n├── dog.6541.jpg\n├── dog.6542.jpg\n├── dog.6543.jpg\n├── dog.6544.jpg\n├── dog.6545.jpg\n├── dog.6546.jpg\n├── dog.6547.jpg\n├── dog.6548.jpg\n├── dog.6549.jpg\n├── dog.654.jpg\n├── dog.6550.jpg\n├── dog.6551.jpg\n├── dog.6552.jpg\n├── dog.6553.jpg\n├── dog.6554.jpg\n├── dog.6555.jpg\n├── dog.6556.jpg\n├── dog.6557.jpg\n├── dog.6558.jpg\n├── dog.6559.jpg\n├── dog.655.jpg\n├── dog.6560.jpg\n├── dog.6561.jpg\n├── dog.6562.jpg\n├── dog.6563.jpg\n├── dog.6564.jpg\n├── dog.6565.jpg\n├── dog.6566.jpg\n├── dog.6567.jpg\n├── dog.6568.jpg\n├── dog.6569.jpg\n├── dog.656.jpg\n├── dog.6570.jpg\n├── dog.6571.jpg\n├── dog.6572.jpg\n├── dog.6573.jpg\n├── dog.6574.jpg\n├── dog.6575.jpg\n├── dog.6576.jpg\n├── dog.6577.jpg\n├── dog.6578.jpg\n├── dog.6579.jpg\n├── dog.657.jpg\n├── dog.6580.jpg\n├── dog.6581.jpg\n├── dog.6582.jpg\n├── dog.6583.jpg\n├── dog.6584.jpg\n├── dog.6585.jpg\n├── dog.6586.jpg\n├── dog.6587.jpg\n├── dog.6588.jpg\n├── dog.6589.jpg\n├── dog.658.jpg\n├── dog.6590.jpg\n├── dog.6591.jpg\n├── dog.6592.jpg\n├── dog.6593.jpg\n├── dog.6594.jpg\n├── dog.6595.jpg\n├── dog.6596.jpg\n├── dog.6597.jpg\n├── dog.6598.jpg\n├── dog.6599.jpg\n├── dog.659.jpg\n├── dog.65.jpg\n├── dog.6600.jpg\n├── dog.6601.jpg\n├── dog.6602.jpg\n├── dog.6603.jpg\n├── dog.6604.jpg\n├── dog.6605.jpg\n├── dog.6606.jpg\n├── dog.6607.jpg\n├── dog.6608.jpg\n├── dog.6609.jpg\n├── dog.660.jpg\n├── dog.6610.jpg\n├── dog.6611.jpg\n├── dog.6612.jpg\n├── dog.6613.jpg\n├── dog.6614.jpg\n├── dog.6615.jpg\n├── dog.6616.jpg\n├── dog.6617.jpg\n├── dog.6618.jpg\n├── dog.6619.jpg\n├── dog.661.jpg\n├── dog.6620.jpg\n├── dog.6621.jpg\n├── dog.6622.jpg\n├── dog.6623.jpg\n├── dog.6624.jpg\n├── dog.6625.jpg\n├── dog.6626.jpg\n├── dog.6627.jpg\n├── dog.6628.jpg\n├── dog.6629.jpg\n├── dog.662.jpg\n├── dog.6630.jpg\n├── dog.6631.jpg\n├── dog.6632.jpg\n├── dog.6633.jpg\n├── dog.6634.jpg\n├── dog.6635.jpg\n├── dog.6636.jpg\n├── dog.6637.jpg\n├── dog.6638.jpg\n├── dog.6639.jpg\n├── dog.663.jpg\n├── dog.6640.jpg\n├── dog.6641.jpg\n├── dog.6642.jpg\n├── dog.6643.jpg\n├── dog.6644.jpg\n├── dog.6645.jpg\n├── dog.6646.jpg\n├── dog.6647.jpg\n├── dog.6648.jpg\n├── dog.6649.jpg\n├── dog.664.jpg\n├── dog.6650.jpg\n├── dog.6651.jpg\n├── dog.6652.jpg\n├── dog.6653.jpg\n├── dog.6654.jpg\n├── dog.6655.jpg\n├── dog.6656.jpg\n├── dog.6657.jpg\n├── dog.6658.jpg\n├── dog.6659.jpg\n├── dog.665.jpg\n├── dog.6660.jpg\n├── dog.6661.jpg\n├── dog.6662.jpg\n├── dog.6663.jpg\n├── dog.6664.jpg\n├── dog.6665.jpg\n├── dog.6666.jpg\n├── dog.6667.jpg\n├── dog.6668.jpg\n├── dog.6669.jpg\n├── dog.666.jpg\n├── dog.6670.jpg\n├── dog.6671.jpg\n├── dog.6672.jpg\n├── dog.6673.jpg\n├── dog.6674.jpg\n├── dog.6675.jpg\n├── dog.6676.jpg\n├── dog.6677.jpg\n├── dog.6678.jpg\n├── dog.6679.jpg\n├── dog.667.jpg\n├── dog.6680.jpg\n├── dog.6681.jpg\n├── dog.6682.jpg\n├── dog.6683.jpg\n├── dog.6684.jpg\n├── dog.6685.jpg\n├── dog.6686.jpg\n├── dog.6687.jpg\n├── dog.6688.jpg\n├── dog.6689.jpg\n├── dog.668.jpg\n├── dog.6690.jpg\n├── dog.6691.jpg\n├── dog.6692.jpg\n├── dog.6693.jpg\n├── dog.6694.jpg\n├── dog.6695.jpg\n├── dog.6696.jpg\n├── dog.6697.jpg\n├── dog.6698.jpg\n├── dog.6699.jpg\n├── dog.669.jpg\n├── dog.66.jpg\n├── dog.6700.jpg\n├── dog.6701.jpg\n├── dog.6702.jpg\n├── dog.6703.jpg\n├── dog.6704.jpg\n├── dog.6705.jpg\n├── dog.6706.jpg\n├── dog.6707.jpg\n├── dog.6708.jpg\n├── dog.6709.jpg\n├── dog.670.jpg\n├── dog.6710.jpg\n├── dog.6711.jpg\n├── dog.6712.jpg\n├── dog.6713.jpg\n├── dog.6714.jpg\n├── dog.6715.jpg\n├── dog.6716.jpg\n├── dog.6717.jpg\n├── dog.6718.jpg\n├── dog.6719.jpg\n├── dog.671.jpg\n├── dog.6720.jpg\n├── dog.6721.jpg\n├── dog.6722.jpg\n├── dog.6723.jpg\n├── dog.6724.jpg\n├── dog.6725.jpg\n├── dog.6726.jpg\n├── dog.6727.jpg\n├── dog.6728.jpg\n├── dog.6729.jpg\n├── dog.672.jpg\n├── dog.6730.jpg\n├── dog.6731.jpg\n├── dog.6732.jpg\n├── dog.6733.jpg\n├── dog.6734.jpg\n├── dog.6735.jpg\n├── dog.6736.jpg\n├── dog.6737.jpg\n├── dog.6738.jpg\n├── dog.6739.jpg\n├── dog.673.jpg\n├── dog.6740.jpg\n├── dog.6741.jpg\n├── dog.6742.jpg\n├── dog.6743.jpg\n├── dog.6744.jpg\n├── dog.6745.jpg\n├── dog.6746.jpg\n├── dog.6747.jpg\n├── dog.6748.jpg\n├── dog.6749.jpg\n├── dog.674.jpg\n├── dog.6750.jpg\n├── dog.6751.jpg\n├── dog.6752.jpg\n├── dog.6753.jpg\n├── dog.6754.jpg\n├── dog.6755.jpg\n├── dog.6756.jpg\n├── dog.6757.jpg\n├── dog.6758.jpg\n├── dog.6759.jpg\n├── dog.675.jpg\n├── dog.6760.jpg\n├── dog.6761.jpg\n├── dog.6762.jpg\n├── dog.6763.jpg\n├── dog.6764.jpg\n├── dog.6765.jpg\n├── dog.6766.jpg\n├── dog.6767.jpg\n├── dog.6768.jpg\n├── dog.6769.jpg\n├── dog.676.jpg\n├── dog.6770.jpg\n├── dog.6771.jpg\n├── dog.6772.jpg\n├── dog.6773.jpg\n├── dog.6774.jpg\n├── dog.6775.jpg\n├── dog.6776.jpg\n├── dog.6777.jpg\n├── dog.6778.jpg\n├── dog.6779.jpg\n├── dog.677.jpg\n├── dog.6780.jpg\n├── dog.6781.jpg\n├── dog.6782.jpg\n├── dog.6783.jpg\n├── dog.6784.jpg\n├── dog.6785.jpg\n├── dog.6786.jpg\n├── dog.6787.jpg\n├── dog.6788.jpg\n├── dog.6789.jpg\n├── dog.678.jpg\n├── dog.6790.jpg\n├── dog.6791.jpg\n├── dog.6792.jpg\n├── dog.6793.jpg\n├── dog.6794.jpg\n├── dog.6795.jpg\n├── dog.6796.jpg\n├── dog.6797.jpg\n├── dog.6798.jpg\n├── dog.6799.jpg\n├── dog.679.jpg\n├── dog.67.jpg\n├── dog.6800.jpg\n├── dog.6801.jpg\n├── dog.6802.jpg\n├── dog.6803.jpg\n├── dog.6804.jpg\n├── dog.6805.jpg\n├── dog.6806.jpg\n├── dog.6807.jpg\n├── dog.6808.jpg\n├── dog.6809.jpg\n├── dog.680.jpg\n├── dog.6810.jpg\n├── dog.6811.jpg\n├── dog.6812.jpg\n├── dog.6813.jpg\n├── dog.6814.jpg\n├── dog.6815.jpg\n├── dog.6816.jpg\n├── dog.6817.jpg\n├── dog.6818.jpg\n├── dog.6819.jpg\n├── dog.681.jpg\n├── dog.6820.jpg\n├── dog.6821.jpg\n├── dog.6822.jpg\n├── dog.6823.jpg\n├── dog.6824.jpg\n├── dog.6825.jpg\n├── dog.6826.jpg\n├── dog.6827.jpg\n├── dog.6828.jpg\n├── dog.6829.jpg\n├── dog.682.jpg\n├── dog.6830.jpg\n├── dog.6831.jpg\n├── dog.6832.jpg\n├── dog.6833.jpg\n├── dog.6834.jpg\n├── dog.6835.jpg\n├── dog.6836.jpg\n├── dog.6837.jpg\n├── dog.6838.jpg\n├── dog.6839.jpg\n├── dog.683.jpg\n├── dog.6840.jpg\n├── dog.6841.jpg\n├── dog.6842.jpg\n├── dog.6843.jpg\n├── dog.6844.jpg\n├── dog.6845.jpg\n├── dog.6846.jpg\n├── dog.6847.jpg\n├── dog.6848.jpg\n├── dog.6849.jpg\n├── dog.684.jpg\n├── dog.6850.jpg\n├── dog.6851.jpg\n├── dog.6852.jpg\n├── dog.6853.jpg\n├── dog.6854.jpg\n├── dog.6855.jpg\n├── dog.6856.jpg\n├── dog.6857.jpg\n├── dog.6858.jpg\n├── dog.6859.jpg\n├── dog.685.jpg\n├── dog.6860.jpg\n├── dog.6861.jpg\n├── dog.6862.jpg\n├── dog.6863.jpg\n├── dog.6864.jpg\n├── dog.6865.jpg\n├── dog.6866.jpg\n├── dog.6867.jpg\n├── dog.6868.jpg\n├── dog.6869.jpg\n├── dog.686.jpg\n├── dog.6870.jpg\n├── dog.6871.jpg\n├── dog.6872.jpg\n├── dog.6873.jpg\n├── dog.6874.jpg\n├── dog.6875.jpg\n├── dog.6876.jpg\n├── dog.6877.jpg\n├── dog.6878.jpg\n├── dog.6879.jpg\n├── dog.687.jpg\n├── dog.6880.jpg\n├── dog.6881.jpg\n├── dog.6882.jpg\n├── dog.6883.jpg\n├── dog.6884.jpg\n├── dog.6885.jpg\n├── dog.6886.jpg\n├── dog.6887.jpg\n├── dog.6888.jpg\n├── dog.6889.jpg\n├── dog.688.jpg\n├── dog.6890.jpg\n├── dog.6891.jpg\n├── dog.6892.jpg\n├── dog.6893.jpg\n├── dog.6894.jpg\n├── dog.6895.jpg\n├── dog.6896.jpg\n├── dog.6897.jpg\n├── dog.6898.jpg\n├── dog.6899.jpg\n├── dog.689.jpg\n├── dog.68.jpg\n├── dog.6900.jpg\n├── dog.6901.jpg\n├── dog.6902.jpg\n├── dog.6903.jpg\n├── dog.6904.jpg\n├── dog.6905.jpg\n├── dog.6906.jpg\n├── dog.6907.jpg\n├── dog.6908.jpg\n├── dog.6909.jpg\n├── dog.690.jpg\n├── dog.6910.jpg\n├── dog.6911.jpg\n├── dog.6912.jpg\n├── dog.6913.jpg\n├── dog.6914.jpg\n├── dog.6915.jpg\n├── dog.6916.jpg\n├── dog.6917.jpg\n├── dog.6918.jpg\n├── dog.6919.jpg\n├── dog.691.jpg\n├── dog.6920.jpg\n├── dog.6921.jpg\n├── dog.6922.jpg\n├── dog.6923.jpg\n├── dog.6924.jpg\n├── dog.6925.jpg\n├── dog.6926.jpg\n├── dog.6927.jpg\n├── dog.6928.jpg\n├── dog.6929.jpg\n├── dog.692.jpg\n├── dog.6930.jpg\n├── dog.6931.jpg\n├── dog.6932.jpg\n├── dog.6933.jpg\n├── dog.6934.jpg\n├── dog.6935.jpg\n├── dog.6936.jpg\n├── dog.6937.jpg\n├── dog.6938.jpg\n├── dog.6939.jpg\n├── dog.693.jpg\n├── dog.6940.jpg\n├── dog.6941.jpg\n├── dog.6942.jpg\n├── dog.6943.jpg\n├── dog.6944.jpg\n├── dog.6945.jpg\n├── dog.6946.jpg\n├── dog.6947.jpg\n├── dog.6948.jpg\n├── dog.6949.jpg\n├── dog.694.jpg\n├── dog.6950.jpg\n├── dog.6951.jpg\n├── dog.6952.jpg\n├── dog.6953.jpg\n├── dog.6954.jpg\n├── dog.6955.jpg\n├── dog.6956.jpg\n├── dog.6957.jpg\n├── dog.6958.jpg\n├── dog.6959.jpg\n├── dog.695.jpg\n├── dog.6960.jpg\n├── dog.6961.jpg\n├── dog.6962.jpg\n├── dog.6963.jpg\n├── dog.6964.jpg\n├── dog.6965.jpg\n├── dog.6966.jpg\n├── dog.6967.jpg\n├── dog.6968.jpg\n├── dog.6969.jpg\n├── dog.696.jpg\n├── dog.6970.jpg\n├── dog.6971.jpg\n├── dog.6972.jpg\n├── dog.6973.jpg\n├── dog.6974.jpg\n├── dog.6975.jpg\n├── dog.6976.jpg\n├── dog.6977.jpg\n├── dog.6978.jpg\n├── dog.6979.jpg\n├── dog.697.jpg\n├── dog.6980.jpg\n├── dog.6981.jpg\n├── dog.6982.jpg\n├── dog.6983.jpg\n├── dog.6984.jpg\n├── dog.6985.jpg\n├── dog.6986.jpg\n├── dog.6987.jpg\n├── dog.6988.jpg\n├── dog.6989.jpg\n├── dog.698.jpg\n├── dog.6990.jpg\n├── dog.6991.jpg\n├── dog.6992.jpg\n├── dog.6993.jpg\n├── dog.6994.jpg\n├── dog.6995.jpg\n├── dog.6996.jpg\n├── dog.6997.jpg\n├── dog.6998.jpg\n├── dog.6999.jpg\n├── dog.699.jpg\n├── dog.69.jpg\n├── dog.6.jpg\n├── dog.7000.jpg\n├── dog.7001.jpg\n├── dog.7002.jpg\n├── dog.7003.jpg\n├── dog.7004.jpg\n├── dog.7005.jpg\n├── dog.7006.jpg\n├── dog.7007.jpg\n├── dog.7008.jpg\n├── dog.7009.jpg\n├── dog.700.jpg\n├── dog.7010.jpg\n├── dog.7011.jpg\n├── dog.7012.jpg\n├── dog.7013.jpg\n├── dog.7014.jpg\n├── dog.7015.jpg\n├── dog.7016.jpg\n├── dog.7017.jpg\n├── dog.7018.jpg\n├── dog.7019.jpg\n├── dog.701.jpg\n├── dog.7020.jpg\n├── dog.7021.jpg\n├── dog.7022.jpg\n├── dog.7023.jpg\n├── dog.7024.jpg\n├── dog.7025.jpg\n├── dog.7026.jpg\n├── dog.7027.jpg\n├── dog.7028.jpg\n├── dog.7029.jpg\n├── dog.702.jpg\n├── dog.7030.jpg\n├── dog.7031.jpg\n├── dog.7032.jpg\n├── dog.7033.jpg\n├── dog.7034.jpg\n├── dog.7035.jpg\n├── dog.7036.jpg\n├── dog.7037.jpg\n├── dog.7038.jpg\n├── dog.7039.jpg\n├── dog.703.jpg\n├── dog.7040.jpg\n├── dog.7041.jpg\n├── dog.7042.jpg\n├── dog.7043.jpg\n├── dog.7044.jpg\n├── dog.7045.jpg\n├── dog.7046.jpg\n├── dog.7047.jpg\n├── dog.7048.jpg\n├── dog.7049.jpg\n├── dog.704.jpg\n├── dog.7050.jpg\n├── dog.7051.jpg\n├── dog.7052.jpg\n├── dog.7053.jpg\n├── dog.7054.jpg\n├── dog.7055.jpg\n├── dog.7056.jpg\n├── dog.7057.jpg\n├── dog.7058.jpg\n├── dog.7059.jpg\n├── dog.705.jpg\n├── dog.7060.jpg\n├── dog.7061.jpg\n├── dog.7062.jpg\n├── dog.7063.jpg\n├── dog.7064.jpg\n├── dog.7065.jpg\n├── dog.7066.jpg\n├── dog.7067.jpg\n├── dog.7068.jpg\n├── dog.7069.jpg\n├── dog.706.jpg\n├── dog.7070.jpg\n├── dog.7071.jpg\n├── dog.7072.jpg\n├── dog.7073.jpg\n├── dog.7074.jpg\n├── dog.7075.jpg\n├── dog.7076.jpg\n├── dog.7077.jpg\n├── dog.7078.jpg\n├── dog.7079.jpg\n├── dog.707.jpg\n├── dog.7080.jpg\n├── dog.7081.jpg\n├── dog.7082.jpg\n├── dog.7083.jpg\n├── dog.7084.jpg\n├── dog.7085.jpg\n├── dog.7086.jpg\n├── dog.7087.jpg\n├── dog.7088.jpg\n├── dog.7089.jpg\n├── dog.708.jpg\n├── dog.7090.jpg\n├── dog.7091.jpg\n├── dog.7092.jpg\n├── dog.7093.jpg\n├── dog.7094.jpg\n├── dog.7095.jpg\n├── dog.7096.jpg\n├── dog.7097.jpg\n├── dog.7098.jpg\n├── dog.7099.jpg\n├── dog.709.jpg\n├── dog.70.jpg\n├── dog.7100.jpg\n├── dog.7101.jpg\n├── dog.7102.jpg\n├── dog.7103.jpg\n├── dog.7104.jpg\n├── dog.7105.jpg\n├── dog.7106.jpg\n├── dog.7107.jpg\n├── dog.7108.jpg\n├── dog.7109.jpg\n├── dog.710.jpg\n├── dog.7110.jpg\n├── dog.7111.jpg\n├── dog.7112.jpg\n├── dog.7113.jpg\n├── dog.7114.jpg\n├── dog.7115.jpg\n├── dog.7116.jpg\n├── dog.7117.jpg\n├── dog.7118.jpg\n├── dog.7119.jpg\n├── dog.711.jpg\n├── dog.7120.jpg\n├── dog.7121.jpg\n├── dog.7122.jpg\n├── dog.7123.jpg\n├── dog.7124.jpg\n├── dog.7125.jpg\n├── dog.7126.jpg\n├── dog.7127.jpg\n├── dog.7128.jpg\n├── dog.7129.jpg\n├── dog.712.jpg\n├── dog.7130.jpg\n├── dog.7131.jpg\n├── dog.7132.jpg\n├── dog.7133.jpg\n├── dog.7134.jpg\n├── dog.7135.jpg\n├── dog.7136.jpg\n├── dog.7137.jpg\n├── dog.7138.jpg\n├── dog.7139.jpg\n├── dog.713.jpg\n├── dog.7140.jpg\n├── dog.7141.jpg\n├── dog.7142.jpg\n├── dog.7143.jpg\n├── dog.7144.jpg\n├── dog.7145.jpg\n├── dog.7146.jpg\n├── dog.7147.jpg\n├── dog.7148.jpg\n├── dog.7149.jpg\n├── dog.714.jpg\n├── dog.7150.jpg\n├── dog.7151.jpg\n├── dog.7152.jpg\n├── dog.7153.jpg\n├── dog.7154.jpg\n├── dog.7155.jpg\n├── dog.7156.jpg\n├── dog.7157.jpg\n├── dog.7158.jpg\n├── dog.7159.jpg\n├── dog.715.jpg\n├── dog.7160.jpg\n├── dog.7161.jpg\n├── dog.7162.jpg\n├── dog.7163.jpg\n├── dog.7164.jpg\n├── dog.7165.jpg\n├── dog.7166.jpg\n├── dog.7167.jpg\n├── dog.7168.jpg\n├── dog.7169.jpg\n├── dog.716.jpg\n├── dog.7170.jpg\n├── dog.7171.jpg\n├── dog.7172.jpg\n├── dog.7173.jpg\n├── dog.7174.jpg\n├── dog.7175.jpg\n├── dog.7176.jpg\n├── dog.7177.jpg\n├── dog.7178.jpg\n├── dog.7179.jpg\n├── dog.717.jpg\n├── dog.7180.jpg\n├── dog.7181.jpg\n├── dog.7182.jpg\n├── dog.7183.jpg\n├── dog.7184.jpg\n├── dog.7185.jpg\n├── dog.7186.jpg\n├── dog.7187.jpg\n├── dog.7188.jpg\n├── dog.7189.jpg\n├── dog.718.jpg\n├── dog.7190.jpg\n├── dog.7191.jpg\n├── dog.7192.jpg\n├── dog.7193.jpg\n├── dog.7194.jpg\n├── dog.7195.jpg\n├── dog.7196.jpg\n├── dog.7197.jpg\n├── dog.7198.jpg\n├── dog.7199.jpg\n├── dog.719.jpg\n├── dog.71.jpg\n├── dog.7200.jpg\n├── dog.7201.jpg\n├── dog.7202.jpg\n├── dog.7203.jpg\n├── dog.7204.jpg\n├── dog.7205.jpg\n├── dog.7206.jpg\n├── dog.7207.jpg\n├── dog.7208.jpg\n├── dog.7209.jpg\n├── dog.720.jpg\n├── dog.7210.jpg\n├── dog.7211.jpg\n├── dog.7212.jpg\n├── dog.7213.jpg\n├── dog.7214.jpg\n├── dog.7215.jpg\n├── dog.7216.jpg\n├── dog.7217.jpg\n├── dog.7218.jpg\n├── dog.7219.jpg\n├── dog.721.jpg\n├── dog.7220.jpg\n├── dog.7221.jpg\n├── dog.7222.jpg\n├── dog.7223.jpg\n├── dog.7224.jpg\n├── dog.7225.jpg\n├── dog.7226.jpg\n├── dog.7227.jpg\n├── dog.7228.jpg\n├── dog.7229.jpg\n├── dog.722.jpg\n├── dog.7230.jpg\n├── dog.7231.jpg\n├── dog.7232.jpg\n├── dog.7233.jpg\n├── dog.7234.jpg\n├── dog.7235.jpg\n├── dog.7236.jpg\n├── dog.7237.jpg\n├── dog.7238.jpg\n├── dog.7239.jpg\n├── dog.723.jpg\n├── dog.7240.jpg\n├── dog.7241.jpg\n├── dog.7242.jpg\n├── dog.7243.jpg\n├── dog.7244.jpg\n├── dog.7245.jpg\n├── dog.7246.jpg\n├── dog.7247.jpg\n├── dog.7248.jpg\n├── dog.7249.jpg\n├── dog.724.jpg\n├── dog.7250.jpg\n├── dog.7251.jpg\n├── dog.7252.jpg\n├── dog.7253.jpg\n├── dog.7254.jpg\n├── dog.7255.jpg\n├── dog.7256.jpg\n├── dog.7257.jpg\n├── dog.7258.jpg\n├── dog.7259.jpg\n├── dog.725.jpg\n├── dog.7260.jpg\n├── dog.7261.jpg\n├── dog.7262.jpg\n├── dog.7263.jpg\n├── dog.7264.jpg\n├── dog.7265.jpg\n├── dog.7266.jpg\n├── dog.7267.jpg\n├── dog.7268.jpg\n├── dog.7269.jpg\n├── dog.726.jpg\n├── dog.7270.jpg\n├── dog.7271.jpg\n├── dog.7272.jpg\n├── dog.7273.jpg\n├── dog.7274.jpg\n├── dog.7275.jpg\n├── dog.7276.jpg\n├── dog.7277.jpg\n├── dog.7278.jpg\n├── dog.7279.jpg\n├── dog.727.jpg\n├── dog.7280.jpg\n├── dog.7281.jpg\n├── dog.7282.jpg\n├── dog.7283.jpg\n├── dog.7284.jpg\n├── dog.7285.jpg\n├── dog.7286.jpg\n├── dog.7287.jpg\n├── dog.7288.jpg\n├── dog.7289.jpg\n├── dog.728.jpg\n├── dog.7290.jpg\n├── dog.7291.jpg\n├── dog.7292.jpg\n├── dog.7293.jpg\n├── dog.7294.jpg\n├── dog.7295.jpg\n├── dog.7296.jpg\n├── dog.7297.jpg\n├── dog.7298.jpg\n├── dog.7299.jpg\n├── dog.729.jpg\n├── dog.72.jpg\n├── dog.7300.jpg\n├── dog.7301.jpg\n├── dog.7302.jpg\n├── dog.7303.jpg\n├── dog.7304.jpg\n├── dog.7305.jpg\n├── dog.7306.jpg\n├── dog.7307.jpg\n├── dog.7308.jpg\n├── dog.7309.jpg\n├── dog.730.jpg\n├── dog.7310.jpg\n├── dog.7311.jpg\n├── dog.7312.jpg\n├── dog.7313.jpg\n├── dog.7314.jpg\n├── dog.7315.jpg\n├── dog.7316.jpg\n├── dog.7317.jpg\n├── dog.7318.jpg\n├── dog.7319.jpg\n├── dog.731.jpg\n├── dog.7320.jpg\n├── dog.7321.jpg\n├── dog.7322.jpg\n├── dog.7323.jpg\n├── dog.7324.jpg\n├── dog.7325.jpg\n├── dog.7326.jpg\n├── dog.7327.jpg\n├── dog.7328.jpg\n├── dog.7329.jpg\n├── dog.732.jpg\n├── dog.7330.jpg\n├── dog.7331.jpg\n├── dog.7332.jpg\n├── dog.7333.jpg\n├── dog.7334.jpg\n├── dog.7335.jpg\n├── dog.7336.jpg\n├── dog.7337.jpg\n├── dog.7338.jpg\n├── dog.7339.jpg\n├── dog.733.jpg\n├── dog.7340.jpg\n├── dog.7341.jpg\n├── dog.7342.jpg\n├── dog.7343.jpg\n├── dog.7344.jpg\n├── dog.7345.jpg\n├── dog.7346.jpg\n├── dog.7347.jpg\n├── dog.7348.jpg\n├── dog.7349.jpg\n├── dog.734.jpg\n├── dog.7350.jpg\n├── dog.7351.jpg\n├── dog.7352.jpg\n├── dog.7353.jpg\n├── dog.7354.jpg\n├── dog.7355.jpg\n├── dog.7356.jpg\n├── dog.7357.jpg\n├── dog.7358.jpg\n├── dog.7359.jpg\n├── dog.735.jpg\n├── dog.7360.jpg\n├── dog.7361.jpg\n├── dog.7362.jpg\n├── dog.7363.jpg\n├── dog.7364.jpg\n├── dog.7365.jpg\n├── dog.7366.jpg\n├── dog.7367.jpg\n├── dog.7368.jpg\n├── dog.7369.jpg\n├── dog.736.jpg\n├── dog.7370.jpg\n├── dog.7371.jpg\n├── dog.7372.jpg\n├── dog.7373.jpg\n├── dog.7374.jpg\n├── dog.7375.jpg\n├── dog.7376.jpg\n├── dog.7377.jpg\n├── dog.7378.jpg\n├── dog.7379.jpg\n├── dog.737.jpg\n├── dog.7380.jpg\n├── dog.7381.jpg\n├── dog.7382.jpg\n├── dog.7383.jpg\n├── dog.7384.jpg\n├── dog.7385.jpg\n├── dog.7386.jpg\n├── dog.7387.jpg\n├── dog.7388.jpg\n├── dog.7389.jpg\n├── dog.738.jpg\n├── dog.7390.jpg\n├── dog.7391.jpg\n├── dog.7392.jpg\n├── dog.7393.jpg\n├── dog.7394.jpg\n├── dog.7395.jpg\n├── dog.7396.jpg\n├── dog.7397.jpg\n├── dog.7398.jpg\n├── dog.7399.jpg\n├── dog.739.jpg\n├── dog.73.jpg\n├── dog.7400.jpg\n├── dog.7401.jpg\n├── dog.7402.jpg\n├── dog.7403.jpg\n├── dog.7404.jpg\n├── dog.7405.jpg\n├── dog.7406.jpg\n├── dog.7407.jpg\n├── dog.7408.jpg\n├── dog.7409.jpg\n├── dog.740.jpg\n├── dog.7410.jpg\n├── dog.7411.jpg\n├── dog.7412.jpg\n├── dog.7413.jpg\n├── dog.7414.jpg\n├── dog.7415.jpg\n├── dog.7416.jpg\n├── dog.7417.jpg\n├── dog.7418.jpg\n├── dog.7419.jpg\n├── dog.741.jpg\n├── dog.7420.jpg\n├── dog.7421.jpg\n├── dog.7422.jpg\n├── dog.7423.jpg\n├── dog.7424.jpg\n├── dog.7425.jpg\n├── dog.7426.jpg\n├── dog.7427.jpg\n├── dog.7428.jpg\n├── dog.7429.jpg\n├── dog.742.jpg\n├── dog.7430.jpg\n├── dog.7431.jpg\n├── dog.7432.jpg\n├── dog.7433.jpg\n├── dog.7434.jpg\n├── dog.7435.jpg\n├── dog.7436.jpg\n├── dog.7437.jpg\n├── dog.7438.jpg\n├── dog.7439.jpg\n├── dog.743.jpg\n├── dog.7440.jpg\n├── dog.7441.jpg\n├── dog.7442.jpg\n├── dog.7443.jpg\n├── dog.7444.jpg\n├── dog.7445.jpg\n├── dog.7446.jpg\n├── dog.7447.jpg\n├── dog.7448.jpg\n├── dog.7449.jpg\n├── dog.744.jpg\n├── dog.7450.jpg\n├── dog.7451.jpg\n├── dog.7452.jpg\n├── dog.7453.jpg\n├── dog.7454.jpg\n├── dog.7455.jpg\n├── dog.7456.jpg\n├── dog.7457.jpg\n├── dog.7458.jpg\n├── dog.7459.jpg\n├── dog.745.jpg\n├── dog.7460.jpg\n├── dog.7461.jpg\n├── dog.7462.jpg\n├── dog.7463.jpg\n├── dog.7464.jpg\n├── dog.7465.jpg\n├── dog.7466.jpg\n├── dog.7467.jpg\n├── dog.7468.jpg\n├── dog.7469.jpg\n├── dog.746.jpg\n├── dog.7470.jpg\n├── dog.7471.jpg\n├── dog.7472.jpg\n├── dog.7473.jpg\n├── dog.7474.jpg\n├── dog.7475.jpg\n├── dog.7476.jpg\n├── dog.7477.jpg\n├── dog.7478.jpg\n├── dog.7479.jpg\n├── dog.747.jpg\n├── dog.7480.jpg\n├── dog.7481.jpg\n├── dog.7482.jpg\n├── dog.7483.jpg\n├── dog.7484.jpg\n├── dog.7485.jpg\n├── dog.7486.jpg\n├── dog.7487.jpg\n├── dog.7488.jpg\n├── dog.7489.jpg\n├── dog.748.jpg\n├── dog.7490.jpg\n├── dog.7491.jpg\n├── dog.7492.jpg\n├── dog.7493.jpg\n├── dog.7494.jpg\n├── dog.7495.jpg\n├── dog.7496.jpg\n├── dog.7497.jpg\n├── dog.7498.jpg\n├── dog.7499.jpg\n├── dog.749.jpg\n├── dog.74.jpg\n├── dog.7500.jpg\n├── dog.7501.jpg\n├── dog.7502.jpg\n├── dog.7503.jpg\n├── dog.7504.jpg\n├── dog.7505.jpg\n├── dog.7506.jpg\n├── dog.7507.jpg\n├── dog.7508.jpg\n├── dog.7509.jpg\n├── dog.750.jpg\n├── dog.7510.jpg\n├── dog.7511.jpg\n├── dog.7512.jpg\n├── dog.7513.jpg\n├── dog.7514.jpg\n├── dog.7515.jpg\n├── dog.7516.jpg\n├── dog.7517.jpg\n├── dog.7518.jpg\n├── dog.7519.jpg\n├── dog.751.jpg\n├── dog.7520.jpg\n├── dog.7521.jpg\n├── dog.7522.jpg\n├── dog.7523.jpg\n├── dog.7524.jpg\n├── dog.7525.jpg\n├── dog.7526.jpg\n├── dog.7527.jpg\n├── dog.7528.jpg\n├── dog.7529.jpg\n├── dog.752.jpg\n├── dog.7530.jpg\n├── dog.7531.jpg\n├── dog.7532.jpg\n├── dog.7533.jpg\n├── dog.7534.jpg\n├── dog.7535.jpg\n├── dog.7536.jpg\n├── dog.7537.jpg\n├── dog.7538.jpg\n├── dog.7539.jpg\n├── dog.753.jpg\n├── dog.7540.jpg\n├── dog.7541.jpg\n├── dog.7542.jpg\n├── dog.7543.jpg\n├── dog.7544.jpg\n├── dog.7545.jpg\n├── dog.7546.jpg\n├── dog.7547.jpg\n├── dog.7548.jpg\n├── dog.7549.jpg\n├── dog.754.jpg\n├── dog.7550.jpg\n├── dog.7551.jpg\n├── dog.7552.jpg\n├── dog.7553.jpg\n├── dog.7554.jpg\n├── dog.7555.jpg\n├── dog.7556.jpg\n├── dog.7557.jpg\n├── dog.7558.jpg\n├── dog.7559.jpg\n├── dog.755.jpg\n├── dog.7560.jpg\n├── dog.7561.jpg\n├── dog.7562.jpg\n├── dog.7563.jpg\n├── dog.7564.jpg\n├── dog.7565.jpg\n├── dog.7566.jpg\n├── dog.7567.jpg\n├── dog.7568.jpg\n├── dog.7569.jpg\n├── dog.756.jpg\n├── dog.7570.jpg\n├── dog.7571.jpg\n├── dog.7572.jpg\n├── dog.7573.jpg\n├── dog.7574.jpg\n├── dog.7575.jpg\n├── dog.7576.jpg\n├── dog.7577.jpg\n├── dog.7578.jpg\n├── dog.7579.jpg\n├── dog.757.jpg\n├── dog.7580.jpg\n├── dog.7581.jpg\n├── dog.7582.jpg\n├── dog.7583.jpg\n├── dog.7584.jpg\n├── dog.7585.jpg\n├── dog.7586.jpg\n├── dog.7587.jpg\n├── dog.7588.jpg\n├── dog.7589.jpg\n├── dog.758.jpg\n├── dog.7590.jpg\n├── dog.7591.jpg\n├── dog.7592.jpg\n├── dog.7593.jpg\n├── dog.7594.jpg\n├── dog.7595.jpg\n├── dog.7596.jpg\n├── dog.7597.jpg\n├── dog.7598.jpg\n├── dog.7599.jpg\n├── dog.759.jpg\n├── dog.75.jpg\n├── dog.7600.jpg\n├── dog.7601.jpg\n├── dog.7602.jpg\n├── dog.7603.jpg\n├── dog.7604.jpg\n├── dog.7605.jpg\n├── dog.7606.jpg\n├── dog.7607.jpg\n├── dog.7608.jpg\n├── dog.7609.jpg\n├── dog.760.jpg\n├── dog.7610.jpg\n├── dog.7611.jpg\n├── dog.7612.jpg\n├── dog.7613.jpg\n├── dog.7614.jpg\n├── dog.7615.jpg\n├── dog.7616.jpg\n├── dog.7617.jpg\n├── dog.7618.jpg\n├── dog.7619.jpg\n├── dog.761.jpg\n├── dog.7620.jpg\n├── dog.7621.jpg\n├── dog.7622.jpg\n├── dog.7623.jpg\n├── dog.7624.jpg\n├── dog.7625.jpg\n├── dog.7626.jpg\n├── dog.7627.jpg\n├── dog.7628.jpg\n├── dog.7629.jpg\n├── dog.762.jpg\n├── dog.7630.jpg\n├── dog.7631.jpg\n├── dog.7632.jpg\n├── dog.7633.jpg\n├── dog.7634.jpg\n├── dog.7635.jpg\n├── dog.7636.jpg\n├── dog.7637.jpg\n├── dog.7638.jpg\n├── dog.7639.jpg\n├── dog.763.jpg\n├── dog.7640.jpg\n├── dog.7641.jpg\n├── dog.7642.jpg\n├── dog.7643.jpg\n├── dog.7644.jpg\n├── dog.7645.jpg\n├── dog.7646.jpg\n├── dog.7647.jpg\n├── dog.7648.jpg\n├── dog.7649.jpg\n├── dog.764.jpg\n├── dog.7650.jpg\n├── dog.7651.jpg\n├── dog.7652.jpg\n├── dog.7653.jpg\n├── dog.7654.jpg\n├── dog.7655.jpg\n├── dog.7656.jpg\n├── dog.7657.jpg\n├── dog.7658.jpg\n├── dog.7659.jpg\n├── dog.765.jpg\n├── dog.7660.jpg\n├── dog.7661.jpg\n├── dog.7662.jpg\n├── dog.7663.jpg\n├── dog.7664.jpg\n├── dog.7665.jpg\n├── dog.7666.jpg\n├── dog.7667.jpg\n├── dog.7668.jpg\n├── dog.7669.jpg\n├── dog.766.jpg\n├── dog.7670.jpg\n├── dog.7671.jpg\n├── dog.7672.jpg\n├── dog.7673.jpg\n├── dog.7674.jpg\n├── dog.7675.jpg\n├── dog.7676.jpg\n├── dog.7677.jpg\n├── dog.7678.jpg\n├── dog.7679.jpg\n├── dog.767.jpg\n├── dog.7680.jpg\n├── dog.7681.jpg\n├── dog.7682.jpg\n├── dog.7683.jpg\n├── dog.7684.jpg\n├── dog.7685.jpg\n├── dog.7686.jpg\n├── dog.7687.jpg\n├── dog.7688.jpg\n├── dog.7689.jpg\n├── dog.768.jpg\n├── dog.7690.jpg\n├── dog.7691.jpg\n├── dog.7692.jpg\n├── dog.7693.jpg\n├── dog.7694.jpg\n├── dog.7695.jpg\n├── dog.7696.jpg\n├── dog.7697.jpg\n├── dog.7698.jpg\n├── dog.7699.jpg\n├── dog.769.jpg\n├── dog.76.jpg\n├── dog.7700.jpg\n├── dog.7701.jpg\n├── dog.7702.jpg\n├── dog.7703.jpg\n├── dog.7704.jpg\n├── dog.7705.jpg\n├── dog.7706.jpg\n├── dog.7707.jpg\n├── dog.7708.jpg\n├── dog.7709.jpg\n├── dog.770.jpg\n├── dog.7710.jpg\n├── dog.7711.jpg\n├── dog.7712.jpg\n├── dog.7713.jpg\n├── dog.7714.jpg\n├── dog.7715.jpg\n├── dog.7716.jpg\n├── dog.7717.jpg\n├── dog.7718.jpg\n├── dog.7719.jpg\n├── dog.771.jpg\n├── dog.7720.jpg\n├── dog.7721.jpg\n├── dog.7722.jpg\n├── dog.7723.jpg\n├── dog.7724.jpg\n├── dog.7725.jpg\n├── dog.7726.jpg\n├── dog.7727.jpg\n├── dog.7728.jpg\n├── dog.7729.jpg\n├── dog.772.jpg\n├── dog.7730.jpg\n├── dog.7731.jpg\n├── dog.7732.jpg\n├── dog.7733.jpg\n├── dog.7734.jpg\n├── dog.7735.jpg\n├── dog.7736.jpg\n├── dog.7737.jpg\n├── dog.7738.jpg\n├── dog.7739.jpg\n├── dog.773.jpg\n├── dog.7740.jpg\n├── dog.7741.jpg\n├── dog.7742.jpg\n├── dog.7743.jpg\n├── dog.7744.jpg\n├── dog.7745.jpg\n├── dog.7746.jpg\n├── dog.7747.jpg\n├── dog.7748.jpg\n├── dog.7749.jpg\n├── dog.774.jpg\n├── dog.7750.jpg\n├── dog.7751.jpg\n├── dog.7752.jpg\n├── dog.7753.jpg\n├── dog.7754.jpg\n├── dog.7755.jpg\n├── dog.7756.jpg\n├── dog.7757.jpg\n├── dog.7758.jpg\n├── dog.7759.jpg\n├── dog.775.jpg\n├── dog.7760.jpg\n├── dog.7761.jpg\n├── dog.7762.jpg\n├── dog.7763.jpg\n├── dog.7764.jpg\n├── dog.7765.jpg\n├── dog.7766.jpg\n├── dog.7767.jpg\n├── dog.7768.jpg\n├── dog.7769.jpg\n├── dog.776.jpg\n├── dog.7770.jpg\n├── dog.7771.jpg\n├── dog.7772.jpg\n├── dog.7773.jpg\n├── dog.7774.jpg\n├── dog.7775.jpg\n├── dog.7776.jpg\n├── dog.7777.jpg\n├── dog.7778.jpg\n├── dog.7779.jpg\n├── dog.777.jpg\n├── dog.7780.jpg\n├── dog.7781.jpg\n├── dog.7782.jpg\n├── dog.7783.jpg\n├── dog.7784.jpg\n├── dog.7785.jpg\n├── dog.7786.jpg\n├── dog.7787.jpg\n├── dog.7788.jpg\n├── dog.7789.jpg\n├── dog.778.jpg\n├── dog.7790.jpg\n├── dog.7791.jpg\n├── dog.7792.jpg\n├── dog.7793.jpg\n├── dog.7794.jpg\n├── dog.7795.jpg\n├── dog.7796.jpg\n├── dog.7797.jpg\n├── dog.7798.jpg\n├── dog.7799.jpg\n├── dog.779.jpg\n├── dog.77.jpg\n├── dog.7800.jpg\n├── dog.7801.jpg\n├── dog.7802.jpg\n├── dog.7803.jpg\n├── dog.7804.jpg\n├── dog.7805.jpg\n├── dog.7806.jpg\n├── dog.7807.jpg\n├── dog.7808.jpg\n├── dog.7809.jpg\n├── dog.780.jpg\n├── dog.7810.jpg\n├── dog.7811.jpg\n├── dog.7812.jpg\n├── dog.7813.jpg\n├── dog.7814.jpg\n├── dog.7815.jpg\n├── dog.7816.jpg\n├── dog.7817.jpg\n├── dog.7818.jpg\n├── dog.7819.jpg\n├── dog.781.jpg\n├── dog.7820.jpg\n├── dog.7821.jpg\n├── dog.7822.jpg\n├── dog.7823.jpg\n├── dog.7824.jpg\n├── dog.7825.jpg\n├── dog.7826.jpg\n├── dog.7827.jpg\n├── dog.7828.jpg\n├── dog.7829.jpg\n├── dog.782.jpg\n├── dog.7830.jpg\n├── dog.7831.jpg\n├── dog.7832.jpg\n├── dog.7833.jpg\n├── dog.7834.jpg\n├── dog.7835.jpg\n├── dog.7836.jpg\n├── dog.7837.jpg\n├── dog.7838.jpg\n├── dog.7839.jpg\n├── dog.783.jpg\n├── dog.7840.jpg\n├── dog.7841.jpg\n├── dog.7842.jpg\n├── dog.7843.jpg\n├── dog.7844.jpg\n├── dog.7845.jpg\n├── dog.7846.jpg\n├── dog.7847.jpg\n├── dog.7848.jpg\n├── dog.7849.jpg\n├── dog.784.jpg\n├── dog.7850.jpg\n├── dog.7851.jpg\n├── dog.7852.jpg\n├── dog.7853.jpg\n├── dog.7854.jpg\n├── dog.7855.jpg\n├── dog.7856.jpg\n├── dog.7857.jpg\n├── dog.7858.jpg\n├── dog.7859.jpg\n├── dog.785.jpg\n├── dog.7860.jpg\n├── dog.7861.jpg\n├── dog.7862.jpg\n├── dog.7863.jpg\n├── dog.7864.jpg\n├── dog.7865.jpg\n├── dog.7866.jpg\n├── dog.7867.jpg\n├── dog.7868.jpg\n├── dog.7869.jpg\n├── dog.786.jpg\n├── dog.7870.jpg\n├── dog.7871.jpg\n├── dog.7872.jpg\n├── dog.7873.jpg\n├── dog.7874.jpg\n├── dog.7875.jpg\n├── dog.7876.jpg\n├── dog.7877.jpg\n├── dog.7878.jpg\n├── dog.7879.jpg\n├── dog.787.jpg\n├── dog.7880.jpg\n├── dog.7881.jpg\n├── dog.7882.jpg\n├── dog.7883.jpg\n├── dog.7884.jpg\n├── dog.7885.jpg\n├── dog.7886.jpg\n├── dog.7887.jpg\n├── dog.7888.jpg\n├── dog.7889.jpg\n├── dog.788.jpg\n├── dog.7890.jpg\n├── dog.7891.jpg\n├── dog.7892.jpg\n├── dog.7893.jpg\n├── dog.7894.jpg\n├── dog.7895.jpg\n├── dog.7896.jpg\n├── dog.7897.jpg\n├── dog.7898.jpg\n├── dog.7899.jpg\n├── dog.789.jpg\n├── dog.78.jpg\n├── dog.7900.jpg\n├── dog.7901.jpg\n├── dog.7902.jpg\n├── dog.7903.jpg\n├── dog.7904.jpg\n├── dog.7905.jpg\n├── dog.7906.jpg\n├── dog.7907.jpg\n├── dog.7908.jpg\n├── dog.7909.jpg\n├── dog.790.jpg\n├── dog.7910.jpg\n├── dog.7911.jpg\n├── dog.7912.jpg\n├── dog.7913.jpg\n├── dog.7914.jpg\n├── dog.7915.jpg\n├── dog.7916.jpg\n├── dog.7917.jpg\n├── dog.7918.jpg\n├── dog.7919.jpg\n├── dog.791.jpg\n├── dog.7920.jpg\n├── dog.7921.jpg\n├── dog.7922.jpg\n├── dog.7923.jpg\n├── dog.7924.jpg\n├── dog.7925.jpg\n├── dog.7926.jpg\n├── dog.7927.jpg\n├── dog.7928.jpg\n├── dog.7929.jpg\n├── dog.792.jpg\n├── dog.7930.jpg\n├── dog.7931.jpg\n├── dog.7932.jpg\n├── dog.7933.jpg\n├── dog.7934.jpg\n├── dog.7935.jpg\n├── dog.7936.jpg\n├── dog.7937.jpg\n├── dog.7938.jpg\n├── dog.7939.jpg\n├── dog.793.jpg\n├── dog.7940.jpg\n├── dog.7941.jpg\n├── dog.7942.jpg\n├── dog.7943.jpg\n├── dog.7944.jpg\n├── dog.7945.jpg\n├── dog.7946.jpg\n├── dog.7947.jpg\n├── dog.7948.jpg\n├── dog.7949.jpg\n├── dog.794.jpg\n├── dog.7950.jpg\n├── dog.7951.jpg\n├── dog.7952.jpg\n├── dog.7953.jpg\n├── dog.7954.jpg\n├── dog.7955.jpg\n├── dog.7956.jpg\n├── dog.7957.jpg\n├── dog.7958.jpg\n├── dog.7959.jpg\n├── dog.795.jpg\n├── dog.7960.jpg\n├── dog.7961.jpg\n├── dog.7962.jpg\n├── dog.7963.jpg\n├── dog.7964.jpg\n├── dog.7965.jpg\n├── dog.7966.jpg\n├── dog.7967.jpg\n├── dog.7968.jpg\n├── dog.7969.jpg\n├── dog.796.jpg\n├── dog.7970.jpg\n├── dog.7971.jpg\n├── dog.7972.jpg\n├── dog.7973.jpg\n├── dog.7974.jpg\n├── dog.7975.jpg\n├── dog.7976.jpg\n├── dog.7977.jpg\n├── dog.7978.jpg\n├── dog.7979.jpg\n├── dog.797.jpg\n├── dog.7980.jpg\n├── dog.7981.jpg\n├── dog.7982.jpg\n├── dog.7983.jpg\n├── dog.7984.jpg\n├── dog.7985.jpg\n├── dog.7986.jpg\n├── dog.7987.jpg\n├── dog.7988.jpg\n├── dog.7989.jpg\n├── dog.798.jpg\n├── dog.7990.jpg\n├── dog.7991.jpg\n├── dog.7992.jpg\n├── dog.7993.jpg\n├── dog.7994.jpg\n├── dog.7995.jpg\n├── dog.7996.jpg\n├── dog.7997.jpg\n├── dog.7998.jpg\n├── dog.7999.jpg\n├── dog.799.jpg\n├── dog.79.jpg\n├── dog.7.jpg\n├── dog.8000.jpg\n├── dog.8001.jpg\n├── dog.8002.jpg\n├── dog.8003.jpg\n├── dog.8004.jpg\n├── dog.8005.jpg\n├── dog.8006.jpg\n├── dog.8007.jpg\n├── dog.8008.jpg\n├── dog.8009.jpg\n├── dog.800.jpg\n├── dog.8010.jpg\n├── dog.8011.jpg\n├── dog.8012.jpg\n├── dog.8013.jpg\n├── dog.8014.jpg\n├── dog.8015.jpg\n├── dog.8016.jpg\n├── dog.8017.jpg\n├── dog.8018.jpg\n├── dog.8019.jpg\n├── dog.801.jpg\n├── dog.8020.jpg\n├── dog.8021.jpg\n├── dog.8022.jpg\n├── dog.8023.jpg\n├── dog.8024.jpg\n├── dog.8025.jpg\n├── dog.8026.jpg\n├── dog.8027.jpg\n├── dog.8028.jpg\n├── dog.8029.jpg\n├── dog.802.jpg\n├── dog.8030.jpg\n├── dog.8031.jpg\n├── dog.8032.jpg\n├── dog.8033.jpg\n├── dog.8034.jpg\n├── dog.8035.jpg\n├── dog.8036.jpg\n├── dog.8037.jpg\n├── dog.8038.jpg\n├── dog.8039.jpg\n├── dog.803.jpg\n├── dog.8040.jpg\n├── dog.8041.jpg\n├── dog.8042.jpg\n├── dog.8043.jpg\n├── dog.8044.jpg\n├── dog.8045.jpg\n├── dog.8046.jpg\n├── dog.8047.jpg\n├── dog.8048.jpg\n├── dog.8049.jpg\n├── dog.804.jpg\n├── dog.8050.jpg\n├── dog.8051.jpg\n├── dog.8052.jpg\n├── dog.8053.jpg\n├── dog.8054.jpg\n├── dog.8055.jpg\n├── dog.8056.jpg\n├── dog.8057.jpg\n├── dog.8058.jpg\n├── dog.8059.jpg\n├── dog.805.jpg\n├── dog.8060.jpg\n├── dog.8061.jpg\n├── dog.8062.jpg\n├── dog.8063.jpg\n├── dog.8064.jpg\n├── dog.8065.jpg\n├── dog.8066.jpg\n├── dog.8067.jpg\n├── dog.8068.jpg\n├── dog.8069.jpg\n├── dog.806.jpg\n├── dog.8070.jpg\n├── dog.8071.jpg\n├── dog.8072.jpg\n├── dog.8073.jpg\n├── dog.8074.jpg\n├── dog.8075.jpg\n├── dog.8076.jpg\n├── dog.8077.jpg\n├── dog.8078.jpg\n├── dog.8079.jpg\n├── dog.807.jpg\n├── dog.8080.jpg\n├── dog.8081.jpg\n├── dog.8082.jpg\n├── dog.8083.jpg\n├── dog.8084.jpg\n├── dog.8085.jpg\n├── dog.8086.jpg\n├── dog.8087.jpg\n├── dog.8088.jpg\n├── dog.8089.jpg\n├── dog.808.jpg\n├── dog.8090.jpg\n├── dog.8091.jpg\n├── dog.8092.jpg\n├── dog.8093.jpg\n├── dog.8094.jpg\n├── dog.8095.jpg\n├── dog.8096.jpg\n├── dog.8097.jpg\n├── dog.8098.jpg\n├── dog.8099.jpg\n├── dog.809.jpg\n├── dog.80.jpg\n├── dog.8100.jpg\n├── dog.8101.jpg\n├── dog.8102.jpg\n├── dog.8103.jpg\n├── dog.8104.jpg\n├── dog.8105.jpg\n├── dog.8106.jpg\n├── dog.8107.jpg\n├── dog.8108.jpg\n├── dog.8109.jpg\n├── dog.810.jpg\n├── dog.8110.jpg\n├── dog.8111.jpg\n├── dog.8112.jpg\n├── dog.8113.jpg\n├── dog.8114.jpg\n├── dog.8115.jpg\n├── dog.8116.jpg\n├── dog.8117.jpg\n├── dog.8118.jpg\n├── dog.8119.jpg\n├── dog.811.jpg\n├── dog.8120.jpg\n├── dog.8121.jpg\n├── dog.8122.jpg\n├── dog.8123.jpg\n├── dog.8124.jpg\n├── dog.8125.jpg\n├── dog.8126.jpg\n├── dog.8127.jpg\n├── dog.8128.jpg\n├── dog.8129.jpg\n├── dog.812.jpg\n├── dog.8130.jpg\n├── dog.8131.jpg\n├── dog.8132.jpg\n├── dog.8133.jpg\n├── dog.8134.jpg\n├── dog.8135.jpg\n├── dog.8136.jpg\n├── dog.8137.jpg\n├── dog.8138.jpg\n├── dog.8139.jpg\n├── dog.813.jpg\n├── dog.8140.jpg\n├── dog.8141.jpg\n├── dog.8142.jpg\n├── dog.8143.jpg\n├── dog.8144.jpg\n├── dog.8145.jpg\n├── dog.8146.jpg\n├── dog.8147.jpg\n├── dog.8148.jpg\n├── dog.8149.jpg\n├── dog.814.jpg\n├── dog.8150.jpg\n├── dog.8151.jpg\n├── dog.8152.jpg\n├── dog.8153.jpg\n├── dog.8154.jpg\n├── dog.8155.jpg\n├── dog.8156.jpg\n├── dog.8157.jpg\n├── dog.8158.jpg\n├── dog.8159.jpg\n├── dog.815.jpg\n├── dog.8160.jpg\n├── dog.8161.jpg\n├── dog.8162.jpg\n├── dog.8163.jpg\n├── dog.8164.jpg\n├── dog.8165.jpg\n├── dog.8166.jpg\n├── dog.8167.jpg\n├── dog.8168.jpg\n├── dog.8169.jpg\n├── dog.816.jpg\n├── dog.8170.jpg\n├── dog.8171.jpg\n├── dog.8172.jpg\n├── dog.8173.jpg\n├── dog.8174.jpg\n├── dog.8175.jpg\n├── dog.8176.jpg\n├── dog.8177.jpg\n├── dog.8178.jpg\n├── dog.8179.jpg\n├── dog.817.jpg\n├── dog.8180.jpg\n├── dog.8181.jpg\n├── dog.8182.jpg\n├── dog.8183.jpg\n├── dog.8184.jpg\n├── dog.8185.jpg\n├── dog.8186.jpg\n├── dog.8187.jpg\n├── dog.8188.jpg\n├── dog.8189.jpg\n├── dog.818.jpg\n├── dog.8190.jpg\n├── dog.8191.jpg\n├── dog.8192.jpg\n├── dog.8193.jpg\n├── dog.8194.jpg\n├── dog.8195.jpg\n├── dog.8196.jpg\n├── dog.8197.jpg\n├── dog.8198.jpg\n├── dog.8199.jpg\n├── dog.819.jpg\n├── dog.81.jpg\n├── dog.8200.jpg\n├── dog.8201.jpg\n├── dog.8202.jpg\n├── dog.8203.jpg\n├── dog.8204.jpg\n├── dog.8205.jpg\n├── dog.8206.jpg\n├── dog.8207.jpg\n├── dog.8208.jpg\n├── dog.8209.jpg\n├── dog.820.jpg\n├── dog.8210.jpg\n├── dog.8211.jpg\n├── dog.8212.jpg\n├── dog.8213.jpg\n├── dog.8214.jpg\n├── dog.8215.jpg\n├── dog.8216.jpg\n├── dog.8217.jpg\n├── dog.8218.jpg\n├── dog.8219.jpg\n├── dog.821.jpg\n├── dog.8220.jpg\n├── dog.8221.jpg\n├── dog.8222.jpg\n├── dog.8223.jpg\n├── dog.8224.jpg\n├── dog.8225.jpg\n├── dog.8226.jpg\n├── dog.8227.jpg\n├── dog.8228.jpg\n├── dog.8229.jpg\n├── dog.822.jpg\n├── dog.8230.jpg\n├── dog.8231.jpg\n├── dog.8232.jpg\n├── dog.8233.jpg\n├── dog.8234.jpg\n├── dog.8235.jpg\n├── dog.8236.jpg\n├── dog.8237.jpg\n├── dog.8238.jpg\n├── dog.8239.jpg\n├── dog.823.jpg\n├── dog.8240.jpg\n├── dog.8241.jpg\n├── dog.8242.jpg\n├── dog.8243.jpg\n├── dog.8244.jpg\n├── dog.8245.jpg\n├── dog.8246.jpg\n├── dog.8247.jpg\n├── dog.8248.jpg\n├── dog.8249.jpg\n├── dog.824.jpg\n├── dog.8250.jpg\n├── dog.8251.jpg\n├── dog.8252.jpg\n├── dog.8253.jpg\n├── dog.8254.jpg\n├── dog.8255.jpg\n├── dog.8256.jpg\n├── dog.8257.jpg\n├── dog.8258.jpg\n├── dog.8259.jpg\n├── dog.825.jpg\n├── dog.8260.jpg\n├── dog.8261.jpg\n├── dog.8262.jpg\n├── dog.8263.jpg\n├── dog.8264.jpg\n├── dog.8265.jpg\n├── dog.8266.jpg\n├── dog.8267.jpg\n├── dog.8268.jpg\n├── dog.8269.jpg\n├── dog.826.jpg\n├── dog.8270.jpg\n├── dog.8271.jpg\n├── dog.8272.jpg\n├── dog.8273.jpg\n├── dog.8274.jpg\n├── dog.8275.jpg\n├── dog.8276.jpg\n├── dog.8277.jpg\n├── dog.8278.jpg\n├── dog.8279.jpg\n├── dog.827.jpg\n├── dog.8280.jpg\n├── dog.8281.jpg\n├── dog.8282.jpg\n├── dog.8283.jpg\n├── dog.8284.jpg\n├── dog.8285.jpg\n├── dog.8286.jpg\n├── dog.8287.jpg\n├── dog.8288.jpg\n├── dog.8289.jpg\n├── dog.828.jpg\n├── dog.8290.jpg\n├── dog.8291.jpg\n├── dog.8292.jpg\n├── dog.8293.jpg\n├── dog.8294.jpg\n├── dog.8295.jpg\n├── dog.8296.jpg\n├── dog.8297.jpg\n├── dog.8298.jpg\n├── dog.8299.jpg\n├── dog.829.jpg\n├── dog.82.jpg\n├── dog.8300.jpg\n├── dog.8301.jpg\n├── dog.8302.jpg\n├── dog.8303.jpg\n├── dog.8304.jpg\n├── dog.8305.jpg\n├── dog.8306.jpg\n├── dog.8307.jpg\n├── dog.8308.jpg\n├── dog.8309.jpg\n├── dog.830.jpg\n├── dog.8310.jpg\n├── dog.8311.jpg\n├── dog.8312.jpg\n├── dog.8313.jpg\n├── dog.8314.jpg\n├── dog.8315.jpg\n├── dog.8316.jpg\n├── dog.8317.jpg\n├── dog.8318.jpg\n├── dog.8319.jpg\n├── dog.831.jpg\n├── dog.8320.jpg\n├── dog.8321.jpg\n├── dog.8322.jpg\n├── dog.8323.jpg\n├── dog.8324.jpg\n├── dog.8325.jpg\n├── dog.8326.jpg\n├── dog.8327.jpg\n├── dog.8328.jpg\n├── dog.8329.jpg\n├── dog.832.jpg\n├── dog.8330.jpg\n├── dog.8331.jpg\n├── dog.8332.jpg\n├── dog.8333.jpg\n├── dog.8334.jpg\n├── dog.8335.jpg\n├── dog.8336.jpg\n├── dog.8337.jpg\n├── dog.8338.jpg\n├── dog.8339.jpg\n├── dog.833.jpg\n├── dog.8340.jpg\n├── dog.8341.jpg\n├── dog.8342.jpg\n├── dog.8343.jpg\n├── dog.8344.jpg\n├── dog.8345.jpg\n├── dog.8346.jpg\n├── dog.8347.jpg\n├── dog.8348.jpg\n├── dog.8349.jpg\n├── dog.834.jpg\n├── dog.8350.jpg\n├── dog.8351.jpg\n├── dog.8352.jpg\n├── dog.8353.jpg\n├── dog.8354.jpg\n├── dog.8355.jpg\n├── dog.8356.jpg\n├── dog.8357.jpg\n├── dog.8358.jpg\n├── dog.8359.jpg\n├── dog.835.jpg\n├── dog.8360.jpg\n├── dog.8361.jpg\n├── dog.8362.jpg\n├── dog.8363.jpg\n├── dog.8364.jpg\n├── dog.8365.jpg\n├── dog.8366.jpg\n├── dog.8367.jpg\n├── dog.8368.jpg\n├── dog.8369.jpg\n├── dog.836.jpg\n├── dog.8370.jpg\n├── dog.8371.jpg\n├── dog.8372.jpg\n├── dog.8373.jpg\n├── dog.8374.jpg\n├── dog.8375.jpg\n├── dog.8376.jpg\n├── dog.8377.jpg\n├── dog.8378.jpg\n├── dog.8379.jpg\n├── dog.837.jpg\n├── dog.8380.jpg\n├── dog.8381.jpg\n├── dog.8382.jpg\n├── dog.8383.jpg\n├── dog.8384.jpg\n├── dog.8385.jpg\n├── dog.8386.jpg\n├── dog.8387.jpg\n├── dog.8388.jpg\n├── dog.8389.jpg\n├── dog.838.jpg\n├── dog.8390.jpg\n├── dog.8391.jpg\n├── dog.8392.jpg\n├── dog.8393.jpg\n├── dog.8394.jpg\n├── dog.8395.jpg\n├── dog.8396.jpg\n├── dog.8397.jpg\n├── dog.8398.jpg\n├── dog.8399.jpg\n├── dog.839.jpg\n├── dog.83.jpg\n├── dog.8400.jpg\n├── dog.8401.jpg\n├── dog.8402.jpg\n├── dog.8403.jpg\n├── dog.8404.jpg\n├── dog.8405.jpg\n├── dog.8406.jpg\n├── dog.8407.jpg\n├── dog.8408.jpg\n├── dog.8409.jpg\n├── dog.840.jpg\n├── dog.8410.jpg\n├── dog.8411.jpg\n├── dog.8412.jpg\n├── dog.8413.jpg\n├── dog.8414.jpg\n├── dog.8415.jpg\n├── dog.8416.jpg\n├── dog.8417.jpg\n├── dog.8418.jpg\n├── dog.8419.jpg\n├── dog.841.jpg\n├── dog.8420.jpg\n├── dog.8421.jpg\n├── dog.8422.jpg\n├── dog.8423.jpg\n├── dog.8424.jpg\n├── dog.8425.jpg\n├── dog.8426.jpg\n├── dog.8427.jpg\n├── dog.8428.jpg\n├── dog.8429.jpg\n├── dog.842.jpg\n├── dog.8430.jpg\n├── dog.8431.jpg\n├── dog.8432.jpg\n├── dog.8433.jpg\n├── dog.8434.jpg\n├── dog.8435.jpg\n├── dog.8436.jpg\n├── dog.8437.jpg\n├── dog.8438.jpg\n├── dog.8439.jpg\n├── dog.843.jpg\n├── dog.8440.jpg\n├── dog.8441.jpg\n├── dog.8442.jpg\n├── dog.8443.jpg\n├── dog.8444.jpg\n├── dog.8445.jpg\n├── dog.8446.jpg\n├── dog.8447.jpg\n├── dog.8448.jpg\n├── dog.8449.jpg\n├── dog.844.jpg\n├── dog.8450.jpg\n├── dog.8451.jpg\n├── dog.8452.jpg\n├── dog.8453.jpg\n├── dog.8454.jpg\n├── dog.8455.jpg\n├── dog.8456.jpg\n├── dog.8457.jpg\n├── dog.8458.jpg\n├── dog.8459.jpg\n├── dog.845.jpg\n├── dog.8460.jpg\n├── dog.8461.jpg\n├── dog.8462.jpg\n├── dog.8463.jpg\n├── dog.8464.jpg\n├── dog.8465.jpg\n├── dog.8466.jpg\n├── dog.8467.jpg\n├── dog.8468.jpg\n├── dog.8469.jpg\n├── dog.846.jpg\n├── dog.8470.jpg\n├── dog.8471.jpg\n├── dog.8472.jpg\n├── dog.8473.jpg\n├── dog.8474.jpg\n├── dog.8475.jpg\n├── dog.8476.jpg\n├── dog.8477.jpg\n├── dog.8478.jpg\n├── dog.8479.jpg\n├── dog.847.jpg\n├── dog.8480.jpg\n├── dog.8481.jpg\n├── dog.8482.jpg\n├── dog.8483.jpg\n├── dog.8484.jpg\n├── dog.8485.jpg\n├── dog.8486.jpg\n├── dog.8487.jpg\n├── dog.8488.jpg\n├── dog.8489.jpg\n├── dog.848.jpg\n├── dog.8490.jpg\n├── dog.8491.jpg\n├── dog.8492.jpg\n├── dog.8493.jpg\n├── dog.8494.jpg\n├── dog.8495.jpg\n├── dog.8496.jpg\n├── dog.8497.jpg\n├── dog.8498.jpg\n├── dog.8499.jpg\n├── dog.849.jpg\n├── dog.84.jpg\n├── dog.8500.jpg\n├── dog.8501.jpg\n├── dog.8502.jpg\n├── dog.8503.jpg\n├── dog.8504.jpg\n├── dog.8505.jpg\n├── dog.8506.jpg\n├── dog.8507.jpg\n├── dog.8508.jpg\n├── dog.8509.jpg\n├── dog.850.jpg\n├── dog.8510.jpg\n├── dog.8511.jpg\n├── dog.8512.jpg\n├── dog.8513.jpg\n├── dog.8514.jpg\n├── dog.8515.jpg\n├── dog.8516.jpg\n├── dog.8517.jpg\n├── dog.8518.jpg\n├── dog.8519.jpg\n├── dog.851.jpg\n├── dog.8520.jpg\n├── dog.8521.jpg\n├── dog.8522.jpg\n├── dog.8523.jpg\n├── dog.8524.jpg\n├── dog.8525.jpg\n├── dog.8526.jpg\n├── dog.8527.jpg\n├── dog.8528.jpg\n├── dog.8529.jpg\n├── dog.852.jpg\n├── dog.8530.jpg\n├── dog.8531.jpg\n├── dog.8532.jpg\n├── dog.8533.jpg\n├── dog.8534.jpg\n├── dog.8535.jpg\n├── dog.8536.jpg\n├── dog.8537.jpg\n├── dog.8538.jpg\n├── dog.8539.jpg\n├── dog.853.jpg\n├── dog.8540.jpg\n├── dog.8541.jpg\n├── dog.8542.jpg\n├── dog.8543.jpg\n├── dog.8544.jpg\n├── dog.8545.jpg\n├── dog.8546.jpg\n├── dog.8547.jpg\n├── dog.8548.jpg\n├── dog.8549.jpg\n├── dog.854.jpg\n├── dog.8550.jpg\n├── dog.8551.jpg\n├── dog.8552.jpg\n├── dog.8553.jpg\n├── dog.8554.jpg\n├── dog.8555.jpg\n├── dog.8556.jpg\n├── dog.8557.jpg\n├── dog.8558.jpg\n├── dog.8559.jpg\n├── dog.855.jpg\n├── dog.8560.jpg\n├── dog.8561.jpg\n├── dog.8562.jpg\n├── dog.8563.jpg\n├── dog.8564.jpg\n├── dog.8565.jpg\n├── dog.8566.jpg\n├── dog.8567.jpg\n├── dog.8568.jpg\n├── dog.8569.jpg\n├── dog.856.jpg\n├── dog.8570.jpg\n├── dog.8571.jpg\n├── dog.8572.jpg\n├── dog.8573.jpg\n├── dog.8574.jpg\n├── dog.8575.jpg\n├── dog.8576.jpg\n├── dog.8577.jpg\n├── dog.8578.jpg\n├── dog.8579.jpg\n├── dog.857.jpg\n├── dog.8580.jpg\n├── dog.8581.jpg\n├── dog.8582.jpg\n├── dog.8583.jpg\n├── dog.8584.jpg\n├── dog.8585.jpg\n├── dog.8586.jpg\n├── dog.8587.jpg\n├── dog.8588.jpg\n├── dog.8589.jpg\n├── dog.858.jpg\n├── dog.8590.jpg\n├── dog.8591.jpg\n├── dog.8592.jpg\n├── dog.8593.jpg\n├── dog.8594.jpg\n├── dog.8595.jpg\n├── dog.8596.jpg\n├── dog.8597.jpg\n├── dog.8598.jpg\n├── dog.8599.jpg\n├── dog.859.jpg\n├── dog.85.jpg\n├── dog.8600.jpg\n├── dog.8601.jpg\n├── dog.8602.jpg\n├── dog.8603.jpg\n├── dog.8604.jpg\n├── dog.8605.jpg\n├── dog.8606.jpg\n├── dog.8607.jpg\n├── dog.8608.jpg\n├── dog.8609.jpg\n├── dog.860.jpg\n├── dog.8610.jpg\n├── dog.8611.jpg\n├── dog.8612.jpg\n├── dog.8613.jpg\n├── dog.8614.jpg\n├── dog.8615.jpg\n├── dog.8616.jpg\n├── dog.8617.jpg\n├── dog.8618.jpg\n├── dog.8619.jpg\n├── dog.861.jpg\n├── dog.8620.jpg\n├── dog.8621.jpg\n├── dog.8622.jpg\n├── dog.8623.jpg\n├── dog.8624.jpg\n├── dog.8625.jpg\n├── dog.8626.jpg\n├── dog.8627.jpg\n├── dog.8628.jpg\n├── dog.8629.jpg\n├── dog.862.jpg\n├── dog.8630.jpg\n├── dog.8631.jpg\n├── dog.8632.jpg\n├── dog.8633.jpg\n├── dog.8634.jpg\n├── dog.8635.jpg\n├── dog.8636.jpg\n├── dog.8637.jpg\n├── dog.8638.jpg\n├── dog.8639.jpg\n├── dog.863.jpg\n├── dog.8640.jpg\n├── dog.8641.jpg\n├── dog.8642.jpg\n├── dog.8643.jpg\n├── dog.8644.jpg\n├── dog.8645.jpg\n├── dog.8646.jpg\n├── dog.8647.jpg\n├── dog.8648.jpg\n├── dog.8649.jpg\n├── dog.864.jpg\n├── dog.8650.jpg\n├── dog.8651.jpg\n├── dog.8652.jpg\n├── dog.8653.jpg\n├── dog.8654.jpg\n├── dog.8655.jpg\n├── dog.8656.jpg\n├── dog.8657.jpg\n├── dog.8658.jpg\n├── dog.8659.jpg\n├── dog.865.jpg\n├── dog.8660.jpg\n├── dog.8661.jpg\n├── dog.8662.jpg\n├── dog.8663.jpg\n├── dog.8664.jpg\n├── dog.8665.jpg\n├── dog.8666.jpg\n├── dog.8667.jpg\n├── dog.8668.jpg\n├── dog.8669.jpg\n├── dog.866.jpg\n├── dog.8670.jpg\n├── dog.8671.jpg\n├── dog.8672.jpg\n├── dog.8673.jpg\n├── dog.8674.jpg\n├── dog.8675.jpg\n├── dog.8676.jpg\n├── dog.8677.jpg\n├── dog.8678.jpg\n├── dog.8679.jpg\n├── dog.867.jpg\n├── dog.8680.jpg\n├── dog.8681.jpg\n├── dog.8682.jpg\n├── dog.8683.jpg\n├── dog.8684.jpg\n├── dog.8685.jpg\n├── dog.8686.jpg\n├── dog.8687.jpg\n├── dog.8688.jpg\n├── dog.8689.jpg\n├── dog.868.jpg\n├── dog.8690.jpg\n├── dog.8691.jpg\n├── dog.8692.jpg\n├── dog.8693.jpg\n├── dog.8694.jpg\n├── dog.8695.jpg\n├── dog.8696.jpg\n├── dog.8697.jpg\n├── dog.8698.jpg\n├── dog.8699.jpg\n├── dog.869.jpg\n├── dog.86.jpg\n├── dog.8700.jpg\n├── dog.8701.jpg\n├── dog.8702.jpg\n├── dog.8703.jpg\n├── dog.8704.jpg\n├── dog.8705.jpg\n├── dog.8706.jpg\n├── dog.8707.jpg\n├── dog.8708.jpg\n├── dog.8709.jpg\n├── dog.870.jpg\n├── dog.8710.jpg\n├── dog.8711.jpg\n├── dog.8712.jpg\n├── dog.8713.jpg\n├── dog.8714.jpg\n├── dog.8715.jpg\n├── dog.8716.jpg\n├── dog.8717.jpg\n├── dog.8718.jpg\n├── dog.8719.jpg\n├── dog.871.jpg\n├── dog.8720.jpg\n├── dog.8721.jpg\n├── dog.8722.jpg\n├── dog.8723.jpg\n├── dog.8724.jpg\n├── dog.8725.jpg\n├── dog.8726.jpg\n├── dog.8727.jpg\n├── dog.8728.jpg\n├── dog.8729.jpg\n├── dog.872.jpg\n├── dog.8730.jpg\n├── dog.8731.jpg\n├── dog.8732.jpg\n├── dog.8733.jpg\n├── dog.8734.jpg\n├── dog.8735.jpg\n├── dog.8736.jpg\n├── dog.8737.jpg\n├── dog.8738.jpg\n├── dog.8739.jpg\n├── dog.873.jpg\n├── dog.8740.jpg\n├── dog.8741.jpg\n├── dog.8742.jpg\n├── dog.8743.jpg\n├── dog.8744.jpg\n├── dog.8745.jpg\n├── dog.8746.jpg\n├── dog.8747.jpg\n├── dog.8748.jpg\n├── dog.8749.jpg\n├── dog.874.jpg\n├── dog.8750.jpg\n├── dog.8751.jpg\n├── dog.8752.jpg\n├── dog.8753.jpg\n├── dog.8754.jpg\n├── dog.8755.jpg\n├── dog.8756.jpg\n├── dog.8757.jpg\n├── dog.8758.jpg\n├── dog.8759.jpg\n├── dog.875.jpg\n├── dog.8760.jpg\n├── dog.8761.jpg\n├── dog.8762.jpg\n├── dog.8763.jpg\n├── dog.8764.jpg\n├── dog.8765.jpg\n├── dog.8766.jpg\n├── dog.8767.jpg\n├── dog.8768.jpg\n├── dog.8769.jpg\n├── dog.876.jpg\n├── dog.8770.jpg\n├── dog.8771.jpg\n├── dog.8772.jpg\n├── dog.8773.jpg\n├── dog.8774.jpg\n├── dog.8775.jpg\n├── dog.8776.jpg\n├── dog.8777.jpg\n├── dog.8778.jpg\n├── dog.8779.jpg\n├── dog.877.jpg\n├── dog.8780.jpg\n├── dog.8781.jpg\n├── dog.8782.jpg\n├── dog.8783.jpg\n├── dog.8784.jpg\n├── dog.8785.jpg\n├── dog.8786.jpg\n├── dog.8787.jpg\n├── dog.8788.jpg\n├── dog.8789.jpg\n├── dog.878.jpg\n├── dog.8790.jpg\n├── dog.8791.jpg\n├── dog.8792.jpg\n├── dog.8793.jpg\n├── dog.8794.jpg\n├── dog.8795.jpg\n├── dog.8796.jpg\n├── dog.8797.jpg\n├── dog.8798.jpg\n├── dog.8799.jpg\n├── dog.879.jpg\n├── dog.87.jpg\n├── dog.8800.jpg\n├── dog.8801.jpg\n├── dog.8802.jpg\n├── dog.8803.jpg\n├── dog.8804.jpg\n├── dog.8805.jpg\n├── dog.8806.jpg\n├── dog.8807.jpg\n├── dog.8808.jpg\n├── dog.8809.jpg\n├── dog.880.jpg\n├── dog.8810.jpg\n├── dog.8811.jpg\n├── dog.8812.jpg\n├── dog.8813.jpg\n├── dog.8814.jpg\n├── dog.8815.jpg\n├── dog.8816.jpg\n├── dog.8817.jpg\n├── dog.8818.jpg\n├── dog.8819.jpg\n├── dog.881.jpg\n├── dog.8820.jpg\n├── dog.8821.jpg\n├── dog.8822.jpg\n├── dog.8823.jpg\n├── dog.8824.jpg\n├── dog.8825.jpg\n├── dog.8826.jpg\n├── dog.8827.jpg\n├── dog.8828.jpg\n├── dog.8829.jpg\n├── dog.882.jpg\n├── dog.8830.jpg\n├── dog.8831.jpg\n├── dog.8832.jpg\n├── dog.8833.jpg\n├── dog.8834.jpg\n├── dog.8835.jpg\n├── dog.8836.jpg\n├── dog.8837.jpg\n├── dog.8838.jpg\n├── dog.8839.jpg\n├── dog.883.jpg\n├── dog.8840.jpg\n├── dog.8841.jpg\n├── dog.8842.jpg\n├── dog.8843.jpg\n├── dog.8844.jpg\n├── dog.8845.jpg\n├── dog.8846.jpg\n├── dog.8847.jpg\n├── dog.8848.jpg\n├── dog.8849.jpg\n├── dog.884.jpg\n├── dog.8850.jpg\n├── dog.8851.jpg\n├── dog.8852.jpg\n├── dog.8853.jpg\n├── dog.8854.jpg\n├── dog.8855.jpg\n├── dog.8856.jpg\n├── dog.8857.jpg\n├── dog.8858.jpg\n├── dog.8859.jpg\n├── dog.885.jpg\n├── dog.8860.jpg\n├── dog.8861.jpg\n├── dog.8862.jpg\n├── dog.8863.jpg\n├── dog.8864.jpg\n├── dog.8865.jpg\n├── dog.8866.jpg\n├── dog.8867.jpg\n├── dog.8868.jpg\n├── dog.8869.jpg\n├── dog.886.jpg\n├── dog.8870.jpg\n├── dog.8871.jpg\n├── dog.8872.jpg\n├── dog.8873.jpg\n├── dog.8874.jpg\n├── dog.8875.jpg\n├── dog.8876.jpg\n├── dog.8877.jpg\n├── dog.8878.jpg\n├── dog.8879.jpg\n├── dog.887.jpg\n├── dog.8880.jpg\n├── dog.8881.jpg\n├── dog.8882.jpg\n├── dog.8883.jpg\n├── dog.8884.jpg\n├── dog.8885.jpg\n├── dog.8886.jpg\n├── dog.8887.jpg\n├── dog.8888.jpg\n├── dog.8889.jpg\n├── dog.888.jpg\n├── dog.8890.jpg\n├── dog.8891.jpg\n├── dog.8892.jpg\n├── dog.8893.jpg\n├── dog.8894.jpg\n├── dog.8895.jpg\n├── dog.8896.jpg\n├── dog.8897.jpg\n├── dog.8898.jpg\n├── dog.8899.jpg\n├── dog.889.jpg\n├── dog.88.jpg\n├── dog.8900.jpg\n├── dog.8901.jpg\n├── dog.8902.jpg\n├── dog.8903.jpg\n├── dog.8904.jpg\n├── dog.8905.jpg\n├── dog.8906.jpg\n├── dog.8907.jpg\n├── dog.8908.jpg\n├── dog.8909.jpg\n├── dog.890.jpg\n├── dog.8910.jpg\n├── dog.8911.jpg\n├── dog.8912.jpg\n├── dog.8913.jpg\n├── dog.8914.jpg\n├── dog.8915.jpg\n├── dog.8916.jpg\n├── dog.8917.jpg\n├── dog.8918.jpg\n├── dog.8919.jpg\n├── dog.891.jpg\n├── dog.8920.jpg\n├── dog.8921.jpg\n├── dog.8922.jpg\n├── dog.8923.jpg\n├── dog.8924.jpg\n├── dog.8925.jpg\n├── dog.8926.jpg\n├── dog.8927.jpg\n├── dog.8928.jpg\n├── dog.8929.jpg\n├── dog.892.jpg\n├── dog.8930.jpg\n├── dog.8931.jpg\n├── dog.8932.jpg\n├── dog.8933.jpg\n├── dog.8934.jpg\n├── dog.8935.jpg\n├── dog.8936.jpg\n├── dog.8937.jpg\n├── dog.8938.jpg\n├── dog.8939.jpg\n├── dog.893.jpg\n├── dog.8940.jpg\n├── dog.8941.jpg\n├── dog.8942.jpg\n├── dog.8943.jpg\n├── dog.8944.jpg\n├── dog.8945.jpg\n├── dog.8946.jpg\n├── dog.8947.jpg\n├── dog.8948.jpg\n├── dog.8949.jpg\n├── dog.894.jpg\n├── dog.8950.jpg\n├── dog.8951.jpg\n├── dog.8952.jpg\n├── dog.8953.jpg\n├── dog.8954.jpg\n├── dog.8955.jpg\n├── dog.8956.jpg\n├── dog.8957.jpg\n├── dog.8958.jpg\n├── dog.8959.jpg\n├── dog.895.jpg\n├── dog.8960.jpg\n├── dog.8961.jpg\n├── dog.8962.jpg\n├── dog.8963.jpg\n├── dog.8964.jpg\n├── dog.8965.jpg\n├── dog.8966.jpg\n├── dog.8967.jpg\n├── dog.8968.jpg\n├── dog.8969.jpg\n├── dog.896.jpg\n├── dog.8970.jpg\n├── dog.8971.jpg\n├── dog.8972.jpg\n├── dog.8973.jpg\n├── dog.8974.jpg\n├── dog.8975.jpg\n├── dog.8976.jpg\n├── dog.8977.jpg\n├── dog.8978.jpg\n├── dog.8979.jpg\n├── dog.897.jpg\n├── dog.8980.jpg\n├── dog.8981.jpg\n├── dog.8982.jpg\n├── dog.8983.jpg\n├── dog.8984.jpg\n├── dog.8985.jpg\n├── dog.8986.jpg\n├── dog.8987.jpg\n├── dog.8988.jpg\n├── dog.8989.jpg\n├── dog.898.jpg\n├── dog.8990.jpg\n├── dog.8991.jpg\n├── dog.8992.jpg\n├── dog.8993.jpg\n├── dog.8994.jpg\n├── dog.8995.jpg\n├── dog.8996.jpg\n├── dog.8997.jpg\n├── dog.8998.jpg\n├── dog.8999.jpg\n├── dog.899.jpg\n├── dog.89.jpg\n├── dog.8.jpg\n├── dog.9000.jpg\n├── dog.9001.jpg\n├── dog.9002.jpg\n├── dog.9003.jpg\n├── dog.9004.jpg\n├── dog.9005.jpg\n├── dog.9006.jpg\n├── dog.9007.jpg\n├── dog.9008.jpg\n├── dog.9009.jpg\n├── dog.900.jpg\n├── dog.9010.jpg\n├── dog.9011.jpg\n├── dog.9012.jpg\n├── dog.9013.jpg\n├── dog.9014.jpg\n├── dog.9015.jpg\n├── dog.9016.jpg\n├── dog.9017.jpg\n├── dog.9018.jpg\n├── dog.9019.jpg\n├── dog.901.jpg\n├── dog.9020.jpg\n├── dog.9021.jpg\n├── dog.9022.jpg\n├── dog.9023.jpg\n├── dog.9024.jpg\n├── dog.9025.jpg\n├── dog.9026.jpg\n├── dog.9027.jpg\n├── dog.9028.jpg\n├── dog.9029.jpg\n├── dog.902.jpg\n├── dog.9030.jpg\n├── dog.9031.jpg\n├── dog.9032.jpg\n├── dog.9033.jpg\n├── dog.9034.jpg\n├── dog.9035.jpg\n├── dog.9036.jpg\n├── dog.9037.jpg\n├── dog.9038.jpg\n├── dog.9039.jpg\n├── dog.903.jpg\n├── dog.9040.jpg\n├── dog.9041.jpg\n├── dog.9042.jpg\n├── dog.9043.jpg\n├── dog.9044.jpg\n├── dog.9045.jpg\n├── dog.9046.jpg\n├── dog.9047.jpg\n├── dog.9048.jpg\n├── dog.9049.jpg\n├── dog.904.jpg\n├── dog.9050.jpg\n├── dog.9051.jpg\n├── dog.9052.jpg\n├── dog.9053.jpg\n├── dog.9054.jpg\n├── dog.9055.jpg\n├── dog.9056.jpg\n├── dog.9057.jpg\n├── dog.9058.jpg\n├── dog.9059.jpg\n├── dog.905.jpg\n├── dog.9060.jpg\n├── dog.9061.jpg\n├── dog.9062.jpg\n├── dog.9063.jpg\n├── dog.9064.jpg\n├── dog.9065.jpg\n├── dog.9066.jpg\n├── dog.9067.jpg\n├── dog.9068.jpg\n├── dog.9069.jpg\n├── dog.906.jpg\n├── dog.9070.jpg\n├── dog.9071.jpg\n├── dog.9072.jpg\n├── dog.9073.jpg\n├── dog.9074.jpg\n├── dog.9075.jpg\n├── dog.9076.jpg\n├── dog.9077.jpg\n├── dog.9078.jpg\n├── dog.9079.jpg\n├── dog.907.jpg\n├── dog.9080.jpg\n├── dog.9081.jpg\n├── dog.9082.jpg\n├── dog.9083.jpg\n├── dog.9084.jpg\n├── dog.9085.jpg\n├── dog.9086.jpg\n├── dog.9087.jpg\n├── dog.9088.jpg\n├── dog.9089.jpg\n├── dog.908.jpg\n├── dog.9090.jpg\n├── dog.9091.jpg\n├── dog.9092.jpg\n├── dog.9093.jpg\n├── dog.9094.jpg\n├── dog.9095.jpg\n├── dog.9096.jpg\n├── dog.9097.jpg\n├── dog.9098.jpg\n├── dog.9099.jpg\n├── dog.909.jpg\n├── dog.90.jpg\n├── dog.9100.jpg\n├── dog.9101.jpg\n├── dog.9102.jpg\n├── dog.9103.jpg\n├── dog.9104.jpg\n├── dog.9105.jpg\n├── dog.9106.jpg\n├── dog.9107.jpg\n├── dog.9108.jpg\n├── dog.9109.jpg\n├── dog.910.jpg\n├── dog.9110.jpg\n├── dog.9111.jpg\n├── dog.9112.jpg\n├── dog.9113.jpg\n├── dog.9114.jpg\n├── dog.9115.jpg\n├── dog.9116.jpg\n├── dog.9117.jpg\n├── dog.9118.jpg\n├── dog.9119.jpg\n├── dog.911.jpg\n├── dog.9120.jpg\n├── dog.9121.jpg\n├── dog.9122.jpg\n├── dog.9123.jpg\n├── dog.9124.jpg\n├── dog.9125.jpg\n├── dog.9126.jpg\n├── dog.9127.jpg\n├── dog.9128.jpg\n├── dog.9129.jpg\n├── dog.912.jpg\n├── dog.9130.jpg\n├── dog.9131.jpg\n├── dog.9132.jpg\n├── dog.9133.jpg\n├── dog.9134.jpg\n├── dog.9135.jpg\n├── dog.9136.jpg\n├── dog.9137.jpg\n├── dog.9138.jpg\n├── dog.9139.jpg\n├── dog.913.jpg\n├── dog.9140.jpg\n├── dog.9141.jpg\n├── dog.9142.jpg\n├── dog.9143.jpg\n├── dog.9144.jpg\n├── dog.9145.jpg\n├── dog.9146.jpg\n├── dog.9147.jpg\n├── dog.9148.jpg\n├── dog.9149.jpg\n├── dog.914.jpg\n├── dog.9150.jpg\n├── dog.9151.jpg\n├── dog.9152.jpg\n├── dog.9153.jpg\n├── dog.9154.jpg\n├── dog.9155.jpg\n├── dog.9156.jpg\n├── dog.9157.jpg\n├── dog.9158.jpg\n├── dog.9159.jpg\n├── dog.915.jpg\n├── dog.9160.jpg\n├── dog.9161.jpg\n├── dog.9162.jpg\n├── dog.9163.jpg\n├── dog.9164.jpg\n├── dog.9165.jpg\n├── dog.9166.jpg\n├── dog.9167.jpg\n├── dog.9168.jpg\n├── dog.9169.jpg\n├── dog.916.jpg\n├── dog.9170.jpg\n├── dog.9171.jpg\n├── dog.9172.jpg\n├── dog.9173.jpg\n├── dog.9174.jpg\n├── dog.9175.jpg\n├── dog.9176.jpg\n├── dog.9177.jpg\n├── dog.9178.jpg\n├── dog.9179.jpg\n├── dog.917.jpg\n├── dog.9180.jpg\n├── dog.9181.jpg\n├── dog.9182.jpg\n├── dog.9183.jpg\n├── dog.9184.jpg\n├── dog.9185.jpg\n├── dog.9186.jpg\n├── dog.9187.jpg\n├── dog.9188.jpg\n├── dog.9189.jpg\n├── dog.918.jpg\n├── dog.9190.jpg\n├── dog.9191.jpg\n├── dog.9192.jpg\n├── dog.9193.jpg\n├── dog.9194.jpg\n├── dog.9195.jpg\n├── dog.9196.jpg\n├── dog.9197.jpg\n├── dog.9198.jpg\n├── dog.9199.jpg\n├── dog.919.jpg\n├── dog.91.jpg\n├── dog.9200.jpg\n├── dog.9201.jpg\n├── dog.9202.jpg\n├── dog.9203.jpg\n├── dog.9204.jpg\n├── dog.9205.jpg\n├── dog.9206.jpg\n├── dog.9207.jpg\n├── dog.9208.jpg\n├── dog.9209.jpg\n├── dog.920.jpg\n├── dog.9210.jpg\n├── dog.9211.jpg\n├── dog.9212.jpg\n├── dog.9213.jpg\n├── dog.9214.jpg\n├── dog.9215.jpg\n├── dog.9216.jpg\n├── dog.9217.jpg\n├── dog.9218.jpg\n├── dog.9219.jpg\n├── dog.921.jpg\n├── dog.9220.jpg\n├── dog.9221.jpg\n├── dog.9222.jpg\n├── dog.9223.jpg\n├── dog.9224.jpg\n├── dog.9225.jpg\n├── dog.9226.jpg\n├── dog.9227.jpg\n├── dog.9228.jpg\n├── dog.9229.jpg\n├── dog.922.jpg\n├── dog.9230.jpg\n├── dog.9231.jpg\n├── dog.9232.jpg\n├── dog.9233.jpg\n├── dog.9234.jpg\n├── dog.9235.jpg\n├── dog.9236.jpg\n├── dog.9237.jpg\n├── dog.9238.jpg\n├── dog.9239.jpg\n├── dog.923.jpg\n├── dog.9240.jpg\n├── dog.9241.jpg\n├── dog.9242.jpg\n├── dog.9243.jpg\n├── dog.9244.jpg\n├── dog.9245.jpg\n├── dog.9246.jpg\n├── dog.9247.jpg\n├── dog.9248.jpg\n├── dog.9249.jpg\n├── dog.924.jpg\n├── dog.9250.jpg\n├── dog.9251.jpg\n├── dog.9252.jpg\n├── dog.9253.jpg\n├── dog.9254.jpg\n├── dog.9255.jpg\n├── dog.9256.jpg\n├── dog.9257.jpg\n├── dog.9258.jpg\n├── dog.9259.jpg\n├── dog.925.jpg\n├── dog.9260.jpg\n├── dog.9261.jpg\n├── dog.9262.jpg\n├── dog.9263.jpg\n├── dog.9264.jpg\n├── dog.9265.jpg\n├── dog.9266.jpg\n├── dog.9267.jpg\n├── dog.9268.jpg\n├── dog.9269.jpg\n├── dog.926.jpg\n├── dog.9270.jpg\n├── dog.9271.jpg\n├── dog.9272.jpg\n├── dog.9273.jpg\n├── dog.9274.jpg\n├── dog.9275.jpg\n├── dog.9276.jpg\n├── dog.9277.jpg\n├── dog.9278.jpg\n├── dog.9279.jpg\n├── dog.927.jpg\n├── dog.9280.jpg\n├── dog.9281.jpg\n├── dog.9282.jpg\n├── dog.9283.jpg\n├── dog.9284.jpg\n├── dog.9285.jpg\n├── dog.9286.jpg\n├── dog.9287.jpg\n├── dog.9288.jpg\n├── dog.9289.jpg\n├── dog.928.jpg\n├── dog.9290.jpg\n├── dog.9291.jpg\n├── dog.9292.jpg\n├── dog.9293.jpg\n├── dog.9294.jpg\n├── dog.9295.jpg\n├── dog.9296.jpg\n├── dog.9297.jpg\n├── dog.9298.jpg\n├── dog.9299.jpg\n├── dog.929.jpg\n├── dog.92.jpg\n├── dog.9300.jpg\n├── dog.9301.jpg\n├── dog.9302.jpg\n├── dog.9303.jpg\n├── dog.9304.jpg\n├── dog.9305.jpg\n├── dog.9306.jpg\n├── dog.9307.jpg\n├── dog.9308.jpg\n├── dog.9309.jpg\n├── dog.930.jpg\n├── dog.9310.jpg\n├── dog.9311.jpg\n├── dog.9312.jpg\n├── dog.9313.jpg\n├── dog.9314.jpg\n├── dog.9315.jpg\n├── dog.9316.jpg\n├── dog.9317.jpg\n├── dog.9318.jpg\n├── dog.9319.jpg\n├── dog.931.jpg\n├── dog.9320.jpg\n├── dog.9321.jpg\n├── dog.9322.jpg\n├── dog.9323.jpg\n├── dog.9324.jpg\n├── dog.9325.jpg\n├── dog.9326.jpg\n├── dog.9327.jpg\n├── dog.9328.jpg\n├── dog.9329.jpg\n├── dog.932.jpg\n├── dog.9330.jpg\n├── dog.9331.jpg\n├── dog.9332.jpg\n├── dog.9333.jpg\n├── dog.9334.jpg\n├── dog.9335.jpg\n├── dog.9336.jpg\n├── dog.9337.jpg\n├── dog.9338.jpg\n├── dog.9339.jpg\n├── dog.933.jpg\n├── dog.9340.jpg\n├── dog.9341.jpg\n├── dog.9342.jpg\n├── dog.9343.jpg\n├── dog.9344.jpg\n├── dog.9345.jpg\n├── dog.9346.jpg\n├── dog.9347.jpg\n├── dog.9348.jpg\n├── dog.9349.jpg\n├── dog.934.jpg\n├── dog.9350.jpg\n├── dog.9351.jpg\n├── dog.9352.jpg\n├── dog.9353.jpg\n├── dog.9354.jpg\n├── dog.9355.jpg\n├── dog.9356.jpg\n├── dog.9357.jpg\n├── dog.9358.jpg\n├── dog.9359.jpg\n├── dog.935.jpg\n├── dog.9360.jpg\n├── dog.9361.jpg\n├── dog.9362.jpg\n├── dog.9363.jpg\n├── dog.9364.jpg\n├── dog.9365.jpg\n├── dog.9366.jpg\n├── dog.9367.jpg\n├── dog.9368.jpg\n├── dog.9369.jpg\n├── dog.936.jpg\n├── dog.9370.jpg\n├── dog.9371.jpg\n├── dog.9372.jpg\n├── dog.9373.jpg\n├── dog.9374.jpg\n├── dog.9375.jpg\n├── dog.9376.jpg\n├── dog.9377.jpg\n├── dog.9378.jpg\n├── dog.9379.jpg\n├── dog.937.jpg\n├── dog.9380.jpg\n├── dog.9381.jpg\n├── dog.9382.jpg\n├── dog.9383.jpg\n├── dog.9384.jpg\n├── dog.9385.jpg\n├── dog.9386.jpg\n├── dog.9387.jpg\n├── dog.9388.jpg\n├── dog.9389.jpg\n├── dog.938.jpg\n├── dog.9390.jpg\n├── dog.9391.jpg\n├── dog.9392.jpg\n├── dog.9393.jpg\n├── dog.9394.jpg\n├── dog.9395.jpg\n├── dog.9396.jpg\n├── dog.9397.jpg\n├── dog.9398.jpg\n├── dog.9399.jpg\n├── dog.939.jpg\n├── dog.93.jpg\n├── dog.9400.jpg\n├── dog.9401.jpg\n├── dog.9402.jpg\n├── dog.9403.jpg\n├── dog.9404.jpg\n├── dog.9405.jpg\n├── dog.9406.jpg\n├── dog.9407.jpg\n├── dog.9408.jpg\n├── dog.9409.jpg\n├── dog.940.jpg\n├── dog.9410.jpg\n├── dog.9411.jpg\n├── dog.9412.jpg\n├── dog.9413.jpg\n├── dog.9414.jpg\n├── dog.9415.jpg\n├── dog.9416.jpg\n├── dog.9417.jpg\n├── dog.9418.jpg\n├── dog.9419.jpg\n├── dog.941.jpg\n├── dog.9420.jpg\n├── dog.9421.jpg\n├── dog.9422.jpg\n├── dog.9423.jpg\n├── dog.9424.jpg\n├── dog.9425.jpg\n├── dog.9426.jpg\n├── dog.9427.jpg\n├── dog.9428.jpg\n├── dog.9429.jpg\n├── dog.942.jpg\n├── dog.9430.jpg\n├── dog.9431.jpg\n├── dog.9432.jpg\n├── dog.9433.jpg\n├── dog.9434.jpg\n├── dog.9435.jpg\n├── dog.9436.jpg\n├── dog.9437.jpg\n├── dog.9438.jpg\n├── dog.9439.jpg\n├── dog.943.jpg\n├── dog.9440.jpg\n├── dog.9441.jpg\n├── dog.9442.jpg\n├── dog.9443.jpg\n├── dog.9444.jpg\n├── dog.9445.jpg\n├── dog.9446.jpg\n├── dog.9447.jpg\n├── dog.9448.jpg\n├── dog.9449.jpg\n├── dog.944.jpg\n├── dog.9450.jpg\n├── dog.9451.jpg\n├── dog.9452.jpg\n├── dog.9453.jpg\n├── dog.9454.jpg\n├── dog.9455.jpg\n├── dog.9456.jpg\n├── dog.9457.jpg\n├── dog.9458.jpg\n├── dog.9459.jpg\n├── dog.945.jpg\n├── dog.9460.jpg\n├── dog.9461.jpg\n├── dog.9462.jpg\n├── dog.9463.jpg\n├── dog.9464.jpg\n├── dog.9465.jpg\n├── dog.9466.jpg\n├── dog.9467.jpg\n├── dog.9468.jpg\n├── dog.9469.jpg\n├── dog.946.jpg\n├── dog.9470.jpg\n├── dog.9471.jpg\n├── dog.9472.jpg\n├── dog.9473.jpg\n├── dog.9474.jpg\n├── dog.9475.jpg\n├── dog.9476.jpg\n├── dog.9477.jpg\n├── dog.9478.jpg\n├── dog.9479.jpg\n├── dog.947.jpg\n├── dog.9480.jpg\n├── dog.9481.jpg\n├── dog.9482.jpg\n├── dog.9483.jpg\n├── dog.9484.jpg\n├── dog.9485.jpg\n├── dog.9486.jpg\n├── dog.9487.jpg\n├── dog.9488.jpg\n├── dog.9489.jpg\n├── dog.948.jpg\n├── dog.9490.jpg\n├── dog.9491.jpg\n├── dog.9492.jpg\n├── dog.9493.jpg\n├── dog.9494.jpg\n├── dog.9495.jpg\n├── dog.9496.jpg\n├── dog.9497.jpg\n├── dog.9498.jpg\n├── dog.9499.jpg\n├── dog.949.jpg\n├── dog.94.jpg\n├── dog.9500.jpg\n├── dog.9501.jpg\n├── dog.9502.jpg\n├── dog.9503.jpg\n├── dog.9504.jpg\n├── dog.9505.jpg\n├── dog.9506.jpg\n├── dog.9507.jpg\n├── dog.9508.jpg\n├── dog.9509.jpg\n├── dog.950.jpg\n├── dog.9510.jpg\n├── dog.9511.jpg\n├── dog.9512.jpg\n├── dog.9513.jpg\n├── dog.9514.jpg\n├── dog.9515.jpg\n├── dog.9516.jpg\n├── dog.9517.jpg\n├── dog.9518.jpg\n├── dog.9519.jpg\n├── dog.951.jpg\n├── dog.9520.jpg\n├── dog.9521.jpg\n├── dog.9522.jpg\n├── dog.9523.jpg\n├── dog.9524.jpg\n├── dog.9525.jpg\n├── dog.9526.jpg\n├── dog.9527.jpg\n├── dog.9528.jpg\n├── dog.9529.jpg\n├── dog.952.jpg\n├── dog.9530.jpg\n├── dog.9531.jpg\n├── dog.9532.jpg\n├── dog.9533.jpg\n├── dog.9534.jpg\n├── dog.9535.jpg\n├── dog.9536.jpg\n├── dog.9537.jpg\n├── dog.9538.jpg\n├── dog.9539.jpg\n├── dog.953.jpg\n├── dog.9540.jpg\n├── dog.9541.jpg\n├── dog.9542.jpg\n├── dog.9543.jpg\n├── dog.9544.jpg\n├── dog.9545.jpg\n├── dog.9546.jpg\n├── dog.9547.jpg\n├── dog.9548.jpg\n├── dog.9549.jpg\n├── dog.954.jpg\n├── dog.9550.jpg\n├── dog.9551.jpg\n├── dog.9552.jpg\n├── dog.9553.jpg\n├── dog.9554.jpg\n├── dog.9555.jpg\n├── dog.9556.jpg\n├── dog.9557.jpg\n├── dog.9558.jpg\n├── dog.9559.jpg\n├── dog.955.jpg\n├── dog.9560.jpg\n├── dog.9561.jpg\n├── dog.9562.jpg\n├── dog.9563.jpg\n├── dog.9564.jpg\n├── dog.9565.jpg\n├── dog.9566.jpg\n├── dog.9567.jpg\n├── dog.9568.jpg\n├── dog.9569.jpg\n├── dog.956.jpg\n├── dog.9570.jpg\n├── dog.9571.jpg\n├── dog.9572.jpg\n├── dog.9573.jpg\n├── dog.9574.jpg\n├── dog.9575.jpg\n├── dog.9576.jpg\n├── dog.9577.jpg\n├── dog.9578.jpg\n├── dog.9579.jpg\n├── dog.957.jpg\n├── dog.9580.jpg\n├── dog.9581.jpg\n├── dog.9582.jpg\n├── dog.9583.jpg\n├── dog.9584.jpg\n├── dog.9585.jpg\n├── dog.9586.jpg\n├── dog.9587.jpg\n├── dog.9588.jpg\n├── dog.9589.jpg\n├── dog.958.jpg\n├── dog.9590.jpg\n├── dog.9591.jpg\n├── dog.9592.jpg\n├── dog.9593.jpg\n├── dog.9594.jpg\n├── dog.9595.jpg\n├── dog.9596.jpg\n├── dog.9597.jpg\n├── dog.9598.jpg\n├── dog.9599.jpg\n├── dog.959.jpg\n├── dog.95.jpg\n├── dog.9600.jpg\n├── dog.9601.jpg\n├── dog.9602.jpg\n├── dog.9603.jpg\n├── dog.9604.jpg\n├── dog.9605.jpg\n├── dog.9606.jpg\n├── dog.9607.jpg\n├── dog.9608.jpg\n├── dog.9609.jpg\n├── dog.960.jpg\n├── dog.9610.jpg\n├── dog.9611.jpg\n├── dog.9612.jpg\n├── dog.9613.jpg\n├── dog.9614.jpg\n├── dog.9615.jpg\n├── dog.9616.jpg\n├── dog.9617.jpg\n├── dog.9618.jpg\n├── dog.9619.jpg\n├── dog.961.jpg\n├── dog.9620.jpg\n├── dog.9621.jpg\n├── dog.9622.jpg\n├── dog.9623.jpg\n├── dog.9624.jpg\n├── dog.9625.jpg\n├── dog.9626.jpg\n├── dog.9627.jpg\n├── dog.9628.jpg\n├── dog.9629.jpg\n├── dog.962.jpg\n├── dog.9630.jpg\n├── dog.9631.jpg\n├── dog.9632.jpg\n├── dog.9633.jpg\n├── dog.9634.jpg\n├── dog.9635.jpg\n├── dog.9636.jpg\n├── dog.9637.jpg\n├── dog.9638.jpg\n├── dog.9639.jpg\n├── dog.963.jpg\n├── dog.9640.jpg\n├── dog.9641.jpg\n├── dog.9642.jpg\n├── dog.9643.jpg\n├── dog.9644.jpg\n├── dog.9645.jpg\n├── dog.9646.jpg\n├── dog.9647.jpg\n├── dog.9648.jpg\n├── dog.9649.jpg\n├── dog.964.jpg\n├── dog.9650.jpg\n├── dog.9651.jpg\n├── dog.9652.jpg\n├── dog.9653.jpg\n├── dog.9654.jpg\n├── dog.9655.jpg\n├── dog.9656.jpg\n├── dog.9657.jpg\n├── dog.9658.jpg\n├── dog.9659.jpg\n├── dog.965.jpg\n├── dog.9660.jpg\n├── dog.9661.jpg\n├── dog.9662.jpg\n├── dog.9663.jpg\n├── dog.9664.jpg\n├── dog.9665.jpg\n├── dog.9666.jpg\n├── dog.9667.jpg\n├── dog.9668.jpg\n├── dog.9669.jpg\n├── dog.966.jpg\n├── dog.9670.jpg\n├── dog.9671.jpg\n├── dog.9672.jpg\n├── dog.9673.jpg\n├── dog.9674.jpg\n├── dog.9675.jpg\n├── dog.9676.jpg\n├── dog.9677.jpg\n├── dog.9678.jpg\n├── dog.9679.jpg\n├── dog.967.jpg\n├── dog.9680.jpg\n├── dog.9681.jpg\n├── dog.9682.jpg\n├── dog.9683.jpg\n├── dog.9684.jpg\n├── dog.9685.jpg\n├── dog.9686.jpg\n├── dog.9687.jpg\n├── dog.9688.jpg\n├── dog.9689.jpg\n├── dog.968.jpg\n├── dog.9690.jpg\n├── dog.9691.jpg\n├── dog.9692.jpg\n├── dog.9693.jpg\n├── dog.9694.jpg\n├── dog.9695.jpg\n├── dog.9696.jpg\n├── dog.9697.jpg\n├── dog.9698.jpg\n├── dog.9699.jpg\n├── dog.969.jpg\n├── dog.96.jpg\n├── dog.9700.jpg\n├── dog.9701.jpg\n├── dog.9702.jpg\n├── dog.9703.jpg\n├── dog.9704.jpg\n├── dog.9705.jpg\n├── dog.9706.jpg\n├── dog.9707.jpg\n├── dog.9708.jpg\n├── dog.9709.jpg\n├── dog.970.jpg\n├── dog.9710.jpg\n├── dog.9711.jpg\n├── dog.9712.jpg\n├── dog.9713.jpg\n├── dog.9714.jpg\n├── dog.9715.jpg\n├── dog.9716.jpg\n├── dog.9717.jpg\n├── dog.9718.jpg\n├── dog.9719.jpg\n├── dog.971.jpg\n├── dog.9720.jpg\n├── dog.9721.jpg\n├── dog.9722.jpg\n├── dog.9723.jpg\n├── dog.9724.jpg\n├── dog.9725.jpg\n├── dog.9726.jpg\n├── dog.9727.jpg\n├── dog.9728.jpg\n├── dog.9729.jpg\n├── dog.972.jpg\n├── dog.9730.jpg\n├── dog.9731.jpg\n├── dog.9732.jpg\n├── dog.9733.jpg\n├── dog.9734.jpg\n├── dog.9735.jpg\n├── dog.9736.jpg\n├── dog.9737.jpg\n├── dog.9738.jpg\n├── dog.9739.jpg\n├── dog.973.jpg\n├── dog.9740.jpg\n├── dog.9741.jpg\n├── dog.9742.jpg\n├── dog.9743.jpg\n├── dog.9744.jpg\n├── dog.9745.jpg\n├── dog.9746.jpg\n├── dog.9747.jpg\n├── dog.9748.jpg\n├── dog.9749.jpg\n├── dog.974.jpg\n├── dog.9750.jpg\n├── dog.9751.jpg\n├── dog.9752.jpg\n├── dog.9753.jpg\n├── dog.9754.jpg\n├── dog.9755.jpg\n├── dog.9756.jpg\n├── dog.9757.jpg\n├── dog.9758.jpg\n├── dog.9759.jpg\n├── dog.975.jpg\n├── dog.9760.jpg\n├── dog.9761.jpg\n├── dog.9762.jpg\n├── dog.9763.jpg\n├── dog.9764.jpg\n├── dog.9765.jpg\n├── dog.9766.jpg\n├── dog.9767.jpg\n├── dog.9768.jpg\n├── dog.9769.jpg\n├── dog.976.jpg\n├── dog.9770.jpg\n├── dog.9771.jpg\n├── dog.9772.jpg\n├── dog.9773.jpg\n├── dog.9774.jpg\n├── dog.9775.jpg\n├── dog.9776.jpg\n├── dog.9777.jpg\n├── dog.9778.jpg\n├── dog.9779.jpg\n├── dog.977.jpg\n├── dog.9780.jpg\n├── dog.9781.jpg\n├── dog.9782.jpg\n├── dog.9783.jpg\n├── dog.9784.jpg\n├── dog.9785.jpg\n├── dog.9786.jpg\n├── dog.9787.jpg\n├── dog.9788.jpg\n├── dog.9789.jpg\n├── dog.978.jpg\n├── dog.9790.jpg\n├── dog.9791.jpg\n├── dog.9792.jpg\n├── dog.9793.jpg\n├── dog.9794.jpg\n├── dog.9795.jpg\n├── dog.9796.jpg\n├── dog.9797.jpg\n├── dog.9798.jpg\n├── dog.9799.jpg\n├── dog.979.jpg\n├── dog.97.jpg\n├── dog.9800.jpg\n├── dog.9801.jpg\n├── dog.9802.jpg\n├── dog.9803.jpg\n├── dog.9804.jpg\n├── dog.9805.jpg\n├── dog.9806.jpg\n├── dog.9807.jpg\n├── dog.9808.jpg\n├── dog.9809.jpg\n├── dog.980.jpg\n├── dog.9810.jpg\n├── dog.9811.jpg\n├── dog.9812.jpg\n├── dog.9813.jpg\n├── dog.9814.jpg\n├── dog.9815.jpg\n├── dog.9816.jpg\n├── dog.9817.jpg\n├── dog.9818.jpg\n├── dog.9819.jpg\n├── dog.981.jpg\n├── dog.9820.jpg\n├── dog.9821.jpg\n├── dog.9822.jpg\n├── dog.9823.jpg\n├── dog.9824.jpg\n├── dog.9825.jpg\n├── dog.9826.jpg\n├── dog.9827.jpg\n├── dog.9828.jpg\n├── dog.9829.jpg\n├── dog.982.jpg\n├── dog.9830.jpg\n├── dog.9831.jpg\n├── dog.9832.jpg\n├── dog.9833.jpg\n├── dog.9834.jpg\n├── dog.9835.jpg\n├── dog.9836.jpg\n├── dog.9837.jpg\n├── dog.9838.jpg\n├── dog.9839.jpg\n├── dog.983.jpg\n├── dog.9840.jpg\n├── dog.9841.jpg\n├── dog.9842.jpg\n├── dog.9843.jpg\n├── dog.9844.jpg\n├── dog.9845.jpg\n├── dog.9846.jpg\n├── dog.9847.jpg\n├── dog.9848.jpg\n├── dog.9849.jpg\n├── dog.984.jpg\n├── dog.9850.jpg\n├── dog.9851.jpg\n├── dog.9852.jpg\n├── dog.9853.jpg\n├── dog.9854.jpg\n├── dog.9855.jpg\n├── dog.9856.jpg\n├── dog.9857.jpg\n├── dog.9858.jpg\n├── dog.9859.jpg\n├── dog.985.jpg\n├── dog.9860.jpg\n├── dog.9861.jpg\n├── dog.9862.jpg\n├── dog.9863.jpg\n├── dog.9864.jpg\n├── dog.9865.jpg\n├── dog.9866.jpg\n├── dog.9867.jpg\n├── dog.9868.jpg\n├── dog.9869.jpg\n├── dog.986.jpg\n├── dog.9870.jpg\n├── dog.9871.jpg\n├── dog.9872.jpg\n├── dog.9873.jpg\n├── dog.9874.jpg\n├── dog.9875.jpg\n├── dog.9876.jpg\n├── dog.9877.jpg\n├── dog.9878.jpg\n├── dog.9879.jpg\n├── dog.987.jpg\n├── dog.9880.jpg\n├── dog.9881.jpg\n├── dog.9882.jpg\n├── dog.9883.jpg\n├── dog.9884.jpg\n├── dog.9885.jpg\n├── dog.9886.jpg\n├── dog.9887.jpg\n├── dog.9888.jpg\n├── dog.9889.jpg\n├── dog.988.jpg\n├── dog.9890.jpg\n├── dog.9891.jpg\n├── dog.9892.jpg\n├── dog.9893.jpg\n├── dog.9894.jpg\n├── dog.9895.jpg\n├── dog.9896.jpg\n├── dog.9897.jpg\n├── dog.9898.jpg\n├── dog.9899.jpg\n├── dog.989.jpg\n├── dog.98.jpg\n├── dog.9900.jpg\n├── dog.9901.jpg\n├── dog.9902.jpg\n├── dog.9903.jpg\n├── dog.9904.jpg\n├── dog.9905.jpg\n├── dog.9906.jpg\n├── dog.9907.jpg\n├── dog.9908.jpg\n├── dog.9909.jpg\n├── dog.990.jpg\n├── dog.9910.jpg\n├── dog.9911.jpg\n├── dog.9912.jpg\n├── dog.9913.jpg\n├── dog.9914.jpg\n├── dog.9915.jpg\n├── dog.9916.jpg\n├── dog.9917.jpg\n├── dog.9918.jpg\n├── dog.9919.jpg\n├── dog.991.jpg\n├── dog.9920.jpg\n├── dog.9921.jpg\n├── dog.9922.jpg\n├── dog.9923.jpg\n├── dog.9924.jpg\n├── dog.9925.jpg\n├── dog.9926.jpg\n├── dog.9927.jpg\n├── dog.9928.jpg\n├── dog.9929.jpg\n├── dog.992.jpg\n├── dog.9930.jpg\n├── dog.9931.jpg\n├── dog.9932.jpg\n├── dog.9933.jpg\n├── dog.9934.jpg\n├── dog.9935.jpg\n├── dog.9936.jpg\n├── dog.9937.jpg\n├── dog.9938.jpg\n├── dog.9939.jpg\n├── dog.993.jpg\n├── dog.9940.jpg\n├── dog.9941.jpg\n├── dog.9942.jpg\n├── dog.9943.jpg\n├── dog.9944.jpg\n├── dog.9945.jpg\n├── dog.9946.jpg\n├── dog.9947.jpg\n├── dog.9948.jpg\n├── dog.9949.jpg\n├── dog.994.jpg\n├── dog.9950.jpg\n├── dog.9951.jpg\n├── dog.9952.jpg\n├── dog.9953.jpg\n├── dog.9954.jpg\n├── dog.9955.jpg\n├── dog.9956.jpg\n├── dog.9957.jpg\n├── dog.9958.jpg\n├── dog.9959.jpg\n├── dog.995.jpg\n├── dog.9960.jpg\n├── dog.9961.jpg\n├── dog.9962.jpg\n├── dog.9963.jpg\n├── dog.9964.jpg\n├── dog.9965.jpg\n├── dog.9966.jpg\n├── dog.9967.jpg\n├── dog.9968.jpg\n├── dog.9969.jpg\n├── dog.996.jpg\n├── dog.9970.jpg\n├── dog.9971.jpg\n├── dog.9972.jpg\n├── dog.9973.jpg\n├── dog.9974.jpg\n├── dog.9975.jpg\n├── dog.9976.jpg\n├── dog.9977.jpg\n├── dog.9978.jpg\n├── dog.9979.jpg\n├── dog.997.jpg\n├── dog.9980.jpg\n├── dog.9981.jpg\n├── dog.9982.jpg\n├── dog.9983.jpg\n├── dog.9984.jpg\n├── dog.9985.jpg\n├── dog.9986.jpg\n├── dog.9987.jpg\n├── dog.9988.jpg\n├── dog.9989.jpg\n├── dog.998.jpg\n├── dog.9990.jpg\n├── dog.9991.jpg\n├── dog.9992.jpg\n├── dog.9993.jpg\n├── dog.9994.jpg\n├── dog.9995.jpg\n├── dog.9996.jpg\n├── dog.9997.jpg\n├── dog.9998.jpg\n├── dog.9999.jpg\n├── dog.999.jpg\n├── dog.99.jpg\n└── dog.9.jpg\n\n0 directories, 25000 files\n\n\n\noriginal_dir = pathlib.Path(\"train\")\nnew_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n\ndef make_subset(subset_name, start_index, end_index):\n    for category in (\"cat\", \"dog\"):\n        dir = new_base_dir / subset_name / category\n        os.makedirs(dir)\n        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n        for fname in fnames:\n            shutil.copyfile(src=original_dir / fname,\n                            dst=dir / fname)\n\nmake_subset(\"train\", start_index=0, end_index=1000)\nmake_subset(\"validation\", start_index=1000, end_index=1500)\nmake_subset(\"test\", start_index=1500, end_index=2500)\n\n\n!tree cats_vs_dogs_small -L 2\n\ncats_vs_dogs_small\n├── test\n│   ├── cat\n│   └── dog\n├── train\n│   ├── cat\n│   └── dog\n└── validation\n    ├── cat\n    └── dog\n\n9 directories, 0 files\n\n\nWe now have 2,000 training images, 1,000 validation images, and 2,000 test images. Each split contains the same number of samples from each class: this is a balanced binary-classification problem, which means classification accuracy will be an appropriate measure of success.\n\n\nE.3.3 Building the model\nThe convnet will be a stack of alternated Conv2D (with relu activation) and MaxPool2D layers. But because we’re dealing with bigger images and a more complex problem, we’ll make our model larger, accordingly: it will have two more Conv2D and MaxPool2D stages. This serves both to augment the capacity of the model and to further reduce the size of the feature maps so they aren’t overly large when we reach the Flatten layer.\nHere, because we start from inputs of size 180 pixels × 180 pixels, we end up with feature maps of size 7 × 7 just before the Flatten layer. Because we’re looking at a binary-classification problem, we’ll end the model with a single unit (a Dense layer of size 1) and a sigmoid activation. This unit will encode the probability that the model is looking at one class or the other.\n\nlearn = None\nmodel = None\ngc.collect()\ntorch.cuda.empty_cache()\n\n\nclass CustomModel(nn.Module):\n    def __init__(self):\n        super(CustomModel, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=3, padding=0),\n            nn.ReLU()\n        )\n        self.flatten = nn.Flatten()\n        self.fc = nn.Sequential(\n            nn.Linear(7 * 7 * 256, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = self.flatten(x)\n        x = self.fc(x)\n        return x\n\nmodel = CustomModel()\n\n\nsummary(model, input_size=(32, 3, 180, 180))\n\n/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  action_fn=lambda data: sys.getsizeof(data.storage()),\n/usr/local/lib/python3.10/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return super().__sizeof__() + self.nbytes()\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCustomModel                              [32, 1]                   --\n├─Sequential: 1-1                        [32, 32, 89, 89]          --\n│    └─Conv2d: 2-1                       [32, 32, 178, 178]        896\n│    └─ReLU: 2-2                         [32, 32, 178, 178]        --\n│    └─MaxPool2d: 2-3                    [32, 32, 89, 89]          --\n├─Sequential: 1-2                        [32, 64, 43, 43]          --\n│    └─Conv2d: 2-4                       [32, 64, 87, 87]          18,496\n│    └─ReLU: 2-5                         [32, 64, 87, 87]          --\n│    └─MaxPool2d: 2-6                    [32, 64, 43, 43]          --\n├─Sequential: 1-3                        [32, 128, 20, 20]         --\n│    └─Conv2d: 2-7                       [32, 128, 41, 41]         73,856\n│    └─ReLU: 2-8                         [32, 128, 41, 41]         --\n│    └─MaxPool2d: 2-9                    [32, 128, 20, 20]         --\n├─Sequential: 1-4                        [32, 256, 9, 9]           --\n│    └─Conv2d: 2-10                      [32, 256, 18, 18]         295,168\n│    └─ReLU: 2-11                        [32, 256, 18, 18]         --\n│    └─MaxPool2d: 2-12                   [32, 256, 9, 9]           --\n├─Sequential: 1-5                        [32, 256, 7, 7]           --\n│    └─Conv2d: 2-13                      [32, 256, 7, 7]           590,080\n│    └─ReLU: 2-14                        [32, 256, 7, 7]           --\n├─Flatten: 1-6                           [32, 12544]               --\n├─Sequential: 1-7                        [32, 1]                   --\n│    └─Linear: 2-15                      [32, 1]                   12,545\n│    └─Sigmoid: 2-16                     [32, 1]                   --\n==========================================================================================\nTotal params: 991,041\nTrainable params: 991,041\nNon-trainable params: 0\nTotal mult-adds (G): 13.35\n==========================================================================================\nInput size (MB): 12.44\nForward/backward pass size (MB): 463.09\nParams size (MB): 3.96\nEstimated Total Size (MB): 479.50\n==========================================================================================\n\n\n\n\nE.3.4 Data preprocessing\nAs you know by now, data should be formatted into appropriately preprocessed floatingpoint tensors before being fed into the model. Currently, the data sits on a drive as JPEG files, so the steps for getting it into the model are roughly as follows:\n\nRead the picture files.\nDecode the JPEG content to RGB grids of pixels.\nConvert these into floating-point tensors.\nResize them to a shared size (we’ll use 180 × 180).\nPack them into batches (we’ll use batches of 32 images).\n\nIt may seem a bit daunting, but fortunately Pytorch has utilities to take care of these steps automatically. In particular, Pytorch features the utility function datasets.ImageFolder(), which lets you quickly set up a data pipeline that can automatically turn image files on disk into batches of preprocessed tensors. This is what we’ll use here.\nCalling datasets.ImageFolder() will first list the subdirectories of directory and assume each one contains images from one of our classes. It will then index the image files in each subdirectory. Finally, it will create and return a DataLoader object configured to read these files, shuffle them, decode them to tensors, resize them to a shared size, and pack them into batches.\n\nimage_size = (180, 180)\nbatch_size = 32\n#new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\ntransform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Create the ImageFolder datasets\ntrain_dataset = datasets.ImageFolder(os.path.join(new_base_dir, \"train\"), transform=transform)\nvalidation_dataset = datasets.ImageFolder(os.path.join(new_base_dir, \"validation\"), transform=transform)\ntest_dataset = datasets.ImageFolder(os.path.join(new_base_dir, \"test\"), transform=transform)\n\n# Create the DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\nvalidation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n\nLet’s look at the output of one of these Dataset objects: it yields batches of 180 × 180 RGB images (shape (32, 3, 180, 180)) and integer labels (shape (32,)). There are 32 samples in each batch (the batch size).\n\nfor data_batch, labels_batch in train_loader:\n    print(\"data batch shape:\", data_batch.shape)\n    print(\"labels batch shape:\", labels_batch.shape)\n    break\n\ndata batch shape: torch.Size([32, 3, 180, 180])\nlabels batch shape: torch.Size([32])\n\n\n\n\nE.3.5 Fitting the model\nLet’s fit the model on our dataset. We’ll use the validation_data argument in fit() to monitor validation metrics on a separate Dataset object.\n\ndef custom_binary_cross_entropy(output, target):\n    batch_size = output.shape[0]  # Get the current batch size\n    return F.binary_cross_entropy(output, target.reshape(batch_size, 1).float())\n\ndef binary_accuracy(output, target):\n    preds = (output &gt; 0.5).float()\n    return (preds == target.float()).float().mean()\n\ndata = DataLoaders(train_loader, validation_loader)\nlearn = Learner(data, model, loss_func=custom_binary_cross_entropy, opt_func=Adam, metrics=[binary_accuracy])\n\n\nlearn.fit_one_cycle(30, 0.001)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbinary_accuracy\ntime\n\n\n\n\n0\n0.692652\n0.691543\n0.602000\n00:12\n\n\n1\n0.688626\n0.695021\n0.500000\n00:12\n\n\n2\n0.689949\n0.685752\n0.545750\n00:11\n\n\n3\n0.686162\n0.677458\n0.581500\n00:12\n\n\n4\n0.661260\n0.626807\n0.625250\n00:12\n\n\n5\n0.620587\n0.633627\n0.641000\n00:11\n\n\n6\n0.593634\n0.615299\n0.652250\n00:13\n\n\n7\n0.571257\n0.635371\n0.680250\n00:11\n\n\n8\n0.524003\n0.601736\n0.666000\n00:11\n\n\n9\n0.495714\n0.538874\n0.718000\n00:11\n\n\n10\n0.465912\n0.536926\n0.725750\n00:12\n\n\n11\n0.395629\n0.576634\n0.744000\n00:12\n\n\n12\n0.340584\n0.596848\n0.758250\n00:11\n\n\n13\n0.271378\n0.574540\n0.753750\n00:12\n\n\n14\n0.205846\n0.678262\n0.749000\n00:11\n\n\n15\n0.149247\n0.692949\n0.752000\n00:12\n\n\n16\n0.106399\n0.840231\n0.768250\n00:12\n\n\n17\n0.052970\n1.088746\n0.776500\n00:12\n\n\n18\n0.024501\n1.264751\n0.772750\n00:12\n\n\n19\n0.011347\n1.528437\n0.763750\n00:13\n\n\n20\n0.004973\n1.978761\n0.771500\n00:12\n\n\n21\n0.002348\n2.146611\n0.775250\n00:12\n\n\n22\n0.001133\n2.257327\n0.776750\n00:12\n\n\n23\n0.000648\n2.379216\n0.780750\n00:12\n\n\n24\n0.000421\n2.409948\n0.780750\n00:11\n\n\n25\n0.000328\n2.435015\n0.776750\n00:11\n\n\n26\n0.000278\n2.454705\n0.777750\n00:12\n\n\n27\n0.000243\n2.463802\n0.781750\n00:11\n\n\n28\n0.000224\n2.467732\n0.780750\n00:13\n\n\n29\n0.000214\n2.468883\n0.780750\n00:15\n\n\n\n\n\nLet’s plot the loss of the model over the training and validation data during training\n\nlearn.recorder.plot_loss()\n\n\n\n\nThese plots are characteristic of overfitting. Let’s check the test accuracy. We’ll reload the model from its saved file to evaluate it as it was before it started overfitting.\n\nfastai_loss, fastai_accuracy = learn.validate(dl=test_loader)\nfastai_accuracy\n\n\n\n\n\n\n\n\n0.7599999904632568\n\n\nWe get a test accuracy of about 75%. Because we have relatively few training samples (2,000), overfitting will be our number one concern. You already know about a number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). We’re now going to work with a new one, specific to computer vision and used almost universally when processing images with deep learning models: data augmentation.\n\n\nE.3.6 Using data augmentation\nOverfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your model would be exposed to every possible aspect of the data distribution at hand: you would never overfit. Data augmentation takes the approach of generating more training data from existing training samples by augmenting the samples via a number of random transformations that yield believable-looking images.\nThe goal is that, at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data so it can generalize better. In Keras, this can be done by adding a number of data augmentation layers at the start of your model. Let’s get started with an example: the following Sequential model chains several random image transformations. In our model, we’d include it right before the Rescaling layer.\n\nlearn = None\nmodel = None\ngc.collect()\ntorch.cuda.empty_cache()\n\n\ndata_augmentation = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(36),\n    transforms.RandomAffine(degrees=0, translate=None, scale=(0.8, 1.2), shear=None),\n    #transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n\nplt.figure(figsize=(10, 10))\n\nfor images, _ in train_loader:\n    augmented_images = data_augmentation(images)\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        image_to_plot = augmented_images[i].numpy().transpose((1, 2, 0))\n        image_to_plot = (image_to_plot - image_to_plot.min())/(image_to_plot.max()-image_to_plot.min()) # Normalize to [0..1] range\n        plt.imshow(image_to_plot)\n        plt.axis(\"off\")\n    break  # Sample only 1 batch from the dataset\nplt.show()\n\n\n\n\nOn the other hand, we can also use albumentations for data augmentation:\n\nimage_size = (180, 180)\nbatch_size = 32\n\nclass CustomImageFolder(datasets.ImageFolder):\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        sample = np.array(sample)\n\n        if self.transform is not None:\n            transformed = self.transform(image=sample)\n            sample = transformed[\"image\"]\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target\n\ntrain_transform = A.Compose(\n    [\n        A.Resize(image_size[0], image_size[1]),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.2, rotate_limit=36, p=0.5),\n        A.HorizontalFlip(p=0.5),\n        A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ToTensorV2(),\n    ]\n)\n\nval_transform = A.Compose(\n    [\n        A.Resize(image_size[0], image_size[1]),\n        A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ToTensorV2(),\n    ]\n)\n\n# Create the ImageFolder datasets\ntrain_dataset = CustomImageFolder(os.path.join(new_base_dir, \"train\"), transform=train_transform)\nvalidation_dataset = CustomImageFolder(os.path.join(new_base_dir, \"validation\"), transform=val_transform)\ntest_dataset = CustomImageFolder(os.path.join(new_base_dir, \"test\"), transform=val_transform)\n\n# Create the DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\nvalidation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n\ndef visualize_augmentations(dataset, idx=0, samples=10, cols=5):\n    dataset = copy.deepcopy(dataset)\n    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n    rows = samples // cols\n    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n    for i in range(samples):\n        image, _ = dataset[idx]\n        ax.ravel()[i].imshow(image)\n        ax.ravel()[i].set_axis_off()\n    plt.tight_layout()\n    plt.show()\n\n\nvisualize_augmentations(train_dataset)\n\n\n\n\nIf we train a new model using this data-augmentation configuration, the model will never see the same input twice. But the inputs it sees are still heavily intercorrelated because they come from a small number of original images—we can’t produce new information; we can only remix existing information. As such, this may not be enough to completely get rid of overfitting. To further fight overfitting, we’ll also add a Dropout layer to our model right before the densely connected classifier.\nOne last thing you should know about random image augmentation layers: just like Dropout, they’re inactive during inference (when we call predict() or evaluate()). During evaluation, our model will behave just the same as when it did not include data augmentation and dropout.\n\nclass CustomModel(nn.Module):\n    def __init__(self):\n        super(CustomModel, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=3, padding=0),\n            nn.ReLU()\n        )\n        self.flatten = nn.Flatten()\n        self.fc = nn.Sequential(\n            nn.Linear(7 * 7 * 256, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = self.flatten(x)\n        x = self.fc(x)\n        return x\n\nmodel = CustomModel()\n\n\ndef custom_binary_cross_entropy(output, target):\n    batch_size = output.shape[0]  # Get the current batch size\n    return F.binary_cross_entropy(output, target.reshape(batch_size, 1).float())\n\ndef binary_accuracy(output, target):\n    preds = (output &gt; 0.5).float()\n    return (preds == target.float()).float().mean()\n\ndata = DataLoaders(train_loader, validation_loader)\nlearn = Learner(data, model, loss_func=custom_binary_cross_entropy, opt_func=Adam, metrics=[binary_accuracy])\n\nThe training loop is slow here, you could try to directly use Pytorch instead, refere to https://albumentations.ai/docs/examples/pytorch_classification/ for more information.\n\nlearn.fit_one_cycle(60, 0.001)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbinary_accuracy\ntime\n\n\n\n\n0\n0.693066\n0.691918\n0.500000\n00:38\n\n\n1\n0.690652\n0.686164\n0.600750\n00:39\n\n\n2\n0.678950\n0.651391\n0.617250\n00:38\n\n\n3\n0.656565\n0.629399\n0.637000\n00:38\n\n\n4\n0.642592\n0.623475\n0.666500\n00:38\n\n\n5\n0.616005\n0.580928\n0.689500\n00:38\n\n\n6\n0.589917\n0.570550\n0.698000\n00:38\n\n\n7\n0.573144\n0.546748\n0.719500\n00:37\n\n\n8\n0.556366\n0.601920\n0.684500\n00:37\n\n\n9\n0.541922\n0.553274\n0.697250\n00:37\n\n\n10\n0.523928\n0.503581\n0.752000\n00:38\n\n\n11\n0.516340\n0.497255\n0.762500\n00:38\n\n\n12\n0.486024\n0.571045\n0.719500\n00:37\n\n\n13\n0.470749\n0.485204\n0.747000\n00:37\n\n\n14\n0.441717\n0.454703\n0.780000\n00:37\n\n\n15\n0.411646\n0.498344\n0.762500\n00:37\n\n\n16\n0.402160\n0.495557\n0.765000\n00:37\n\n\n17\n0.396506\n0.473726\n0.783250\n00:37\n\n\n18\n0.381660\n0.490383\n0.787750\n00:38\n\n\n19\n0.360372\n0.457263\n0.787500\n00:37\n\n\n20\n0.342842\n0.465778\n0.780750\n00:37\n\n\n21\n0.328554\n0.484067\n0.793500\n00:37\n\n\n22\n0.314617\n0.468121\n0.786250\n00:37\n\n\n23\n0.301147\n0.415992\n0.799500\n00:38\n\n\n24\n0.278657\n0.434241\n0.811750\n00:37\n\n\n25\n0.247953\n0.449205\n0.810000\n00:37\n\n\n26\n0.235257\n0.427547\n0.819500\n00:37\n\n\n27\n0.233322\n0.455889\n0.808000\n00:37\n\n\n28\n0.208548\n0.445234\n0.805500\n00:38\n\n\n29\n0.201965\n0.496131\n0.821750\n00:38\n\n\n30\n0.187354\n0.470819\n0.830750\n00:39\n\n\n31\n0.168621\n0.463303\n0.827000\n00:39\n\n\n32\n0.159545\n0.659526\n0.794000\n00:39\n\n\n33\n0.162341\n0.525368\n0.817750\n00:39\n\n\n34\n0.149857\n0.459367\n0.832000\n00:39\n\n\n35\n0.141797\n0.449607\n0.841250\n00:38\n\n\n36\n0.137166\n0.500571\n0.822750\n00:39\n\n\n37\n0.124995\n0.418119\n0.842000\n00:41\n\n\n38\n0.111028\n0.487745\n0.842250\n00:39\n\n\n39\n0.096113\n0.461577\n0.846000\n00:37\n\n\n40\n0.076184\n0.517702\n0.849500\n00:37\n\n\n41\n0.083818\n0.532772\n0.843250\n00:38\n\n\n42\n0.070036\n0.582350\n0.837250\n00:37\n\n\n43\n0.070664\n0.603945\n0.841250\n00:37\n\n\n44\n0.084810\n0.591764\n0.850250\n00:37\n\n\n45\n0.078341\n0.488943\n0.852000\n00:38\n\n\n46\n0.065318\n0.624858\n0.854250\n00:38\n\n\n47\n0.065456\n0.477966\n0.855750\n00:37\n\n\n48\n0.061500\n0.486319\n0.858500\n00:37\n\n\n49\n0.054900\n0.489795\n0.855750\n00:37\n\n\n50\n0.057763\n0.487532\n0.853750\n00:38\n\n\n51\n0.045038\n0.497306\n0.853750\n00:38\n\n\n52\n0.038785\n0.498204\n0.864750\n00:37\n\n\n53\n0.043548\n0.488389\n0.858750\n00:37\n\n\n54\n0.038572\n0.493007\n0.859750\n00:37\n\n\n55\n0.037272\n0.501999\n0.857500\n00:38\n\n\n56\n0.049886\n0.497700\n0.859750\n00:38\n\n\n57\n0.039522\n0.496305\n0.859750\n00:37\n\n\n58\n0.042081\n0.498379\n0.858750\n00:37\n\n\n59\n0.038153\n0.498430\n0.859750\n00:37\n\n\n\n\n\nLet’s train the model using data augmentation and dropout. Because we expect overfitting to occur much later during training, we will train for three times as many epochs — 60.\nLet’s plot the results again: Thanks to data augmentation and dropout, we start overfitting much later, around epochs 20-30 (compared to epoch 10 for the original model). The validation accuracy ends up consistently in the 80–85% range—a big improvement over our first try.\n\nlearn.recorder.plot_loss()\n\n\n\n\nLet’s check the test accuracy.\n\nfastai_loss, fastai_accuracy = learn.validate(dl=test_loader)\nfastai_accuracy\n\n\n\n\n\n\n\n\n0.8554999828338623\n\n\nWe get a test accuracy over 85%. It’s starting to look good! By further tuning the model’s configuration (such as the number of filters per convolution layer, or the number of layers in the model), we might be able to get an even better accuracy, likely up to 90%. But it would prove difficult to go any higher just by training our own convnet from scratch, because we have so little data to work with. As a next step to improve our accuracy on this problem, we’ll have to use a pretrained model as we will see later on."
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_pytorch.html#object-detection-and-segmentation-with-detectorn2-optional",
    "href": "09_Convolutional_NeuralNetworks_pytorch.html#object-detection-and-segmentation-with-detectorn2-optional",
    "title": "Appendix E — Image processing with Convolutional Neural Networks - Pytorch",
    "section": "E.4 Object detection and segmentation with detectorn2 (Optional)",
    "text": "E.4 Object detection and segmentation with detectorn2 (Optional)\nIn this section, we show how to train an existing detectron2 model on a custom dataset in a new format.\nWe use the balloon segmentation dataset which only has one class: balloon. We’ll train a balloon segmentation model from an existing model pre-trained on COCO dataset, available in detectron2’s model zoo.\nNote that COCO dataset does not have the “balloon” category. We’ll be able to recognize this new class in a few minutes.\n\nsetup_logger()\n\n&lt;Logger detectron2 (DEBUG)&gt;\n\n\n\nE.4.1 Prepare the dataset\n\n# download, decompress the data\n!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n!unzip balloon_dataset.zip &gt; /dev/null\n\n--2023-04-30 06:53:44--  https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\nResolving github.com (github.com)... 140.82.113.3\nConnecting to github.com (github.com)|140.82.113.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://objects.githubusercontent.com/github-production-release-asset-2e65be/107595270/737339e2-2b83-11e8-856a-188034eb3468?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230430%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230430T065344Z&X-Amz-Expires=300&X-Amz-Signature=ea904acad73f8792f74d922b5b08311500fd5d72a41e8c6215e74e94069a3edf&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=107595270&response-content-disposition=attachment%3B%20filename%3Dballoon_dataset.zip&response-content-type=application%2Foctet-stream [following]\n--2023-04-30 06:53:44--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/107595270/737339e2-2b83-11e8-856a-188034eb3468?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230430%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230430T065344Z&X-Amz-Expires=300&X-Amz-Signature=ea904acad73f8792f74d922b5b08311500fd5d72a41e8c6215e74e94069a3edf&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=107595270&response-content-disposition=attachment%3B%20filename%3Dballoon_dataset.zip&response-content-type=application%2Foctet-stream\nResolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 38741381 (37M) [application/octet-stream]\nSaving to: ‘balloon_dataset.zip’\n\nballoon_dataset.zip 100%[===================&gt;]  36.95M   145MB/s    in 0.3s    \n\n2023-04-30 06:53:44 (145 MB/s) - ‘balloon_dataset.zip’ saved [38741381/38741381]\n\n\n\nRegister the balloon dataset to detectron2, following the detectron2 custom dataset tutorial. Here, the dataset is in its custom format, therefore we write a function to parse it and prepare it into detectron2’s standard format. User should write such a function when using a dataset in custom format. See the tutorial for more details.\n\n# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n# from detectron2.data.datasets import register_coco_instances\n# register_coco_instances(\"my_dataset_train\", {}, \"json_annotation_train.json\", \"path/to/image/dir\")\n# register_coco_instances(\"my_dataset_val\", {}, \"json_annotation_val.json\", \"path/to/image/dir\")\n\ndef get_balloon_dicts(img_dir):\n    json_file = os.path.join(img_dir, \"via_region_data.json\")\n    with open(json_file) as f:\n        imgs_anns = json.load(f)\n\n    dataset_dicts = []\n    for idx, v in enumerate(imgs_anns.values()):\n        record = {}\n        \n        filename = os.path.join(img_dir, v[\"filename\"])\n        height, width = cv2.imread(filename).shape[:2]\n        \n        record[\"file_name\"] = filename\n        record[\"image_id\"] = idx\n        record[\"height\"] = height\n        record[\"width\"] = width\n      \n        annos = v[\"regions\"]\n        objs = []\n        for _, anno in annos.items():\n            assert not anno[\"region_attributes\"]\n            anno = anno[\"shape_attributes\"]\n            px = anno[\"all_points_x\"]\n            py = anno[\"all_points_y\"]\n            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n            poly = [p for x in poly for p in x]\n\n            obj = {\n                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n                \"bbox_mode\": BoxMode.XYXY_ABS,\n                \"segmentation\": [poly],\n                \"category_id\": 0,\n            }\n            objs.append(obj)\n        record[\"annotations\"] = objs\n        dataset_dicts.append(record)\n    return dataset_dicts\n\nfor d in [\"train\", \"val\"]:\n    DatasetCatalog.register(\"balloon_\" + d, lambda d=d: get_balloon_dicts(\"balloon/\" + d))\n    MetadataCatalog.get(\"balloon_\" + d).set(thing_classes=[\"balloon\"])\nballoon_metadata = MetadataCatalog.get(\"balloon_train\")\n\nTo verify the dataset is in correct format, let’s visualize the annotations of randomly selected samples in the training set:\n\ndataset_dicts = get_balloon_dicts(\"balloon/train\")\nfor d in random.sample(dataset_dicts, 3):\n    img = cv2.imread(d[\"file_name\"])\n    visualizer = Visualizer(img[:, :, ::-1], metadata=balloon_metadata, scale=0.5)\n    out = visualizer.draw_dataset_dict(d)\n    cv2_imshow(out.get_image()[:, :, ::-1])\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\nE.4.2 Train the model\nNow, let’s fine-tune a COCO-pretrained R50-FPN Mask R-CNN model on the balloon dataset.\n\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"balloon_train\",)\ncfg.DATASETS.TEST = ()\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\ncfg.SOLVER.IMS_PER_BATCH = 2  # This is the real \"batch size\" commonly known to deep learning people\ncfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\ncfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\ncfg.SOLVER.STEPS = []        # do not decay learning rate\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512)\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = DefaultTrainer(cfg) \ntrainer.resume_or_load(resume=False)\ntrainer.train()\n\n[04/30 06:54:01 d2.engine.defaults]: Model:\nGeneralizedRCNN(\n  (backbone): FPN(\n    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (top_block): LastLevelMaxPool()\n    (bottom_up): ResNet(\n      (stem): BasicStem(\n        (conv1): Conv2d(\n          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n      )\n      (res2): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n      )\n      (res3): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n      )\n      (res4): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (4): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (5): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n      )\n      (res5): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (rpn_head): StandardRPNHead(\n      (conv): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (anchor_generator): DefaultAnchorGenerator(\n      (cell_anchors): BufferList()\n    )\n  )\n  (roi_heads): StandardROIHeads(\n    (box_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (box_head): FastRCNNConvFCHead(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc_relu1): ReLU()\n      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n      (fc_relu2): ReLU()\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n    )\n    (mask_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (mask_head): MaskRCNNConvUpsampleHead(\n      (mask_fcn1): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn2): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn3): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn4): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n      (deconv_relu): ReLU()\n      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)\n[04/30 06:54:03 d2.data.build]: Removed 0 images with no usable annotations. 61 images left.\n[04/30 06:54:03 d2.data.build]: Distribution of instances among all 1 categories:\n|  category  | #instances   |\n|:----------:|:-------------|\n|  balloon   | 255          |\n|            |              |\n[04/30 06:54:03 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n[04/30 06:54:03 d2.data.build]: Using training sampler TrainingSampler\n[04/30 06:54:03 d2.data.common]: Serializing the dataset using: &lt;class 'detectron2.data.common._TorchSerializedList'&gt;\n[04/30 06:54:03 d2.data.common]: Serializing 61 elements to byte tensors and concatenating them all ...\n[04/30 06:54:03 d2.data.common]: Serialized dataset takes 0.17 MiB\n[04/30 06:54:03 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n\n\nmodel_final_f10217.pkl: 178MB [00:02, 75.3MB/s]                          \nWARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\nWARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\nWARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\nWARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\nWARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\nWARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\nWARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\nroi_heads.box_predictor.bbox_pred.{bias, weight}\nroi_heads.box_predictor.cls_score.{bias, weight}\nroi_heads.mask_head.predictor.{bias, weight}\n\n\n[04/30 06:54:05 d2.engine.train_loop]: Starting training from iteration 0\n\n\n/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\n[04/30 06:54:18 d2.utils.events]:  eta: 0:02:03  iter: 19  total_loss: 2.146  loss_cls: 0.7301  loss_box_reg: 0.7054  loss_mask: 0.6942  loss_rpn_cls: 0.03187  loss_rpn_loc: 0.00954    time: 0.4561  last_time: 0.4256  data_time: 0.0337  last_data_time: 0.0340   lr: 1.6068e-05  max_mem: 2455M\n[04/30 06:54:30 d2.utils.events]:  eta: 0:01:55  iter: 39  total_loss: 1.767  loss_cls: 0.6171  loss_box_reg: 0.5889  loss_mask: 0.6012  loss_rpn_cls: 0.02424  loss_rpn_loc: 0.004564    time: 0.4497  last_time: 0.5170  data_time: 0.0092  last_data_time: 0.0233   lr: 3.2718e-05  max_mem: 2455M\n[04/30 06:54:39 d2.utils.events]:  eta: 0:01:49  iter: 59  total_loss: 1.835  loss_cls: 0.5053  loss_box_reg: 0.7199  loss_mask: 0.4906  loss_rpn_cls: 0.04432  loss_rpn_loc: 0.01324    time: 0.4561  last_time: 0.4568  data_time: 0.0130  last_data_time: 0.0266   lr: 4.9367e-05  max_mem: 2456M\n[04/30 06:54:51 d2.utils.events]:  eta: 0:01:43  iter: 79  total_loss: 1.449  loss_cls: 0.3847  loss_box_reg: 0.6427  loss_mask: 0.365  loss_rpn_cls: 0.02072  loss_rpn_loc: 0.007872    time: 0.4891  last_time: 0.4607  data_time: 0.0469  last_data_time: 0.0160   lr: 6.6017e-05  max_mem: 2574M\n[04/30 06:55:00 d2.utils.events]:  eta: 0:01:32  iter: 99  total_loss: 1.237  loss_cls: 0.3137  loss_box_reg: 0.5958  loss_mask: 0.2853  loss_rpn_cls: 0.03939  loss_rpn_loc: 0.008389    time: 0.4855  last_time: 0.4037  data_time: 0.0119  last_data_time: 0.0043   lr: 8.2668e-05  max_mem: 2574M\n[04/30 06:55:10 d2.utils.events]:  eta: 0:01:23  iter: 119  total_loss: 1.149  loss_cls: 0.2453  loss_box_reg: 0.6494  loss_mask: 0.2161  loss_rpn_cls: 0.01339  loss_rpn_loc: 0.005409    time: 0.4822  last_time: 0.4801  data_time: 0.0148  last_data_time: 0.0132   lr: 9.9318e-05  max_mem: 2612M\n[04/30 06:55:20 d2.utils.events]:  eta: 0:01:14  iter: 139  total_loss: 1.032  loss_cls: 0.2096  loss_box_reg: 0.6288  loss_mask: 0.2124  loss_rpn_cls: 0.01758  loss_rpn_loc: 0.008476    time: 0.4860  last_time: 0.3769  data_time: 0.0170  last_data_time: 0.0038   lr: 0.00011597  max_mem: 2612M\n[04/30 06:55:30 d2.utils.events]:  eta: 0:01:05  iter: 159  total_loss: 0.7927  loss_cls: 0.1471  loss_box_reg: 0.5167  loss_mask: 0.1499  loss_rpn_cls: 0.01585  loss_rpn_loc: 0.00623    time: 0.4867  last_time: 0.3468  data_time: 0.0214  last_data_time: 0.0058   lr: 0.00013262  max_mem: 2612M\n[04/30 06:55:38 d2.utils.events]:  eta: 0:00:55  iter: 179  total_loss: 0.7589  loss_cls: 0.1219  loss_box_reg: 0.4812  loss_mask: 0.1269  loss_rpn_cls: 0.01871  loss_rpn_loc: 0.006172    time: 0.4814  last_time: 0.4045  data_time: 0.0121  last_data_time: 0.0099   lr: 0.00014927  max_mem: 2612M\n[04/30 06:55:49 d2.utils.events]:  eta: 0:00:46  iter: 199  total_loss: 0.5453  loss_cls: 0.09887  loss_box_reg: 0.3309  loss_mask: 0.1008  loss_rpn_cls: 0.01691  loss_rpn_loc: 0.007972    time: 0.4842  last_time: 0.5318  data_time: 0.0193  last_data_time: 0.0350   lr: 0.00016592  max_mem: 2612M\n[04/30 06:55:59 d2.utils.events]:  eta: 0:00:37  iter: 219  total_loss: 0.4851  loss_cls: 0.1042  loss_box_reg: 0.2436  loss_mask: 0.1031  loss_rpn_cls: 0.01303  loss_rpn_loc: 0.009417    time: 0.4850  last_time: 0.5056  data_time: 0.0196  last_data_time: 0.0212   lr: 0.00018257  max_mem: 2612M\n[04/30 06:56:09 d2.utils.events]:  eta: 0:00:28  iter: 239  total_loss: 0.34  loss_cls: 0.06758  loss_box_reg: 0.1757  loss_mask: 0.07039  loss_rpn_cls: 0.01167  loss_rpn_loc: 0.006367    time: 0.4899  last_time: 0.7319  data_time: 0.0421  last_data_time: 0.1122   lr: 0.00019922  max_mem: 2612M\n[04/30 06:56:19 d2.utils.events]:  eta: 0:00:18  iter: 259  total_loss: 0.4182  loss_cls: 0.08279  loss_box_reg: 0.1719  loss_mask: 0.1069  loss_rpn_cls: 0.0144  loss_rpn_loc: 0.01126    time: 0.4894  last_time: 0.5146  data_time: 0.0205  last_data_time: 0.0237   lr: 0.00021587  max_mem: 2612M\n[04/30 06:56:29 d2.utils.events]:  eta: 0:00:09  iter: 279  total_loss: 0.3486  loss_cls: 0.06866  loss_box_reg: 0.1945  loss_mask: 0.07559  loss_rpn_cls: 0.01046  loss_rpn_loc: 0.007445    time: 0.4885  last_time: 0.3725  data_time: 0.0156  last_data_time: 0.0048   lr: 0.00023252  max_mem: 2612M\n[04/30 06:56:40 d2.utils.events]:  eta: 0:00:00  iter: 299  total_loss: 0.261  loss_cls: 0.06409  loss_box_reg: 0.1413  loss_mask: 0.06687  loss_rpn_cls: 0.006927  loss_rpn_loc: 0.003735    time: 0.4887  last_time: 0.6228  data_time: 0.0236  last_data_time: 0.0724   lr: 0.00024917  max_mem: 2612M\n[04/30 06:56:40 d2.engine.hooks]: Overall training speed: 298 iterations in 0:02:25 (0.4887 s / it)\n[04/30 06:56:40 d2.engine.hooks]: Total training time: 0:02:30 (0:00:04 on hooks)\n\n\n\n\nE.4.3 Inference & evaluation using the trained model\nNow, let’s run inference with the trained model on the balloon validation dataset. First, let’s create a predictor using the model we just trained:\n\n# Inference should use the config with parameters that are used in training\n# cfg now already contains everything we've set previously. We changed it a little bit for inference:\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\npredictor = DefaultPredictor(cfg)\n\n[04/30 06:57:36 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./output/model_final.pth ...\n\n\nThen, we randomly select several samples to visualize the prediction results.\n\ndataset_dicts = get_balloon_dicts(\"balloon/val\")\nfor d in random.sample(dataset_dicts, 3):    \n    im = cv2.imread(d[\"file_name\"])\n    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n    v = Visualizer(im[:, :, ::-1],\n                   metadata=balloon_metadata, \n                   scale=0.5, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n    )\n    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    cv2_imshow(out.get_image()[:, :, ::-1])\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nWe can also evaluate its performance using AP metric implemented in COCO API. This gives an AP of ~70. Not bad!\n\nevaluator = COCOEvaluator(\"balloon_val\", output_dir=\"./output\")\nval_loader = build_detection_test_loader(cfg, \"balloon_val\")\nprint(inference_on_dataset(predictor.model, val_loader, evaluator))\n# another equivalent way to evaluate the model is to use `trainer.test`\n\n[04/30 06:57:52 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n[04/30 06:57:52 d2.evaluation.coco_evaluation]: Trying to convert 'balloon_val' to COCO format ...\n[04/30 06:57:52 d2.data.datasets.coco]: Converting annotations of dataset 'balloon_val' to COCO format ...)\n[04/30 06:57:53 d2.data.datasets.coco]: Converting dataset dicts into COCO format\n[04/30 06:57:53 d2.data.datasets.coco]: Conversion finished, #images: 13, #annotations: 50\n[04/30 06:57:53 d2.data.datasets.coco]: Caching COCO format annotations at './output/balloon_val_coco_format.json' ...\n[04/30 06:57:53 d2.data.build]: Distribution of instances among all 1 categories:\n|  category  | #instances   |\n|:----------:|:-------------|\n|  balloon   | 50           |\n|            |              |\n[04/30 06:57:53 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n[04/30 06:57:53 d2.data.common]: Serializing the dataset using: &lt;class 'detectron2.data.common._TorchSerializedList'&gt;\n[04/30 06:57:53 d2.data.common]: Serializing 13 elements to byte tensors and concatenating them all ...\n[04/30 06:57:53 d2.data.common]: Serialized dataset takes 0.04 MiB\n[04/30 06:57:53 d2.evaluation.evaluator]: Start inference on 13 batches\n[04/30 06:57:56 d2.evaluation.evaluator]: Inference done 11/13. Dataloading: 0.0014 s/iter. Inference: 0.1043 s/iter. Eval: 0.0053 s/iter. Total: 0.1110 s/iter. ETA=0:00:00\n[04/30 06:57:56 d2.evaluation.evaluator]: Total inference time: 0:00:00.923665 (0.115458 s / iter per device, on 1 devices)\n[04/30 06:57:56 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:00 (0.103552 s / iter per device, on 1 devices)\n[04/30 06:57:56 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n[04/30 06:57:56 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n[04/30 06:57:56 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\nLoading and preparing results...\nDONE (t=0.00s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *bbox*\nDONE (t=0.01s).\nAccumulating evaluation results...\nDONE (t=0.01s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.746\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.859\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.838\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.269\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.551\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.901\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.238\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.764\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.764\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.267\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.571\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.923\n[04/30 06:57:56 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n|:------:|:------:|:------:|:------:|:------:|:------:|\n| 74.610 | 85.871 | 83.829 | 26.931 | 55.069 | 90.110 |\nLoading and preparing results...\nDONE (t=0.00s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=0.02s).\nAccumulating evaluation results...\nDONE (t=0.01s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.779\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.838\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.838\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.236\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.964\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.254\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.788\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.788\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.233\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.559\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.973\n[04/30 06:57:56 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n|:------:|:------:|:------:|:------:|:------:|:------:|\n| 77.919 | 83.829 | 83.829 | 23.564 | 53.851 | 96.356 |\nOrderedDict([('bbox', {'AP': 74.60969255997529, 'AP50': 85.87131716204989, 'AP75': 83.82940778549906, 'APs': 26.930693069306926, 'APm': 55.068926123381566, 'APl': 90.10970121205669}), ('segm', {'AP': 77.91911061287945, 'AP50': 83.82940778549906, 'AP75': 83.82940778549906, 'APs': 23.564356435643557, 'APm': 53.85148514851485, 'APl': 96.3558721256741})])\n\n\n\n\nE.4.4 Other types of builtin models\nWe showcase simple demos of other types of models below:\n\n# Inference with a keypoint detection model\ncfg = get_cfg()   # get a fresh new config\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\npredictor = DefaultPredictor(cfg)\noutputs = predictor(im)\nv = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\nout = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\ncv2_imshow(out.get_image()[:, :, ::-1])\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n# Inference with a panoptic segmentation model\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\npredictor = DefaultPredictor(cfg)\npanoptic_seg, segments_info = predictor(im)[\"panoptic_seg\"]\nv = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\nout = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\ncv2_imshow(out.get_image()[:, :, ::-1])\n\n[04/30 06:58:22 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl ...\n\n\nmodel_final_cafdb1.pkl: 261MB [00:02, 97.8MB/s]"
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_pytorch.html#image-segmentation-via-fastai",
    "href": "09_Convolutional_NeuralNetworks_pytorch.html#image-segmentation-via-fastai",
    "title": "Appendix E — Image processing with Convolutional Neural Networks - Pytorch",
    "section": "E.5 Image segmentation via fastai",
    "text": "E.5 Image segmentation via fastai\nCreating a model that can recognize the content of every individual pixel in an image is called segmentation. Here is how we can train a segmentation model with fastai, using a subset of the Camvid dataset from the paper “Semantic Object Classes in Video: A High-Definition Ground Truth Database” by Gabruel J. Brostow, Julien Fauqueur, and Roberto Cipolla:\n\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n    codes = np.loadtxt(path/'codes.txt', dtype=str)\n)\n\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(8)\n\n\n\n\n\n\n    \n      \n      100.18% [2318336/2314212 00:00&lt;00:00]\n    \n    \n\n\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00&lt;00:00, 130MB/s]\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n3.086789\n2.394335\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.912205\n1.644341\n00:01\n\n\n1\n1.613313\n1.217818\n00:01\n\n\n2\n1.505060\n1.243426\n00:01\n\n\n3\n1.352298\n0.977219\n00:01\n\n\n4\n1.208431\n0.888610\n00:01\n\n\n5\n1.088054\n0.788914\n00:01\n\n\n6\n0.990660\n0.758133\n00:01\n\n\n7\n0.914747\n0.755208\n00:01\n\n\n\n\n\nWe can visualize how well it achieved its task, by asking the model to color-code each pixel of an image. As you can see, it nearly perfectly classifies every pixel in every object. For instance, notice that all of the cars are overlaid with the same color and all of the trees are overlaid with the same color (in each pair of images, the lefthand image is the ground truth label and the right is the prediction from the model):\n\nlearn.show_results(max_n=4, figsize=(7,8))"
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_pytorch.html#data-cleaning-with-cleanvision",
    "href": "09_Convolutional_NeuralNetworks_pytorch.html#data-cleaning-with-cleanvision",
    "title": "Appendix E — Image processing with Convolutional Neural Networks - Pytorch",
    "section": "E.6 Data cleaning with CleanVision",
    "text": "E.6 Data cleaning with CleanVision\nCleanVision is built to automatically detects various issues in image datasets. This data-centric AI package is designed as a quick first step for any computer vision project to find problems in your dataset, which you may want to address before applying machine learning. The following Issue Key column specifies the name for each type of issue in CleanVision code.\n\n\n\n\n\n\n\n\n\n\nIssue Type\nDescription\nIssue Key\n\n\n\n\n1\nLight\nImages that are too bright/washed out in the dataset\nlight\n\n\n2\nDark\nImages that are irregularly dark\ndark\n\n\n3\nOdd Aspect Ratio\nImages with an unusual aspect ratio (i.e. overly skinny/wide)\nodd_aspect_ratio\n\n\n4\nExact Duplicates\nImages that are exact duplicates of each other\nexact_duplicates\n\n\n5\nNear Duplicates\nImages that are almost visually identical to each other (e.g. same image with different filters)\nnear_duplicates\n\n\n6\nBlurry\nImages that are blurry or out of focus\nblurry\n\n\n7\nGrayscale\nImages that are grayscale (lacking color)\ngrayscale\n\n\n8\nLow Information\nImages that lack much information (e.g. a completely black image with a few white dots)\nlow_information\n\n\n\n\n!wget - nc 'https://cleanlab-public.s3.amazonaws.com/CleanVision/image_files.zip'\n!unzip -q image_files.zip\n\n--2023-04-30 06:34:14--  http://-/\nResolving - (-)... failed: Name or service not known.\nwget: unable to resolve host address ‘-’\n--2023-04-30 06:34:14--  http://nc/\nResolving nc (nc)... failed: No address associated with hostname.\nwget: unable to resolve host address ‘nc’\n--2023-04-30 06:34:14--  https://cleanlab-public.s3.amazonaws.com/CleanVision/image_files.zip\nResolving cleanlab-public.s3.amazonaws.com (cleanlab-public.s3.amazonaws.com)... 52.216.108.19, 52.217.203.193, 52.217.33.132, ...\nConnecting to cleanlab-public.s3.amazonaws.com (cleanlab-public.s3.amazonaws.com)|52.216.108.19|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 78293407 (75M) [application/zip]\nSaving to: ‘image_files.zip’\n\nimage_files.zip     100%[===================&gt;]  74.67M  67.9MB/s    in 1.1s    \n\n2023-04-30 06:34:16 (67.9 MB/s) - ‘image_files.zip’ saved [78293407/78293407]\n\nFINISHED --2023-04-30 06:34:16--\nTotal wall clock time: 1.4s\nDownloaded: 1 files, 75M in 1.1s (67.9 MB/s)\n\n\n\n# Path to your dataset, you can specify your own dataset path\ndataset_path = \"./image_files/\"\n\n# Initialize imagelab with your dataset\nimagelab = Imagelab(data_path=dataset_path)\n\n# Visualize a few sample images from the dataset\nimagelab.visualize(num_images=8)\n\nReading images from /content/image_files\nSample images from the dataset\n\n\n\n\n\n\n# Find issues\n# You can also specify issue types to detect, for example\n# issue_types = {\"dark\": {}}\n# imagelab.find_issues(issue_types)\nimagelab.find_issues()\n\nChecking for dark, light, odd_aspect_ratio, low_information, exact_duplicates, near_duplicates, blurry, grayscale images ...\n\n\n100%|██████████| 595/595 [00:01&lt;00:00, 417.82it/s]\n100%|██████████| 595/595 [00:00&lt;00:00, 728.59it/s] \n\n\nIssue checks completed. To see a detailed report of issues found, use imagelab.report().\n\n\n\n\n\nThe report() method helps you quickly understand the major issues detected in the dataset. It reports the number of images in the dataset that exhibit each type of issue, and shows example images corresponding to the most severe instances of each issue.\n\nimagelab.report()\n\nIssues found in order of severity in the dataset\n\n|    | issue_type       |   num_images |\n|---:|:-----------------|-------------:|\n|  0 | grayscale        |           20 |\n|  1 | near_duplicates  |           20 |\n|  2 | exact_duplicates |           19 |\n|  3 | dark             |           13 |\n|  4 | blurry           |           10 |\n|  5 | odd_aspect_ratio |            8 |\n|  6 | light            |            5 |\n|  7 | low_information  |            4 | \n\n\nTop 4 examples with grayscale issue in the dataset.\n\n\n\n\n\n\nTop 4 sets of images with near_duplicates issue\nSet: 0\n\n\n\n\n\nSet: 1\n\n\n\n\n\nSet: 2\n\n\n\n\n\nSet: 3\n\n\n\n\n\n\nTop 4 sets of images with exact_duplicates issue\nSet: 0\n\n\n\n\n\nSet: 1\n\n\n\n\n\nSet: 2\n\n\n\n\n\nSet: 3\n\n\n\n\n\n\nTop 4 examples with dark issue in the dataset.\n\n\n\n\n\n\nTop 4 examples with blurry issue in the dataset.\n\n\n\n\n\n\nTop 4 examples with odd_aspect_ratio issue in the dataset.\n\n\n\n\n\n\nTop 4 examples with light issue in the dataset.\n\n\n\n\n\n\nTop 4 examples with low_information issue in the dataset.\n\n\n\n\n\nThe main way to interface with your data is via the Imagelab class. This class can be used to understand the issues in your dataset at a high level (global overview) and low level (issues and quality scores for each image) as well as additional information about the dataset. It has three main attributes:\n\nImagelab.issue_summary\nImagelab.issues\nImagelab.info\n\n\nE.6.1 imagelab.issue_summary\nDataframe with global summary of all issue types detected in your dataset and the overall prevalence of each type.\nIn each row: - issue_type - name of the issue - num_images - number of images of that issue type found in the dataset\n\nimagelab.issue_summary\n\n\n  \n    \n      \n\n\n\n\n\n\nissue_type\nnum_images\n\n\n\n\n0\ngrayscale\n20\n\n\n1\nnear_duplicates\n20\n\n\n2\nexact_duplicates\n19\n\n\n3\ndark\n13\n\n\n4\nblurry\n10\n\n\n5\nodd_aspect_ratio\n8\n\n\n6\nlight\n5\n\n\n7\nlow_information\n4\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nE.6.2 imagelab.issues\nDataFrame assessing each image in your dataset, reporting which issues each image exhibits and a quality score for each type of issue.\n\nimagelab.issues.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nodd_aspect_ratio_score\nis_odd_aspect_ratio_issue\nlow_information_score\nis_low_information_issue\nlight_score\nis_light_issue\ngrayscale_score\nis_grayscale_issue\ndark_score\nis_dark_issue\nblurry_score\nis_blurry_issue\nis_exact_duplicates_issue\nis_near_duplicates_issue\n\n\n\n\n/content/image_files/image_0.png\n1.0\nFalse\n0.806332\nFalse\n0.925490\nFalse\n1\nFalse\n1.000000\nFalse\n0.373038\nFalse\nFalse\nFalse\n\n\n/content/image_files/image_1.png\n1.0\nFalse\n0.923116\nFalse\n0.906609\nFalse\n1\nFalse\n0.990676\nFalse\n0.345064\nFalse\nFalse\nFalse\n\n\n/content/image_files/image_10.png\n1.0\nFalse\n0.875129\nFalse\n0.995127\nFalse\n1\nFalse\n0.795937\nFalse\n0.534317\nFalse\nFalse\nFalse\n\n\n/content/image_files/image_100.png\n1.0\nFalse\n0.916140\nFalse\n0.889762\nFalse\n1\nFalse\n0.827587\nFalse\n0.494283\nFalse\nFalse\nFalse\n\n\n/content/image_files/image_101.png\n1.0\nFalse\n0.779338\nFalse\n0.960784\nFalse\n0\nTrue\n0.992157\nFalse\n0.471333\nFalse\nFalse\nFalse\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThere is a Boolean column for each issue type, showing whether each image exhibits that type of issue or not. For example, the rows where the is_dark_issue column contains True, those rows correspond to images that appear too dark. For the dark issue type (and more generally for other types of issues), there is a numeric column dark_score, which assesses how severe this issue is in each image. These quality scores lie between 0 and 1, where lower values indicate more severe instances of the issue (images which are darker in this example).\nOne use-case for imagelab.issues is to filter out all images exhibiting one particular type of issue and rank them by their quality score. Here’s how to get all blurry images ranked by their blurry_score, note lower scores indicate higher severity:\n\nblurry_images = imagelab.issues[imagelab.issues[\"is_blurry_issue\"] == True].sort_values(by=['blurry_score'])\nblurry_image_files = blurry_images.index.tolist()\n\n\nimagelab.visualize(image_files=blurry_image_files[:4])\n\n\n\n\nThe imagelab.visualize() also allows you can use to see examples of specific issues in your dataset. num_images and cell_size are optional arguments, that you can use to control number of examples of each issue type and size of each image in the grid respectively.\n\nissue_types = [\"grayscale\"]\nimagelab.visualize(issue_types=issue_types, num_images=8, cell_size=(3, 3))\n\n\nTop 8 examples with grayscale issue in the dataset.\n\n\n\n\n\n\n\nE.6.3 imagelab.info\nThis is a nested dictionary containing statistics about the images and other miscellaneous information stored while checking for issues in the dataset Possible keys in this dict are statistics and a key corresponding to each issue type\n\nimagelab.info.keys()\n\ndict_keys(['statistics', 'dark', 'light', 'odd_aspect_ratio', 'low_information', 'blurry', 'grayscale', 'exact_duplicates', 'near_duplicates'])\n\n\nimagelab.info['statistics'] is also a dict containing statistics calculated on images that are used for checking for issues in the dataset.\n\nimagelab.info['statistics'].keys()\n\ndict_keys(['brightness', 'aspect_ratio', 'entropy', 'blurriness', 'color_space'])\n\n\nimagelab.info can also be used to retrieve which images are near or exact duplicates of each other. issue.summary shows the number of exact duplicate images but does not show how many such sets of duplicates images exist in the dataset. To see the number of exact duplicate sets, you can use imagelab.info:\n\nimagelab.info['exact_duplicates']['num_sets']\n\n9\n\n\nYou can also get exactly which images are there in each (exact/near) duplicated set using imagelab.info.\n\nimagelab.info['exact_duplicates']['sets']\n\n[['/content/image_files/image_142.png', '/content/image_files/image_236.png'],\n ['/content/image_files/image_170.png', '/content/image_files/image_299.png'],\n ['/content/image_files/image_190.png', '/content/image_files/image_197.png'],\n ['/content/image_files/image_288.png', '/content/image_files/image_289.png'],\n ['/content/image_files/image_292.png',\n  '/content/image_files/image_348.png',\n  '/content/image_files/image_492.png'],\n ['/content/image_files/image_30.png', '/content/image_files/image_55.png'],\n ['/content/image_files/image_351.png', '/content/image_files/image_372.png'],\n ['/content/image_files/image_379.png', '/content/image_files/image_579.png'],\n ['/content/image_files/image_550.png', '/content/image_files/image_7.png']]\n\n\n\n\nE.6.4 Check for an issue with a different threshold\nYou can use the loaded imagelab instance to check for an issue type with a custom hyperparameter. Here is a table of hyperparameters that each issue type supports and their permissible values:\n\nthreshold- All images with scores below this threshold will be flagged as an issue.\nhash_size - This controls how much detail about an image we want to keep for getting perceptual hash. Higher sizes imply more detail.\nhash_type - Type of perceptual hash to use. Currently whash and phash are the supported hash types. Check here for more details on these hash types.\n\n\n\n\n\n\n\n\n\n\nIssue Key\nHyperparameters\n\n\n\n\n1\nlight\nthreshold (between 0 and 1)\n\n\n2\ndark\nthreshold (between 0 and 1)\n\n\n3\nodd_aspect_ratio\nthreshold (between 0 and 1)\n\n\n4\nexact_duplicates\nN/A\n\n\n5\nnear_duplicates\nhash_size (power of 2), hash_types (whash, phash)\n\n\n6\nblurry\nthreshold (between 0 and 1)\n\n\n7\ngrayscale\nthreshold (between 0 and 1)\n\n\n8\nlow_information\nthreshold (between 0 and 1)\n\n\n\n\nissue_types = {\"dark\": {\"threshold\": 0.2}}\nimagelab.find_issues(issue_types)\n\nimagelab.report(issue_types)\n\nChecking for dark images ...\nIssue checks completed. To see a detailed report of issues found, use imagelab.report().\nIssues found in order of severity in the dataset\n\n|    | issue_type   |   num_images |\n|---:|:-------------|-------------:|\n|  5 | dark         |            8 | \n\n\nTop 4 examples with dark issue in the dataset.\n\n\n\n\n\nNote the number of images with dark issue has reduced from the previous run!\n\n\nE.6.5 Save and load\nCleanVision also has a save and load functionality that you can use to save the results and load them at a later point in time to see results or run more checks. For saving, specify force=True to overwrite existing files:\n\nsave_path = \"./results\"\nimagelab.save(save_path)\n\nSaved Imagelab to folder: ./results\nThe data path and dataset must be not be changed to maintain consistent state when loading this Imagelab\n\n\n\n## For loading a saved instance, specify `dataset_path` \n## to help check for any inconsistencies between dataset paths in the previous and current run.\nimagelab = Imagelab.load(save_path, dataset_path)\n\nSuccessfully loaded Imagelab"
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_pytorch.html#lable-issue-with-cleanlab",
    "href": "09_Convolutional_NeuralNetworks_pytorch.html#lable-issue-with-cleanlab",
    "title": "Appendix E — Image processing with Convolutional Neural Networks - Pytorch",
    "section": "E.7 Lable issue with Cleanlab",
    "text": "E.7 Lable issue with Cleanlab\n\nmnist = fetch_openml(\"mnist_784\")  # Fetch the MNIST dataset\n\nX = mnist.data.astype(\"float32\").to_numpy() # 2D array (images are flattened into 1D)\nX /= 255.0  # Scale the features to the [0, 1] range\n\nX = X.reshape(len(X), 1, 28, 28)  # reshape into [N, C, H, W] for PyTorch\nlabels = mnist.target.astype(\"int64\").to_numpy()  # 1D array of given labels\n\n/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n\n\n\nE.7.1 Ensure your classifier is scikit-learn compatible\nHere, we define a simple neural network with Pytroch\n\n# We use subclassing API here\nclass ClassifierModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.cnn = nn.Sequential(\n            nn.Conv2d(1, 6, 3),\n            nn.ReLU(),\n            nn.BatchNorm2d(6),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(6, 16, 3),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.out = nn.Sequential(\n            nn.Flatten(),\n            nn.LazyLinear(128), # A torch.nn.Linear module where in_features is inferred!\n            nn.ReLU(),\n            nn.Linear(128, 10),\n            nn.Softmax(dim=-1),\n        )\n\n    def forward(self, X):\n        X = self.cnn(X)\n        X = self.out(X)\n        return X\n\nAs some cleanlab features require scikit-learn compatibility, we adapt the above keras neural net accordingly. skorch is a convenient package that helps with this:\n\nclf = NeuralNetClassifier(ClassifierModule)\n\n\n\nE.7.2 Compute out-of-sample predicted probabilities\nIf we’d like cleanlab to identify potential label errors in the whole dataset and not just the training set, we can consider using the entire dataset when computing the out-of-sample predicted probabilities, pred_probs, via cross-validation.\n\nnum_crossval_folds = 3  # for efficiency; values like 5 or 10 will generally work better\npred_probs = cross_val_predict(\n    clf,\n    X,\n    labels,\n    cv=num_crossval_folds,\n    method=\"predict_proba\",\n)\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n  warnings.warn('Lazy modules are a new feature under heavy development '\n\n\n  epoch    train_loss    valid_acc    valid_loss     dur\n-------  ------------  -----------  ------------  ------\n      1        0.7562       0.9165        0.3194  2.2443\n      2        0.2114       0.9421        0.1991  2.4727\n      3        0.1476       0.9547        0.1553  2.1834\n      4        0.1185       0.9610        0.1323  2.1328\n      5        0.1008       0.9643        0.1175  2.1259\n      6        0.0889       0.9660        0.1079  2.1473\n      7        0.0800       0.9694        0.1005  2.2020\n      8        0.0733       0.9712        0.0949  2.7131\n      9        0.0678       0.9724        0.0899  2.7966\n     10        0.0633       0.9731        0.0862  2.4333\n\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n  warnings.warn('Lazy modules are a new feature under heavy development '\n\n\n  epoch    train_loss    valid_acc    valid_loss     dur\n-------  ------------  -----------  ------------  ------\n      1        0.7694       0.9164        0.3139  2.1668\n      2        0.2229       0.9447        0.2009  2.3349\n      3        0.1564       0.9556        0.1608  2.3366\n      4        0.1261       0.9598        0.1386  2.1405\n      5        0.1074       0.9631        0.1240  2.1643\n      6        0.0944       0.9654        0.1130  2.1670\n      7        0.0846       0.9678        0.1041  2.1175\n      8        0.0767       0.9702        0.0975  2.5494\n      9        0.0704       0.9726        0.0919  2.1594\n     10        0.0651       0.9736        0.0875  2.1436\n\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n  warnings.warn('Lazy modules are a new feature under heavy development '\n\n\n  epoch    train_loss    valid_acc    valid_loss     dur\n-------  ------------  -----------  ------------  ------\n      1        0.7947       0.9176        0.3226  2.3926\n      2        0.2277       0.9468        0.1976  2.4256\n      3        0.1590       0.9559        0.1524  2.4540\n      4        0.1276       0.9618        0.1291  2.1569\n      5        0.1087       0.9651        0.1149  2.1608\n      6        0.0959       0.9674        0.1052  2.1491\n      7        0.0863       0.9687        0.0981  2.1503\n      8        0.0789       0.9714        0.0922  2.2473\n      9        0.0727       0.9728        0.0879  2.3513\n     10        0.0676       0.9738        0.0839  2.1506\n\n\nAn additional benefit of cross-validation is that it facilitates more reliable evaluation of our model than a single training/validation split.\n\npredicted_labels = pred_probs.argmax(axis=1)\nacc = accuracy_score(labels, predicted_labels)\nprint(f\"Cross-validated estimate of accuracy on held-out data: {acc}\")\n\nCross-validated estimate of accuracy on held-out data: 0.9765857142857143\n\n\n\n\nE.7.3 Use cleanlab to find label issues\nBased on the given labels and out-of-sample predicted probabilities, cleanlab can quickly help us identify label issues in our dataset. For a dataset with N examples from K classes, the labels should be a 1D array of length N and predicted probabilities should be a 2D (N x K) array. Here we request that the indices of the identified label issues be sorted by cleanlab’s self-confidence score, which measures the quality of each given label via the probability assigned to it in our model’s prediction.\n\nranked_label_issues = find_label_issues(\n    labels,\n    pred_probs,\n    return_indices_ranked_by=\"self_confidence\",\n)\n\nprint(f\"Cleanlab found {len(ranked_label_issues)} label issues.\")\nprint(f\"Top 15 most likely label errors: \\n {ranked_label_issues[:15]}\")\n\nCleanlab found 136 label issues.\nTop 15 most likely label errors: \n [59915 24798 28556  8200 26882  6448 63520  7010  1604   902 51248 23824\n 20672 22643 53216]\n\n\nranked_label_issues() is a list of indices corresponding to examples that are worth inspecting more closely.\nLet’s look at the top 15 examples cleanlab thinks are most likely to be incorrectly labeled. We can see a few label errors and odd edge cases. Feel free to change the values below to display more/fewer examples.\n\nplot_examples(ranked_label_issues[range(15)], 3, 5)\n\n\n\n\nLet’s zoom into some specific examples from the above set:\nGiven label is 4 but looks more like a 7:\n\nplot_examples([59915])\n\n\n\n\nGiven label is 4 but looks more like a 9:\n\nplot_examples([24798])\n\n\n\n\nA very odd looking 6:\n\nplot_examples([63520])\n\n\n\n\ncleanlab has shortlisted the most likely label errors to speed up your data cleaning process. With this list, you can decide whether to fix label issues or prune some of these examples from the dataset."
  },
  {
    "objectID": "09_Convolutional_NeuralNetworks_pytorch.html#references",
    "href": "09_Convolutional_NeuralNetworks_pytorch.html#references",
    "title": "Appendix E — Image processing with Convolutional Neural Networks - Pytorch",
    "section": "E.8 References",
    "text": "E.8 References\n\nhttps://github.com/ageron/handson-ml3/\nhttps://github.com/fchollet/deep-learning-with-python-notebooks_\nhttps://github.com/fastai/fastbook2e\nhttps://github.com/facebookresearch/detectron2\nhttps://github.com/cleanlab/cleanlab\nhttps://github.com/cleanlab/cleanvision"
  }
]