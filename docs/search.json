[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical and Innovative Analytics in Data Science",
    "section": "",
    "text": "1 Preface\nThis is the companion book for the course Practical and Innovative Analytics in Data Science open in the Department of Applied Mathematics, National Sun Yat-sen University. This course focuses on the practical aspect of data science in the real world. In the course, students will learn to engage in a real-world project requiring them to apply skills from the entire data science pipeline: preparing, organizing, and transforming data, constructing a model, and evaluating results. Moreover, high-level descriptions of how to apply deep learning for computer vision and natural language problems will also be covered.\nThe book is based on several well-known books and resources, including:\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html",
    "href": "01_end_to_end_machine_learning_project.html",
    "title": "2  End-to-end Machine Learning project",
    "section": "",
    "text": "3 Get the Data\nLet’s create a copy so you can play with it without harming the training set\nCode\nhousing = strat_train_set.copy()\n#housing = train_set.copy()\nLet’s separate the predictors and the labels since we don’t necessarily want to apply the same transformations to the predictors and the target values (note that drop() creates a copy of the data and does not affect strat_train_set):\nCode\nhousing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set X\nhousing_labels = strat_train_set[\"median_house_value\"].copy() # y\nAt last! You framed the problem, you got the data and explored it, you sampled a training set and a test set, and you wrote a preprocessing pipeline to automatically clean up and prepare your data for machine learning algorithms. You are now ready to select and train a machine learning model.\nLet’s assume that you now have a shortlist of promising models. You now need to fine-tune them. Let’s look at a few ways you can do that.\nPerfect, you got approval to launch! You now need to get your solution ready for production (e.g., polish the code, write documentation and tests, and so on). Then you can deploy your model to your production environment.\nThe most basic way to do this is just to save the best model you trained, transfer the file to your production environment, and load it. To save the model, you can use the joblib library like this:\nCode\nimport joblib\n\njoblib.dump(final_model, \"my_california_housing_model.pkl\")\n\n\n['my_california_housing_model.pkl']\nOnce your model is transferred to production, you can load it and use it. For this you must first import any custom classes and functions the model relies on (which means transferring the code to production), then load the model using joblib and use it to make predictions:\nCode\nimport joblib\n\nfinal_model_reloaded = joblib.load(\"my_california_housing_model.pkl\")\n\nnew_data = housing.iloc[:5]  # pretend these are new districts\npredictions = final_model_reloaded.predict(new_data)\nFor example, perhaps the model will be used within a website: the user will type in some data about a new district and click the Estimate Price button. This will send a query containing the data to the web server, which will forward it to your web application, and finally your code will simply call the model’s predict() method (you want to load the model upon server startup, rather than every time the model is used).\nAlternatively, you can wrap the model within a dedicated web service that your web application can query through a REST API. This makes it easier to upgrade your model to new versions without interrupting the main application. It also simplifies scaling, since you can start as many web services as needed and load-balance the requests coming from your web application across these web services. Moreover, it allows your web application to use any language, not just Python.\nAnother popular strategy is to deploy your model to the cloud, for example on Google’s Vertex AI (formerly known as Google Cloud AI Platform and Google Cloud ML Engine): just save your model using joblib and upload it to Google Cloud Storage (GCS), then head over to Vertex AI and create a new model version, pointing it to the GCS file. That’s it! This gives you a simple web service that takes care of load balancing and scaling for you.\nIt take JSON requests containing the input data (e.g., of a district) and returns JSON responses containing the predictions. You can then use this web service in your website (or whatever production environment you are using). As we will see in model serving lesson that we will use it on AI Platform is not much different from deploying Scikit-Learn models.\nBut deployment is not the end of the story. You also need to write monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops. This could be a steep drop, likely due to a broken component in your infrastructure, but be aware that it could also be a gentle decay that could easily go unnoticed for a long time. This is quite common because models tend to “rot” over time: indeed, the world changes, so if the model was trained with last year’s data, it may not be adapted to today’s data.\nPyCaret is a high-level, low-code Python library that makes it easy to compare, train, evaluate, tune, and deploy machine learning models with only a few lines of code.\nAt its core, PyCaret is basically just a large wrapper over many data science libraries such as Scikit-learn, Yellowbrick, SHAP, Optuna, and Spacy. Yes, you could use these libraries for the same tasks, but if you don’t want to write a lot of code, PyCaret could save you a lot of time.\nhttps://pycaret.readthedocs.io/en/latest/api/regression.html\nCode\n!pip install --pre pycaret[full] -qq\n\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 480.1/480.1 KB 25.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.2/9.2 MB 93.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.9/79.9 MB 11.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213.0/213.0 KB 28.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.2/280.2 KB 24.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.0/16.0 MB 73.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 81.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.8/106.8 KB 12.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.8/45.8 KB 6.5 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n  WARNING: Requested plotly-resampler&gt;=0.7.2.2 from https://files.pythonhosted.org/packages/d7/5e/71a9e34a36c1855d0c4e30a88405d58c4bbbe7ece802b188628a643f2cda/plotly_resampler-0.8.4rc1.tar.gz#sha256=154ebffa9778813fe457ac9112e71e085ac283644a1e26b869bd5c6b7bf064a2 (from pycaret[full]), but installing version 0.8.4rc1\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.2/81.2 KB 11.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 KB 9.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 KB 6.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 50.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.7/147.7 KB 20.6 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 84.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 99.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 93.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 324.4/324.4 KB 35.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.3/100.3 KB 14.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.2/362.2 KB 29.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.0/92.0 KB 10.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 61.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.6/76.6 MB 11.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.9/56.9 KB 7.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 70.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.0/83.0 KB 10.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.1/317.1 KB 28.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.2/88.2 KB 9.9 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 75.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 277.8/277.8 KB 29.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 575.9/575.9 KB 54.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.1/41.1 KB 6.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.5/177.5 KB 23.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.2/56.2 KB 7.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.2/14.2 MB 84.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.5/101.5 KB 17.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 365.3/365.3 KB 38.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.2/92.2 KB 12.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.4/57.4 MB 14.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.6/193.6 MB 7.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.7/64.7 KB 9.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.0/17.0 MB 68.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.7/132.7 KB 13.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 464.9/464.9 KB 37.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/12.9 MB 71.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.9/240.9 KB 19.2 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 66.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.6/16.6 MB 49.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 58.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/10.4 MB 105.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.6/79.6 KB 10.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.8/227.8 KB 1.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.3/54.3 KB 4.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 KB 8.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 508.4/508.4 KB 45.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/3.5 MB 94.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 KB 26.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.5/135.5 KB 20.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 793.8/793.8 KB 54.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.4/129.4 KB 17.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 KB 5.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 92.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 102.5 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 662.4/662.4 KB 53.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.7/57.7 KB 9.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.0/471.0 KB 50.7 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.0/47.0 KB 5.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 219.7/219.7 KB 25.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.1/83.1 KB 11.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.8/65.8 KB 10.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 KB 15.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 KB 19.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.7/232.7 KB 28.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 187.8/187.8 KB 22.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.1/154.1 KB 20.3 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.6/71.6 KB 8.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.5/227.5 KB 21.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 51.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.5/71.5 KB 9.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.0/107.0 KB 11.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.7/140.7 KB 15.9 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 59.1 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 KB 11.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.5/200.5 KB 21.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.9/8.9 MB 94.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 29.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 300.0/300.0 KB 19.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 965.4/965.4 KB 43.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.3/82.3 KB 11.5 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.5/147.5 KB 16.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.0/184.0 KB 23.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 KB 11.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 KB 6.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210.6/210.6 KB 22.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 27.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.1/51.1 KB 5.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 344.5/344.5 KB 34.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 68.8/68.8 KB 6.7 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.5/45.5 KB 4.8 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.6/44.6 KB 5.0 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.0/96.0 KB 8.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 185.1/185.1 KB 12.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.7/8.7 MB 8.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 KB 7.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.6/12.6 MB 10.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.5/85.5 MB 7.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.0/4.0 MB 22.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 24.0 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 KB 8.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.0/132.0 KB 16.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 983.2/983.2 KB 43.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 KB 11.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.9/55.9 KB 7.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 KB 17.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 934.8/934.8 KB 59.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 144.2/144.2 KB 20.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 KB 9.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 275.7/275.7 KB 32.1 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 72.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 78.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 758.0/758.0 KB 58.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 KB 7.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/18.5 MB 65.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 384.9/384.9 KB 39.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 KB 10.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.3/135.3 KB 15.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 468.5/468.5 KB 37.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.4/64.4 KB 8.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 136.5/136.5 KB 15.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.6/69.6 KB 8.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.0/121.0 KB 16.6 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.7/102.7 KB 10.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.4/9.4 MB 63.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 679.5/679.5 KB 43.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 87.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 296.5/296.5 KB 35.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 261.4/261.4 KB 27.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.4/133.4 KB 13.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.3/57.3 KB 8.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 357.2/357.2 KB 42.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.6/83.6 KB 11.6 MB/s eta 0:00:00\n  Building wheel for pyod (setup.py) ... done\n  Building wheel for umap-learn (setup.py) ... done\n  Building wheel for databricks-cli (setup.py) ... done\n  Building wheel for fugue-sql-antlr (setup.py) ... done\n  Building wheel for pynndescent (setup.py) ... done\n  Building wheel for PyNomaly (setup.py) ... done\n  Building wheel for dash-auth (setup.py) ... done\n  Building wheel for emoji (setup.py) ... done\n  Building wheel for ffmpy (setup.py) ... done\n  Building wheel for python-multipart (setup.py) ... done\n  Building wheel for htmlmin (setup.py) ... done\n  Building wheel for lime (setup.py) ... done\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 7.34.0 which is incompatible.\nRemember to restart the notebook to update the modules\nhttps://pycaret.gitbook.io/docs/get-started/functions/deploy\nhttps://pycaret.gitbook.io/docs/get-started/functions/others#get_config\nCode\ncreate_app(final_lightgbm)\n\n\nColab notebook detected. To show errors in colab notebook, set debug=True in launch()\nNote: opening Chrome Inspector may crash demo inside Colab notebooks.\n\nTo create a public link, set `share=True` in `launch()`.\nCode\n# deploy a model\n# deploy_model(final_lightgbm, model_name = 'final_lightgbm', platform = 'gcp', authentication = {'project': 'gcp-project-name', 'bucket' : 'gcp-bucket-name'})"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#setup",
    "href": "01_end_to_end_machine_learning_project.html#setup",
    "title": "2  End-to-end Machine Learning project",
    "section": "2.1 Setup",
    "text": "2.1 Setup\nFirst, let’s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20.\n\n\nCode\n!pip install scikit-learn -U -qq\n\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/9.8 MB ? eta -:--:--     ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/9.8 MB 64.2 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 5.1/9.8 MB 75.9 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 8.4/9.8 MB 80.8 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 9.8/9.8 MB 81.0 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 9.8/9.8 MB 81.0 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.8/9.8 MB 50.8 MB/s eta 0:00:00\n\n\nRemember to restart the notebook to update the modules\n\n\nCode\n# Python ≥3.7 is required\nimport sys\nassert sys.version_info &gt;= (3, 7)\n\n# Scikit-Learn ≥1.2 is recommend\nfrom packaging import version\nimport sklearn\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\n# Common imports\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os\nimport tarfile\nimport urllib.request\n\n\n# To plot pretty figures # This tells Jupyter to set up Matplotlib so it uses Jupyter’s own backend.\n# https://github.com/pycaret/pycaret/issues/319\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'    \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\nimport seaborn as sns\nsns.set_context(\"notebook\")\n\n# Where to save the figures\nIMAGES_PATH = Path() / \"images\" / \"end_to_end_project\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n\n\n\nCode\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\nMounted at /content/drive"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#download-the-data",
    "href": "01_end_to_end_machine_learning_project.html#download-the-data",
    "title": "2  End-to-end Machine Learning project",
    "section": "3.1 Download the Data",
    "text": "3.1 Download the Data\nIt is preferable to create a small function to do that. It is useful in particular\n\nIf data changes regularly, as it allows you to write a small script that you can run whenever you need to fetch the latest data (or you can set up a scheduled job to do that automatically at regular intervals).\nAutomating the process of fetching the data is also useful if you need to install the dataset on multiple machines.\n\nHere is the function to fetch and load the data:\n\n\nCode\ndef load_housing_data():\n    \"\"\"When load_housing_data() is called, it looks for the datasets/housing.tgz file. If it\n    does not find it, it creates the datasets directory inside the current directory\n    downloads the housing.tgz file from the ageron/data GitHub repository, \n    and extracts its content into the datasets directory\n    \"\"\"\n    tarball_path = Path(\"datasets/housing.tgz\")\n    if not tarball_path.is_file():\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n        urllib.request.urlretrieve(url, tarball_path)\n        with tarfile.open(tarball_path) as housing_tarball:\n            housing_tarball.extractall(path=\"datasets\")\n    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#take-a-quick-look-at-the-data-structure",
    "href": "01_end_to_end_machine_learning_project.html#take-a-quick-look-at-the-data-structure",
    "title": "2  End-to-end Machine Learning project",
    "section": "3.2 Take a Quick Look at the Data Structure",
    "text": "3.2 Take a Quick Look at the Data Structure\nNow let’s load the data using Pandas. Once again you should write a small function to load the data:\n\n\nCode\nhousing = load_housing_data()\n\n\nhttps://www.kaggle.com/camnugent/california-housing-prices\n\n\nCode\n#Let’s take a look at the top five rows.\nhousing.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nEach row represents one district. There are 10 attributes\n\n\nCode\n# The info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values\nhousing.info(), housing.shape \n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\n(None, (20640, 10))\n\n\nNotice that the total_bedrooms attribute has only 20,433 non-null values, meaning that 207 districts are missing this feature. We will need to take care of this later.\nAll attributes are numerical, except for ocean_proximity. Its type is object, so it could hold any kind of Python object. But since you loaded this data from a CSV file, you know that it must be a text attribute. When you looked at the top five rows, you probably noticed that the values in the ocean_proximity column were repetitive, which means that it is probably a categorical attribute. You can find out what categories exist and how many districts belong to each category by using the value_counts() method.\n\n\nCode\nhousing[\"ocean_proximity\"].value_counts()\n\n\n&lt;1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: ocean_proximity, dtype: int64\n\n\nNote that the null values are ignored below (so, for example, count of total_bedrooms is 20,433, not 20,640).\n\n\nCode\nhousing.describe() # The describe() method shows a summary of the numerical attributes\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAnother quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis).\nYou can either plot this one attribute at a time, or you can call the hist() method on the whole dataset (dataframe), and it will plot a histogram for each numerical attribute:\n\n\nCode\nhousing.hist(bins=50, figsize=(12,8))\nplt.show()\n\n\n\n\n\nNotice a few things in these histograms: 1. First, the median income attribute does not look like it is expressed in US dollars (USD). After checking with the team that collected the data, you are told that the data has been scaled and capped at 15 (actually 15.0001) for higher median incomes, and at 0.5 (actually 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about $30,000). Working with preprocessed attributes is common in Machine Learning, and it is not necessarily a problem, but you should try to understand how the data was computed.\n\nThe housing median age and the median house value were also capped. The latter may be a serious problem since it is your target attribute (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond $500,000, then you have mainly two options:\n\n\nCollect proper labels for the districts whose labels were capped.\nRemove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond $500,000).\n\n\nThese attributes have very different scales. We will discuss this later in this chapter when we explore feature scaling.\nFinally, many histograms are heavy tail and skewed right: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes later on to have more bell-shaped distributions."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#create-a-test-set",
    "href": "01_end_to_end_machine_learning_project.html#create-a-test-set",
    "title": "2  End-to-end Machine Learning project",
    "section": "3.3 Create a Test Set",
    "text": "3.3 Create a Test Set\nTo avoid the data snooping bias. Creating a test set! This is theoretically quite simple: just pick some instances randomly, typically 20% of the dataset (or less if your dataset is very large), and set them aside\n\n\nCode\n# to make this notebook's output identical at every run\n# https://en.wikipedia.org/wiki/42_(number)\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\n\n\n\nCode\nlen(train_set)\n\n\n16512\n\n\n\n\nCode\nlen(test_set)\n\n\n4128\n\n\n\n\nCode\ntest_set.head(10)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n20046\n-122.38\n40.67\n10.0\n2281.0\n444.0\n1274.0\n438.0\n2.2120\n65600.0\nINLAND\n\n\n3024\n-118.37\n33.83\n35.0\n1207.0\n207.0\n601.0\n213.0\n4.7308\n353400.0\n&lt;1H OCEAN\n\n\n15663\n-117.24\n32.72\n39.0\n3089.0\n431.0\n1175.0\n432.0\n7.5925\n466700.0\nNEAR OCEAN\n\n\n20484\n-118.44\n34.05\n18.0\n4780.0\n1192.0\n1886.0\n1036.0\n4.4674\n500001.0\n&lt;1H OCEAN\n\n\n9814\n-118.44\n34.18\n33.0\n2127.0\n414.0\n1056.0\n391.0\n4.3750\n286100.0\n&lt;1H OCEAN\n\n\n13311\n-121.76\n36.92\n36.0\n2096.0\n409.0\n1454.0\n394.0\n3.2216\n238300.0\n&lt;1H OCEAN\n\n\n7113\n-120.45\n34.91\n16.0\n712.0\n147.0\n355.0\n162.0\n2.5600\n150000.0\n&lt;1H OCEAN\n\n\n7668\n-117.86\n33.79\n31.0\n3523.0\n922.0\n2660.0\n949.0\n3.1792\n146400.0\n&lt;1H OCEAN\n\n\n18246\n-117.43\n33.91\n15.0\n14281.0\n2511.0\n7540.0\n2245.0\n4.3222\n138000.0\nINLAND\n\n\n5723\n-118.18\n34.13\n44.0\n2734.0\n415.0\n1057.0\n424.0\n7.9213\n477800.0\n&lt;1H OCEAN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n3.3.1 Optional\nSo far we have considered purely random sampling methods. This is generally fine if your dataset is large enough (especially relative to the number of attributes), but if it is not, you run the risk of introducing a significant sampling bias. When a survey company decides to call 1,000 people to ask them a few questions, they don’t just pick 1,000 people randomly in a phone book. They try to ensure that these 1,000 people are representative of the whole population.\nFor example, the US population is composed of 51.3% female and 48.7% male, so a well-conducted survey in the US would try to maintain this ratio in the sample: 513 female and 487 male. This is called stratified sampling: the population is divided into homogeneous subgroups called strata, and the right number of instances is sampled from each stratum to guarantee that the test set is representative of the overall population. If they used purely random sampling, the survey results may be significantly biased.\nSuppose you chatted with experts who told you that the median income is a very important attribute to predict median housing prices. You may want to ensure that the test set is representative of the various categories of incomes in the whole dataset.\nSince the median income is a continuous numerical attribute, you first need to create an income category attribute. Let’s look at the median income histogram more closely.\n\n\nCode\nhousing[\"median_income\"].hist()\nplt.show()\n\n\n\n\n\nMost median income values are clustered around 1.5 to 6 (i.e., $15,000 – $60,000), but some median incomes go far beyond 6. It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of the stratum’s importance may be biased. This means that you should not have too many strata, and each stratum should be large enough. The following code uses the pd.cut() function to create an income category attribute with 5 categories (labeled from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from 1.5 to 3, and so on:\n\n\nCode\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\n\n\n\n\nCode\nhousing[\"income_cat\"].value_counts()\n\n\n3    7236\n2    6581\n4    3639\n5    2362\n1     822\nName: income_cat, dtype: int64\n\n\n\n\nCode\nhousing[\"income_cat\"].hist()\nplt.show()\n\n\n\n\n\nNow you are ready to do stratified sampling based on the income category. For this you can use Scikit-Learn’s StratifiedShuffleSplit class:\nhttps://scikit-learn.org/stable/modules/cross_validation.html#stratified-shuffle-split\n\n\nCode\nstrat_train_set, strat_test_set = train_test_split(\n    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)\n\n\nLet’s see if this worked as expected. You can start by looking at the income category proportions in the test set:\n\n\nCode\nstrat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n\n\n3    0.350533\n2    0.318798\n4    0.176357\n5    0.114341\n1    0.039971\nName: income_cat, dtype: float64\n\n\n\n\nCode\nhousing[\"income_cat\"].value_counts() / len(housing)\n\n\n3    0.350581\n2    0.318847\n4    0.176308\n5    0.114438\n1    0.039826\nName: income_cat, dtype: float64\n\n\nWith similar code you can measure the income category proportions in the full dataset. Below we compare the income category proportions in the overall dataset, in the test set generated with stratified sampling, and in a test set generated using purely random sampling. As you can see, the test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is quite skewed.\n\n\nCode\ndef income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() / len(data)\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall %\": income_cat_proportions(housing),\n    \"Stratified %\": income_cat_proportions(strat_test_set),\n    \"Random %\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props.index.name = \"Income Category\"\ncompare_props[\"Strat. Error %\"] = (compare_props[\"Stratified %\"]/compare_props[\"Overall %\"] - 1)\ncompare_props[\"Rand. Error %\"] = (compare_props[\"Random %\"]/compare_props[\"Overall %\"] - 1)\n(compare_props * 100).round(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nOverall %\nStratified %\nRandom %\nStrat. Error %\nRand. Error %\n\n\nIncome Category\n\n\n\n\n\n\n\n\n\n1\n3.98\n4.00\n4.24\n0.36\n6.45\n\n\n2\n31.88\n31.88\n30.74\n-0.02\n-3.59\n\n\n3\n35.06\n35.05\n34.52\n-0.01\n-1.53\n\n\n4\n17.63\n17.64\n18.41\n0.03\n4.42\n\n\n5\n11.44\n11.43\n12.09\n-0.08\n5.63\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou won’t use the income_cat column again, so you might as well drop it, reverting the data back to its original state:\n\n\nCode\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#visualizing-geographical-data",
    "href": "01_end_to_end_machine_learning_project.html#visualizing-geographical-data",
    "title": "2  End-to-end Machine Learning project",
    "section": "4.1 Visualizing Geographical Data",
    "text": "4.1 Visualizing Geographical Data\nSince there is geographical information (latitude and longitude), it is a good idea to create a scatterplot of all districts to visualize the data\n\n\nCode\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True)\nplt.show()\n\n\n\n\n\nThis looks like California all right, but other than that it is hard to see any particular pattern. Setting the alpha option to 0.2 makes it much easier to visualize the places where there is a high density of data points\n\n\nCode\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2)\nsave_fig(\"better_visualization_plot\")\n\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nNow that’s much better: you can clearly see the high-density areas, namely the Bay Area and around Los Angeles and San Diego, plus a long line of fairly high-density areas in the Central Valley.\nNow let’s look at the housing prices (Figure 2-13). The radius of each circle represents the district’s population (option s), and the color represents the price (option c). We will use a predefined color map (option cmap) called jet, which ranges from blue (low values) to red (high prices):\nThe argument sharex=False fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https://github.com/pandas-dev/pandas/issues/10611 ).\n\n\nCode\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n             s= housing[\"population\"] / 100, label=\"population\",\n             c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n             legend=True, sharex=False, figsize=(10, 7))\nplt.show()\n\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nThis image tells you that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density, as you probably knew already.\nIt will probably be useful to use a clustering algorithm to detect the main clusters, and add new features that measure the proximity to the cluster centers. The ocean proximity attribute may be useful as well, although in Northern California the housing prices in coastal districts are not too high, so it is not a simple rule.\n\n\nCode\n# Download the California image\nfilename = \"california.png\"\nif not (IMAGES_PATH / filename).is_file():\n    homl3_root = \"https://github.com/ageron/handson-ml3/raw/main/\"\n    url = homl3_root + \"images/end_to_end_project/\" + filename\n    print(\"Downloading\", filename)\n    urllib.request.urlretrieve(url, IMAGES_PATH / filename)\n\n\nDownloading california.png\n\n\n\n\nCode\nhousing_renamed = housing.rename(columns={\n    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n    \"population\": \"Population\",\n    \"median_house_value\": \"Median house value (ᴜsᴅ)\"})\nhousing_renamed.plot(\n             kind=\"scatter\", x=\"Longitude\", y=\"Latitude\",\n             s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n             c=\"Median house value (ᴜsᴅ)\", cmap=\"jet\", colorbar=True,\n             legend=True, sharex=False, figsize=(10, 7))\n\ncalifornia_img = plt.imread(IMAGES_PATH / filename)\naxis = -124.55, -113.95, 32.45, 42.05\nplt.axis(axis)\nplt.imshow(california_img, extent=axis)\nplt.show()\n\n\nOutput hidden; open in https://colab.research.google.com to view."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#looking-for-correlations",
    "href": "01_end_to_end_machine_learning_project.html#looking-for-correlations",
    "title": "2  End-to-end Machine Learning project",
    "section": "4.2 Looking for Correlations",
    "text": "4.2 Looking for Correlations\nSince the dataset is not too large, you can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes\n\n\nCode\ncorr_matrix = housing.corr()\n\n\nNow let’s look at how much each attribute correlates with the median house value:\n\n\nCode\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\n\nmedian_house_value    1.000000\nmedian_income         0.688380\ntotal_rooms           0.137455\nhousing_median_age    0.102175\nhouseholds            0.071426\ntotal_bedrooms        0.054635\npopulation           -0.020153\nlongitude            -0.050859\nlatitude             -0.139584\nName: median_house_value, dtype: float64\n\n\nThe correlation coefficient ranges from –1 to 1. When it is close to 1, it means that there is a strong positive correlation; For example, the median house value tends to go up when the median income goes up.\nWhen the coefficient is close to –1, it means that there is a strong negative correlation; you can see a small negative correlation between the latitude and the median house value (i.e., prices have a slight tendency to go down when you go north).\nFinally, coefficients close to zero mean that there is no linear correlation. It may completely miss out on nonlinear relationships (e.g., “if x is close to zero then y generally goes up”)\nAnother way to check for correlation between attributes is to use Pandas’ scatter_matrix function, which plots every numerical attribute against every other numerical attribute. Since there are now 11 numerical attributes, you would get 121 plots (including index), which would not fit on a page, so let’s just focus on a few promising attributes that seem most correlated with the median housing value\n\n\nCode\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\nplt.figure(figsize=[12,8])\nsns.pairplot(housing[attributes])\nplt.show()\n\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nThe most promising attribute to predict the median house value is the median income, so let’s zoom in on their correlation scatterplot\n\n\nCode\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1, grid=True)\nplt.axis([0, 16, 0, 550000])\nplt.show()\n\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nThis plot reveals a few things.\n\nFirst, the correlation is indeed very strong; you can clearly see the upward trend and the points are not too dispersed.\nSecond, the price cap that we noticed earlier is clearly visible as a horizontal line at $500,000. But this plot reveals other less obvious straight lines: a horizontal line around $450,000, another around $350,000, perhaps one around $280,000, and a few more below that. You may want to try removing the corresponding districts to prevent your algorithms from learning to reproduce these data quirks."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#experimenting-with-attribute-combinations",
    "href": "01_end_to_end_machine_learning_project.html#experimenting-with-attribute-combinations",
    "title": "2  End-to-end Machine Learning project",
    "section": "4.3 Experimenting with Attribute Combinations",
    "text": "4.3 Experimenting with Attribute Combinations\nHopefully the previous sections gave you an idea of a few ways you can explore the data and gain insights.\n\nWe identified a few data quirks that you may want to clean up before feeding the data to a machine learning algorithm\nWe found interesting correlations between attributes, in particular with the target attribute\nWe also noticed that some attributes have a skewed-right distribution, so you may want to transform them (e.g., by computing their logarithm or square root).\n\nOf course, your mileage will vary considerably with each project, but the general ideas are similar.\nOne last thing you may want to do before actually preparing the data for Machine Learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you really want is the number of rooms per household.\nSimilarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at.\nLet’s create these new attributes:\n\n\nCode\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\nhousing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"] = housing[\"population\"]/housing[\"households\"]\n\n\nAnd now let’s look at the correlation matrix again:\n\n\nCode\ncorr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\n\nmedian_house_value          1.000000\nmedian_income               0.688380\nrooms_per_household         0.143663\ntotal_rooms                 0.137455\nhousing_median_age          0.102175\nhouseholds                  0.071426\ntotal_bedrooms              0.054635\npopulation                 -0.020153\npopulation_per_household   -0.038224\nlongitude                  -0.050859\nlatitude                   -0.139584\nbedrooms_ratio             -0.256397\nName: median_house_value, dtype: float64\n\n\nHey, not bad! The new bedrooms_ratio attribute is much more correlated with the median house value than the total number of rooms or bedrooms. Apparently houses with a lower bedroom/room ratio tend to be more expensive. The number of rooms per household is also more informative than the total number of rooms in a district—obviously the larger the houses, the more expensive they are.\nThis round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first reasonably good prototype. But this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#data-cleaning",
    "href": "01_end_to_end_machine_learning_project.html#data-cleaning",
    "title": "2  End-to-end Machine Learning project",
    "section": "5.1 Data Cleaning",
    "text": "5.1 Data Cleaning\nMost Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. You noticed earlier that the total_bedrooms attribute has some missing values, so let’s fix this. You have three options:\n\nGet rid of the corresponding districts.\nGet rid of the whole attribute.\nSet the values to some value (zero, the mean, the median, etc.).\n\n\n\nCode\nsample_incomplete_rows = housing[housing.isnull().any(axis=1)]\nsample_incomplete_rows\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n14452\n-120.67\n40.50\n15.0\n5343.0\nNaN\n2503.0\n902.0\n3.5962\nINLAND\n\n\n18217\n-117.96\n34.03\n35.0\n2093.0\nNaN\n1755.0\n403.0\n3.4115\n&lt;1H OCEAN\n\n\n11889\n-118.05\n34.04\n33.0\n1348.0\nNaN\n1098.0\n257.0\n4.2917\n&lt;1H OCEAN\n\n\n20325\n-118.88\n34.17\n15.0\n4260.0\nNaN\n1701.0\n669.0\n5.1033\n&lt;1H OCEAN\n\n\n14360\n-117.87\n33.62\n8.0\n1266.0\nNaN\n375.0\n183.0\n9.8020\n&lt;1H OCEAN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2348\n-122.70\n38.35\n14.0\n2313.0\nNaN\n954.0\n397.0\n3.7813\n&lt;1H OCEAN\n\n\n366\n-122.50\n37.75\n44.0\n1819.0\nNaN\n1137.0\n354.0\n3.4919\nNEAR OCEAN\n\n\n18241\n-121.44\n38.54\n39.0\n2855.0\nNaN\n1217.0\n562.0\n3.2404\nINLAND\n\n\n18493\n-116.21\n33.75\n22.0\n894.0\nNaN\n830.0\n202.0\n3.0673\nINLAND\n\n\n16519\n-117.86\n34.01\n16.0\n4632.0\nNaN\n3038.0\n727.0\n5.1762\n&lt;1H OCEAN\n\n\n\n\n\n168 rows × 9 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nsample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # option 1\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nsample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # option 2\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n14452\n-120.67\n40.50\n15.0\n5343.0\n2503.0\n902.0\n3.5962\nINLAND\n\n\n18217\n-117.96\n34.03\n35.0\n2093.0\n1755.0\n403.0\n3.4115\n&lt;1H OCEAN\n\n\n11889\n-118.05\n34.04\n33.0\n1348.0\n1098.0\n257.0\n4.2917\n&lt;1H OCEAN\n\n\n20325\n-118.88\n34.17\n15.0\n4260.0\n1701.0\n669.0\n5.1033\n&lt;1H OCEAN\n\n\n14360\n-117.87\n33.62\n8.0\n1266.0\n375.0\n183.0\n9.8020\n&lt;1H OCEAN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2348\n-122.70\n38.35\n14.0\n2313.0\n954.0\n397.0\n3.7813\n&lt;1H OCEAN\n\n\n366\n-122.50\n37.75\n44.0\n1819.0\n1137.0\n354.0\n3.4919\nNEAR OCEAN\n\n\n18241\n-121.44\n38.54\n39.0\n2855.0\n1217.0\n562.0\n3.2404\nINLAND\n\n\n18493\n-116.21\n33.75\n22.0\n894.0\n830.0\n202.0\n3.0673\nINLAND\n\n\n16519\n-117.86\n34.01\n16.0\n4632.0\n3038.0\n727.0\n5.1762\n&lt;1H OCEAN\n\n\n\n\n\n168 rows × 8 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nmedian = housing[\"total_bedrooms\"].median()\nsample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3\nsample_incomplete_rows\n\n\n/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py:6392: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  return self._update_inplace(result)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\n\n\n\n\n14452\n-120.67\n40.50\n15.0\n5343.0\n434.0\n2503.0\n902.0\n3.5962\nINLAND\n\n\n18217\n-117.96\n34.03\n35.0\n2093.0\n434.0\n1755.0\n403.0\n3.4115\n&lt;1H OCEAN\n\n\n11889\n-118.05\n34.04\n33.0\n1348.0\n434.0\n1098.0\n257.0\n4.2917\n&lt;1H OCEAN\n\n\n20325\n-118.88\n34.17\n15.0\n4260.0\n434.0\n1701.0\n669.0\n5.1033\n&lt;1H OCEAN\n\n\n14360\n-117.87\n33.62\n8.0\n1266.0\n434.0\n375.0\n183.0\n9.8020\n&lt;1H OCEAN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2348\n-122.70\n38.35\n14.0\n2313.0\n434.0\n954.0\n397.0\n3.7813\n&lt;1H OCEAN\n\n\n366\n-122.50\n37.75\n44.0\n1819.0\n434.0\n1137.0\n354.0\n3.4919\nNEAR OCEAN\n\n\n18241\n-121.44\n38.54\n39.0\n2855.0\n434.0\n1217.0\n562.0\n3.2404\nINLAND\n\n\n18493\n-116.21\n33.75\n22.0\n894.0\n434.0\n830.0\n202.0\n3.0673\nINLAND\n\n\n16519\n-117.86\n34.01\n16.0\n4632.0\n434.0\n3038.0\n727.0\n5.1762\n&lt;1H OCEAN\n\n\n\n\n\n168 rows × 9 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nIf you choose option 3, you should compute the median value on the training set, and use it to fill the missing values in the training set, but also don’t forget to save the median value that you have computed. You will need it later to replace missing values in the test set when you want to evaluate your system, and also once the system goes live to replace missing values in new data.\nScikit-Learn provides a handy class to take care of missing values: SimpleImputer.\n\n\nCode\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")\n\n\nSince the median can only be computed on numerical attributes, you then need to create a copy of the data with only the numerical attributes (this will exclude the text attribute ocean_proximity)\n\n\nCode\nhousing_num = housing.select_dtypes(include=[np.number])\n\n\n\n\nCode\nimputer.fit(housing_num)\n\n\nSimpleImputer(strategy='median')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputerSimpleImputer(strategy='median')\n\n\n\nAll objects in SKlearn share a consistent and simple interface: 1. Estimators: Any object that can estimate some parameters based on a dataset is called an estimator (e.g., a SimpleImputer is an estimator). The estimation itself is performed by the fit() method, and it takes a dataset as a parameter, or two for supervised learning algorithms—the second dataset contains the labels. Any other parameter needed to guide the estimation process is considered a hyperparameter (such as a SimpleImputer’s strategy), and it must be set as an instance variable (generally via a constructor parameter). 2. Transformers: Some estimators (such as a SimpleImputer) can also transform a dataset; these are called transformers. Once again, the API is simple: the transformation is performed by the transform() method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters, as is the case for a SimpleImputer. All transformers also have a convenience method called fit_transform(), which is equivalent to calling fit() and then transform() (but sometimes fit_transform() is optimized and runs much faster). 3. Predictors: Finally, some estimators, given a dataset, are capable of making predictions; they are called predictors. For example, the LinearRegression model was a predictor. A predictor has a predict() method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a score() method that measures the quality of the predictions, given a test set (and the corresponding labels, in the case of supervised learning algorithms).\n\n\nAs with all estimators, it is important to fit the scalers to the training data only: never use fit() or fit_transform() for anything else than the training set. Once you have a trained scaler, you can then use it to transform() any other set, including the validation set, the test set, and new data.\n\nThe imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable. Only the total_bedrooms attribute had missing values, but we cannot be sure that there won’t be any missing values in new data after the system goes live, so it is safer to apply the imputer to all the numerical attributes:\n\n\nCode\nimputer.statistics_\n\n\narray([-118.51  ,   34.26  ,   29.    , 2125.    ,  434.    , 1167.    ,\n        408.    ,    3.5385])\n\n\n\n\nCode\nimputer.feature_names_in_\n\n\narray(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n       'total_bedrooms', 'population', 'households', 'median_income'],\n      dtype=object)\n\n\nNow you can use this “trained” imputer to transform the training set by replacing missing values with the learned medians:\n\n\nCode\nX = imputer.transform(housing_num)\n\n\nThe result is a plain NumPy array containing the transformed features. If you want to put it back into a Pandas DataFrame, it’s simple:\n\n\nCode\n#housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)\nfrom sklearn import config_context\nwith config_context(transform_output=\"pandas\"):\n    housing_tr = imputer.transform(housing_num)\n\n\nSee https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py\n\n\nCode\nhousing_tr.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\n\n\n\n\n13096\n-122.42\n37.80\n52.0\n3321.0\n1115.0\n1576.0\n1034.0\n2.0987\n\n\n14973\n-118.38\n34.14\n40.0\n1965.0\n354.0\n666.0\n357.0\n6.0876\n\n\n3785\n-121.98\n38.36\n33.0\n1083.0\n217.0\n562.0\n203.0\n2.4330\n\n\n14689\n-117.11\n33.75\n17.0\n4174.0\n851.0\n1845.0\n780.0\n2.2618\n\n\n20507\n-118.15\n33.77\n36.0\n4366.0\n1211.0\n1912.0\n1172.0\n3.5292"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#dealing-with-outlier-optional",
    "href": "01_end_to_end_machine_learning_project.html#dealing-with-outlier-optional",
    "title": "2  End-to-end Machine Learning project",
    "section": "5.2 Dealing with outlier (Optional)",
    "text": "5.2 Dealing with outlier (Optional)\n\n\nCode\nfrom sklearn.ensemble import IsolationForest\n\nisolation_forest = IsolationForest(random_state=42)\noutlier_pred = isolation_forest.fit_predict(X)\n\n\nIf you wanted to drop outliers, you would run the following code:\n\n\nCode\noutlier_pred\n\n\n\n\nCode\n#housing = housing.iloc[outlier_pred == 1]\n#housing_labels = housing_labels.iloc[outlier_pred == 1]"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#handling-text-and-categorical-attributes",
    "href": "01_end_to_end_machine_learning_project.html#handling-text-and-categorical-attributes",
    "title": "2  End-to-end Machine Learning project",
    "section": "5.3 Handling Text and Categorical Attributes",
    "text": "5.3 Handling Text and Categorical Attributes\nSo far we have only dealt with numerical attributes, but your data may also contain text attributes. In this dataset, there is just one: the ocean_proximity attribute. Let’s look at its value for the first few instances:\n\n\nCode\nhousing_cat = housing[[\"ocean_proximity\"]] # Note the double square bracket\nhousing_cat.head(10)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nocean_proximity\n\n\n\n\n13096\nNEAR BAY\n\n\n14973\n&lt;1H OCEAN\n\n\n3785\nINLAND\n\n\n14689\nINLAND\n\n\n20507\nNEAR OCEAN\n\n\n1286\nINLAND\n\n\n18078\n&lt;1H OCEAN\n\n\n4396\nNEAR BAY\n\n\n18031\n&lt;1H OCEAN\n\n\n6753\n&lt;1H OCEAN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nMost Machine Learning algorithms prefer to work with numbers anyway, so let’s convert these categories from text to numbers.\n\n\nCode\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:10]\n\n\narray([[3.],\n       [0.],\n       [1.],\n       [1.],\n       [4.],\n       [1.],\n       [0.],\n       [3.],\n       [0.],\n       [0.]])\n\n\nYou can get the list of categories using the categories_ instance variable. It is a list containing a 1D array of categories for each categorical attribute (in this case, a list containing a single array since there is just one categorical attribute):\n\n\nCode\nordinal_encoder.categories_\n\n\n[array(['&lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n       dtype=object)]\n\n\nOne issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad”, “average”, “good”, “excellent”), but it is obviously not the case for the ocean_proximity column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1).\nTo fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is “&lt;1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called dummy attributes. Scikit-Learn provides a OneHotEncoder class to convert categorical values into one-hot vectors\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot\n\n\n&lt;16512x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 16512 stored elements in Compressed Sparse Row format&gt;\n\n\nBy default, the OneHotEncoder class returns a sparse array, but we can convert it to a dense array if needed by calling the toarray() method:\n\n\nCode\nhousing_cat_1hot.toarray()\n\n\narray([[0., 0., 0., 1., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       ...,\n       [0., 0., 0., 0., 1.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1.]])\n\n\n\n\nCode\ncat_encoder.categories_\n\n\n[array(['&lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n       dtype=object)]\n\n\nThe advantage of OneHotEncoder over get_dummies() is that\n\nIt remembers which categories it was trained on. This is very important because once your model is in production, it should be fed exactly the same features as during training: no more, no less.\nOneHotEncoder is smarter: it will detect the unknown category and raise an exception. If you prefer, you can set the handle_unknown hyperparameter to \"ignore\", in which case it will just represent the unknown category with zeros\n\n\n\nCode\ndf_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"&lt;2H OCEAN\", \"ISLAND\"]})\npd.get_dummies(df_test_unknown)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nocean_proximity_&lt;2H OCEAN\nocean_proximity_ISLAND\n\n\n\n\n0\n1\n0\n\n\n1\n0\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\ncat_encoder.handle_unknown = \"ignore\"\ncat_encoder.transform(df_test_unknown).toarray()\n\n\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0.]])\n\n\nIf a categorical attribute has a large number of possible categories (e.g., country code, profession, species, etc.), then one-hot encoding will result in a large number of input features. This may slow down training and degrade performance.\nIf this happens, you may want to replace the categorical input with useful numerical features related to the categories: for example, you could replace the ocean_proximity feature with the distance to the ocean (similarly, a country code could be replaced with the country’s population and GDP per capita).\nAlternatively, you could replace each category with a learnable low dimensional vector called an embedding. Each category’s representation would be learned during training: this is an example of representation learning.\nWhen you fit any Scikit-Learn estimator using a DataFrame, the estimator stores the column names in the feature_names_in_ attribute. Scikit-Learn then ensures that any DataFrame fed to this estimator after that (e.g., to transform() or predict()) has the same column names. Transformers also provide a get_feature_names_out() method that you can use to build a DataFrame around the transformer’s output:\n\n\nCode\ncat_encoder.feature_names_in_\n\n\narray(['ocean_proximity'], dtype=object)\n\n\n\n\nCode\ncat_encoder.get_feature_names_out()\n\n\narray(['ocean_proximity_&lt;1H OCEAN', 'ocean_proximity_INLAND',\n       'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',\n       'ocean_proximity_NEAR OCEAN'], dtype=object)\n\n\n\n\nCode\ndf_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"&lt;2H OCEAN\", \"ISLAND\"]})\ndf_output = pd.DataFrame(cat_encoder.transform(df_test_unknown).toarray(),\n                         columns=cat_encoder.get_feature_names_out(),\n                         index=df_test_unknown.index)\ndf_output\n\n\n\n  \n    \n      \n\n\n\n\n\n\nocean_proximity_&lt;1H OCEAN\nocean_proximity_INLAND\nocean_proximity_ISLAND\nocean_proximity_NEAR BAY\nocean_proximity_NEAR OCEAN\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n1.0\n0.0\n0.0"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#feature-scaling-and-transformation",
    "href": "01_end_to_end_machine_learning_project.html#feature-scaling-and-transformation",
    "title": "2  End-to-end Machine Learning project",
    "section": "5.4 Feature Scaling and Transformation",
    "text": "5.4 Feature Scaling and Transformation\nOne of the most important transformations you need to apply to your data is feature scaling. With few exceptions, machine learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Without any scaling, most models will be biased toward ignoring the median income and focusing more on the number of rooms.\n\n5.4.1 Min-max scaling\nMin-max scaling (many people call this normalization) is the simplest: for each attribute, the values are shifted and rescaled so that they end up ranging from 0 to 1. This is performed by subtracting the min value and dividing by the difference between the min and the max. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if, for some reason, you don’t want 0–1\n\n\nCode\nfrom sklearn.preprocessing import MinMaxScaler\n\nmin_max_scaler = MinMaxScaler(feature_range=(-1, 1))\nhousing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)\n\n\n\n\n5.4.2 Standardization\nStandardization is different: first it subtracts the mean value (so standardized values have a zero mean), then it divides the result by the standard deviation (so standardized values have a standard deviation equal to 1). Unlike min-max scaling, standardization does not restrict values to a specific range. However, standardization is much less affected by outliers.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\nhousing_num_std_scaled = std_scaler.fit_transform(housing_num)\n\n\n\n\n5.4.3 Other scaling (Optional)\nWhen a feature’s distribution has a heavy tail (i.e., when values far from the mean are not exponentially rare), both min-max scaling and standardization will squash most values into a small range. Machine learning models generally don’t like this at all. So before you scale the feature, you should first transform it to shrink the heavy tail, and if possible to make the distribution roughly symmetrical. For example, a common way to do this for positive features with a heavy tail to the right is to replace the feature with its square root (or raise the feature to a power between 0 and 1). If the feature has a really long and heavy tail, such as a power law distribution, then replacing the feature with its logarithm may help.\nFor example, the population feature roughly follows a power law\n\n\nCode\nfig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\nhousing[\"population\"].hist(ax=axs[0], bins=50)\nhousing[\"population\"].apply(np.log).hist(ax=axs[1], bins=50)\naxs[0].set_xlabel(\"Population\")\naxs[1].set_xlabel(\"Log of population\")\naxs[0].set_ylabel(\"Number of districts\")\nplt.show()\n\n\n\n\n\nAnother approach to handle heavy-tailed features consists in bucketizing the feature. This means chopping its distribution into roughly equal-sized buckets, and replacing each feature value with the index of the bucket it belongs to. For example, you could replace each value with its percentile. Bucketizing with equal-sized buckets results in a feature with an almost uniform distribution, so there’s no need for further scaling, or you can just divide by the number of buckets to force the values to the 0–1 range.\n\n\nCode\npercentiles = [np.percentile(housing[\"median_income\"], p) for p in range(1, 100)]\nflattened_median_income = pd.cut(housing[\"median_income\"], bins=[-np.inf] + percentiles + [np.inf], labels=range(1, 100 + 1))\nflattened_median_income.hist(bins=50)\nplt.xlabel(\"Median income percentile\")\nplt.ylabel(\"Number of districts\")\nplt.show()\n# Note: incomes below the 1st percentile are labeled 1, and incomes above the\n# 99th percentile are labeled 100. This is why the distribution below ranges\n# from 1 to 100 (not 0 to 100).\n\n\n\n\n\n\nWhen a feature has a multimodal distribution (i.e., with two or more clear peaks, called modes), such as the housing_median_age feature, it can also be helpful to bucketize it, but this time treating the bucket IDs as categories, rather than as numerical values. This means that the bucket indices must be encoded, for example using a OneHotEncoder (so you usually don’t want to use too many buckets). This approach will allow the regression model to more easily learn different rules for different ranges of this feature value.\n\nFor example, perhaps houses built around 35 years ago have a peculiar style that fell out of fashion, and therefore they’re cheaper than their age alone would suggest.\nAnother approach to transforming multimodal distributions is to add a feature for each of the modes (at least the main ones), representing the similarity between the housing median age and that particular mode. The similarity measure is typically computed using a radial basis function (RBF). Using Scikit-Learn’s rbf_kernel() function, you can create a new Gaussian RBF feature measuring the similarity between the housing median age and 35:\n\n\nCode\nfrom sklearn.metrics.pairwise import rbf_kernel\n\nages = np.linspace(housing[\"housing_median_age\"].min(),\n                   housing[\"housing_median_age\"].max(),\n                   500).reshape(-1, 1)\ngamma1 = 0.1\ngamma2 = 0.03\nrbf1 = rbf_kernel(ages, [[35]], gamma=gamma1)\nrbf2 = rbf_kernel(ages, [[35]], gamma=gamma2)\n\nfig, ax1 = plt.subplots()\n\nax1.set_xlabel(\"Housing median age\")\nax1.set_ylabel(\"Number of districts\")\nax1.hist(housing[\"housing_median_age\"], bins=50)\n\nax2 = ax1.twinx()  # create a twin axis that shares the same x-axis\ncolor = \"blue\"\nax2.plot(ages, rbf1, color=color, label=\"gamma = 0.10\")\nax2.plot(ages, rbf2, color=color, label=\"gamma = 0.03\", linestyle=\"--\")\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylabel(\"Age similarity\", color=color)\n\nplt.legend(loc=\"upper left\")\nplt.show()\n\n\n\n\n\nAs the chart shows, the new age similarity feature peaks at 35, right around the spike in the housing median age distribution: if this particular age group is well correlated with lower prices, there’s a good chance that this new feature will help."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#custom-transformers",
    "href": "01_end_to_end_machine_learning_project.html#custom-transformers",
    "title": "2  End-to-end Machine Learning project",
    "section": "5.5 Custom Transformers",
    "text": "5.5 Custom Transformers\nAlthough Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific attributes.\n\n5.5.1 Function with no training parameter (Optional)\nFor transformations that don’t require any training, you can just write a function that takes a NumPy array as input and outputs the transformed. Let’s create a log-transformer and apply it to the population feature:\n\n\nCode\nfrom sklearn.preprocessing import FunctionTransformer\nlog_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\nlog_pop = log_transformer.transform(housing[[\"population\"]])\n\n\n\n\n5.5.2 Transform with training parameter\nFunctionTransformer is very handy, but what if you would like your transformer to be trainable, learning some parameters in the fit() method and using them later in the transform() method? You can get fit_transform() for free by simply adding TransformerMixin as a base class: the default implementation will just call fit() and then transform(). If you add BaseEstimator as a base class (and avoid using *args and **kwargs in your constructor), you will also get two extra methods: get_params() and set_params(). These will be useful for automatic hyperparameter tuning.\nFor example, the following code demonstrates custom transformer that uses a KMeans clusterer in the fit() method to identify the main clusters in the training data, and then uses rbf_kernel() in the transform() method to measure how similar each sample is to each cluster center:\n\n\nCode\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.cluster import KMeans\n\nclass ClusterSimilarity(BaseEstimator, TransformerMixin):\n    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n        self.n_clusters = n_clusters\n        self.gamma = gamma\n        self.random_state = random_state\n\n    def fit(self, X, y=None, sample_weight=None):\n        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n        self.kmeans_.fit(X, sample_weight=sample_weight)\n        return self  # always return self!\n\n    def transform(self, X):\n        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n    \n    def get_feature_names_out(self, names=None):\n        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n\n\nK-means is a clustering algorithm that locates clusters in the data. How many it searches for is controlled by the n_clusters hyperparameter. After training, the cluster centers are available via the cluster_centers_ attribute. The fit() method of KMeans supports an optional argument sample_weight, which lets the user specify the relative weights of the samples. k-means is a stochastic algorithm, meaning that it relies on randomness to locate the clusters, so if you want reproducible results, you must set the random_state parameter. As you can see, despite the complexity of the task, the code is fairly straightforward. Now let’s use this custom transformer:\n\n\nCode\ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\nsimilarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]], sample_weight=housing_labels)\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nThis code creates a ClusterSimilarity transformer, setting the number of clusters to 10. Then it calls fit_transform() with the latitude and longitude of every district in the training set, weighting each district by its median house value. The transformer uses k-means to locate the clusters, then measures the Gaussian RBF similarity between each district and all 10 cluster centers. The result is a matrix with one row per district, and one column per cluster.\n\nSee https://www.kaggle.com/code/ryanholbrook/clustering-with-k-means for the rationale of using clustering results as features.\n\n\n\nCode\nhousing_renamed = housing.rename(columns={\n    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n    \"population\": \"Population\",\n    \"median_house_value\": \"Median house value (ᴜsᴅ)\"})\nhousing_renamed[\"Max cluster similarity\"] = similarities.max(axis=1)\n\nhousing_renamed.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", grid=True,\n                     s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n                     c=\"Max cluster similarity\",\n                     cmap=\"jet\", colorbar=True,\n                     legend=True, sharex=False, figsize=(10, 7))\nplt.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],\n         cluster_simil.kmeans_.cluster_centers_[:, 0],\n         linestyle=\"\", color=\"black\", marker=\"X\", markersize=20,\n         label=\"Cluster centers\")\nplt.legend(loc=\"upper right\")\nplt.show()\n\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nThe figure shows the 10 cluster centers found by k-means. The districts are colored according to their geographic similarity to their closest cluster center. As you can see, most clusters are located in highly populated and expensive areas."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#transformation-pipelines",
    "href": "01_end_to_end_machine_learning_project.html#transformation-pipelines",
    "title": "2  End-to-end Machine Learning project",
    "section": "5.6 Transformation Pipelines",
    "text": "5.6 Transformation Pipelines\nAs you can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with such sequences of transformations. Here is a small pipeline for numerical attributes, which will first impute then scale the input features:\n\n\nCode\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\nnum_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"standardize\", StandardScaler()),\n])\nnum_pipeline\n\n\nPipeline(steps=[('impute', SimpleImputer(strategy='median')),\n                ('standardize', StandardScaler())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('impute', SimpleImputer(strategy='median')),\n                ('standardize', StandardScaler())])SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()\n\n\nThe Pipeline constructor takes a list of name/estimator pairs (2-tuples) defining a sequence of steps. The names can be anything you like, as long as they are unique and don’t contain double underscores (__). They will be useful later, when we discuss hyperparameter tuning. The estimators must all be transformers (i.e., they must have a fit_transform() method), except for the last one, which can be anything: a transformer, a predictor, or any other type of estimator.\nWhen you call the pipeline’s fit() method, it calls fit_transform() sequentially on all the transformers, passing the output of each call as the parameter to the next call until it reaches the final estimator, for which it just calls the fit() method. The pipeline exposes the same methods as the final estimator. In this example the last estimator is a StandardScaler, which is a transformer, so the pipeline also acts like a transformer.\nIf you call the pipeline’s transform() method, it will sequentially apply all the transformations to the data. If the last estimator were a predictor instead of a transformer, then the pipeline would have a predict() method rather than a transform() method. Calling it would sequentially apply all the transformations to the data and pass the result to the predictor’s predict() method.\nLet’s call the pipeline’s fit_transform() method and look at the output’s first two rows, rounded to two decimal places:\n\n\nCode\nhousing_num_prepared = num_pipeline.fit_transform(housing_num)\nhousing_num_prepared[:2].round(2)\n\n\narray([[-1.42,  1.01,  1.86,  0.31,  1.37,  0.14,  1.39, -0.94],\n       [ 0.6 , -0.7 ,  0.91, -0.31, -0.44, -0.69, -0.37,  1.17]])\n\n\nSo far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer capable of handling all columns, applying the appropriate transformations to each column. For this, you can use a ColumnTransformer. For example, the following ColumnTransformer will apply num_pipeline (the one we just defined) to the numerical attributes and cat_pipeline to the categorical attribute:\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\ncat_attribs = [\"ocean_proximity\"]\n\ncat_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n    ])\n\npreprocessing = ColumnTransformer([\n    (\"num\", num_pipeline, num_attribs),\n    (\"cat\", cat_pipeline, cat_attribs),\n])\n\n\nWe construct a ColumnTransformer. Its constructor requires a list of triplets (3-tuples), each containing a name (which must be unique and not contain double underscores), a transformer, and a list of names (or indices) of columns that the transformer should be applied to.\n\nInstead of using a transformer, you can specify the string “drop” if you want the columns to be dropped, or you can specify “passthrough” if you want the columns to be left untouched. By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but you can set the remainder hyperparameter to any transformer (or to “passthrough”) if you want these columns to be handled differently.\n\nSince listing all the column names is not very convenient, Scikit-Learn provides a make_column_selector() function that returns a selector function you can use to automatically select all the features of a given type, such as numerical or categorical.\nYou can pass this selector function to the ColumnTransformer instead of column names or indices. Moreover, if you don’t care about naming the transformers, you can use make_column_transformer(), which chooses the names for you.\n\n\nCode\nfrom sklearn.compose import make_column_selector, make_column_transformer\n\npreprocessing = make_column_transformer(\n    (num_pipeline, make_column_selector(dtype_include=np.number)),\n    (cat_pipeline, make_column_selector(dtype_include=object)),\n)\n\n\nNow we’re ready to apply this ColumnTransformer to the housing data:\n\n\nCode\n# https://github.com/scikit-learn/scikit-learn/issues/25224\n# with config_context(transform_output=\"pandas\"):\n#  housing_prepared = preprocessing.fit_transform(housing)\n\nhousing_prepared = preprocessing.fit_transform(housing)\n\n\nOnce again this returns a NumPy array, but you can get the column names using preprocessing.get_feature_names_out() and wrap the data in a nice DataFrame as we did before.\n\n\nCode\nhousing_prepared_fr = pd.DataFrame(\n    housing_prepared,\n    columns=preprocessing.get_feature_names_out(),\n    index=housing.index)\nhousing_prepared_fr.head(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\npipeline-1__longitude\npipeline-1__latitude\npipeline-1__housing_median_age\npipeline-1__total_rooms\npipeline-1__total_bedrooms\npipeline-1__population\npipeline-1__households\npipeline-1__median_income\npipeline-2__ocean_proximity_&lt;1H OCEAN\npipeline-2__ocean_proximity_INLAND\npipeline-2__ocean_proximity_ISLAND\npipeline-2__ocean_proximity_NEAR BAY\npipeline-2__ocean_proximity_NEAR OCEAN\n\n\n\n\n13096\n-1.423037\n1.013606\n1.861119\n0.311912\n1.368167\n0.137460\n1.394812\n-0.936491\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n14973\n0.596394\n-0.702103\n0.907630\n-0.308620\n-0.435925\n-0.693771\n-0.373485\n1.171942\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe code that builds the pipeline to do all of this should look familiar to you by now:\n\nMissing values in numerical features will be imputed by replacing them with the median, as most ML algorithms don’t expect missing values. In categorical features, missing values will be replaced by the most frequent category.\nThe categorical feature will be one-hot encoded, as most ML algorithms only accept numerical inputs.\nA few ratio features will be computed and added: bedrooms_ratio,rooms_per_house, and people_per_house. Hopefully these will better correlate with the median house value, and thereby help the ML models.\nA few cluster similarity features will also be added. These will likely be more useful to the model than latitude and longitude.\nFeatures with a long tail will be replaced by their logarithm, as most models prefer features with roughly uniform or Gaussian distributions.\nAll numerical features will be standardized, as most ML algorithms prefer when all features have roughly the same scale.\n\n\nIf you don’t want to name the transformers, you can use the make_pipeline() function instead Pipeline\n\n\n\nCode\ndef column_ratio(X):\n    return X[:, [0]] / X[:, [1]]\n\ndef ratio_name(function_transformer, feature_names_in):\n    return [\"ratio\"]  # feature names out\n\ndef ratio_pipeline():\n    return make_pipeline(\n        SimpleImputer(strategy=\"median\"),\n        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n        StandardScaler())\n\nlog_pipeline = make_pipeline(\n    SimpleImputer(strategy=\"median\"),\n    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n    StandardScaler())\n\ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\ndefault_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n\npreprocessing = ColumnTransformer([\n        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n                               \"households\", \"median_income\"]),\n        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n    ],\n    remainder=default_num_pipeline)  # one column remaining: housing_median_age\n\n\nIf you run this ColumnTransformer, it performs all the transformations and outputs a NumPy array with 24 features:\n\n\nCode\nhousing_prepared = preprocessing.fit_transform(housing)\nhousing_prepared.shape\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n(16512, 24)"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#training-and-evaluating-on-the-training-set",
    "href": "01_end_to_end_machine_learning_project.html#training-and-evaluating-on-the-training-set",
    "title": "2  End-to-end Machine Learning project",
    "section": "6.1 Training and Evaluating on the Training Set",
    "text": "6.1 Training and Evaluating on the Training Set\nLet’s first train a Linear Regression model\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = make_pipeline(preprocessing, LinearRegression())\nlin_reg.fit(housing, housing_labels)\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2...\n                                                   'households',\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                ('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2...\n                                                   'households',\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                ('linearregression', LinearRegression())])columntransformer: ColumnTransformerColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                             SimpleImputer(strategy='median')),\n                                            ('standardscaler',\n                                             StandardScaler())]),\n                  transformers=[('bedrooms',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                                                                      func=&lt;function column_ratio...\n                                 ['total_bedrooms', 'total_rooms', 'population',\n                                  'households', 'median_income']),\n                                ('geo', ClusterSimilarity(random_state=42),\n                                 ['latitude', 'longitude']),\n                                ('cat',\n                                 Pipeline(steps=[('impute',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])bedrooms['total_bedrooms', 'total_rooms']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()rooms_per_house['total_rooms', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()people_per_house['population', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out='one-to-one', func=&lt;ufunc 'log'&gt;)StandardScalerStandardScaler()geo['latitude', 'longitude']ClusterSimilarityClusterSimilarity(random_state=42)cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(handle_unknown='ignore')remainder['housing_median_age']SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()LinearRegressionLinearRegression()\n\n\nYou now have a working linear regression model. You try it out on the training set, looking at the first five predictions and comparing them to the labels:\n\n\nCode\nhousing_predictions = lin_reg.predict(housing)\nhousing_predictions[:5].round(-2)  # -2 = rounded to the nearest hundred\n\n\narray([243700., 372400., 128800.,  94400., 328300.])\n\n\n\n\nCode\nhousing_labels.iloc[:5].values\n\n\narray([458300., 483800., 101700.,  96100., 361800.])\n\n\nRemember that you chose to use the RMSE as your performance measure, so you want to measure this regression model’s RMSE on the whole training set using Scikit-Learn’s mean_squared_error() function, with the squared argument set to False:\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\nlin_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)\nlin_rmse\n\n\n68687.89176590038\n\n\nThis is better than nothing, but clearly not a great score: the median_housing_values of most districts range between $120,000 and $265,000, so a typical prediction error of $68,628 is really not very satisfying. This is an example of a model underfitting the training data. When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough.\nThe main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. This model is not regularized, which rules out the last option. You could try to add more features, but first you want to try a more complex model to see how it does. We decide to try a DecisionTreeRegressor, as this is a fairly powerful model capable of finding complex nonlinear relationships in the data.\n\n\nCode\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\ntree_reg.fit(housing, housing_labels)\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2...\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                ('decisiontreeregressor',\n                 DecisionTreeRegressor(random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2...\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                ('decisiontreeregressor',\n                 DecisionTreeRegressor(random_state=42))])columntransformer: ColumnTransformerColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                             SimpleImputer(strategy='median')),\n                                            ('standardscaler',\n                                             StandardScaler())]),\n                  transformers=[('bedrooms',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                                                                      func=&lt;function column_ratio...\n                                 ['total_bedrooms', 'total_rooms', 'population',\n                                  'households', 'median_income']),\n                                ('geo', ClusterSimilarity(random_state=42),\n                                 ['latitude', 'longitude']),\n                                ('cat',\n                                 Pipeline(steps=[('impute',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])bedrooms['total_bedrooms', 'total_rooms']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()rooms_per_house['total_rooms', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()people_per_house['population', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out='one-to-one', func=&lt;ufunc 'log'&gt;)StandardScalerStandardScaler()geo['latitude', 'longitude']ClusterSimilarityClusterSimilarity(random_state=42)cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(handle_unknown='ignore')remainder['housing_median_age']SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()DecisionTreeRegressorDecisionTreeRegressor(random_state=42)\n\n\n\n\nCode\nhousing_predictions = tree_reg.predict(housing)\ntree_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)\ntree_rmse\n\n\n0.0\n\n\nIt is much more likely that the model has badly overfit the data. How can you be sure? As we saw earlier, you don’t want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training, and part for model validation."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#better-evaluation-using-cross-validation",
    "href": "01_end_to_end_machine_learning_project.html#better-evaluation-using-cross-validation",
    "title": "2  End-to-end Machine Learning project",
    "section": "6.2 Better Evaluation Using Cross-Validation",
    "text": "6.2 Better Evaluation Using Cross-Validation\nOne way to evaluate the Decision Tree model would be to use the train_test_split function to split the training set into a smaller training set and a validation set, then train your models against the smaller training set and evaluate them against the validation set.\nA great alternative is to use Scikit-Learn’s K-fold cross-validation feature. The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores:\n\n\nCode\nfrom sklearn.model_selection import cross_val_score\n\n# Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better),\ntree_rmses = -cross_val_score(tree_reg, housing, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10)\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nLet’s look at the results:\n\n\nCode\npd.Series(tree_rmses).describe()\n\n\ncount       10.000000\nmean     66868.027288\nstd       2060.966425\nmin      63649.536493\n25%      65338.078316\n50%      66801.953094\n75%      68229.934454\nmax      70094.778246\ndtype: float64\n\n\nNow the decision tree doesn’t look as good as it did earlier. In fact, it seems to perform almost as poorly as the linear regression model! Notice that cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The decision tree has an RMSE of about 66,868, with a standard deviation of about 2,061. You would not have this information if you just used one validation set! We know there’s an overfitting problem because the training error is low (actually zero) while the validation error is high.\nLet’s compute the same scores for the Linear Regression model just to be sure\n\n\nCode\nlin_rmses = -cross_val_score(lin_reg, housing, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10)\npd.Series(lin_rmses).describe()\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\ncount       10.000000\nmean     69858.018195\nstd       4182.205077\nmin      65397.780144\n25%      68070.536263\n50%      68619.737842\n75%      69810.076342\nmax      80959.348171\ndtype: float64\n\n\nLet’s try one last model now: the RandomForestRegressor.Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions.\nBuilding a model on top of many other models is called Ensemble Learning, and it is often a great way to push ML algorithms even further.\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = make_pipeline(preprocessing, RandomForestRegressor(random_state=42))\nforest_rmses = -cross_val_score(forest_reg, housing, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10)\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n\n\nCode\npd.Series(forest_rmses).describe()\n\n\ncount       10.000000\nmean     47019.561281\nstd       1033.957120\nmin      45458.112527\n25%      46464.031184\n50%      46967.596354\n75%      47325.694987\nmax      49243.765795\ndtype: float64\n\n\nWow, this is much better: random forests really look very promising for this task! However, if you train a RandomForest and measure the RMSE on the training set, you will find roughly 17,474: that’s much lower, meaning that there’s still quite a lot of overfitting going on.\n\n\nCode\nforest_reg.fit(housing, housing_labels)\nhousing_predictions = forest_reg.predict(housing)\nforest_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)\nforest_rmse\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n17474.619286483998\n\n\nPossible solutions are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data. Before you dive much deeper into random forests, however, you should try out many other models from various categories of machine learning algorithms (e.g., several support vector machines with different kernels, and possibly a neural network), without spending too much time tweaking the hyperparameters. The goal is to shortlist a few (two to five) promising models."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#grid-search",
    "href": "01_end_to_end_machine_learning_project.html#grid-search",
    "title": "2  End-to-end Machine Learning project",
    "section": "7.1 Grid Search",
    "text": "7.1 Grid Search\nOne way to do that would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.\nInstead you should get Scikit-Learn’s GridSearchCV to search for you. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation. For example, the following code searches for the best combination of hyperparameter values for the RandomForestRegressor:\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\nfull_pipeline = Pipeline([\n    (\"preprocessing\", preprocessing),\n    (\"random_forest\", RandomForestRegressor(random_state=42)),\n])\nparam_grid = [\n    {'preprocessing__geo__n_clusters': [5, 8, 10],\n     'random_forest__max_features': [4, 6, 8]},\n    {'preprocessing__geo__n_clusters': [10, 15],\n     'random_forest__max_features': [6, 8, 10]},\n]\ngrid_search = GridSearchCV(full_pipeline, param_grid, cv=3, scoring='neg_root_mean_squared_error')\ngrid_search.fit(housing, housing_labels)\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('preprocessing',\n                                        ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                                                     SimpleImputer(strategy='median')),\n                                                                                    ('standardscaler',\n                                                                                     StandardScaler())]),\n                                                          transformers=[('bedrooms',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('functiontransformer',\n                                                                                          FunctionTransformer(feature_names_out=&lt;f...\n                                                                         &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                                       ('random_forest',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid=[{'preprocessing__geo__n_clusters': [5, 8, 10],\n                          'random_forest__max_features': [4, 6, 8]},\n                         {'preprocessing__geo__n_clusters': [10, 15],\n                          'random_forest__max_features': [6, 8, 10]}],\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('preprocessing',\n                                        ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                                                     SimpleImputer(strategy='median')),\n                                                                                    ('standardscaler',\n                                                                                     StandardScaler())]),\n                                                          transformers=[('bedrooms',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('functiontransformer',\n                                                                                          FunctionTransformer(feature_names_out=&lt;f...\n                                                                         &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                                       ('random_forest',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid=[{'preprocessing__geo__n_clusters': [5, 8, 10],\n                          'random_forest__max_features': [4, 6, 8]},\n                         {'preprocessing__geo__n_clusters': [10, 15],\n                          'random_forest__max_features': [6, 8, 10]}],\n             scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('preprocessing',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14b...\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                ('random_forest', RandomForestRegressor(random_state=42))])preprocessing: ColumnTransformerColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                             SimpleImputer(strategy='median')),\n                                            ('standardscaler',\n                                             StandardScaler())]),\n                  transformers=[('bedrooms',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                                                                      func=&lt;function column_ratio...\n                                 ['total_bedrooms', 'total_rooms', 'population',\n                                  'households', 'median_income']),\n                                ('geo', ClusterSimilarity(random_state=42),\n                                 ['latitude', 'longitude']),\n                                ('cat',\n                                 Pipeline(steps=[('impute',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])bedrooms['total_bedrooms', 'total_rooms']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()rooms_per_house['total_rooms', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()people_per_house['population', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out='one-to-one', func=&lt;ufunc 'log'&gt;)StandardScalerStandardScaler()geo['latitude', 'longitude']ClusterSimilarityClusterSimilarity(random_state=42)cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(handle_unknown='ignore')remainder['housing_median_age']SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\nNotice that you can refer to any hyperparameter of any estimator in a pipeline, even if this estimator is nested deep inside several pipelines and column transformers. For example, when Scikit-Learn sees \"preprocessing__geo__n_clusters\", it splits this string at the double underscores, then it looks for an estimator named \"preprocessing\" in the pipeline and finds the preprocessing ColumnTransformer. Next, it looks for a transformer named \"geo\" inside this ColumnTransformer and finds the ClusterSimilarity transformer we used on the latitude and longitude attributes. Then it finds this transformer’s n_clusters hyperparameter. Similarly, random_forest__max_features refers to the max_features hyperparameter of the estimator named \"random_forest\", which is of course the RandomForest model\nThere are two dictionaries in this param_grid, so GridSearchCV will first evaluate all 3 × 3 = 9 combinations of n_clusters and max_features hyperparameter values specified in the first dict, then it will try all 2 × 3 = 6 combinations of hyperparameter values in the second dict. So in total the grid search will explore 9 + 6 = 15 combinations of hyperparameter values, and it will train the pipeline 3 times per combination, since we are using 3-fold cross validation. This means there will be a grand total of 15 × 3 = 45 rounds of training!\n\n\nCode\ngrid_search.best_params_\n\n# Since 15 is the maximum values that were evaluated, you\n# should probably try searching again with higher values, since the\n# score may continue to improve.\n\n\n{'preprocessing__geo__n_clusters': 15, 'random_forest__max_features': 6}\n\n\n\n\nCode\ngrid_search.best_estimator_\n\n\nPipeline(steps=[('preprocessing',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14b...\n                                                  ClusterSimilarity(n_clusters=15,\n                                                                    random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b14c8a1f0&gt;)])),\n                ('random_forest',\n                 RandomForestRegressor(max_features=6, random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessing',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14b...\n                                                  ClusterSimilarity(n_clusters=15,\n                                                                    random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b14c8a1f0&gt;)])),\n                ('random_forest',\n                 RandomForestRegressor(max_features=6, random_state=42))])preprocessing: ColumnTransformerColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                             SimpleImputer(strategy='median')),\n                                            ('standardscaler',\n                                             StandardScaler())]),\n                  transformers=[('bedrooms',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                                                                      func=&lt;function column_ratio...\n                                 ['total_bedrooms', 'total_rooms', 'population',\n                                  'households', 'median_income']),\n                                ('geo',\n                                 ClusterSimilarity(n_clusters=15,\n                                                   random_state=42),\n                                 ['latitude', 'longitude']),\n                                ('cat',\n                                 Pipeline(steps=[('impute',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b14c8a1f0&gt;)])bedrooms['total_bedrooms', 'total_rooms']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()rooms_per_house['total_rooms', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()people_per_house['population', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out='one-to-one', func=&lt;ufunc 'log'&gt;)StandardScalerStandardScaler()geo['latitude', 'longitude']ClusterSimilarityClusterSimilarity(n_clusters=15, random_state=42)cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b14c8a1f0&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(handle_unknown='ignore')remainder['housing_median_age']SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(max_features=6, random_state=42)\n\n\nYou can access the best estimator using grid_search.best_estimator_. If GridSearchCV is initialized with refit=True (which is the default), then once it finds the best estimator using cross-validation, it retrains it on the whole training set. This is usually a good idea, since feeding it more data will likely improve its performance.\nThe evaluation scores are available using grid_search.cv_results_. This is a dictionary, but if you wrap it in a DataFrame you get a nice list of all the test scores for each combination of hyperparameters and for each cross-validation split, as well as the mean test score across all splits:\nLet’s look at the score of each hyperparameter combination tested during the grid search:\n\n\nCode\ncv_res = pd.DataFrame(grid_search.cv_results_)\ncv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n\ncv_res = cv_res[[\"param_preprocessing__geo__n_clusters\",\n                 \"param_random_forest__max_features\", \"split0_test_score\",\n                 \"split1_test_score\", \"split2_test_score\", \"mean_test_score\"]]\nscore_cols = [\"split0\", \"split1\", \"split2\", \"mean_test_rmse\"]\ncv_res.columns = [\"n_clusters\", \"max_features\"] + score_cols\ncv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)\n\ncv_res.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nn_clusters\nmax_features\nsplit0\nsplit1\nsplit2\nmean_test_rmse\n\n\n\n\n12\n15\n6\n43460\n43919\n44748\n44042\n\n\n13\n15\n8\n44132\n44075\n45010\n44406\n\n\n14\n15\n10\n44374\n44286\n45316\n44659\n\n\n7\n10\n6\n44683\n44655\n45657\n44999\n\n\n9\n10\n6\n44683\n44655\n45657\n44999\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe mean test RMSE score for the best model is 44,042, which is better than the score you got earlier using the default hyperparameter values (which was 47,019). Congratulations, you have successfully fine-tuned your best model!"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#randomized-search",
    "href": "01_end_to_end_machine_learning_project.html#randomized-search",
    "title": "2  End-to-end Machine Learning project",
    "section": "7.2 Randomized Search",
    "text": "7.2 Randomized Search\nThe grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use RandomizedSearchCV instead. This class can be used in much the same way as the GridSearchCV class, but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration.\nFor each hyperparameter, you must provide either a list of possible values, or a probability distribution:\nTry 30 (n_iter × cv) random combinations of hyperparameters:\n\n\nCode\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),\n          'random_forest__max_features': randint(low=2, high=20)}\n\nrnd_search = RandomizedSearchCV(\n    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,\n    scoring='neg_root_mean_squared_error', random_state=42)\n\nrnd_search.fit(housing, housing_labels)\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[('preprocessing',\n                                              ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                                                           SimpleImputer(strategy='median')),\n                                                                                          ('standardscaler',\n                                                                                           StandardScaler())]),\n                                                                transformers=[('bedrooms',\n                                                                               Pipeline(steps=[('simpleimputer',\n                                                                                                SimpleImputer(strategy='median')),\n                                                                                               ('functiontransformer',\n                                                                                                FunctionTransformer(feature_names_...\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                                             ('random_forest',\n                                              RandomForestRegressor(random_state=42))]),\n                   param_distributions={'preprocessing__geo__n_clusters': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f2b14de5280&gt;,\n                                        'random_forest__max_features': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f2b146d3160&gt;},\n                   random_state=42, scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[('preprocessing',\n                                              ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                                                           SimpleImputer(strategy='median')),\n                                                                                          ('standardscaler',\n                                                                                           StandardScaler())]),\n                                                                transformers=[('bedrooms',\n                                                                               Pipeline(steps=[('simpleimputer',\n                                                                                                SimpleImputer(strategy='median')),\n                                                                                               ('functiontransformer',\n                                                                                                FunctionTransformer(feature_names_...\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                                             ('random_forest',\n                                              RandomForestRegressor(random_state=42))]),\n                   param_distributions={'preprocessing__geo__n_clusters': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f2b14de5280&gt;,\n                                        'random_forest__max_features': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f2b146d3160&gt;},\n                   random_state=42, scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('preprocessing',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14b...\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('impute',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])),\n                ('random_forest', RandomForestRegressor(random_state=42))])preprocessing: ColumnTransformerColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                             SimpleImputer(strategy='median')),\n                                            ('standardscaler',\n                                             StandardScaler())]),\n                  transformers=[('bedrooms',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                                                                      func=&lt;function column_ratio...\n                                 ['total_bedrooms', 'total_rooms', 'population',\n                                  'households', 'median_income']),\n                                ('geo', ClusterSimilarity(random_state=42),\n                                 ['latitude', 'longitude']),\n                                ('cat',\n                                 Pipeline(steps=[('impute',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;)])bedrooms['total_bedrooms', 'total_rooms']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()rooms_per_house['total_rooms', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()people_per_house['population', 'households']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x7f2b14bd3d30&gt;,\n                    func=&lt;function column_ratio at 0x7f2b14bd3af0&gt;)StandardScalerStandardScaler()log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputerSimpleImputer(strategy='median')FunctionTransformerFunctionTransformer(feature_names_out='one-to-one', func=&lt;ufunc 'log'&gt;)StandardScalerStandardScaler()geo['latitude', 'longitude']ClusterSimilarityClusterSimilarity(random_state=42)cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2b0e7f3fd0&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(handle_unknown='ignore')remainder['housing_median_age']SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\n\nCode\ncv_res = pd.DataFrame(rnd_search.cv_results_)\ncv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\ncv_res = cv_res[[\"param_preprocessing__geo__n_clusters\",\n                 \"param_random_forest__max_features\", \"split0_test_score\",\n                 \"split1_test_score\", \"split2_test_score\", \"mean_test_score\"]]\ncv_res.columns = [\"n_clusters\", \"max_features\"] + score_cols\ncv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)\ncv_res.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nn_clusters\nmax_features\nsplit0\nsplit1\nsplit2\nmean_test_rmse\n\n\n\n\n1\n45\n9\n41287\n42071\n42627\n41995\n\n\n8\n32\n7\n41690\n42513\n43224\n42475\n\n\n0\n41\n16\n42223\n42959\n43321\n42834\n\n\n5\n42\n4\n41818\n43094\n43817\n42910\n\n\n2\n23\n8\n42264\n42996\n43830\n43030\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAnother way to fine-tune your system is to try to combine the models that perform best. This is especially true if the individual models make very different types of errors."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#analyze-the-best-models-and-their-errors",
    "href": "01_end_to_end_machine_learning_project.html#analyze-the-best-models-and-their-errors",
    "title": "2  End-to-end Machine Learning project",
    "section": "7.3 Analyze the Best Models and Their Errors",
    "text": "7.3 Analyze the Best Models and Their Errors\nYou will often gain good insights on the problem by inspecting the best models. For example, the RandomForestRegressor can indicate the relative importance of each attribute for making accurate predictions\n\n\nCode\nfinal_model = rnd_search.best_estimator_  # includes preprocessing\nfeature_importances = final_model[\"random_forest\"].feature_importances_\nfeature_importances.round(2)\n\n\narray([0.07, 0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.19, 0.04, 0.01, 0.  ,\n       0.01, 0.01, 0.01, 0.01, 0.01, 0.  , 0.01, 0.01, 0.01, 0.  , 0.01,\n       0.01, 0.01, 0.01, 0.01, 0.  , 0.  , 0.02, 0.01, 0.01, 0.01, 0.02,\n       0.01, 0.  , 0.02, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01,\n       0.01, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01, 0.  , 0.07,\n       0.  , 0.  , 0.  , 0.01])\n\n\n\n\nCode\nsorted(zip(feature_importances,\n           final_model[\"preprocessing\"].get_feature_names_out()),\n           reverse=True)\n\n\n[(0.18694559869103852, 'log__median_income'),\n (0.0748194905715524, 'cat__ocean_proximity_INLAND'),\n (0.06926417748515576, 'bedrooms__ratio'),\n (0.05446998753775219, 'rooms_per_house__ratio'),\n (0.05262301809680712, 'people_per_house__ratio'),\n (0.03819415873915732, 'geo__Cluster 0 similarity'),\n (0.02879263999929514, 'geo__Cluster 28 similarity'),\n (0.023530192521380392, 'geo__Cluster 24 similarity'),\n (0.020544786346378206, 'geo__Cluster 27 similarity'),\n (0.019873052631077512, 'geo__Cluster 43 similarity'),\n (0.018597511022930273, 'geo__Cluster 34 similarity'),\n (0.017409085415656868, 'geo__Cluster 37 similarity'),\n (0.015546519677632162, 'geo__Cluster 20 similarity'),\n (0.014230331127504292, 'geo__Cluster 17 similarity'),\n (0.0141032216204026, 'geo__Cluster 39 similarity'),\n (0.014065768027447325, 'geo__Cluster 9 similarity'),\n (0.01354220782825315, 'geo__Cluster 4 similarity'),\n (0.01348963625822907, 'geo__Cluster 3 similarity'),\n (0.01338319626383868, 'geo__Cluster 38 similarity'),\n (0.012240533790212824, 'geo__Cluster 31 similarity'),\n (0.012089046542256785, 'geo__Cluster 7 similarity'),\n (0.01152326329703204, 'geo__Cluster 23 similarity'),\n (0.011397459905603558, 'geo__Cluster 40 similarity'),\n (0.011282340924816439, 'geo__Cluster 36 similarity'),\n (0.01104139770781063, 'remainder__housing_median_age'),\n (0.010671123191312802, 'geo__Cluster 44 similarity'),\n (0.010296376177202627, 'geo__Cluster 5 similarity'),\n (0.010184798445004483, 'geo__Cluster 42 similarity'),\n (0.010121853542225083, 'geo__Cluster 11 similarity'),\n (0.009795219101117579, 'geo__Cluster 35 similarity'),\n (0.00952581084310724, 'geo__Cluster 10 similarity'),\n (0.009433209165984823, 'geo__Cluster 13 similarity'),\n (0.00915075361116215, 'geo__Cluster 1 similarity'),\n (0.009021485619463173, 'geo__Cluster 30 similarity'),\n (0.00894936224917583, 'geo__Cluster 41 similarity'),\n (0.008901832702357514, 'geo__Cluster 25 similarity'),\n (0.008897504713401587, 'geo__Cluster 29 similarity'),\n (0.0086846298524955, 'geo__Cluster 21 similarity'),\n (0.008061104590483955, 'geo__Cluster 15 similarity'),\n (0.00786048176566994, 'geo__Cluster 16 similarity'),\n (0.007793633130749198, 'geo__Cluster 22 similarity'),\n (0.007501766442066527, 'log__total_rooms'),\n (0.0072024111938241275, 'geo__Cluster 32 similarity'),\n (0.006947156598995616, 'log__population'),\n (0.006800076770899128, 'log__households'),\n (0.006736105364684462, 'log__total_bedrooms'),\n (0.006315268213499131, 'geo__Cluster 33 similarity'),\n (0.005796398579893261, 'geo__Cluster 14 similarity'),\n (0.005234954623294958, 'geo__Cluster 6 similarity'),\n (0.0045514083468621595, 'geo__Cluster 12 similarity'),\n (0.004546042080216035, 'geo__Cluster 18 similarity'),\n (0.004314514641115755, 'geo__Cluster 2 similarity'),\n (0.003953528110719969, 'geo__Cluster 19 similarity'),\n (0.003297404747742136, 'geo__Cluster 26 similarity'),\n (0.00289453474290887, 'cat__ocean_proximity_&lt;1H OCEAN'),\n (0.0016978863168109126, 'cat__ocean_proximity_NEAR OCEAN'),\n (0.0016391131530559377, 'geo__Cluster 8 similarity'),\n (0.00015061247730531558, 'cat__ocean_proximity_NEAR BAY'),\n (7.301686597099842e-05, 'cat__ocean_proximity_ISLAND')]\n\n\n\n\nCode\nextra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\n#cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n#cat_one_hot_attribs = list(cat_encoder.categories_[0])\n#attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n\nrel_imp = pd.Series(feature_importances, index=final_model[\"preprocessing\"].get_feature_names_out()).sort_values(inplace=False)\nprint(rel_imp[-15:])\nrel_imp[-15:].T.plot(kind='barh', color='r')\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None\n\n\ngeo__Cluster 39 similarity     0.014103\ngeo__Cluster 17 similarity     0.014230\ngeo__Cluster 20 similarity     0.015547\ngeo__Cluster 37 similarity     0.017409\ngeo__Cluster 34 similarity     0.018598\ngeo__Cluster 43 similarity     0.019873\ngeo__Cluster 27 similarity     0.020545\ngeo__Cluster 24 similarity     0.023530\ngeo__Cluster 28 similarity     0.028793\ngeo__Cluster 0 similarity      0.038194\npeople_per_house__ratio        0.052623\nrooms_per_house__ratio         0.054470\nbedrooms__ratio                0.069264\ncat__ocean_proximity_INLAND    0.074819\nlog__median_income             0.186946\ndtype: float64\n\n\n\n\n\nWith this information, you may want to try dropping some of the less useful features (e.g., apparently only one ocean_proximity category is really useful, so you could try dropping the others)\nYou should also look at the specific errors that your system makes, then try to understand why it makes them and what could fix the problem (adding extra features or, on the contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\n\nNow is also a good time to ensure that your model not only works well on average, but also on all categories of districts, whether they’re rural or urban, rich or poor, northern or southern, minority or not, etc. Creating subsets of your validation set for each category takes a bit of work, but it’s important: if your model performs poorly on a whole category of districts, then it should probably not be deployed until the issue is solved, or at least it should not be used to make predictions for that category, as it may do more harm than good!"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#evaluate-your-system-on-the-test-set",
    "href": "01_end_to_end_machine_learning_project.html#evaluate-your-system-on-the-test-set",
    "title": "2  End-to-end Machine Learning project",
    "section": "7.4 Evaluate Your System on the Test Set",
    "text": "7.4 Evaluate Your System on the Test Set\nAfter tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set. Just get the predictors and the labels from your test set, and run your final_model to transform the data and make predictions, then evaluate these predictions:\n\n\nCode\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nfinal_predictions = final_model.predict(X_test)\n\nfinal_rmse = mean_squared_error(y_test, final_predictions, squared=False)\nprint(final_rmse)\n\n\n41424.40026462184\n\n\nIn some cases, such a point estimate of the generalization error will not be quite enough to convince you to launch: what if it is just 0.1% better than the model currently in production? You might want to have an idea of how precise this estimate is.\nFor this, you can compute a 95% confidence interval for the generalization error using scipy.stats.t.interval():\n\n\nCode\nfrom scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))\n\n\narray([39275.40861216, 43467.27680583])\n\n\nWe could compute the interval manually like this:\n\n\nCode\nm = len(squared_errors)\nmean = squared_errors.mean()\ntscore = stats.t.ppf((1 + confidence) / 2, df=m - 1)\ntmargin = tscore * squared_errors.std(ddof=1) / np.sqrt(m)\nnp.sqrt(mean - tmargin), np.sqrt(mean + tmargin)\n\n\n(39275.40861216077, 43467.2768058342)\n\n\nAlternatively, we could use a z-scores rather than t-scores:\n\n\nCode\nzscore = stats.norm.ppf((1 + confidence) / 2)\nzmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m)\nnp.sqrt(mean - zmargin), np.sqrt(mean + zmargin)\n\n\n(39276.05610140007, 43466.691749969636)\n\n\nThe performance will usually be slightly worse than what you measured using cross-validation if you did a lot of hyperparameter tuning (because your system ends up fine-tuned to perform well on the validation data, and will likely not perform as well on unknown datasets). It is not the case in this example, but when this happens you must resist the temptation to tweak the hyperparameters to make the numbers look good on the test set; the improvements would be unlikely to generalize to new data.\nNow comes the project prelaunch phase: you need to present your solution highlighting what you have learned, what worked and what did not, what assumptions were made, and what your system’s limitations are), document everything, and create nice presentations with clear visualizations and easy-to-remember statements (e.g., “the median income is the number one predictor of housing prices”).\nIn this California housing example, the final performance of the system is not much better than the experts’, but it may still be a good idea to launch it, especially if this frees up some time for the experts so they can work on more interesting and productive tasks."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#deployment",
    "href": "01_end_to_end_machine_learning_project.html#deployment",
    "title": "2  End-to-end Machine Learning project",
    "section": "8.1 Deployment",
    "text": "8.1 Deployment\nSo you need to monitor your model’s live performance. But how do you that? Well, it depends. In some cases, the model’s performance can be inferred from downstream metrics. For example, if your model is part of a recommender system and it suggests products that the users may be interested in, then it’s easy to monitor the number of recommended products sold each day. If this number drops (compared to non-recommended products), then the prime suspect is the model. This may be because the data pipeline is broken, or perhaps the model needs to be retrained on fresh data.\nHowever, it’s not always possible to determine the model’s performance without any human analysis. For example, suppose you trained an image classification model to detect several product defects on a production line. How can you get an alert if the model’s performance drops, before thousands of defective products get shipped to your clients? One solution is to send to human raters a sample of all the pictures that the model classified (especially pictures that the model wasn’t so sure about). Depending on the task, the raters may need to be experts, or they could be nonspecialists, such as workers on a crowdsourcing platform (e.g., Amazon Mechanical Turk). In some applications they could even be the users themselves, responding for example via surveys or repurposed captchas. Either way, you need to put in place a monitoring system (with or without human raters to evaluate the live model), as well as all the relevant processes to define what to do in case of failures and how to prepare for them.\nUnfortunately, this can be a lot of work. In fact, it is often much more work than building and training a model. If the data keeps evolving, you will need to update your datasets and retrain your model regularly. You should probably automate the whole process as much as possible. Here are a few things you can automate:\n\nCollect fresh data regularly and label it (e.g., using human raters).\nWrite a script to train the model and fine-tune the hyperparameters automatically. This script could run automatically, for example every day or every week, depending on your needs.\nWrite another script that will evaluate both the new model and the previous model on the updated test set, and deploy the model to production if the performance has not decreased (if it did, make sure you investigate why). The script should probably test the performance of your model on various subsets of the test set, such as poor or rich districts, rural or urban districts, etc.\n\nYou should also make sure you evaluate the model’s input data quality. Sometimes performance will degrade slightly because of a poor-quality signal (e.g., a malfunctioning sensor sending random values, or another team’s output becoming stale), but it may take a while before your system’s performance degrades enough to trigger an alert. If you monitor your model’s inputs, you may catch this earlier. For example, you could trigger an alert if more and more inputs are missing a feature, or if its mean or standard deviation drifts too far from the training set, or a categorical feature starts containing new categories.\nFinally, make sure you keep backups of every model you create and have the process and tools in place to roll back to a previous model quickly, in case the new model starts failing badly for some reason. Having backups also makes it possible to easily compare new models with previous ones. Similarly, you should keep backups of every version of your datasets so that you can roll back to a previous dataset if the new one ever gets corrupted (e.g., if the fresh data that gets added to it turns out to be full of outliers). Having backups of your datasets also allows you to evaluate any model against any previous dataset.\nAs you can see, Machine Learning involves quite a lot of infrastructure, so don’t be surprised if your first ML project takes a lot of effort and time to build and deploy to production. Fortunately, once all the infrastructure is in place, going from idea to production will be much faster."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#get-the-data-1",
    "href": "01_end_to_end_machine_learning_project.html#get-the-data-1",
    "title": "2  End-to-end Machine Learning project",
    "section": "9.1 Get the data",
    "text": "9.1 Get the data\n\n\nCode\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nhousing = load_housing_data() #Let’s take a look at the top five rows. Each row represents one district. There are 10 attributes\nhousing.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNow that we have the data, we can initialize a PyCaret experiment, which will preprocess the data and enable logging for all of the models that we will train on this dataset."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#explore-the-data",
    "href": "01_end_to_end_machine_learning_project.html#explore-the-data",
    "title": "2  End-to-end Machine Learning project",
    "section": "9.2 Explore the data",
    "text": "9.2 Explore the data\nydata-profiling primary goal is to provide a one-line Exploratory Data Analysis (EDA) experience in a consistent and fast solution. Like pandas df.describe() function, that is so handy, ydata-profiling delivers an extended analysis of a DataFrame while allowing the data analysis to be exported in different formats such as html and json.\nhttps://github.com/ydataai/ydata-profiling\n\n\nCode\nfrom ydata_profiling import ProfileReport\n\n\n\n\nCode\nprofile = ProfileReport(housing, title=\"Profiling Report\")\n\n\n\n\nCode\nprofile.to_notebook_iframe()\n\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nThe setup() function initializes the environment in pycaret and creates the transformation pipeline to prepare the data for modeling and deployment. setup() must be called before executing any other function in pycaret. It takes two mandatory parameters: a pandas dataframe and the name of the target column. All other parameters are optional and are used to customize the pre-processing pipeline.\nWhen setup() is executed, PyCaret’s inference algorithm will automatically infer the data types for all features based on certain properties. The data type should be inferred correctly but this is not always the case. To account for this, PyCaret displays a table containing the features and their inferred data types after setup() is executed.\n\nEnsuring that the data types are correct is of fundamental importance in PyCaret as it automatically performs a few pre-processing tasks which are imperative to any machine learning experiment. These tasks are performed differently for each data type which means it is very important for them to be correctly configured.\n\nhttps://pycaret.gitbook.io/docs/get-started/functions/initialize\n\n\nCode\nfrom pycaret.regression import *\nexp1 = setup(data = housing, target = 'median_house_value', session_id=123)\n\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n123\n\n\n1\nTarget\nmedian_house_value\n\n\n2\nTarget type\nRegression\n\n\n3\nOriginal data shape\n(20640, 10)\n\n\n4\nTransformed data shape\n(20640, 14)\n\n\n5\nTransformed train set shape\n(14447, 14)\n\n\n6\nTransformed test set shape\n(6193, 14)\n\n\n7\nNumeric features\n8\n\n\n8\nCategorical features\n1\n\n\n9\nRows with missing values\n1.0%\n\n\n10\nPreprocess\nTrue\n\n\n11\nImputation type\nsimple\n\n\n12\nNumeric imputation\nmean\n\n\n13\nCategorical imputation\nmode\n\n\n14\nMaximum one-hot encoding\n25\n\n\n15\nEncoding method\nNone\n\n\n16\nFold Generator\nKFold\n\n\n17\nFold Number\n10\n\n\n18\nCPU Jobs\n-1\n\n\n19\nUse GPU\nFalse\n\n\n20\nLog Experiment\nFalse\n\n\n21\nExperiment Name\nreg-default-name\n\n\n22\nUSI\n1e11\n\n\n\n\n\nThe eda() function generates automated Exploratory Data Analysis (EDA) using the AutoViz library.\nhttps://pycaret.gitbook.io/docs/get-started/functions/analyze#eda\n\n\nCode\neda(display_format = 'bokeh')\n\n\nOutput hidden; open in https://colab.research.google.com to view."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#prepare-the-data",
    "href": "01_end_to_end_machine_learning_project.html#prepare-the-data",
    "title": "2  End-to-end Machine Learning project",
    "section": "9.3 Prepare the data",
    "text": "9.3 Prepare the data\nIn order to demonstrate the predict_model() function on unseen data, a sample of 10% records has been withheld from the original dataset to be used for predictions. This should not be confused with a train/test split as this particular split is performed to simulate a real life scenario. Another way to think about this is that these records are not available at the time when the machine learning experiment was performed.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\nhousing, data_unseen = train_test_split(housing, test_size=0.1, random_state=42)\n\n\nhttps://pycaret.gitbook.io/docs/get-started/preprocessing\n\n\nCode\n\"\"\"\nclass columnDropperTransformer(TransformerMixin):\n    def __init__(self,columns):\n        self.columns=columns\n\n    def transform(self,X,y=None):\n        return X.drop(self.columns,axis=1)\n\n    def fit(self, X, y=None):\n        return self \n\"\"\"\n\n\n'\\nclass columnDropperTransformer(TransformerMixin):\\n    def __init__(self,columns):\\n        self.columns=columns\\n\\n    def transform(self,X,y=None):\\n        return X.drop(self.columns,axis=1)\\n\\n    def fit(self, X, y=None):\\n        return self \\n'\n\n\nNote that starting from PyCaret 3.0, it supports OOP API.\n\n\nCode\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\n\n# log_experiment = 'wandb',\nreg_experiment = setup(housing, \n            target = 'median_house_value', \n            train_size = 0.8,\n            data_split_stratify = ['income_cat'],\n            numeric_imputation = 'median',\n            categorical_imputation = 'mode',\n            normalize = True,\n            remove_outliers = True,\n            #custom_pipeline = columnDropperTransformer(['income_cat_1.0','income_cat_2.0','income_cat_3.0','income_cat_4.0','income_cat_5.0']),\n            session_id = 42, \n            #log_experiment=True,\n            profile = True,\n            experiment_name='ca_housing')\n\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n42\n\n\n1\nTarget\nmedian_house_value\n\n\n2\nTarget type\nRegression\n\n\n3\nOriginal data shape\n(18576, 11)\n\n\n4\nTransformed data shape\n(17951, 19)\n\n\n5\nTransformed train set shape\n(14236, 19)\n\n\n6\nTransformed test set shape\n(3716, 19)\n\n\n7\nNumeric features\n8\n\n\n8\nCategorical features\n2\n\n\n9\nRows with missing values\n1.0%\n\n\n10\nPreprocess\nTrue\n\n\n11\nImputation type\nsimple\n\n\n12\nNumeric imputation\nmedian\n\n\n13\nCategorical imputation\nmode\n\n\n14\nMaximum one-hot encoding\n25\n\n\n15\nEncoding method\nNone\n\n\n16\nRemove outliers\nTrue\n\n\n17\nOutliers threshold\n0.050000\n\n\n18\nNormalize\nTrue\n\n\n19\nNormalize method\nzscore\n\n\n20\nFold Generator\nKFold\n\n\n21\nFold Number\n10\n\n\n22\nCPU Jobs\n-1\n\n\n23\nUse GPU\nFalse\n\n\n24\nLog Experiment\nFalse\n\n\n25\nExperiment Name\nca_housing\n\n\n26\nUSI\n6018\n\n\n\n\n\nLoading profile... Please Wait!\n\n\n\n\nCode\nX_train_transformed = get_config(\"X_train_transformed\")\n\n\n\n\nCode\nX_train_transformed\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity_NEAR BAY\nocean_proximity_NEAR OCEAN\nocean_proximity_&lt;1H OCEAN\nocean_proximity_INLAND\nocean_proximity_ISLAND\nincome_cat_2.0\nincome_cat_1.0\nincome_cat_3.0\nincome_cat_4.0\nincome_cat_5.0\n\n\n\n\n0\n-1.364134\n1.046776\n1.859278\n0.197765\n0.481069\n-0.119739\n0.622318\n-1.069305\n2.901312\n-0.374453\n-0.909639\n-0.682598\n-0.014579\n1.421609\n-0.176182\n-0.749410\n-0.459495\n-0.342352\n\n\n1\n0.702926\n-0.861667\n0.235242\n-0.144908\n0.641720\n2.210955\n0.728542\n-0.811247\n-0.344672\n2.670564\n-0.909639\n-0.682598\n-0.014579\n1.421609\n-0.176182\n-0.749410\n-0.459495\n-0.342352\n\n\n3\n-0.889063\n0.379292\n-0.576776\n0.403949\n0.519485\n1.617214\n0.614730\n-0.619632\n-0.344672\n-0.374453\n1.099337\n-0.682598\n-0.014579\n1.421609\n-0.176182\n-0.749410\n-0.459495\n-0.342352\n\n\n4\n0.778736\n-0.894571\n-0.576776\n4.699704\n4.252885\n4.036448\n4.317402\n0.354490\n-0.344672\n-0.374453\n1.099337\n-0.682598\n-0.014579\n-0.703428\n-0.176182\n1.334383\n-0.459495\n-0.342352\n\n\n5\n0.025697\n-0.574930\n-0.414373\n0.763321\n0.376296\n0.597698\n0.550237\n0.781639\n-0.344672\n2.670564\n-0.909639\n-0.682598\n-0.014579\n-0.703428\n-0.176182\n-0.749410\n2.176301\n-0.342352\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n14854\n0.192478\n-0.664242\n0.072838\n-0.925360\n-0.898438\n-0.733011\n-0.861027\n0.026329\n-0.344672\n2.670564\n-0.909639\n-0.682598\n-0.014579\n-0.703428\n-0.176182\n1.334383\n-0.459495\n-0.342352\n\n\n14855\n1.344774\n-1.308224\n-0.901583\n0.901261\n0.868728\n0.729207\n0.975134\n-0.624362\n-0.344672\n-0.374453\n1.099337\n-0.682598\n-0.014579\n1.421609\n-0.176182\n-0.749410\n-0.459495\n-0.342352\n\n\n14856\n1.238642\n-1.275320\n-1.145189\n-0.385215\n-0.884468\n-0.584575\n-0.815502\n2.130376\n-0.344672\n-0.374453\n1.099337\n-0.682598\n-0.014579\n-0.703428\n-0.176182\n-0.749410\n-0.459495\n2.920971\n\n\n14858\n0.611955\n-0.730051\n1.534471\n-1.074917\n-0.814620\n-1.132744\n-0.849646\n-0.115229\n-0.344672\n-0.374453\n1.099337\n-0.682598\n-0.014579\n-0.703428\n-0.176182\n1.334383\n-0.459495\n-0.342352\n\n\n14859\n0.718087\n-0.734750\n0.803655\n0.014086\n-0.154552\n-0.075468\n-0.128839\n-0.016915\n-0.344672\n-0.374453\n1.099337\n-0.682598\n-0.014579\n-0.703428\n-0.176182\n1.334383\n-0.459495\n-0.342352\n\n\n\n\n\n14117 rows × 18 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nhttps://www.kaggle.com/code/uyeanil/titanic-custom-transformer-pipeline-pycaret as AutoML\n\n\nCode\n\"\"\"\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.impute import SimpleImputer\n\nclass ClusterSimilarity(BaseEstimator, TransformerMixin):\n    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n        self.n_clusters = n_clusters\n        self.gamma = gamma\n        self.random_state = random_state\n\n    def fit(self, X, y=None, sample_weight=None):\n        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n        self.kmeans_.fit(X, sample_weight=sample_weight)\n        return self  # always return self!\n\n    def transform(self, X):\n        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n    \n    def get_feature_names_out(self, names=None):\n        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n        \ndef column_ratio(X):\n    return X[:, [0]] / X[:, [1]]\n\ndef ratio_name(function_transformer, feature_names_in):\n    return [\"ratio\"]  # feature names out\n\ndef ratio_pipeline():\n    return make_pipeline(\n        SimpleImputer(strategy=\"median\"),\n        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n        StandardScaler())\n\nlog_pipeline = make_pipeline(\n    SimpleImputer(strategy=\"median\"),\n    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n    StandardScaler())\n\ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\ncat_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n    ])\ndefault_num_pipeline = make_pipeline(StandardScaler())\n\npreprocessing = ColumnTransformer([\n        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n                               \"households\", \"median_income\"]),\n        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n    ],\n    remainder=default_num_pipeline)  # one column remaining: housing_median_age\n\"\"\""
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#select-and-train-a-model-1",
    "href": "01_end_to_end_machine_learning_project.html#select-and-train-a-model-1",
    "title": "2  End-to-end Machine Learning project",
    "section": "9.4 Select and train a model",
    "text": "9.4 Select and train a model\nComparing all models to evaluate performance is the recommended starting point for modeling once the setup is completed (unless you exactly know what kind of model you need, which is often not the case). This function trains all models in the model library and scores them using k-fold cross validation for metric evaluation. The output prints a score grid that shows average MAE, MSE, RMSE,R2, RMSLE and MAPE accross the folds (10 by default) along with training time.\nhttps://pycaret.gitbook.io/docs/get-started/functions/train\n\n\nCode\nbest_model = compare_models(sort = 'RMSE', fold=5, verbose=True)\n\n\n\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\nTT (Sec)\n\n\n\n\ncatboost\nCatBoost Regressor\n30861.9104\n2181034968.8429\n46691.5909\n0.8358\n0.2308\n0.1717\n6.1980\n\n\nlightgbm\nLight Gradient Boosting Machine\n32355.3704\n2354387054.3994\n48510.4899\n0.8227\n0.2380\n0.1806\n0.4200\n\n\nrf\nRandom Forest Regressor\n33150.0424\n2572097820.1463\n50704.3716\n0.8063\n0.2444\n0.1852\n6.1680\n\n\net\nExtra Trees Regressor\n37005.2973\n3043882815.8823\n55163.1011\n0.7707\n0.2641\n0.2075\n4.0100\n\n\ngbr\nGradient Boosting Regressor\n38408.0096\n3067882575.8698\n55381.6992\n0.7690\n0.2719\n0.2180\n2.3660\n\n\nknn\nK Neighbors Regressor\n43466.1945\n4131793971.2000\n64271.9602\n0.6888\n0.3034\n0.2376\n0.2460\n\n\nbr\nBayesian Ridge\n49242.1016\n4720977912.2509\n68696.1739\n0.6445\n0.3801\n0.2878\n0.2680\n\n\nridge\nRidge Regression\n49252.5068\n4722997394.5170\n68710.6807\n0.6443\n0.3779\n0.2880\n0.1380\n\n\nlasso\nLasso Regression\n49254.5804\n4723361290.9467\n68713.2920\n0.6443\n0.3780\n0.2880\n0.1860\n\n\nllar\nLasso Least Angle Regression\n49254.4740\n4723363267.9404\n68713.3056\n0.6443\n0.3780\n0.2880\n0.2560\n\n\nlar\nLeast Angle Regression\n49255.2461\n4723522771.5529\n68714.4540\n0.6443\n0.3780\n0.2880\n0.2480\n\n\nlr\nLinear Regression\n49255.2461\n4723522771.5529\n68714.4540\n0.6443\n0.3780\n0.2880\n1.3200\n\n\nhuber\nHuber Regressor\n47867.1926\n4780685846.9827\n69135.6429\n0.6400\n0.3670\n0.2630\n0.3700\n\n\npar\nPassive Aggressive Regressor\n47814.5390\n4846281518.3859\n69609.0491\n0.6351\n0.3529\n0.2576\n0.9300\n\n\ndt\nDecision Tree Regressor\n44960.7247\n5036168100.5834\n70933.1545\n0.6209\n0.3270\n0.2450\n0.2160\n\n\nen\nElastic Net\n53387.4699\n5226586829.8307\n72292.3985\n0.6065\n0.3573\n0.3180\n0.1400\n\n\nomp\nOrthogonal Matching Pursuit\n62099.6132\n6940291836.2596\n83299.1586\n0.4773\n0.4256\n0.3821\n0.2980\n\n\nada\nAdaBoost Regressor\n78936.3612\n8428966889.7267\n91785.6668\n0.3653\n0.4911\n0.5492\n1.2680\n\n\ndummy\nDummy Regressor\n90681.3531\n13288744960.0000\n115272.3750\n-0.0002\n0.5881\n0.6153\n0.2420\n\n\n\n\n\n\n\n\n\n\n\nThe score grid printed above highlights the highest performing metric for comparison purposes only. By passing sort parameter, compare_models(sort = 'RMSE') will sort the grid by RMSE (lower to higher since lower is better). We also change the fold parameter from the default value of 10 to a different value by using compare_models(fold = 5) which will compare all models on 5 fold cross validation. Reducing the number of folds will improve the training time. By default, compare_models return the best performing model based on default sort order but can be used to return a list of top N models by using n_select parameter. Notice that you can also use exclude parameter to block certain models.\nThere are 25 regressors available in the model library of PyCaret. To see list of all regressors either check the docstring or use models() function to see the library.\n\n\nCode\nmodels()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nName\nReference\nTurbo\n\n\nID\n\n\n\n\n\n\n\nlr\nLinear Regression\nsklearn.linear_model._base.LinearRegression\nTrue\n\n\nlasso\nLasso Regression\nsklearn.linear_model._coordinate_descent.Lasso\nTrue\n\n\nridge\nRidge Regression\nsklearn.linear_model._ridge.Ridge\nTrue\n\n\nen\nElastic Net\nsklearn.linear_model._coordinate_descent.ElasticNet\nTrue\n\n\nlar\nLeast Angle Regression\nsklearn.linear_model._least_angle.Lars\nTrue\n\n\nllar\nLasso Least Angle Regression\nsklearn.linear_model._least_angle.LassoLars\nTrue\n\n\nomp\nOrthogonal Matching Pursuit\nsklearn.linear_model._omp.OrthogonalMatchingPursuit\nTrue\n\n\nbr\nBayesian Ridge\nsklearn.linear_model._bayes.BayesianRidge\nTrue\n\n\nard\nAutomatic Relevance Determination\nsklearn.linear_model._bayes.ARDRegression\nFalse\n\n\npar\nPassive Aggressive Regressor\nsklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor\nTrue\n\n\nransac\nRandom Sample Consensus\nsklearn.linear_model._ransac.RANSACRegressor\nFalse\n\n\ntr\nTheilSen Regressor\nsklearn.linear_model._theil_sen.TheilSenRegressor\nFalse\n\n\nhuber\nHuber Regressor\nsklearn.linear_model._huber.HuberRegressor\nTrue\n\n\nkr\nKernel Ridge\nsklearn.kernel_ridge.KernelRidge\nFalse\n\n\nsvm\nSupport Vector Regression\nsklearn.svm._classes.SVR\nFalse\n\n\nknn\nK Neighbors Regressor\nsklearn.neighbors._regression.KNeighborsRegressor\nTrue\n\n\ndt\nDecision Tree Regressor\nsklearn.tree._classes.DecisionTreeRegressor\nTrue\n\n\nrf\nRandom Forest Regressor\nsklearn.ensemble._forest.RandomForestRegressor\nTrue\n\n\net\nExtra Trees Regressor\nsklearn.ensemble._forest.ExtraTreesRegressor\nTrue\n\n\nada\nAdaBoost Regressor\nsklearn.ensemble._weight_boosting.AdaBoostRegressor\nTrue\n\n\ngbr\nGradient Boosting Regressor\nsklearn.ensemble._gb.GradientBoostingRegressor\nTrue\n\n\nmlp\nMLP Regressor\nsklearn.neural_network._multilayer_perceptron.MLPRegressor\nFalse\n\n\nxgboost\nExtreme Gradient Boosting\nxgboost.sklearn.XGBRegressor\nTrue\n\n\nlightgbm\nLight Gradient Boosting Machine\nlightgbm.sklearn.LGBMRegressor\nTrue\n\n\ncatboost\nCatBoost Regressor\ncatboost.core.CatBoostRegressor\nTrue\n\n\ndummy\nDummy Regressor\nsklearn.dummy.DummyRegressor\nTrue\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\ncreate_model() is the most granular function in PyCaret and is often the foundation behind most of the PyCaret functionalities. As the name suggests this function trains and evaluates a model using cross validation that can be set with fold parameter. The output prints a score grid that shows MAE, MSE, RMSE, R2, RMSLE and MAPE by fold.\n\n\nCode\nlightgbm = create_model('lightgbm')\n\n\n\n\n\n\n\n\n\n\n \nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\nFold\n \n \n \n \n \n \n\n\n\n\n0\n32167.4099\n2367963276.0495\n48661.7229\n0.8085\n0.2347\n0.1779\n\n\n1\n30984.6322\n2126158343.9367\n46110.2846\n0.8432\n0.2215\n0.1683\n\n\n2\n32185.5007\n2194345568.5475\n46843.8424\n0.8403\n0.2336\n0.1781\n\n\n3\n32027.3585\n2474181783.1165\n49741.1478\n0.8163\n0.2314\n0.1733\n\n\n4\n32129.8307\n2404652199.2175\n49037.2532\n0.8279\n0.2486\n0.1844\n\n\n5\n31591.5676\n2244223522.5267\n47373.2364\n0.8248\n0.2421\n0.1823\n\n\n6\n32913.7391\n2408284988.0534\n49074.2803\n0.8144\n0.2315\n0.1744\n\n\n7\n33253.8539\n2476742658.1617\n49766.8831\n0.8121\n0.2603\n0.2009\n\n\n8\n30294.3307\n2075253002.2693\n45554.9449\n0.8428\n0.2271\n0.1742\n\n\n9\n32522.3678\n2296816948.4132\n47925.1181\n0.8304\n0.2328\n0.1789\n\n\nMean\n32007.0591\n2306862229.0292\n48008.8714\n0.8261\n0.2364\n0.1793\n\n\nStd\n828.6481\n135432354.9804\n1417.9208\n0.0124\n0.0107\n0.0084\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlightgbm.get_params()\n\n\n{'boosting_type': 'gbdt',\n 'class_weight': None,\n 'colsample_bytree': 1.0,\n 'importance_type': 'split',\n 'learning_rate': 0.1,\n 'max_depth': -1,\n 'min_child_samples': 20,\n 'min_child_weight': 0.001,\n 'min_split_gain': 0.0,\n 'n_estimators': 100,\n 'n_jobs': -1,\n 'num_leaves': 31,\n 'objective': None,\n 'random_state': 42,\n 'reg_alpha': 0.0,\n 'reg_lambda': 0.0,\n 'silent': 'warn',\n 'subsample': 1.0,\n 'subsample_for_bin': 200000,\n 'subsample_freq': 0}"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#fine-tune-your-model-1",
    "href": "01_end_to_end_machine_learning_project.html#fine-tune-your-model-1",
    "title": "2  End-to-end Machine Learning project",
    "section": "9.5 Fine-Tune your model",
    "text": "9.5 Fine-Tune your model\nhttps://pycaret.gitbook.io/docs/get-started/functions/optimize\nhttps://pycaret.gitbook.io/docs/get-started/functions/others#automl\nhttps://pycaret.gitbook.io/docs/get-started/functions/optimize#ensemble_model\nWhen a model is created using the create_model function it uses the default hyperparameters to train the model. In order to tune hyperparameters, the tune_model function is used. This function automatically tunes the hyperparameters of a model using Random Grid Search on a pre-defined search space. The output prints a score grid that shows MAE, MSE, RMSE, R2, RMSLE and MAPE by fold. To use the custom search grid, you can pass custom_grid parameter in the tune_model function.\n\n\nCode\n#lgbm_params = {'num_leaves': np.arange(10,200,10),\n#                        'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n#                        'learning_rate': np.arange(0.1,1,0.1)\n#                        }\ntuned_lightgbm = tune_model(lightgbm, n_iter = 20, optimize = 'RMSE')\n\n\n\n\n\n\n\n\n\n\n \nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\nFold\n \n \n \n \n \n \n\n\n\n\n0\n32351.2276\n2387101345.3071\n48857.9712\n0.8069\n0.2367\n0.1792\n\n\n1\n31455.2358\n2118511518.6187\n46027.2910\n0.8438\n0.2208\n0.1698\n\n\n2\n32328.5698\n2238357372.3281\n47311.2817\n0.8371\n0.2347\n0.1778\n\n\n3\n31700.3079\n2440822708.2834\n49404.6831\n0.8188\n0.2295\n0.1688\n\n\n4\n32539.5859\n2446707135.0858\n49464.2005\n0.8249\n0.2557\n0.1874\n\n\n5\n31239.9483\n2198566379.3510\n46888.8727\n0.8284\n0.2394\n0.1791\n\n\n6\n32818.6665\n2397818205.7179\n48967.5219\n0.8152\n0.2312\n0.1741\n\n\n7\n33076.8939\n2438819409.6154\n49384.4045\n0.8149\n0.2605\n0.1986\n\n\n8\n30685.3025\n2109069565.3140\n45924.6074\n0.8402\n0.2293\n0.1752\n\n\n9\n32208.8041\n2309496367.8215\n48057.2197\n0.8295\n0.2362\n0.1793\n\n\nMean\n32040.4542\n2308527000.7443\n48028.8054\n0.8260\n0.2374\n0.1789\n\n\nStd\n712.2017\n126737551.3240\n1326.9723\n0.0115\n0.0115\n0.0083\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 20 candidates, totalling 200 fits\n\n\n\n\n\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one)."
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#analyze-the-model",
    "href": "01_end_to_end_machine_learning_project.html#analyze-the-model",
    "title": "2  End-to-end Machine Learning project",
    "section": "9.6 Analyze the model",
    "text": "9.6 Analyze the model\nBefore model finalization, the plot_model() function can be used to analyze the performance across different aspects such as Residuals Plot, Prediction Error, Feature Importance etc. This function takes a trained model object and returns a plot based on the test / hold-out set.\nThe evaluate_model() displays a user interface for analyzing the performance of a trained model. It calls the plot_model() function internally.\n\n\nCode\nevaluate_model(tuned_lightgbm)\n\n\n\n\n\nhttps://pycaret.gitbook.io/docs/get-started/functions/analyze\n\n\nCode\n#plot_model(tuned_lightgbm)\n#plot_model(tuned_lightgbm, plot = 'error')\n#plot_model(tuned_lightgbm, plot = 'feature')\n\n\nThe interpret_model() analyzes the predictions generated from a trained model. Most plots in this function are implemented based on the SHAP (Shapley Additive exPlanations)\n\n\nCode\ninterpret_model(tuned_lightgbm)"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#predict-on-test-set",
    "href": "01_end_to_end_machine_learning_project.html#predict-on-test-set",
    "title": "2  End-to-end Machine Learning project",
    "section": "9.7 Predict on test set",
    "text": "9.7 Predict on test set\nBefore finalizing the model, it is advisable to perform one final check by predicting the test/hold-out set and reviewing the evaluation metrics.\n\n\nCode\npredict_model(tuned_lightgbm)\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\nLight Gradient Boosting Machine\n32502.7734\n2287056237.8276\n47823.1768\n0.8326\n0.2390\n0.1826\n\n\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\nincome_cat\nmedian_house_value\nprediction_label\n\n\n\n\n14860\n-117.650002\n34.020000\n9.0\n2107.0\n411.0\n1138.0\n389.0\n4.4042\nINLAND\n3\n159100.0\n166584.740965\n\n\n14861\n-117.680000\n34.150002\n4.0\n4082.0\n578.0\n1996.0\n580.0\n6.7813\nINLAND\n5\n286300.0\n263679.509928\n\n\n14862\n-118.290001\n34.080002\n23.0\n1864.0\n937.0\n2795.0\n858.0\n1.8495\n&lt;1H OCEAN\n2\n212500.0\n212056.088491\n\n\n14863\n-117.970001\n33.990002\n23.0\n3335.0\n570.0\n1560.0\n555.0\n5.7268\n&lt;1H OCEAN\n4\n300300.0\n271747.627121\n\n\n14864\n-117.320000\n34.099998\n42.0\n801.0\n176.0\n711.0\n183.0\n1.8681\nINLAND\n2\n59700.0\n65521.582721\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n18571\n-121.000000\n37.259998\n45.0\n1750.0\n371.0\n847.0\n354.0\n1.7062\nINLAND\n2\n77400.0\n89862.446840\n\n\n18572\n-122.169998\n37.720001\n43.0\n3783.0\n814.0\n2139.0\n789.0\n4.0202\nNEAR BAY\n3\n166300.0\n196701.827793\n\n\n18573\n-117.120003\n32.580002\n34.0\n2003.0\n466.0\n1226.0\n443.0\n3.0613\nNEAR OCEAN\n3\n136700.0\n135883.426084\n\n\n18574\n-122.040001\n38.250000\n52.0\n582.0\n131.0\n241.0\n106.0\n2.4000\nINLAND\n2\n125000.0\n130034.478754\n\n\n18575\n-120.849998\n37.490002\n42.0\n264.0\n72.0\n310.0\n70.0\n1.4063\nINLAND\n1\n61500.0\n95609.262658\n\n\n\n\n\n3716 rows × 12 columns"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#finalize-model",
    "href": "01_end_to_end_machine_learning_project.html#finalize-model",
    "title": "2  End-to-end Machine Learning project",
    "section": "9.8 Finalize model",
    "text": "9.8 Finalize model\nModel finalization is the last step in the experiment. A normal machine learning workflow in PyCaret starts with setup(), followed by comparing all models using compare_models() and shortlisting a few candidate models (based on the metric of interest) to perform several modeling techniques such as hyperparameter tuning, ensembling, stacking etc. This workflow will eventually lead you to the best model for use in making predictions on new and unseen data. The finalize_model() function fits the model onto the complete dataset including the test/hold-out sample (20% in this case). The purpose of this function is to train the model on the complete dataset before it is deployed in production.\n\n\nCode\nfinal_lightgbm = finalize_model(tuned_lightgbm)\n\n\n\n\nCode\npredict_model(final_lightgbm)\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\nLight Gradient Boosting Machine\n29413.3892\n1872817647.7416\n43276.0632\n0.8629\n0.2199\n0.1665\n\n\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\nincome_cat\nmedian_house_value\nprediction_label\n\n\n\n\n14860\n-117.650002\n34.020000\n9.0\n2107.0\n411.0\n1138.0\n389.0\n4.4042\nINLAND\n3\n159100.0\n162164.238793\n\n\n14861\n-117.680000\n34.150002\n4.0\n4082.0\n578.0\n1996.0\n580.0\n6.7813\nINLAND\n5\n286300.0\n273877.534930\n\n\n14862\n-118.290001\n34.080002\n23.0\n1864.0\n937.0\n2795.0\n858.0\n1.8495\n&lt;1H OCEAN\n2\n212500.0\n209148.262865\n\n\n14863\n-117.970001\n33.990002\n23.0\n3335.0\n570.0\n1560.0\n555.0\n5.7268\n&lt;1H OCEAN\n4\n300300.0\n277503.747233\n\n\n14864\n-117.320000\n34.099998\n42.0\n801.0\n176.0\n711.0\n183.0\n1.8681\nINLAND\n2\n59700.0\n63081.895276\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n18571\n-121.000000\n37.259998\n45.0\n1750.0\n371.0\n847.0\n354.0\n1.7062\nINLAND\n2\n77400.0\n93080.033327\n\n\n18572\n-122.169998\n37.720001\n43.0\n3783.0\n814.0\n2139.0\n789.0\n4.0202\nNEAR BAY\n3\n166300.0\n193502.069024\n\n\n18573\n-117.120003\n32.580002\n34.0\n2003.0\n466.0\n1226.0\n443.0\n3.0613\nNEAR OCEAN\n3\n136700.0\n130554.077380\n\n\n18574\n-122.040001\n38.250000\n52.0\n582.0\n131.0\n241.0\n106.0\n2.4000\nINLAND\n2\n125000.0\n111131.581617\n\n\n18575\n-120.849998\n37.490002\n42.0\n264.0\n72.0\n310.0\n70.0\n1.4063\nINLAND\n1\n61500.0\n90052.157496\n\n\n\n\n\n3716 rows × 12 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe predict_model() function is also used to predict on the unseen dataset. data_unseen is the variable created at the beginning of the tutorial and contains 10% of the original dataset which was never exposed to PyCaret\n\n\nCode\ndata_unseen[\"income_cat\"] = pd.cut(data_unseen[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\nunseen_predictions = predict_model(final_lightgbm, data=data_unseen)\nunseen_predictions.head()\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\nLight Gradient Boosting Machine\n32474.6118\n2364513076.8764\n48626.2591\n0.8164\n0.2404\n0.1824\n\n\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nocean_proximity\nincome_cat\nmedian_house_value\nprediction_label\n\n\n\n\n0\n-122.379997\n40.669998\n10.0\n2281.0\n444.0\n1274.0\n438.0\n2.2120\nINLAND\n2\n65600.0\n68770.661309\n\n\n1\n-118.370003\n33.830002\n35.0\n1207.0\n207.0\n601.0\n213.0\n4.7308\n&lt;1H OCEAN\n4\n353400.0\n316812.741266\n\n\n2\n-117.239998\n32.720001\n39.0\n3089.0\n431.0\n1175.0\n432.0\n7.5925\nNEAR OCEAN\n5\n466700.0\n442420.543218\n\n\n3\n-118.440002\n34.049999\n18.0\n4780.0\n1192.0\n1886.0\n1036.0\n4.4674\n&lt;1H OCEAN\n3\n500001.0\n431798.750579\n\n\n4\n-118.440002\n34.180000\n33.0\n2127.0\n414.0\n1056.0\n391.0\n4.3750\n&lt;1H OCEAN\n3\n286100.0\n262450.257792\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nfrom pycaret.utils.generic import check_metric\ncheck_metric(unseen_predictions.median_house_value, unseen_predictions.prediction_label, 'RMSE')\n\n\n48626.2591"
  },
  {
    "objectID": "01_end_to_end_machine_learning_project.html#save-and-load-the-model",
    "href": "01_end_to_end_machine_learning_project.html#save-and-load-the-model",
    "title": "2  End-to-end Machine Learning project",
    "section": "9.9 Save and load the model",
    "text": "9.9 Save and load the model\nWe have now finished the experiment by finalizing the tuned_lightgbm model which is now stored in final_lightgbm variable. We have also used the model stored in final_lightgbm to predict data_unseen. This brings us to the end of our experiment, but one question is still to be asked: What happens when you have more new data to predict? Do you have to go through the entire experiment again? The answer is no, PyCaret’s inbuilt function save_model() allows you to save the model along with entire transformation pipeline for later use.\n\n\nCode\nsave_model(final_lightgbm,'Final LightGBM Model 02Feb2022')\n\n\nTransformation Pipeline and Model Successfully Saved\n\n\n(Pipeline(memory=FastMemory(location=/tmp/joblib),\n          steps=[('numerical_imputer',\n                  TransformerWrapper(include=['longitude', 'latitude',\n                                              'housing_median_age',\n                                              'total_rooms', 'total_bedrooms',\n                                              'population', 'households',\n                                              'median_income'],\n                                     transformer=SimpleImputer(strategy='median'))),\n                 ('categorical_imputer',\n                  TransformerWrapper(include=['ocean_proximity', 'income_cat...\n                  TransformerWrapper(include=['ocean_proximity', 'income_cat'],\n                                     transformer=OneHotEncoder(cols=['ocean_proximity',\n                                                                     'income_cat'],\n                                                               handle_missing='return_nan',\n                                                               use_cat_names=True))),\n                 ('remove_outliers',\n                  TransformerWrapper(transformer=RemoveOutliers())),\n                 ('normalize', TransformerWrapper(transformer=StandardScaler())),\n                 ('actual_estimator', LGBMRegressor(random_state=42))]),\n 'Final LightGBM Model 02Feb2022.pkl')\n\n\nTo load a saved model at a future date in the same or an alternative environment, we would use PyCaret’s load_model() function and then easily apply the saved model on new unseen data for prediction.\n\n\nCode\nsaved_final_lightgbm = load_model('Final LightGBM Model 02Feb2022')\n\n\nTransformation Pipeline and Model Successfully Loaded\n\n\n\n\nCode\nnew_prediction = predict_model(saved_final_lightgbm, data=data_unseen)\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\nLight Gradient Boosting Machine\n32474.6118\n2364513076.8764\n48626.2591\n0.8164\n0.2404\n0.1824\n\n\n\n\n\n\n\nCode\nfrom pycaret.utils.generic import check_metric\ncheck_metric(new_prediction.median_house_value, new_prediction.prediction_label, 'RMSE')\n\n\n48626.2591"
  },
  {
    "objectID": "02_Dataset.html#setup",
    "href": "02_Dataset.html#setup",
    "title": "3  Framing the problem and constructing the dataset",
    "section": "3.1 Setup",
    "text": "3.1 Setup\n\n\nCode\n!pip install git+https://github.com/phonchi/pigeonXT.git \n!pip install cleanlab -qq\n!pip install modAL -qq\n!pip install snorkel -qq\n\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting git+https://github.com/phonchi/pigeonXT.git\n  Cloning https://github.com/phonchi/pigeonXT.git to /tmp/pip-req-build-3jpt3q7q\n  Running command git clone --filter=blob:none --quiet https://github.com/phonchi/pigeonXT.git /tmp/pip-req-build-3jpt3q7q\n  Resolved https://github.com/phonchi/pigeonXT.git to commit 6564faf5101f33724a63a36231f74a3c4fa2ff3c\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting numpy&lt;2.0,&gt;=1.23\n  Downloading numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 38.4 MB/s eta 0:00:00\nRequirement already satisfied: pandas&lt;2.0,&gt;=1.3 in /usr/local/lib/python3.8/dist-packages (from pigeonxt-jupyter==0.7.3) (1.3.5)\nRequirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas&lt;2.0,&gt;=1.3-&gt;pigeonxt-jupyter==0.7.3) (2022.7.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas&lt;2.0,&gt;=1.3-&gt;pigeonxt-jupyter==0.7.3) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&lt;2.0,&gt;=1.3-&gt;pigeonxt-jupyter==0.7.3) (1.15.0)\nBuilding wheels for collected packages: pigeonxt-jupyter\n  Building wheel for pigeonxt-jupyter (pyproject.toml) ... done\n  Created wheel for pigeonxt-jupyter: filename=pigeonxt_jupyter-0.7.3-py3-none-any.whl size=12846 sha256=7c5f5b55a53ee9fc099d85c70fc907a29f7561eaf5a99a13f3b4e79d2ee8270d\n  Stored in directory: /tmp/pip-ephem-wheel-cache-t836gymf/wheels/18/f0/1e/314aebea9e2497eb5ebe4d5eddf3243311bd319fc13a6aee71\nSuccessfully built pigeonxt-jupyter\nInstalling collected packages: numpy, pigeonxt-jupyter\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscipy 1.7.3 requires numpy&lt;1.23.0,&gt;=1.16.5, but you have numpy 1.24.2 which is incompatible.\nnumba 0.56.4 requires numpy&lt;1.24,&gt;=1.18, but you have numpy 1.24.2 which is incompatible.\nSuccessfully installed numpy-1.24.2 pigeonxt-jupyter-0.7.3\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 157.5/157.5 KB 6.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.9/16.9 MB 49.1 MB/s eta 0:00:00\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npigeonxt-jupyter 0.7.3 requires numpy&lt;2.0,&gt;=1.23, but you have numpy 1.22.4 which is incompatible.\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.3/103.3 KB 6.0 MB/s eta 0:00:00\n\n\n\n\nCode\n# Built-in function\nimport glob\nimport os\nimport subprocess\nimport re\nimport random\n\n# Scientific computing\nimport numpy as np\nimport pandas as pd\nfrom IPython import display\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# Modeling\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\n# Manual labeling\nimport pigeonXT as pixt\n\n# Concensus algorithm\nfrom cleanlab.multiannotator import get_label_quality_multiannotator, get_majority_vote_label\nfrom cleanlab.benchmarking.noise_generation import generate_noise_matrix_from_trace\nfrom cleanlab.benchmarking.noise_generation import generate_noisy_labels\n\n# Active learning\nfrom modAL.models import ActiveLearner\nfrom modAL.uncertainty import uncertainty_sampling\n\n# Weak supervision\nfrom snorkel.labeling import PandasLFApplier\nfrom snorkel.labeling import labeling_function\nfrom snorkel.labeling import LFAnalysis\nfrom snorkel.labeling import LabelingFunction\nfrom snorkel.labeling.model import MajorityLabelVoter\nfrom snorkel.labeling.model import LabelModel\nfrom snorkel.labeling import filter_unlabeled_dataframe\nfrom snorkel.utils import probs_to_preds\nfrom snorkel.preprocess.nlp import SpacyPreprocessor\nfrom snorkel.labeling.lf.nlp import nlp_labeling_function\n\n# Data augmentation\nfrom skimage import io\nimport albumentations as A\n\n\n/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n\n\n\n\nCode\ndef visualize(image):\n    plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    plt.imshow(image)"
  },
  {
    "objectID": "02_Dataset.html#scraping-data-using-beautifulsoup",
    "href": "02_Dataset.html#scraping-data-using-beautifulsoup",
    "title": "3  Framing the problem and constructing the dataset",
    "section": "3.2 Scraping data using BeautifulSoup",
    "text": "3.2 Scraping data using BeautifulSoup\nSometimes we have to build our dataset using crawler. See 1. https://github.com/marvelje/weather_forecasting (Constructing weather dataset) 2. https://www.kaggle.com/code/pandula/a-simple-tutorial-on-beautifulsoup/notebook (Constructing IMDB dataset) 3. https://github.com/sharmasapna/Web-scraping-for-images (Constructing image dataset)"
  },
  {
    "objectID": "02_Dataset.html#manual-labeling-with-pigeonxt",
    "href": "02_Dataset.html#manual-labeling-with-pigeonxt",
    "title": "3  Framing the problem and constructing the dataset",
    "section": "3.3 Manual Labeling with pigeonXT",
    "text": "3.3 Manual Labeling with pigeonXT\nIn many data science projects, the first step at which the data science team gets involved is in labeling the image data. Even if the labeling will be automated, the first few images in a proof of concept are almost always hand-labeled. The form and organization will differ based on the problem type (image classification or object detection) and whether an image can have multiple labels or only one. To hand-label images, a rater views the image, determines the label(s), and records the label(s). There are two typical approaches to doing this recording: using a folder structure and a metadata table.\nIn a folder organization, raters simply move images to different folders depending on what their label is. All flowers that are daisies are stored in a folder named daisy, for example. Raters can do this quickly because most operating systems provide previews of images and handy ways to select groups of images and move them into folders.\nThe problem with the folder approach is that it leads to duplication if an image can have multiple labels—for example, if an image contains both roses and daisies. The alternative, and recommended, approach is to record the label(s) in a metadata table (such as in a spreadsheet or a CSV file) that has at least two columns - one column is the filename of the image file, and the other is the list of labels that are valid for the image.\nA labeling tool should have a facility to display the image, and enable the rater to quickly select valid categories and save the rating to a database. We will use pigeonXT which is a wrapper of Jupyter widget next.\n\n3.3.1 Image classification\n\n\nCode\n%%bash\nmkdir flower_images\nfor filename in 100080576_f52e8ee070_n.jpg 10140303196_b88d3d6cec.jpg 10172379554_b296050f82_n.jpg; do\n  gsutil cp gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/$filename flower_images\ndone\n\n\nCopying gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/100080576_f52e8ee070_n.jpg...\n/ [0 files][    0.0 B/ 26.2 KiB]                                                / [1 files][ 26.2 KiB/ 26.2 KiB]                                                \nOperation completed over 1 objects/26.2 KiB.                                     \nCopying gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/10140303196_b88d3d6cec.jpg...\n/ [0 files][    0.0 B/114.5 KiB]                                                / [1 files][114.5 KiB/114.5 KiB]                                                \nOperation completed over 1 objects/114.5 KiB.                                    \nCopying gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/10172379554_b296050f82_n.jpg...\n/ [0 files][    0.0 B/ 35.6 KiB]                                                / [1 files][ 35.6 KiB/ 35.6 KiB]                                                \nOperation completed over 1 objects/35.6 KiB.                                     \n\n\n\n\nCode\nfilenames = glob.glob('flower_images/*.jpg')\nprint(filenames)\n\n\n['flower_images/10140303196_b88d3d6cec.jpg', 'flower_images/100080576_f52e8ee070_n.jpg', 'flower_images/10172379554_b296050f82_n.jpg']\n\n\n\n\nCode\nannotations = pixt.annotate(\n      filenames,\n      options=['daisy', 'tulip', 'rose'],\n      display_fn=lambda filename: display.display(display.Image(filename))\n)\n\n\n\n\n\n\n\n\n\n\n\nAnnotation done.\n\n\n\n\nCode\nannotations\n\n\n\n  \n    \n      \n\n\n\n\n\n\nexample\nchanged\nlabel\n\n\n\n\n0\nflower_images/10140303196_b88d3d6cec.jpg\nFalse\n\n\n\n1\nflower_images/100080576_f52e8ee070_n.jpg\nTrue\ndaisy\n\n\n2\nflower_images/10172379554_b296050f82_n.jpg\nTrue\ndaisy\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n3.3.2 Binary or multi-class text classification\n\n\nCode\nannotations = pixt.annotate(\n        ['I love this movie', 'I was really disappointed by the book'],\n        options=['positive', 'negative', 'inbetween']\n    )\n\n\n\n\n\n\n\n\n\n\n\nAnnotation done.\n\n\n\n\nCode\nannotations\n\n\n\n  \n    \n      \n\n\n\n\n\n\nexample\nchanged\nlabel\n\n\n\n\n0\nI love this movie\nTrue\npositive\n\n\n1\nI was really disappointed by the book\nTrue\nnegative\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe output can be save to a CSV or JSON file.\n\n\nCode\nannotations.to_csv(\"text_class_dataset.csv\")\n\n\n\n\n3.3.3 Multi-label text classification\n\n\nCode\ndf = pd.DataFrame([\n        {'example': 'Star wars'},    \n        {'example': 'The Positively True Adventures of the Alleged Texas Cheerleader-Murdering Mom'},\n        {'example': 'Eternal Sunshine of the Spotless Mind'},\n        {'example': 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb'},    \n        {'example': 'Killer klowns from outer space'},    \n    ])\n\nlabels = ['Adventure', 'Romance', 'Fantasy', 'Science fiction', 'Horror', 'Thriller']\n    \nannotations = pixt.annotate(\n        df, \n        options=labels, \n        task_type='multilabel-classification',\n        buttons_in_a_row=3,\n        reset_buttons_after_click=True,\n        include_next=True,\n        include_back=True,\n    )  \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nannotations\n\n\n\n  \n    \n      \n\n\n\n\n\n\nexample\nchanged\nAdventure\nRomance\nFantasy\nScience fiction\nHorror\nThriller\n\n\n\n\n0\nStar wars\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1\nThe Positively True Adventures of the Alleged ...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nEternal Sunshine of the Spotless Mind\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nDr. Strangelove or: How I Learned to Stop Worr...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nKiller klowns from outer space\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nannotations.to_csv(\"text_multilabel_dataset.csv\")\n\n\nAnything that can be displayed on Jupyter (text, images, audio, graphs, etc.)can be displayed by pigeonXT by providing the appropriate display_fn argument.\nAdditionally, custom hooks can be attached to each row update (example_process_fn), or when the annotating task is complete(final_process_fn). See https://github.com/dennisbakhuis/pigeonXT for more details."
  },
  {
    "objectID": "02_Dataset.html#improve-consensus-labels-for-multiannotator-data-with-cleanlab",
    "href": "02_Dataset.html#improve-consensus-labels-for-multiannotator-data-with-cleanlab",
    "title": "3  Framing the problem and constructing the dataset",
    "section": "3.4 Improve Consensus Labels for Multiannotator Data with Cleanlab",
    "text": "3.4 Improve Consensus Labels for Multiannotator Data with Cleanlab\nThis example contains classification data that has been labeled by multiple annotators (where each example has been labeled by at least one annotator, but not every annotator has labeled every example)\nFor this part, we will generate a toy dataset that has 50 annotators and 300 examples. There are three possible classes, 0, 1 and 2.\nEach annotator annotates approximately 10% of the examples. We also synthetically made the last 5 annotators in our toy dataset have much noisier labels than the rest of the annotators. To generate our multiannotator data, we define a make_data() method\n\n\nCode\nSEED = 111 # set to None for non-reproducible randomness\nnp.random.seed(seed=SEED)\n\ndef make_data(\n    means=[[3, 2], [7, 7], [0, 8]],\n    covs=[[[5, -1.5], [-1.5, 1]], [[1, 0.5], [0.5, 4]], [[5, 1], [1, 5]]],\n    sizes=[150, 75, 75],\n    num_annotators=50,\n):\n\n    m = len(means)  # number of classes\n    n = sum(sizes)\n    local_data = []\n    labels = []\n\n    for idx in range(m):\n        local_data.append(\n            np.random.multivariate_normal(mean=means[idx], cov=covs[idx], size=sizes[idx])\n        )\n        labels.append(np.array([idx for i in range(sizes[idx])]))\n    X_train = np.vstack(local_data)\n    true_labels_train = np.hstack(labels)\n\n    # Compute p(true_label=k)\n    py = np.bincount(true_labels_train) / float(len(true_labels_train))\n\n    noise_matrix_better = generate_noise_matrix_from_trace(\n        m,\n        trace=0.8 * m,\n        py=py,\n        valid_noise_matrix=True,\n        seed=SEED,\n    )\n\n    noise_matrix_worse = generate_noise_matrix_from_trace(\n        m,\n        trace=0.35 * m,\n        py=py,\n        valid_noise_matrix=True,\n        seed=SEED,\n    )\n\n    # Generate our noisy labels using the noise_matrix for specified number of annotators.\n    s = pd.DataFrame(\n        np.vstack(\n            [\n                generate_noisy_labels(true_labels_train, noise_matrix_better)\n                if i &lt; num_annotators - 5\n                else generate_noisy_labels(true_labels_train, noise_matrix_worse)\n                for i in range(num_annotators)\n            ]\n        ).transpose()\n    )\n\n    # Each annotator only labels approximately 10% of the dataset\n    # (unlabeled points represented with NaN)\n    s = s.apply(lambda x: x.mask(np.random.random(n) &lt; 0.9)).astype(\"Int64\")\n    s.dropna(axis=1, how=\"all\", inplace=True)\n    s.columns = [\"A\" + str(i).zfill(4) for i in range(1, num_annotators+1)]\n\n    row_NA_check = pd.notna(s).any(axis=1)\n\n    return {\n        \"X_train\": X_train[row_NA_check],\n        \"true_labels_train\": true_labels_train[row_NA_check],\n        \"multiannotator_labels\": s[row_NA_check].reset_index(drop=True),\n    }\n\n\nLet’s view the first few rows of the data. Here are the labels selected by each annotator for the first few examples:\n\n\nCode\ndata_dict = make_data()\n\nX = data_dict[\"X_train\"]\nmultiannotator_labels = data_dict[\"multiannotator_labels\"]\ntrue_labels = data_dict[\"true_labels_train\"] # used for comparing the accuracy of consensus labels\n\n\n\n\nCode\nmultiannotator_labels.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nA0001\nA0002\nA0003\nA0004\nA0005\nA0006\nA0007\nA0008\nA0009\nA0010\n...\nA0041\nA0042\nA0043\nA0044\nA0045\nA0046\nA0047\nA0048\nA0049\nA0050\n\n\n\n\n0\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n...\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n1\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n0\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n...\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n2\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n...\n&lt;NA&gt;\n0\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n2\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n3\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n2\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n...\n0\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n4\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n...\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n2\n&lt;NA&gt;\n&lt;NA&gt;\n0\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n\n\n\n5 rows × 50 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nmultiannotator_labels contains the class labels that each annotator chose for each example, with examples that a particular annotator did not label represented using np.nan. X contains the features for each example.\n\n3.4.1 Get majority vote labels and compute out-of-sample predicted probabilites\nBefore training a machine learning model, we must first obtain the consensus labels from the annotators that labeled the data. The simplest way to obtain an initial set of consensus labels is to select it using majority vote.\n\n\nCode\nmajority_vote_label = get_majority_vote_label(multiannotator_labels)\n\n\nNext, we will train a model on the consensus labels obtained using majority vote to compute out-of-sample predicted probabilities. Here, we use a simple logistic regression model.\n\n\nCode\nmodel = LogisticRegression()\n\nnum_crossval_folds = 5\npred_probs = cross_val_predict(\n    estimator=model, X=X, y=majority_vote_label, cv=num_crossval_folds, method=\"predict_proba\"\n)\n\n\n\n\n3.4.2 Use cleanlab to get better consensus labels and other statistics\nUsing the annotators’ labels and the out-of-sample predicted probabilites from the model, cleanlab can help us obtain improved consensus labels for our data.\n\n\nCode\nresults = get_label_quality_multiannotator(multiannotator_labels, pred_probs, verbose=True)\n\n\nAnnotator(s) ['A0002' 'A0027' 'A0028' 'A0029' 'A0035'] did not annotate any examples that overlap with other annotators,                 \nusing the average annotator agreeement among other annotators as this annotator's agreement.\nAnnotator(s) ['A0002' 'A0027' 'A0028' 'A0029' 'A0035'] did not annotate any examples that overlap with other annotators,                 \nusing the average annotator agreeement among other annotators as this annotator's agreement.\n\n\nHere, we use the multiannotator.get_label_quality_multiannotator() function which returns a dictionary containing three items:\n\nlabel_quality which gives us the improved consensus labels using information from each of the annotators and the model. The DataFrame also contains information about the number of annotations, annotator agreement and consensus quality score for each example.\n\n\n\nCode\nresults[\"label_quality\"]\n\n\n\n  \n    \n      \n\n\n\n\n\n\nconsensus_label\nconsensus_quality_score\nannotator_agreement\nnum_annotations\n\n\n\n\n0\n0\n0.778153\n0.500000\n2\n\n\n1\n0\n0.840381\n1.000000\n3\n\n\n2\n0\n0.811415\n0.600000\n5\n\n\n3\n0\n0.764981\n0.600000\n5\n\n\n4\n0\n0.848161\n0.800000\n5\n\n\n...\n...\n...\n...\n...\n\n\n291\n2\n0.799866\n0.833333\n6\n\n\n292\n2\n0.774352\n0.666667\n6\n\n\n293\n2\n0.860814\n1.000000\n8\n\n\n294\n2\n0.946146\n0.500000\n4\n\n\n295\n2\n0.835593\n0.714286\n7\n\n\n\n\n\n296 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndetailed_label_quality which returns the label quality score for each label given by every annotator\n\n\n\nCode\nresults[\"detailed_label_quality\"]\n\n\n\n  \n    \n      \n\n\n\n\n\n\nquality_annotator_A0001\nquality_annotator_A0002\nquality_annotator_A0003\nquality_annotator_A0004\nquality_annotator_A0005\nquality_annotator_A0006\nquality_annotator_A0007\nquality_annotator_A0008\nquality_annotator_A0009\nquality_annotator_A0010\n...\nquality_annotator_A0041\nquality_annotator_A0042\nquality_annotator_A0043\nquality_annotator_A0044\nquality_annotator_A0045\nquality_annotator_A0046\nquality_annotator_A0047\nquality_annotator_A0048\nquality_annotator_A0049\nquality_annotator_A0050\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.840381\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\n0.811415\nNaN\nNaN\nNaN\nNaN\nNaN\n0.051990\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.172521\nNaN\nNaN\nNaN\n...\n0.764981\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n0.101573\nNaN\nNaN\n0.848161\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n291\n0.799866\nNaN\nNaN\nNaN\nNaN\nNaN\n0.799866\n0.131487\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n292\nNaN\nNaN\nNaN\n0.774352\nNaN\n0.774352\nNaN\nNaN\nNaN\n0.167851\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n293\nNaN\nNaN\n0.860814\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n0.860814\nNaN\nNaN\nNaN\nNaN\nNaN\n0.860814\nNaN\n\n\n294\nNaN\nNaN\n0.946146\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n0.026924\nNaN\nNaN\n0.026924\nNaN\n\n\n295\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n0.835593\nNaN\nNaN\nNaN\nNaN\n0.080003\nNaN\n0.080003\nNaN\nNaN\n\n\n\n\n\n296 rows × 50 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nannotator_stats which gives us the annotator quality score for each annotator, alongisde other information such as the number of examples each annotator labeled, their agreement with the consensus labels and the class they perform the worst at.\n\n\n\nCode\nresults[\"annotator_stats\"]\n\n\n\n  \n    \n      \n\n\n\n\n\n\nannotator_quality\nagreement_with_consensus\nworst_class\nnum_examples_labeled\n\n\n\n\nA0050\n0.262121\n0.250000\n2\n24\n\n\nA0047\n0.296369\n0.294118\n2\n34\n\n\nA0049\n0.320576\n0.310345\n1\n29\n\n\nA0046\n0.357324\n0.346154\n1\n26\n\n\nA0048\n0.468168\n0.520000\n2\n25\n\n\nA0031\n0.525641\n0.580645\n2\n31\n\n\nA0034\n0.544269\n0.607143\n2\n28\n\n\nA0021\n0.585709\n0.656250\n1\n32\n\n\nA0019\n0.610918\n0.678571\n2\n28\n\n\nA0015\n0.620574\n0.678571\n2\n28\n\n\nA0011\n0.622605\n0.692308\n1\n26\n\n\nA0025\n0.624784\n0.702703\n2\n37\n\n\nA0017\n0.632781\n0.680000\n2\n25\n\n\nA0003\n0.650724\n0.722222\n2\n36\n\n\nA0038\n0.654490\n0.714286\n1\n28\n\n\nA0037\n0.655256\n0.729730\n2\n37\n\n\nA0044\n0.656257\n0.738095\n2\n42\n\n\nA0040\n0.660856\n0.740741\n2\n27\n\n\nA0009\n0.677454\n0.750000\n2\n28\n\n\nA0006\n0.680579\n0.740741\n2\n27\n\n\nA0041\n0.697971\n0.766667\n2\n30\n\n\nA0033\n0.699469\n0.800000\n2\n20\n\n\nA0012\n0.701831\n0.772727\n0\n22\n\n\nA0014\n0.703866\n0.785714\n2\n28\n\n\nA0024\n0.703875\n0.794118\n2\n34\n\n\nA0004\n0.706232\n0.809524\n1\n21\n\n\nA0029\n0.707322\n0.781250\n2\n32\n\n\nA0002\n0.707889\n0.793103\n2\n29\n\n\nA0028\n0.709388\n0.815789\n1\n38\n\n\nA0045\n0.713798\n0.814815\n2\n27\n\n\nA0007\n0.714058\n0.805556\n2\n36\n\n\nA0042\n0.719011\n0.821429\n2\n28\n\n\nA0018\n0.725979\n0.823529\n2\n34\n\n\nA0043\n0.726029\n0.823529\n2\n34\n\n\nA0010\n0.726405\n0.833333\n0\n24\n\n\nA0030\n0.730296\n0.857143\n2\n28\n\n\nA0020\n0.731679\n0.827586\n2\n29\n\n\nA0036\n0.734518\n0.821429\n2\n28\n\n\nA0013\n0.735184\n0.863636\n2\n22\n\n\nA0016\n0.743784\n0.838710\n2\n31\n\n\nA0001\n0.743842\n0.848485\n2\n33\n\n\nA0026\n0.745471\n0.869565\n1\n23\n\n\nA0039\n0.746901\n0.857143\n2\n28\n\n\nA0027\n0.759070\n0.891892\n1\n37\n\n\nA0022\n0.767128\n0.862069\n2\n29\n\n\nA0035\n0.773640\n0.870968\n2\n31\n\n\nA0023\n0.781378\n0.909091\n1\n22\n\n\nA0005\n0.791291\n0.916667\n0\n24\n\n\nA0008\n0.792155\n0.916667\n1\n36\n\n\nA0032\n0.823238\n0.950000\n0\n20\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe can get the improved consensus labels from the label_quality DataFrame shown above. We can compare the accuracy of the consensus labels obtained using majority vote, and the improved consensus labels obtained using cleanlab.\n\n\nCode\nimproved_consensus_label = results[\"label_quality\"][\"consensus_label\"].values\n\nmajority_vote_accuracy = np.mean(true_labels == majority_vote_label)\ncleanlab_label_accuracy = np.mean(true_labels == improved_consensus_label)\n\nprint(f\"Accuracy of majority vote labels = {majority_vote_accuracy}\")\nprint(f\"Accuracy of cleanlab consensus labels = {cleanlab_label_accuracy}\")\n\n\nAccuracy of majority vote labels = 0.9054054054054054\nAccuracy of cleanlab consensus labels = 0.9763513513513513\n\n\nWe can see that the accuracy of the consensus labels improved as a result of using cleanlab!\nAfter obtaining the improved consensus labels, we can now retrain a better version of our machine learning model using these newly obtained labels. You can also repeatedly iterate this process of getting better consensus labels using the model’s out-of-sample predicted probabilites and then retraining the model with the improved labels to get even better predicted probabilities!"
  },
  {
    "objectID": "02_Dataset.html#active-learning-with-modal",
    "href": "02_Dataset.html#active-learning-with-modal",
    "title": "3  Framing the problem and constructing the dataset",
    "section": "3.5 Active Learning with modAL",
    "text": "3.5 Active Learning with modAL\nIn this example, the active learning workflow of modAL is demonstrated - with you in the loop! By running this notebook, you’ll be queried to label digits using the DIGITS dataset.\nHere we use the pool-based sampling. In this setting, we assume a small set of labeled data L and a large set of unlabeled data U such that |L|≪|U|.\nThe high level description about this strategy is as follows: Queries are selectively drawn from the pool, which is usually assumed to be closed (i.e., static or non-changing), although this is not strictly necessary. Typically, instances are queried in a greedy fashion, according to an informativeness measure used to evaluate all instances in the pool (or, perhaps if U is very large, some subsample thereof).\nNow we set up the initial training set for our classifier. If you would like to play around, you can try to modifiy the value n_initial below and see if it impacts the algorithm!\n\n\nCode\ndigits = load_digits()\n\nfig = plt.figure(figsize=(8, 8))  # figure size in inches\nfig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n\nfor i in range(64):\n    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n    # label the image with the target value\n    ax.text(0, 7, str(digits.target[i]))\n\n\n\n\n\n\n\nCode\nn_initial = 100\nX, y = load_digits(return_X_y=True)\nprint(X.shape)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\ninitial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)\n\nX_initial, y_initial = X_train[initial_idx], y_train[initial_idx]\nX_pool, y_pool = np.delete(X_train, initial_idx, axis=0), np.delete(y_train, initial_idx, axis=0)\nprint(X_pool.shape)\n\n\n(1797, 64)\n(1247, 64)\n\n\n\n3.5.1 Initializing the learner\nAlong with our pool-based sampling strategy, modAL’s modular design allows you to vary parameters surrounding the active learning process, including the core estimator and query strategy.\nNow we initialize the active learner. Feel free to change the underlying RandomForestClassifier or the uncertainty_sampling!\n\n\nCode\nlearner = ActiveLearner(\n    estimator=RandomForestClassifier(),\n    query_strategy=uncertainty_sampling,\n    X_training=X_initial, y_training=y_initial\n)\n\n## We also set how many queries we want to make. The more the better!\nn_queries = 20\n\n\n\n\n3.5.2 The active learning loop\n\n\nCode\naccuracy_scores = [learner.score(X_test, y_test)]\n\nfor i in range(n_queries):\n    display.clear_output(wait=True)\n    query_idx, query_inst = learner.query(X_pool)\n    with plt.style.context('seaborn-white'):\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.title('Digit to label')\n        plt.imshow(query_inst.reshape(8, 8))\n        plt.subplot(1, 2, 2)\n        plt.title('Accuracy of your model')\n        plt.plot(range(i+1), accuracy_scores)\n        plt.scatter(range(i+1), accuracy_scores)\n        plt.xlabel('number of queries')\n        plt.ylabel('accuracy')\n        display.display(plt.gcf())\n        plt.close('all')\n    # Query the rater\n    print(\"Which digit is this?\")\n    y_new = np.array([int(input())], dtype=int)\n    learner.teach(query_inst.reshape(1, -1), y_new)\n    X_pool, y_pool = np.delete(X_pool, query_idx, axis=0), np.delete(y_pool, query_idx, axis=0)\n    accuracy_scores.append(learner.score(X_test, y_test))\n\n\n\n\n\nWhich digit is this?\n3\n\n\nBy querying hard example to the rater, the performance increase from 83% to 86%!\n\n\n3.5.3 Iris example\nIn this example, we use scikit-learn’s k-nearest neighbors classifier as our estimator and default to modAL’s uncertainty sampling query strategy.\n\n\nCode\n# Set our RNG seed for reproducibility\nRANDOM_STATE_SEED = 123\nnp.random.seed(RANDOM_STATE_SEED)\n\n\n\n\nCode\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX_raw = iris['data']\ny_raw = iris['target']\nprint(X_raw.shape)\n\n\n(150, 4)\n\n\n\n\nCode\n# Define our PCA transformer and fit it onto our raw dataset.\npca = PCA(n_components=2, random_state=RANDOM_STATE_SEED)\ntransformed_iris = pca.fit_transform(X=X_raw)\n\n\n\n\nCode\n# Isolate the data we'll need for plotting.\nx_component, y_component = transformed_iris[:, 0], transformed_iris[:, 1]\n\n# Plot our dimensionality-reduced (via PCA) dataset.\nplt.figure(figsize=(8.5, 6), dpi=130)\nplt.scatter(x=x_component, y=y_component, c=y_raw, cmap='viridis', s=50, alpha=8/10)\nplt.title('Iris classes after PCA transformation')\nplt.show()\n\n\n\n\n\nNow we partition our iris dataset into a training set L and U. We first specify our training set L consisting of 3 random examples. The remaining examples go to our “unlabeled” pool U.\n\n\nCode\n# Isolate our examples for our labeled dataset.\nn_labeled_examples = X_raw.shape[0]\ntraining_indices = np.random.randint(low=0, high=n_labeled_examples + 1, size=3)\n\nX_train = X_raw[training_indices]\ny_train = y_raw[training_indices]\n\n# Isolate the non-training examples we'll be querying.\nX_pool = np.delete(X_raw, training_indices, axis=0)\ny_pool = np.delete(y_raw, training_indices, axis=0)\n\n\nFor the classification, we are going to use a simple k-nearest neighbors classifier. In this step, we are also going to initialize the ActiveLearner.\n\n\nCode\n# Specify our core estimator along with it's active learning model.\nknn = KNeighborsClassifier(n_neighbors=3)\nlearner = ActiveLearner(estimator=knn, X_training=X_train, y_training=y_train)\n\n\nLet’s see how our classifier performs on the initial training set!\n\n\nCode\n# Isolate the data we'll need for plotting.\nprint(y_train)\npredictions = learner.predict(X_raw)\nis_correct = (predictions == y_raw)\n\npredictions\n\n\n[2 2 1]\n\n\narray([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n\n\nCode\n# Record our learner's score on the raw data.\nunqueried_score = learner.score(X_raw, y_raw)\n\n# Plot our classification results.\nfig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\nax.scatter(x=x_component[is_correct],  y=y_component[is_correct],  c='g', marker='+', label='Correct',   alpha=8/10)\nax.scatter(x=x_component[~is_correct], y=y_component[~is_correct], c='r', marker='x', label='Incorrect', alpha=8/10)\nax.legend(loc='lower right')\nax.set_title(\"ActiveLearner class predictions (Accuracy: {score:.3f})\".format(score=unqueried_score))\nplt.show()\n\n\n\n\n\n\n3.5.3.1 Update our model by pool-based sampling our “unlabeled” dataset U\nAs we can see, our model is unable to properly learn the underlying data distribution. All of its predictions are for the third class label, and as such it is only as competitive as defaulting its predictions to a single class – if only we had more data!\nBelow, we tune our classifier by allowing it to query 20 instances it hasn’t seen before. Using uncertainty sampling, our classifier aims to reduce the amount of uncertainty in its predictions using a variety of measures — see https://modal-python.readthedocs.io/en/latest/index.html for more on specific classification uncertainty measures. With each requested query, we remove that record from our pool U and record our model’s accuracy on the raw dataset.\n\n\nCode\nN_QUERIES = 20\nperformance_history = [unqueried_score]\n\n# Allow our model to query our unlabeled dataset for the most\n# informative points according to our query strategy (uncertainty sampling).\nfor index in range(N_QUERIES):\n  query_index, query_instance = learner.query(X_pool)\n\n  # Teach our ActiveLearner model the record it has requested.\n  # Here we assume the label comes from the true label!!!\n  X, y = X_pool[query_index].reshape(1, -1), y_pool[query_index].reshape(1, )\n  learner.teach(X=X, y=y)\n\n  # Remove the queried instance from the unlabeled pool.\n  X_pool, y_pool = np.delete(X_pool, query_index, axis=0), np.delete(y_pool, query_index)\n\n  # Calculate and report our model's accuracy.\n  model_accuracy = learner.score(X_raw, y_raw)\n  print('Accuracy after query {n}: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy))\n\n  # Save our model's performance for plotting.\n  performance_history.append(model_accuracy)\n\n\nAccuracy after query 1: 0.6667\nAccuracy after query 2: 0.6667\nAccuracy after query 3: 0.8800\nAccuracy after query 4: 0.8800\nAccuracy after query 5: 0.8733\nAccuracy after query 6: 0.8400\nAccuracy after query 7: 0.7400\nAccuracy after query 8: 0.7267\nAccuracy after query 9: 0.7267\nAccuracy after query 10: 0.7267\nAccuracy after query 11: 0.7267\nAccuracy after query 12: 0.7267\nAccuracy after query 13: 0.7267\nAccuracy after query 14: 0.7267\nAccuracy after query 15: 0.7200\nAccuracy after query 16: 0.8400\nAccuracy after query 17: 0.8800\nAccuracy after query 18: 0.8933\nAccuracy after query 19: 0.9267\nAccuracy after query 20: 0.9267\n\n\nHere, we first plot the query iteration index against model accuracy. To visualize the performance of our classifier, we also plot the correct and incorrect predictions on the full dataset.\n\n\nCode\n# Plot our performance over time.\nfig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n\nax.plot(performance_history)\nax.scatter(range(len(performance_history)), performance_history, s=13)\n\nax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=5, integer=True))\nax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10))\nax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n\nax.set_ylim(bottom=0, top=1)\nax.grid(True)\n\nax.set_title('Incremental classification accuracy')\nax.set_xlabel('Query iteration')\nax.set_ylabel('Classification Accuracy')\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# Isolate the data we'll need for plotting.\npredictions = learner.predict(X_raw)\nis_correct = (predictions == y_raw)\n\n# Plot our updated classification results once we've trained our learner.\nfig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n\nax.scatter(x=x_component[is_correct],  y=y_component[is_correct],  c='g', marker='+', label='Correct',   alpha=8/10)\nax.scatter(x=x_component[~is_correct], y=y_component[~is_correct], c='r', marker='x', label='Incorrect', alpha=8/10)\n\nax.set_title('Classification accuracy after {n} queries: {final_acc:.3f}'.format(n=N_QUERIES, final_acc=performance_history[-1]))\nax.legend(loc='lower right')\n\nplt.show()"
  },
  {
    "objectID": "02_Dataset.html#weak-supervison-using-snorkel",
    "href": "02_Dataset.html#weak-supervison-using-snorkel",
    "title": "3  Framing the problem and constructing the dataset",
    "section": "3.6 Weak Supervison using Snorkel",
    "text": "3.6 Weak Supervison using Snorkel\nWe will walk through the process of using Snorkel to build a training set for classifying YouTube comments as spam or not spam. The goal of this tutorial is to illustrate the basic components and concepts of Snorkel in a simple way, but also to dive into the actual process of iteratively developing real applications in Snorkel.\nOur goal is to train a classifier over the comment data that can predict whether a comment is spam or not spam. We have access to a large amount of unlabeled data in the form of YouTube comments with some metadata. In order to train a classifier, we need to label our data, but doing so by hand for real world applications can often be prohibitively slow and expensive.\nIn these cases, we can turn to a weak supervision approach, using labeling functions (LFs) in Snorkel: noisy, programmatic rules and heuristics that assign labels to unlabeled training data.\nWe’ll dive into the Snorkel API and how we write labeling functions later in this tutorial, but as an example, we can write a LF that labels data points with “http” in the comment text as spam since many spam comments contain links.\nWe use a YouTube comments dataset that consists of YouTube comments from 5 videos. The task is to classify each comment as being\nHAM: comments relevant to the video (even very simple ones), or SPAM: irrelevant (often trying to advertise something) or inappropriate messages\n\n\nCode\n%%file download_data.sh\n\n#!/bin/bash\nset -euxo pipefail\n\nFILES=( \"Youtube01-Psy.csv\" \"Youtube02-KatyPerry.csv\" \"Youtube03-LMFAO.csv\" \"Youtube04-Eminem.csv\" \"Youtube05-Shakira.csv\" )\nDATA_URL=\"http://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip\"\nRELOAD=false\n\n# Check if at least any file is missing. If so, reload all data.\nfor filename in \"${FILES[@]}\"\ndo\n    if [ ! -e \"data/$filename\" ]; then\n        RELOAD=true\n    fi\ndone\n\nif [ \"$RELOAD\" = true ]; then\n    if [ -d \"data/\" ]; then rm -Rf \"data/\"; fi\n    mkdir -p data\n    curl $DATA_URL &gt; data.zip\n    mv data.zip data/\n    cd data\n    unzip data.zip\n    rm data.zip\n    rm -rf __MACOSX\n    cd ..\nfi\n\n\nWriting download_data.sh\n\n\n\n\nCode\ndef load_spam_dataset(load_train_labels: bool = False, split_dev_valid: bool = False):\n    if os.path.basename(os.getcwd()) == \"snorkel-tutorials\":\n        os.chdir(\"spam\")\n    try:\n        subprocess.run([\"bash\", \"download_data.sh\"], check=True, stderr=subprocess.PIPE)\n    except subprocess.CalledProcessError as e:\n        print(e.stderr.decode())\n        raise e\n    filenames = sorted(glob.glob(\"data/Youtube*.csv\"))\n\n    dfs = []\n    for i, filename in enumerate(filenames, start=1):\n        df = pd.read_csv(filename)\n        # Lowercase column names\n        df.columns = map(str.lower, df.columns)\n        # Remove comment_id field\n        df = df.drop(\"comment_id\", axis=1)\n        # Add field indicating source video\n        df[\"video\"] = [i] * len(df)\n        # Rename fields\n        df = df.rename(columns={\"class\": \"label\", \"content\": \"text\"})\n        # Shuffle order\n        df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n        dfs.append(df)\n\n    df_train = pd.concat(dfs[:4])\n    df_dev = df_train.sample(100, random_state=123)\n\n    if not load_train_labels:\n        df_train[\"label\"] = np.ones(len(df_train[\"label\"])) * -1\n    df_valid_test = dfs[4]\n    df_valid, df_test = train_test_split(\n        df_valid_test, test_size=250, random_state=123, stratify=df_valid_test.label\n    )\n\n    if split_dev_valid:\n        return df_train, df_dev, df_valid, df_test\n    else:\n        return df_train, df_test\n\n\nWe split our data into two sets:\n\nTraining Set: The largest split of the dataset, and the one without any ground truth (“gold”) labels. We will generate labels for these data points with weak supervision.\nTest Set: A small, standard held-out blind hand-labeled set for final evaluation of our classifier. This set should only be used for final evaluation, not error analysis.\n\n\nNote that in more advanced production settings, we will often further split up the available hand-labeled data into a development split, for getting ideas to write labeling functions, and a validation split for e.g. checking our performance without looking at test set scores, hyperparameter tuning, etc. These splits are omitted for simplicity here.\n\n\n3.6.1 Loading Data\nWe load the YouTube comments dataset and create Pandas DataFrame objects for the train and test sets. Snorkel provides native support for several DataFrame-like data structures, including Pandas, Dask, and PySpark.\nEach DataFrame consists of the following fields:\n\nauthor: Username of the comment author\ndata: Date and time the comment was posted\ntext: Raw text content of the comment\nlabel: Whether the comment is SPAM (1), HAM (0), or UNKNOWN/ABSTAIN (-1)\nvideo: Video the comment is associated with\n\nWe start by loading our data. The load_spam_dataset() method downloads the raw CSV files from the internet, divides them into splits, converts them into DataFrames, and shuffles them. As mentioned above, the dataset contains comments from 5 of the most popular YouTube videos during a period between 2014 and 2015. The first four videos’ comments are combined to form the train set. This set has no gold labels. The fifth video is part of the test set.\n\n\nCode\ndf_train, df_test = load_spam_dataset()\n\n# We pull out the label vectors for ease of use later\nY_test = df_test.label.values\n\n\n\n\nCode\ndf_train\n\n\n\n  \n    \n      \n\n\n\n\n\n\nauthor\ndate\ntext\nlabel\nvideo\n\n\n\n\n0\nAlessandro leite\n2014-11-05T22:21:36\npls http://www10.vakinha.com.br/VaquinhaE.aspx...\n-1.0\n1\n\n\n1\nSalim Tayara\n2014-11-02T14:33:30\nif your like drones, plz subscribe to Kamal Ta...\n-1.0\n1\n\n\n2\nPhuc Ly\n2014-01-20T15:27:47\ngo here to check the views :3﻿\n-1.0\n1\n\n\n3\nDropShotSk8r\n2014-01-19T04:27:18\nCame here to check the views, goodbye.﻿\n-1.0\n1\n\n\n4\ncss403\n2014-11-07T14:25:48\ni am 2,126,492,636 viewer :D﻿\n-1.0\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n443\nThemayerlife\nNaN\nCheck out my mummy chanel!\n-1.0\n4\n\n\n444\nFill Reseni\n2015-05-27T17:10:53.724000\nThe rap: cool Rihanna: STTUUPID﻿\n-1.0\n4\n\n\n445\nGreg Fils Aimé\nNaN\nI hope everyone is in good spirits I&#39;m a h...\n-1.0\n4\n\n\n446\nLil M\nNaN\nLil m !!!!! Check hi out!!!!! Does live the wa...\n-1.0\n4\n\n\n447\nAvidorFilms\nNaN\nPlease check out my youtube channel! Just uplo...\n-1.0\n4\n\n\n\n\n\n1586 rows × 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n# For clarity, we define constants to represent the class labels for spam, ham, and abstaining.\nABSTAIN = -1\nHAM = 0\nSPAM = 1\n\n\n\n\n3.6.2 Writing Labeling Functions (LFs)\nLabeling functions (LFs) help users encode domain knowledge and other supervision sources programmatically.\nLFs are heuristics that take as input a data point and either assign a label to it (in this case, HAM or SPAM) or abstain (don’t assign any label). Labeling functions can be noisy: they don’t have perfect accuracy and don’t have to label every data point. Moreover, different labeling functions can overlap (label the same data point) and even conflict (assign different labels to the same data point). This is expected, and we demonstrate how we deal with this later.\nBecause their only requirement is that they map a data point a label (or abstain), they can wrap a wide variety of forms of supervision.\nTypical LF development cycles include multiple iterations of ideation, refining, evaluation, and debugging. A typical cycle consists of the following steps:\n\nLook at examples to generate ideas for LFs\nWrite an initial version of an LF\nSpot check its performance by looking at its output on data points in the training set (or development set if available)\nRefine and debug to improve coverage or accuracy as necessary\n\nOur goal for LF development is to create a high quality set of training labels for our unlabeled dataset, not to label everything or directly create a model for inference using the LFs. The training labels are used to train a separate discriminative model (in this case, one which just uses the comment text) in order to generalize to new, unseen data points. Using this model, we can make predictions for data points that our LFs don’t cover.\n\nPattern-matching LFs (regular expressions)\n\nLabeling functions in Snorkel are created with the @labeling_function decorator. The decorator can be applied to any Python function that returns a label for a single data point.\n\nSee https://realpython.com/primer-on-python-decorators/ for more details about decorators.\n\nLet’s start developing an LF to catch instances of commenters trying to get people to “check out” their channel, video, or website. We’ll start by just looking for the exact string “check out” in the text, and see how that compares to looking for just “check” in the text. For the two versions of our rule, we’ll write a Python function over a single data point that express it, then add the decorator.\nOne dominant pattern in the comments that look like spam (which we might know from prior domain experience, or from inspection of a few training data points) is the use of the phrase “check out” (e.g. “check out my channel”). Let’s start with that.\n\n\nCode\n@labeling_function()\ndef regex_check_out(x):\n    return SPAM if re.search(r\"check.*out\", x.text, flags=re.I) else ABSTAIN\n\n\n\nKeyword LFs\n\nFor text applications, some of the simplest LFs to write are often just keyword lookups. These will often follow the same execution pattern, so we can create a template and use the resources parameter to pass in LF-specific keywords. Similar to the labeling_function decorator, the LabelingFunction class wraps a Python function (the f parameter), and we can use the resources parameter to pass in keyword arguments (here, our keywords to lookup) to said function.\n\n\nCode\ndef keyword_lookup(x, keywords, label):\n    if any(word in x.text.lower() for word in keywords):\n        return label\n    return ABSTAIN\n\n\ndef make_keyword_lf(keywords, label=SPAM):\n    return LabelingFunction(\n        name=f\"keyword_{keywords[0]}\",\n        f=keyword_lookup,\n        resources=dict(keywords=keywords, label=label),\n    )\n\n\n\"\"\"Spam comments talk about 'my channel', 'my video', etc.\"\"\"\nkeyword_my = make_keyword_lf(keywords=[\"my\"])\n\n\"\"\"Spam comments ask users to subscribe to their channels.\"\"\"\nkeyword_subscribe = make_keyword_lf(keywords=[\"subscribe\"])\n\n\"\"\"Spam comments post links to other channels.\"\"\"\nkeyword_link = make_keyword_lf(keywords=[\"http\"])\n\n\"\"\"Spam comments suspicious.\"\"\"\nkeyword_guys = make_keyword_lf(keywords=[\"guys\"])\n\n\"\"\"Spam comments make requests rather than commenting.\"\"\"\nkeyword_please = make_keyword_lf(keywords=[\"please\", \"plz\"])\n\n\n\n\"\"\"Ham comments actually talk about the video's content.\"\"\"\nkeyword_song = make_keyword_lf(keywords=[\"song\"], label=HAM)\n\n\n\nHeuristic LFs There may other heuristics or “rules of thumb” that you come up with as you look at the data. So long as you can express it in a function, it’s a viable LF!\n\n\n\nCode\n@labeling_function()\ndef short_comment(x):\n    \"\"\"Ham comments are often short, such as 'cool video!'\"\"\"\n    return HAM if len(x.text.split()) &lt; 5 else ABSTAIN\n\n\n\n\nCode\nlfs = [\n    keyword_my,\n    keyword_subscribe,\n    keyword_link,\n    keyword_guys,\n    keyword_please,\n    keyword_song,\n    regex_check_out,\n    short_comment,\n]\n\n\nTo apply one or more LFs that we’ve written to a collection of data points, we use an LFApplier. Because our data points are represented with a Pandas DataFrame in this tutorial, we use the PandasLFApplier. Correspondingly, a single data point x that’s passed into our LFs will be a Pandas Series object.\nIt’s important to note that these LFs will work for any object with an attribute named text, not just Pandas objects. Snorkel has several other appliers for different data point collection types which you can browse in the https://snorkel.readthedocs.io/en/master/packages/labeling.html.\nThe output of the apply(...) method is a label matrix, a fundamental concept in Snorkel. It’s a NumPy array L with one column for each LF and one row for each data point, where L[i, j] is the label that the jth labeling function output for the ith data point. We’ll create a label matrix for the train set.\n\n\nCode\napplier = PandasLFApplier(lfs=lfs)\nL_train = applier.apply(df=df_train)\nL_test = applier.apply(df=df_test)\n\n\n100%|██████████| 1586/1586 [00:00&lt;00:00, 9646.05it/s]\n100%|██████████| 250/250 [00:00&lt;00:00, 8692.21it/s]\n\n\nWe can easily calculate the coverage of these LFs (i.e., the percentage of the dataset that they label) as follows:\n\n\nCode\ncoverage = (L_train != ABSTAIN).mean(axis=0)\ncoverage\n\n\narray([0.19861286, 0.12736444, 0.11916772, 0.05674653, 0.11223203,\n       0.14186633, 0.23392182, 0.22572509])\n\n\n\n\nCode\nL_train\n\n\narray([[-1, -1,  1, ..., -1, -1, -1],\n       [-1,  1, -1, ..., -1, -1, -1],\n       [-1, -1, -1, ..., -1, -1, -1],\n       ...,\n       [ 1,  1, -1, ..., -1,  1, -1],\n       [-1,  1, -1, ..., -1,  1, -1],\n       [ 1, -1, -1, ..., -1,  1, -1]])\n\n\nLots of statistics about labeling functions - like coverage - are useful when building any Snorkel application. So Snorkel provides tooling for common LF analyses using the LFAnalysis utility.\nCheckout the statistics here https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.LFAnalysis.html\n\n\nCode\nLFAnalysis(L=L_train, lfs=lfs).lf_summary()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nj\nPolarity\nCoverage\nOverlaps\nConflicts\n\n\n\n\nkeyword_my\n0\n[1]\n0.198613\n0.172131\n0.044136\n\n\nkeyword_subscribe\n1\n[1]\n0.127364\n0.092055\n0.026482\n\n\nkeyword_http\n2\n[1]\n0.119168\n0.083859\n0.051702\n\n\nkeyword_guys\n3\n[1]\n0.056747\n0.055485\n0.005044\n\n\nkeyword_please\n4\n[1]\n0.112232\n0.104666\n0.023960\n\n\nkeyword_song\n5\n[0]\n0.141866\n0.071879\n0.045397\n\n\nregex_check_out\n6\n[1]\n0.233922\n0.116015\n0.022068\n\n\nshort_comment\n7\n[0]\n0.225725\n0.100883\n0.074401\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nOur goal is now to convert the labels from our LFs into a single noise-aware probabilistic (or confidence-weighted) label per data point. A simple baseline for doing this is to take the majority vote on a per-data point basis: if more LFs voted SPAM than HAM, label it SPAM (and vice versa). We can test this with the MajorityLabelVoter baseline model.\n\n\nCode\nmajority_model = MajorityLabelVoter()\npreds_train = majority_model.predict(L=L_train)\n\n\nThe LFs may have varying properties and should not be treated identically. In addition to having varied accuracies and coverages, LFs may be correlated, resulting in certain signals being overrepresented in a majority-vote-based model.\nTo handle these issues appropriately, we will instead use a more sophisticated Snorkel LabelModel to combine the outputs of the LFs. This model will ultimately produce a single set of noise-aware training labels, which are probabilistic or confidence-weighted labels. We will then use these labels to train a classifier for our task.\nNote that no gold labels are used during the training process. The only information we need is the label matrix, which contains the output of the LFs on our training set. The LabelModel is able to learn weights for the labeling functions using only the label matrix as input. We also specify the cardinality, or number of classes.\n\n\nCode\nlabel_model = LabelModel(cardinality=2, verbose=True)\nlabel_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123)\n\n\n100%|██████████| 500/500 [00:00&lt;00:00, 1051.93epoch/s]\n\n\n\n\nCode\nmajority_acc = majority_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n    \"accuracy\"\n]\nprint(f\"{'Majority Vote Accuracy:':&lt;25} {majority_acc * 100:.1f}%\")\n\nlabel_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n    \"accuracy\"\n]\nprint(f\"{'Label Model Accuracy:':&lt;25} {label_model_acc * 100:.1f}%\")\n\n\nMajority Vote Accuracy:   86.4%\nLabel Model Accuracy:     84.8%\n\n\nThe majority vote model or more sophisticated LabelModel could in principle be used directly as a classifier if the outputs of our labeling functions were made available at test time. However, these models (i.e. these re-weighted combinations of our labeling function’s votes) will abstain on the data points that our labeling functions don’t cover.\nWe will instead use the outputs of the LabelModel as training labels to train a discriminative classifier which can generalize beyond the labeling function outputs to see if we can improve performance further. This classifier will also only need the text of the comment to make predictions, making it much more suitable for inference over unseen comments.\nLet’s briefly confirm that the labels the LabelModel produces are indeed probabilistic in nature. The following histogram shows the confidences we have that each data point has the label SPAM. The points we are least certain about will have labels close to 0.5.\n\n\nCode\ndef plot_probabilities_histogram(Y):\n    plt.hist(Y, bins=10)\n    plt.xlabel(\"Probability of SPAM\")\n    plt.ylabel(\"Number of data points\")\n    plt.show()\n\n\nprobs_train = label_model.predict_proba(L=L_train)\nplot_probabilities_histogram(probs_train[:, SPAM])\n\n\n\n\n\nAs we saw earlier, some of the data points in our train set received no labels from any of our LFs. These data points convey no supervision signal and tend to hurt performance, so we filter them out before training using a built-in utility.\n\n\nCode\ndf_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n    X=df_train, y=probs_train, L=L_train\n)\n\n\n\n\n3.6.3 Training a Classifier\nIn this final section of the tutorial, we’ll use the probabilistic training labels we generated in the last section to train a classifier for our task. The output of the Snorkel LabelModel is just a set of labels which can be used with most popular libraries for performing supervised learning. Note that typically, Snorkel is used (and really shines!) with much more complex, training data-hungry models, but we will use Logistic Regression here for simplicity of exposition.\nFor simplicity and speed, we use a simple “bag of n-grams” feature representation: each data point is represented by a one-hot vector marking which words or 2-word combinations are present in the comment text.\n\n\nCode\nvectorizer = CountVectorizer(ngram_range=(1, 5))\nX_train = vectorizer.fit_transform(df_train_filtered.text.tolist())\nX_test = vectorizer.transform(df_test.text.tolist())\n\n\nIf we want to use a library or model that doesn’t accept probabilistic labels (such as Scikit-Learn), we can instead replace each label distribution with the label of the class that has the maximum probability. This can easily be done using the probs_to_preds helper method. We do note, however, that this transformation is lossy, as we no longer have values for our confidence in each label.\n\n\nCode\npreds_train_filtered = probs_to_preds(probs=probs_train_filtered)\n\n\nWe then use these labels to train a classifier as usual.\n\n\nCode\nsklearn_model = LogisticRegression(C=1e2, solver=\"liblinear\")\nsklearn_model.fit(X=X_train, y=preds_train_filtered)\n\n\nLogisticRegression(C=100.0, solver='liblinear')\n\n\n\n\nCode\nprint(f\"Test Accuracy: {sklearn_model.score(X=X_test, y=Y_test) * 100:.1f}%\")\n\n\nTest Accuracy: 91.2%\n\n\nWe observe an additional boost in accuracy over the LabelModel by multiple points! This is in part because the discriminative model generalizes beyond the labeling function’s labels and makes good predictions on all data points, not just the ones covered by labeling functions. By using the label model to transfer the domain knowledge encoded in our LFs to the discriminative model, we were able to generalize beyond the noisy labeling heuristics."
  },
  {
    "objectID": "02_Dataset.html#data-augmentation-using-albumentations",
    "href": "02_Dataset.html#data-augmentation-using-albumentations",
    "title": "3  Framing the problem and constructing the dataset",
    "section": "3.7 Data augmentation using Albumentations",
    "text": "3.7 Data augmentation using Albumentations\nScikit-image reads an image in RGB format which is consistent with Albumentations.\n\n\nCode\n!wget https://raw.githubusercontent.com/albumentations-team/albumentations_examples/master/notebooks/images/image_3.jpg\n\n\n--2023-02-20 03:54:21--  https://raw.githubusercontent.com/albumentations-team/albumentations_examples/master/notebooks/images/image_3.jpg\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 58095 (57K) [image/jpeg]\nSaving to: ‘image_3.jpg’\n\nimage_3.jpg         100%[===================&gt;]  56.73K  --.-KB/s    in 0.007s  \n\n2023-02-20 03:54:21 (7.89 MB/s) - ‘image_3.jpg’ saved [58095/58095]\n\n\n\n\n\nCode\nimage = io.imread('image_3.jpg')\nvisualize(image)\n\n\n\n\n\nWe fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, you shouldn’t fix the random seed before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time\n\n\nCode\n# Define a single augmentation, pass the image to it and receive the augmented image\ntransform = A.HorizontalFlip(p=0.5)\nrandom.seed(7)\naugmented_image = transform(image=image)['image']\nvisualize(augmented_image)\n\n\n\n\n\n\n\nCode\ntransform = A.ShiftScaleRotate(p=0.5)\nrandom.seed(7) \naugmented_image = transform(image=image)['image']\nvisualize(augmented_image)\n\n\n\n\n\n\n\nCode\n#Define an augmentation pipeline using Compose, pass the image to it and receive the augmented imag\ntransform = A.Compose([\n    A.CLAHE(),\n    A.RandomRotate90(),\n    A.Transpose(),\n    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n    A.Blur(blur_limit=3),\n    A.OpticalDistortion(),\n    A.GridDistortion(),\n    A.HueSaturationValue(),\n])\nrandom.seed(42) \naugmented_image = transform(image=image)['image']\nvisualize(augmented_image)\n\n\n\n\n\n\n\nCode\nio.imsave('augmented_image.jpg', augmented_image)\n\n\n\n\nCode\nimage = io.imread('augmented_image.jpg')\nvisualize(image)\n\n\n\n\n\nFor more information, please refer to https://albumentations.ai/docs/#examples."
  },
  {
    "objectID": "02_Dataset.html#reference",
    "href": "02_Dataset.html#reference",
    "title": "3  Framing the problem and constructing the dataset",
    "section": "3.8 Reference",
    "text": "3.8 Reference\nThis notebook contains the sample from\n\nhttps://github.com/dennisbakhuis/pigeonXT/blob/master/pigeonXT_Examples.ipynb\nhttps://docs.cleanlab.ai/stable/tutorials/multiannotator.html\nhttps://modal-python.readthedocs.io/en/latest/content/examples/interactive_labeling.html_\nhttps://modal-python.readthedocs.io/en/latest/content/examples/pool-based_sampling.html\nhttps://www.snorkel.org/use-cases/01-spam-tutorial\nhttps://albumentations.ai/docs/#examples"
  },
  {
    "objectID": "03_Relational_Database_and_data_wrangling.html#queringing-data-with-bigquery",
    "href": "03_Relational_Database_and_data_wrangling.html#queringing-data-with-bigquery",
    "title": "4  Relational Database and data wrangling",
    "section": "4.1 Queringing data with BigQuery",
    "text": "4.1 Queringing data with BigQuery\n\n\nCode\n# For SQL\nfrom google.cloud import bigquery\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n\nStructured Query Language, or SQL, is the programming language used with databases, and it is an important skill for any data scientist. In this example, you’ll build your SQL skills using BigQuery, a web service work as database management system that lets you apply SQL to huge datasets.\n\n4.1.1 Preliminaries for google colab (optional)\nWe want to start exploring the Google BiqQuery public datasets. Let’s start by walking through the required setup steps, and then we can load and explore some data.\nIf you are using colab. Follow this quickstart guide, which will explain how to: 1. Create a Cloud Platform project if you don’t have one already. 2. Enable billing for the project (If you apply the free trial, you already satisfy this condition.) 3. Enable the BigQuery API 4. Enabling the Service account\nNow we need to authenticate to gain access to the BigQuery API. We will create a client, specifying the service account key file (replace ‘lunar-pact-378812-7a28b789bde2.json’ with your key file).\n\n\nCode\n# @title Setup\nfrom google.oauth2 import service_account\n\n# TODO(developer): Set key_path to the path to the service account key file.\n\nkey_path = \"lunar-pact-378812-7a28b789bde2.json\"\n\ncredentials = service_account.Credentials.from_service_account_file(\n    key_path\n)\n\n\n\n\nCode\n# @title Alternative Setup\n#from google.colab import auth\n#from google.cloud import bigquery\n#from google.colab import data_table\n\n#project = 'lunar-pact-378812' # Project ID inserted based on the query results selected to explore\n#location = 'US' # Location inserted based on the query results selected to explore\n#client = bigquery.Client(project=project, location=location)\n#data_table.enable_dataframe_formatter()\n#auth.authenticate_user()\n\n\nNow that we’re authenticated, we need to load the BigQuery package, and the google.colab.data_table package that can be used to display large pandas dataframes as an interactive data. Loading data_table is optional, but it will be useful for working with data in pandas.\n\n\nCode\n#%load_ext google.cloud.bigquery\n#%load_ext google.colab.data_table\n\n\n\n\nCode\nclient = bigquery.Client(credentials=credentials, project=credentials.project_id,)\n\n\n\n\n4.1.2 Create the reference\nYou can also work with Kaggle, which provide bigquery integration that you do not need to setup a google account. Each Kaggle user can scan 5TB every 30 days for free. Once you hit that limit, you’ll have to wait for it to reset. See https://www.kaggle.com/product-feedback/48573 for more details.\nThe first step in the workflow is to create a Client object. As you’ll soon see, this Client object will play a central role in retrieving information from BigQuery datasets.\n\n\nCode\n# Uncomment below and create a \"Client\" object if you are using Kaggle\n# client = bigquery.Client()\n\n\nWe’ll work with a dataset of posts on Hacker News, a website focusing on computer science and cybersecurity news. In BigQuery, each dataset is contained in a corresponding project. In this case, our hacker_news dataset is contained in the bigquery-public-data project.\nTo access the dataset, We begin by constructing a reference to the dataset with the dataset() method. Next, we use the get_dataset() method, along with the reference we just constructed, to fetch the dataset.\nSee the full list of public datasets or the kaggle bigquery dataset if you want to explore others.\n\n\nCode\n# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n\nEvery dataset is just a collection of tables. You can think of a dataset as a spreadsheet file containing multiple tables, all composed of rows and columns.We use the list_tables() method to list the tables in the dataset.\n\n\nCode\n# List all the tables in the \"hacker_news\" dataset\ntables = list(client.list_tables(dataset))\n\n# Print names of all tables in the dataset (there are four!)\nfor table in tables:  \n    print(table.table_id)\n\n\ncomments\nfull\nfull_201510\nstories\n\n\nSimilar to how we fetched a dataset, we can fetch a table. In the code cell below, we fetch the full table in the hacker_news dataset\n\n\nCode\n# Construct a reference to the \"full\" table\ntable_ref = dataset_ref.table(\"full\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n\nIn the next section, you’ll explore the contents of this table in more detail. For now, take the time to use the image below to consolidate what you’ve learned so far.\n\n\n\nfirst_commands\n\n\n\n\n4.1.3 Table schema\nThe structure of a table is called its schema. We need to understand a table’s schema to effectively pull out the data we want.\nIn this example, we’ll investigate the full table that we fetched above.\n\n\nCode\n# Print information on all the columns in the \"full\" table in the \"hacker_news\" dataset\ntable.schema\n\n\n[SchemaField('title', 'STRING', 'NULLABLE', None, 'Story title', (), None),\n SchemaField('url', 'STRING', 'NULLABLE', None, 'Story url', (), None),\n SchemaField('text', 'STRING', 'NULLABLE', None, 'Story or comment text', (), None),\n SchemaField('dead', 'BOOLEAN', 'NULLABLE', None, 'Is dead?', (), None),\n SchemaField('by', 'STRING', 'NULLABLE', None, \"The username of the item's author.\", (), None),\n SchemaField('score', 'INTEGER', 'NULLABLE', None, 'Story score', (), None),\n SchemaField('time', 'INTEGER', 'NULLABLE', None, 'Unix time', (), None),\n SchemaField('timestamp', 'TIMESTAMP', 'NULLABLE', None, 'Timestamp for the unix time', (), None),\n SchemaField('type', 'STRING', 'NULLABLE', None, 'Type of details (comment, comment_ranking, poll, story, job, pollopt)', (), None),\n SchemaField('id', 'INTEGER', 'NULLABLE', None, \"The item's unique id.\", (), None),\n SchemaField('parent', 'INTEGER', 'NULLABLE', None, 'Parent comment ID', (), None),\n SchemaField('descendants', 'INTEGER', 'NULLABLE', None, 'Number of story or poll descendants', (), None),\n SchemaField('ranking', 'INTEGER', 'NULLABLE', None, 'Comment ranking', (), None),\n SchemaField('deleted', 'BOOLEAN', 'NULLABLE', None, 'Is deleted?', (), None)]\n\n\nEach SchemaField tells us about a specific column (which we also refer to as a field). In order, the information is:\n\nThe name of the column\nThe field type (or datatype) in the column\nThe mode of the column ('NULLABLE' means that a column allows NULL values, and is the default)\nA description of the data in that column\n\nFor instance, the field has the SchemaField:\nSchemaField('by', 'string', 'NULLABLE', \"The username of the item's author.\",())\nThis tells us: - the field (or column) is called by, - the data in this field is strings, - NULL values are allowed, and - it contains the usernames corresponding to each item’s author.\nWe can use the list_rows() method to check just the first five lines of of the full table to make sure this is right. This returns a BigQuery RowIterator object that can quickly be converted to a pandas DataFrame with the to_dataframe() method.\n\n\nCode\n# Preview the first five lines of the \"full\" table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n\n  \n    \n      \n\n\n\n\n\n\ntitle\nurl\ntext\ndead\nby\nscore\ntime\ntimestamp\ntype\nid\nparent\ndescendants\nranking\ndeleted\n\n\n\n\n0\nNone\nNone\nI would rather just have wired earbuds, period...\n&lt;NA&gt;\nzeveb\n&lt;NA&gt;\n1591717736\n2020-06-09 15:48:56+00:00\ncomment\n23467666\n23456782\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n1\nNone\nNone\nDNS?\n&lt;NA&gt;\nnly\n&lt;NA&gt;\n1572810465\n2019-11-03 19:47:45+00:00\ncomment\n21436112\n21435130\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n2\nNone\nNone\nThese benchmarks seem pretty good. Filterable...\n&lt;NA&gt;\nmrkeen\n&lt;NA&gt;\n1591717727\n2020-06-09 15:48:47+00:00\ncomment\n23467665\n23467426\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n3\nNone\nNone\nOh really?&lt;p&gt;* Excel alone uses 86.1MB of priv...\n&lt;NA&gt;\noceanswave\n&lt;NA&gt;\n1462987532\n2016-05-11 17:25:32+00:00\ncomment\n11677248\n11676886\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n4\nNone\nNone\nThese systems are useless. Of the many flaws:...\n&lt;NA&gt;\nnyxxie\n&lt;NA&gt;\n1572810473\n2019-11-03 19:47:53+00:00\ncomment\n21436113\n21435025\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe list_rows() method will also let us look at just the information in a specific column. If we want to see the first five entries in the by column, for example, we can do that!\n\n\nCode\n# Preview the first five entries in the \"by\" column of the \"full\" table\nclient.list_rows(table, selected_fields=table.schema[4:5], max_results=5).to_dataframe()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nby\n\n\n\n\n0\nzeveb\n\n\n1\nnly\n\n\n2\nmrkeen\n\n\n3\noceanswave\n\n\n4\nnyxxie\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4.1.4 Select, From & Where\nNow that you know how to access and examine a dataset, you’re ready to write your first SQL query! As you’ll soon see, SQL queries will help you sort through a massive dataset, to retrieve only the information that you need. We’ll begin by using the keywords SELECT, FROM, and WHERE to get data from specific columns based on conditions you specify.\nWe’ll use an OpenAQ dataset about air quality. First, we’ll set up everything we need to run queries and take a quick peek at what tables are in our database.\n\n\nCode\n# Construct a reference to the \"openaq\" dataset\ndataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# List all the tables in the \"openaq\" dataset\ntables = list(client.list_tables(dataset))\n\n# Print names of all tables in the dataset (there's only one!)\nfor table in tables:  \n    print(table.table_id)\n\n\nglobal_air_quality\n\n\nThe dataset contains only one table, called global_air_quality. We’ll fetch the table and take a peek at the first few rows to see what sort of data it contains.\n\n\nCode\n# Construct a reference to the \"global_air_quality\" table\ntable_ref = dataset_ref.table(\"global_air_quality\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"global_air_quality\" table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlocation\ncity\ncountry\npollutant\nvalue\ntimestamp\nunit\nsource_name\nlatitude\nlongitude\naveraged_over_in_hours\nlocation_geom\n\n\n\n\n0\nBorówiec, ul. Drapałka\nBorówiec\nPL\nbc\n0.85217\n2022-04-28 07:00:00+00:00\nµg/m³\nGIOS\n1.0\n52.276794\n17.074114\nPOINT(52.276794 1)\n\n\n1\nKraków, ul. Bulwarowa\nKraków\nPL\nbc\n0.91284\n2022-04-27 23:00:00+00:00\nµg/m³\nGIOS\n1.0\n50.069308\n20.053492\nPOINT(50.069308 1)\n\n\n2\nPłock, ul. Reja\nPłock\nPL\nbc\n1.41000\n2022-03-30 04:00:00+00:00\nµg/m³\nGIOS\n1.0\n52.550938\n19.709791\nPOINT(52.550938 1)\n\n\n3\nElbląg, ul. Bażyńskiego\nElbląg\nPL\nbc\n0.33607\n2022-05-03 13:00:00+00:00\nµg/m³\nGIOS\n1.0\n54.167847\n19.410942\nPOINT(54.167847 1)\n\n\n4\nPiastów, ul. Pułaskiego\nPiastów\nPL\nbc\n0.51000\n2022-05-11 05:00:00+00:00\nµg/m³\nGIOS\n1.0\n52.191728\n20.837489\nPOINT(52.191728 1)\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nlet’s put together a query. Say we want to select all the values from the city column that are in rows where the country column is 'US' (for “United States”).\n\n\nCode\n# Query to select all the items from the \"city\" column where the \"country\" column is 'US'\n# SQL is almost completely case and indentation insensitive. The capitalization and\n# indentation style here is preferred style.\nquery = \"\"\"\n        SELECT city\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country = 'US'\n        \"\"\"\n\n\nNotice also that SQL statements requires single quotes for its strings inside python string (we use triple quotation mark here). We begin by setting up the query with the query() method.\n\n\nCode\n# Set up the query\nquery_job = client.query(query)\n\n# API request - run the query, and return a pandas DataFrame\nus_cities = query_job.to_dataframe()\n\n\nNow we’ve got a pandas DataFrame called us_cities, which we can use like any other DataFrame.\n\n\nCode\n# What five cities have the most measurements?\nus_cities.city.value_counts().head()\n\n\nPhoenix-Mesa-Scottsdale                     39414\nLos Angeles-Long Beach-Santa Ana            27479\nRiverside-San Bernardino-Ontario            26887\nNew York-Northern New Jersey-Long Island    25417\nSan Francisco-Oakland-Fremont               22710\nName: city, dtype: int64\n\n\nIf you want multiple columns, you can select them with a comma between the names:\n\n\nCode\nquery = \"\"\"\n        SELECT city, country\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country = 'US'\n        \"\"\"\n\n\nYou can select all columns with a * like this:\n\n\nCode\nquery = \"\"\"\n        SELECT *\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country = 'US'\n        \"\"\"\n\n\n\n\n4.1.5 Querying big dataset\nYou can estimate the size of any query before running it. Here is an example using the Hacker News dataset. To see how much data a query will scan, we create a QueryJobConfig object and set the dry_run parameter to True.\n\n\nCode\n# Query to get the score column from every row where the type column has value \"job\"\nquery = \"\"\"\n        SELECT score, title\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE type = \"job\" \n        \"\"\"\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(query, job_config=dry_run_config)\n\nprint(\"This query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed))\n\n\nThis query will process 553320240 bytes.\n\n\nYou can also specify a parameter when running the query to limit how much data you are willing to scan. Here’s an example with a low limit.\n\n\nCode\n# Only run the query if it's less than 1 MB\nONE_MB = 1000*1000\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_MB)\n\n# Set up the query (will only run if it's less than 1 MB)\nsafe_query_job = client.query(query, job_config=safe_config)\n\n# API request - try to run the query, and return a pandas DataFrame\nsafe_query_job.to_dataframe()\n\n\nInternalServerError: ignored\n\n\nIn this case, the query was cancelled, because the limit of 1 MB was exceeded. However, we can also increase the limit to run the query successfully!\n\n\n4.1.6 Group By, Having & Count\nNow that you can select raw data, you’re ready to learn how to group your data and count things within those groups.\nThe Hacker News dataset contains information on stories and comments from the Hacker News social networking site. We’ll work with the comments table and begin by printing the first few rows\n\n\nCode\n# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"comments\" table\ntable_ref = dataset_ref.table(\"comments\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"comments\" table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nid\nby\nauthor\ntime\ntime_ts\ntext\nparent\ndeleted\ndead\nranking\n\n\n\n\n0\n9734136\nNone\nNone\n1434565400\n2015-06-17 18:23:20+00:00\nNone\n9733698\nTrue\n&lt;NA&gt;\n0\n\n\n1\n4921158\nNone\nNone\n1355496966\n2012-12-14 14:56:06+00:00\nNone\n4921100\nTrue\n&lt;NA&gt;\n0\n\n\n2\n7500568\nNone\nNone\n1396261158\n2014-03-31 10:19:18+00:00\nNone\n7499385\nTrue\n&lt;NA&gt;\n0\n\n\n3\n8909635\nNone\nNone\n1421627275\n2015-01-19 00:27:55+00:00\nNone\n8901135\nTrue\n&lt;NA&gt;\n0\n\n\n4\n9256463\nNone\nNone\n1427204705\n2015-03-24 13:45:05+00:00\nNone\n9256346\nTrue\n&lt;NA&gt;\n0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nLet’s use the table to see which comments generated the most replies. Since: - the parent column indicates the comment that was replied to, and - the id column has the unique ID used to identify each comment,\nwe can GROUP BY the parent column and COUNT() the id column in order to figure out the number of comments that were made as responses to a specific comment.\nFurthermore, since we’re only interested in popular comments, we’ll look at comments with more than ten replies. So, we’ll only return groups HAVING more than ten ID’s.\n\n\nCode\n# Query to select comments that received more than 10 replies\nquery_popular = \"\"\"\n                SELECT parent, COUNT(id)\n                FROM `bigquery-public-data.hacker_news.comments`\n                GROUP BY parent\n                HAVING COUNT(id) &gt; 10\n                \"\"\"\n\n\n\n\nCode\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_popular, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\npopular_comments = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\npopular_comments.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nparent\nf0_\n\n\n\n\n0\n2385424\n55\n\n\n1\n8441979\n57\n\n\n2\n7634152\n46\n\n\n3\n9062758\n49\n\n\n4\n5694173\n61\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\npopular_comments\n\n\n\n  \n    \n      \n\n\n\n\n\n\nparent\nf0_\n\n\n\n\n0\n2385424\n55\n\n\n1\n8441979\n57\n\n\n2\n7634152\n46\n\n\n3\n9062758\n49\n\n\n4\n5694173\n61\n\n\n...\n...\n...\n\n\n77363\n1748827\n37\n\n\n77364\n3657756\n37\n\n\n77365\n2873865\n37\n\n\n77366\n9395540\n37\n\n\n77367\n1772903\n37\n\n\n\n\n\n77368 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nEach row in the popular_comments DataFrame corresponds to a comment that received more than ten replies.\nA couple hints to make your queries even better: - The column resulting from COUNT(id) was called f0__. That’s not a very descriptive name. You can change the name by adding AS NumPosts after you specify the aggregation. This is called aliasing. - If you are ever unsure what to put inside the COUNT() function, you can do COUNT(1) to count the rows in each group. Most people find it especially readable, because we know it’s not focusing on other columns. It also scans less data than if supplied column names (making it faster and using less of your data access quota).\nUsing these tricks, we can rewrite our query:\n\n\nCode\n# Improved version of earlier query, now with aliasing & improved readability\nquery_improved = \"\"\"\n                 SELECT parent, COUNT(1) AS NumPosts\n                 FROM `bigquery-public-data.hacker_news.comments`\n                 GROUP BY parent\n                 HAVING COUNT(1) &gt; 10\n                 \"\"\"\n\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_improved, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nimproved_df = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\nimproved_df.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nparent\nNumPosts\n\n\n\n\n0\n2970550\n63\n\n\n1\n8254532\n40\n\n\n2\n7687784\n44\n\n\n3\n4101992\n53\n\n\n4\n1632878\n39\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nimproved_df\n\n\n\n  \n    \n      \n\n\n\n\n\n\nparent\nNumPosts\n\n\n\n\n0\n2970550\n63\n\n\n1\n8254532\n40\n\n\n2\n7687784\n44\n\n\n3\n4101992\n53\n\n\n4\n1632878\n39\n\n\n...\n...\n...\n\n\n77363\n7844298\n37\n\n\n77364\n7864644\n37\n\n\n77365\n2866332\n37\n\n\n77366\n1885594\n37\n\n\n77367\n2175321\n37\n\n\n\n\n\n77368 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNow you have the data you want, and it has descriptive names.\n\n4.1.6.1 Note on using GROUP BY\nNote that because it tells SQL how to apply aggregate functions (like COUNT()), it doesn’t make sense to use GROUP BY without an aggregate function. Similarly, if you have any GROUP BY clause, then all variables must be passed to either a 1. GROUP BY command, or 2. an aggregation function.\nConsider the query below:\n\n\nCode\nquery_good = \"\"\"\n             SELECT parent, COUNT(id)\n             FROM `bigquery-public-data.hacker_news.comments`\n             GROUP BY parent\n             \"\"\"\n\n\nNote that there are two variables: parent and id. - parent was passed to a GROUP BY command (in GROUP BY parent), and - id was passed to an aggregate function (in COUNT(id)).\nAnd the query below won’t work, because the author column isn’t passed to an aggregate function or a GROUP BY clause:\n\n\nCode\nquery_bad = \"\"\"\n            SELECT author, parent, COUNT(id)\n            FROM `bigquery-public-data.hacker_news.comments`\n            GROUP BY parent\n            \"\"\"\n\n\n\n\nCode\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_bad, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nimproved_df = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\nimproved_df.head()\n\n\nBadRequest: ignored\n\n\n\n\n\n4.1.7 Order By\nFrequently, you’ll want to sort your results. Let’s use the US Traffic Fatality Records database, which contains information on traffic accidents in the US where at least one person died.\nWe’ll investigate the accident_2015 table. Here is a view of the first few rows.\n\n\nCode\n# Construct a reference to the \"nhtsa_traffic_fatalities\" dataset\ndataset_ref = client.dataset(\"nhtsa_traffic_fatalities\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"accident_2015\" table\ntable_ref = dataset_ref.table(\"accident_2015\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"accident_2015\" table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\nLet’s use the table to determine how the number of accidents varies with the day of the week. Since: - the consecutive_number column contains a unique ID for each accident, and - the timestamp_of_crash column contains the date of the accident in DATETIME format,\nwe can: - EXTRACT the day of the week (as day_of_week in the query below) from the timestamp_of_crash column, and - GROUP BY the day of the week, before we COUNT the consecutive_number column to determine the number of accidents for each day of the week.\nThen we sort the table with an ORDER BY clause, so the days with the most accidents are returned first.\n\n\nCode\n# Query to find out the number of accidents for each day of the week\nquery = \"\"\"\n        SELECT COUNT(consecutive_number) AS num_accidents, \n               EXTRACT(DAYOFWEEK FROM timestamp_of_crash) AS day_of_week\n        FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n        GROUP BY day_of_week\n        ORDER BY num_accidents DESC\n        \"\"\"\n\n\n\n\nCode\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 1 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**9)\nquery_job = client.query(query, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\naccidents_by_day = query_job.to_dataframe()\n\n# Print the DataFrame\naccidents_by_day\n\n\n\n  \n    \n      \n\n\n\n\n\n\nnum_accidents\nday_of_week\n\n\n\n\n0\n5659\n7\n\n\n1\n5298\n1\n\n\n2\n4916\n6\n\n\n3\n4460\n5\n\n\n4\n4182\n4\n\n\n5\n4038\n2\n\n\n6\n3985\n3\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNotice that the data is sorted by the num_accidents column, where the days with more traffic accidents appear first.\nTo map the numbers returned for the day_of_week column to the actual day, you might consult the BigQuery documentation on the DAYOFWEEK function. It says that it returns “an integer between 1 (Sunday) and 7 (Saturday), inclusively”. So, in 2015, most fatal motor accidents in the US occured on Sunday and Saturday, while the fewest happened on Tuesday.\n\n\n4.1.8 As and With\nOn its own, AS is a convenient way to clean up the data returned by your query. We’re going to use a common table expression (CTE) to find out how many Bitcoin transactions were made each day for the entire timespan of a bitcoin transaction dataset.\nWe’ll investigate the transactions table. Here is a view of the first few rows.\n\n\nCode\n# Construct a reference to the \"crypto_bitcoin\" dataset\ndataset_ref = client.dataset(\"crypto_bitcoin\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"transactions\" table\ntable_ref = dataset_ref.table(\"transactions\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"transactions\" table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nhash\nsize\nvirtual_size\nversion\nlock_time\nblock_hash\nblock_number\nblock_timestamp\nblock_timestamp_month\ninput_count\noutput_count\ninput_value\noutput_value\nis_coinbase\nfee\ninputs\noutputs\n\n\n\n\n0\na16f3ce4dd5deb92d98ef5cf8afeaf0775ebca408f708b...\n275\n275\n1\n0\n00000000dc55860c8a29c58d45209318fa9e9dc2c1833a...\n181\n2009-01-12 06:02:13+00:00\n2009-01-01\n1\n2\n4000000000.000000000\n4000000000.000000000\nFalse\n0E-9\n[{'index': 0, 'spent_transaction_hash': 'f4184...\n[{'index': 0, 'script_asm': '04b5abd412d4341b4...\n\n\n1\n591e91f809d716912ca1d4a9295e70c3e78bab077683f7...\n275\n275\n1\n0\n0000000054487811fc4ff7a95be738aa5ad9320c394c48...\n182\n2009-01-12 06:12:16+00:00\n2009-01-01\n1\n2\n3000000000.000000000\n3000000000.000000000\nFalse\n0E-9\n[{'index': 0, 'spent_transaction_hash': 'a16f3...\n[{'index': 0, 'script_asm': '0401518fa1d1e1e3e...\n\n\n2\n12b5633bad1f9c167d523ad1aa1947b2732a865bf5414e...\n276\n276\n1\n0\n00000000f46e513f038baf6f2d9a95b2a28d8a6c985bcf...\n183\n2009-01-12 06:34:22+00:00\n2009-01-01\n1\n2\n2900000000.000000000\n2900000000.000000000\nFalse\n0E-9\n[{'index': 0, 'spent_transaction_hash': '591e9...\n[{'index': 0, 'script_asm': '04baa9d3665315562...\n\n\n3\n828ef3b079f9c23829c56fe86e85b4a69d9e06e5b54ea5...\n276\n276\n1\n0\n00000000fb5b44edc7a1aa105075564a179d65506e2bd2...\n248\n2009-01-12 20:04:20+00:00\n2009-01-01\n1\n2\n2800000000.000000000\n2800000000.000000000\nFalse\n0E-9\n[{'index': 0, 'spent_transaction_hash': '12b56...\n[{'index': 0, 'script_asm': '04bed827d37474bef...\n\n\n4\n35288d269cee1941eaebb2ea85e32b42cdb2b04284a56d...\n277\n277\n1\n0\n00000000689051c09ff2cd091cc4c22c10b965eb8db3ad...\n545\n2009-01-15 05:48:32+00:00\n2009-01-01\n1\n2\n2500000000.000000000\n2500000000.000000000\nFalse\n0E-9\n[{'index': 0, 'spent_transaction_hash': 'd71fd...\n[{'index': 0, 'script_asm': '044a656f065871a35...\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSince the block_timestamp column contains the date of each transaction in DATETIME format, we’ll convert these into DATE format using the DATE() command.\nWe do that using a CTE, and then the next part of the query counts the number of transactions for each date and sorts the table so that earlier dates appear first.\n\n\nCode\n# Query to select the number of transactions per date, sorted by date\nquery_with_CTE = \"\"\" \n                 WITH time AS \n                 (\n                     SELECT DATE(block_timestamp) AS trans_date\n                     FROM `bigquery-public-data.crypto_bitcoin.transactions`\n                 )\n                 SELECT COUNT(1) AS transactions,\n                        trans_date\n                 FROM time\n                 GROUP BY trans_date\n                 ORDER BY trans_date\n                 \"\"\"\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_with_CTE, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\ntransactions_by_date = query_job.to_dataframe()\n\n# Print the first five rows\ntransactions_by_date.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\ntransactions\ntrans_date\n\n\n\n\n0\n1\n2009-01-03\n\n\n1\n14\n2009-01-09\n\n\n2\n61\n2009-01-10\n\n\n3\n93\n2009-01-11\n\n\n4\n101\n2009-01-12\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSince they’re returned sorted, we can easily plot the raw results to show us the number of Bitcoin transactions per day over the whole timespan of this dataset.\n\n\nCode\ntransactions_by_date.set_index('trans_date').plot()\n\n\n&lt;AxesSubplot:xlabel='trans_date'&gt;\n\n\n\n\n\nAs you can see, common table expressions (CTEs) let you shift a lot of your data cleaning into SQL. That’s an especially good thing in the case of BigQuery, because it is vastly faster than doing the work in Pandas.\n\n\n4.1.9 Joining data\nWhen our data lives across different tables, how do we analyze it? By JOINing the tables together. A JOIN combines rows in the left table with corresponding rows in the right table, where the meaning of “corresponding” is based on how we specify the join.\nGitHub is the most popular place to collaborate on software projects. A GitHub repository (or repo) is a collection of files associated with a specific project. Most repos on GitHub are shared under a specific legal license, which determines the legal restrictions on how they are used. For our example, we’re going to look at how many different files have been released under each license.\nWe’ll work with two tables in the database. The first table is the licenses table, which provides the name of each GitHub repo (in the repo_name column) and its corresponding license. Here’s a view of the first five rows.\n\n\nCode\n# Construct a reference to the \"github_repos\" dataset\ndataset_ref = client.dataset(\"github_repos\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"licenses\" table\nlicenses_ref = dataset_ref.table(\"licenses\")\n\n# API request - fetch the table\nlicenses_table = client.get_table(licenses_ref)\n\n# Preview the first five lines of the \"licenses\" table\nclient.list_rows(licenses_table, max_results=5).to_dataframe()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nrepo_name\nlicense\n\n\n\n\n0\nautarch/Dist-Zilla-Plugin-Test-TidyAll\nartistic-2.0\n\n\n1\nthundergnat/Prime-Factor\nartistic-2.0\n\n\n2\nkusha-b-k/Turabian_Engin_Fan\nartistic-2.0\n\n\n3\nonlinepremiumoutlet/onlinepremiumoutlet.github.io\nartistic-2.0\n\n\n4\nhuangyuanlove/LiaoBa_Service\nartistic-2.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe second table is the sample_files table, which provides, among other information, the GitHub repo that each file belongs to (in the repo_name column). The first several rows of this table are printed below.\n\n\nCode\n# Construct a reference to the \"sample_files\" table\nfiles_ref = dataset_ref.table(\"sample_files\")\n\n# API request - fetch the table\nfiles_table = client.get_table(files_ref)\n\n# Preview the first five lines of the \"sample_files\" table\nclient.list_rows(files_table, max_results=5).to_dataframe()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nrepo_name\nref\npath\nmode\nid\nsymlink_target\n\n\n\n\n0\nEOL/eol\nrefs/heads/master\ngenerate/vendor/railties\n40960\n0338c33fb3fda57db9e812ac7de969317cad4959\n/usr/share/rails-ruby1.8/railties\n\n\n1\nnp/ling\nrefs/heads/master\ntests/success/merger_seq_inferred.t/merger_seq...\n40960\ndd4bb3d5ecabe5044d3fa5a36e0a9bf7ca878209\n../../../fixtures/all/merger_seq_inferred.ll\n\n\n2\nnp/ling\nrefs/heads/master\nfixtures/sequence/lettype.ll\n40960\n8fdf536def2633116d65b92b3b9257bcf06e3e45\n../all/lettype.ll\n\n\n3\nnp/ling\nrefs/heads/master\nfixtures/failure/wrong_order_seq3.ll\n40960\nc2509ae1196c4bb79d7e60a3d679488ca4a753e9\n../all/wrong_order_seq3.ll\n\n\n4\nnp/ling\nrefs/heads/master\nissues/sequence/keep.t\n40960\n5721de3488fb32745dfc11ec482e5dd0331fecaf\n../keep.t\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNext, we write a query that uses information in both tables to determine how many files are released in each license.\n\n\nCode\n# Query to determine the number of files per license, sorted by number of files\nquery = \"\"\"\n        SELECT L.license, COUNT(1) AS number_of_files\n        FROM `bigquery-public-data.github_repos.sample_files` AS sf\n        INNER JOIN `bigquery-public-data.github_repos.licenses` AS L \n            ON sf.repo_name = L.repo_name\n        GROUP BY L.license\n        ORDER BY number_of_files DESC\n        \"\"\"\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nfile_count_by_license = query_job.to_dataframe()\n\n\nIt’s a big query, and so we’ll investigate each piece separately.\n\nWe’ll begin with the JOIN (highlighted in blue above). This specifies the sources of data and how to join them. We use ON to specify that we combine the tables by matching the values in the repo_name columns in the tables.\nNext, we’ll talk about SELECT and GROUP BY (highlighted in yellow). The GROUP BY breaks the data into a different group for each license, before we COUNT the number of rows in the sample_files table that corresponds to each license. (Remember that you can count the number of rows with COUNT(1).)\nFinally, the ORDER BY (highlighted in purple) sorts the results so that licenses with more files appear first.\nIt was a big query, but it gave us a nice table summarizing how many files have been committed under each license:\n\n\nCode\n# Print the DataFrame\nfile_count_by_license\n\n\n\n  \n    \n      \n\n\n\n\n\n\nlicense\nnumber_of_files\n\n\n\n\n0\nmit\n20560894\n\n\n1\ngpl-2.0\n16608922\n\n\n2\napache-2.0\n7201141\n\n\n3\ngpl-3.0\n5107676\n\n\n4\nbsd-3-clause\n3465437\n\n\n5\nagpl-3.0\n1372100\n\n\n6\nlgpl-2.1\n799664\n\n\n7\nbsd-2-clause\n692357\n\n\n8\nlgpl-3.0\n582277\n\n\n9\nmpl-2.0\n457000\n\n\n10\ncc0-1.0\n449149\n\n\n11\nepl-1.0\n322255\n\n\n12\nunlicense\n208602\n\n\n13\nartistic-2.0\n147391\n\n\n14\nisc\n118332\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThere are a few more types of JOIN, along with how to use UNIONs to pull information from multiple tables. We’ll work with the Hacker News dataset. We begin by reviewing the first several rows of the comments table.\n\n\nCode\n# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"comments\" table\ntable_ref = dataset_ref.table(\"comments\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nid\nby\nauthor\ntime\ntime_ts\ntext\nparent\ndeleted\ndead\nranking\n\n\n\n\n0\n9734136\nNone\nNone\n1434565400\n2015-06-17 18:23:20+00:00\nNone\n9733698\nTrue\n&lt;NA&gt;\n0\n\n\n1\n4921158\nNone\nNone\n1355496966\n2012-12-14 14:56:06+00:00\nNone\n4921100\nTrue\n&lt;NA&gt;\n0\n\n\n2\n7500568\nNone\nNone\n1396261158\n2014-03-31 10:19:18+00:00\nNone\n7499385\nTrue\n&lt;NA&gt;\n0\n\n\n3\n8909635\nNone\nNone\n1421627275\n2015-01-19 00:27:55+00:00\nNone\n8901135\nTrue\n&lt;NA&gt;\n0\n\n\n4\n9256463\nNone\nNone\n1427204705\n2015-03-24 13:45:05+00:00\nNone\n9256346\nTrue\n&lt;NA&gt;\n0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n# Construct a reference to the \"stories\" table\ntable_ref = dataset_ref.table(\"stories\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nid\nby\nscore\ntime\ntime_ts\ntitle\nurl\ntext\ndeleted\ndead\ndescendants\nauthor\n\n\n\n\n0\n6988445\ncflick\n0\n1388454902\n2013-12-31 01:55:02+00:00\nAppshare\nhttp://chadflick.ws/appshare.html\nDid facebook or angrybirds pay you? We will!\n&lt;NA&gt;\nTrue\n&lt;NA&gt;\ncflick\n\n\n1\n7047571\nRd2\n1\n1389562985\n2014-01-12 21:43:05+00:00\nJava in startups\n\nHello, hacker news!&lt;p&gt;Have any of you used jav...\n&lt;NA&gt;\nTrue\n&lt;NA&gt;\nRd2\n\n\n2\n9157712\nmo0\n1\n1425657937\n2015-03-06 16:05:37+00:00\nShow HN: Discover what songs were used in YouT...\nhttp://www.mooma.sh/\nThe user can paste a media url(currently only ...\n&lt;NA&gt;\nTrue\n&lt;NA&gt;\nmo0\n\n\n3\n8127403\nad11\n1\n1407052667\n2014-08-03 07:57:47+00:00\nMy poker project, what do you think?\n\nHi guys, what do you think about my poker proj...\n&lt;NA&gt;\nTrue\n&lt;NA&gt;\nad11\n\n\n4\n6933158\nemyy\n1\n1387432701\n2013-12-19 05:58:21+00:00\nChristmas Crafts Ideas - Easy and Simple Famil...\nhttp://www.winxdvd.com/resource/christmas-craf...\nThere are some free Christmas craft ideas to m...\n&lt;NA&gt;\nTrue\n&lt;NA&gt;\nemyy\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe query below pulls information from the stories and comments tables to create a table showing all stories posted on January 1, 2012, along with the corresponding number of comments. We use a LEFT JOIN so that the results include stories that didn’t receive any comments.\n\n\nCode\n# Query to select all stories posted on January 1, 2012, with number of comments\njoin_query = \"\"\"\n             WITH c AS\n             (\n             SELECT parent, COUNT(*) as num_comments\n             FROM `bigquery-public-data.hacker_news.comments` \n             GROUP BY parent\n             )\n             SELECT s.id as story_id, s.by, s.title, c.num_comments\n             FROM `bigquery-public-data.hacker_news.stories` AS s\n             LEFT JOIN c\n             ON s.id = c.parent\n             WHERE EXTRACT(DATE FROM s.time_ts) = '2012-01-01'\n             ORDER BY c.num_comments DESC\n             \"\"\"\n\n# Run the query, and return a pandas DataFrame\njoin_result = client.query(join_query).result().to_dataframe()\njoin_result.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nstory_id\nby\ntitle\nnum_comments\n\n\n\n\n0\n3412900\nwhoishiring\nAsk HN: Who is Hiring? (January 2012)\n154\n\n\n1\n3412901\nwhoishiring\nAsk HN: Freelancer? Seeking freelancer? (Janua...\n97\n\n\n2\n3412643\njemeshsu\nAvoid Apress\n30\n\n\n3\n3412891\nBrajeshwar\nThere's no shame in code that is simply \"good ...\n27\n\n\n4\n3414012\nramanujam\nImpress.js - a Prezi like implementation using...\n27\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSince the results are ordered by the num_comments column, stories without comments appear at the end of the DataFrame. (Remember that NaN stands for “not a number”.)\n\n\nCode\n# None of these stories received any comments\njoin_result.tail()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nstory_id\nby\ntitle\nnum_comments\n\n\n\n\n439\n3413342\njonsteiman\nThe List You Can't Miss: Top 5 Blogs of 2011\n&lt;NA&gt;\n\n\n440\n3412327\narroyo\nCan your business card be followed?\n&lt;NA&gt;\n\n\n441\n3413203\nnorris_tony44\n10 Popular iPhone Games you Must Play\n&lt;NA&gt;\n\n\n442\n3412940\njulelara\nWashington Redskins vs Philadelphia Eagles liv...\n&lt;NA&gt;\n\n\n443\n3412632\nUsedCarFleetCom\nUsed Car fleet\n&lt;NA&gt;\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAs you’ve seen, JOINs horizontally combine results from different tables. If you instead would like to vertically concatenate columns, you can do so with a UNION.\nNext, we write a query to select all usernames corresponding to users who wrote stories or comments on January 1, 2014. We use UNION DISTINCT (instead of UNION ALL) to ensure that each user appears in the table at most once.\n\n\nCode\n# Query to select all users who posted stories or comments on January 1, 2014\nunion_query = \"\"\"\n              SELECT c.by\n              FROM `bigquery-public-data.hacker_news.comments` AS c\n              WHERE EXTRACT(DATE FROM c.time_ts) = '2014-01-01'\n              UNION DISTINCT\n              SELECT s.by\n              FROM `bigquery-public-data.hacker_news.stories` AS s\n              WHERE EXTRACT(DATE FROM s.time_ts) = '2014-01-01'\n              \"\"\"\n\n# Run the query, and return a pandas DataFrame\nunion_result = client.query(union_query).result().to_dataframe()\nunion_result.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nby\n\n\n\n\n0\nvidarh\n\n\n1\ntlarkworthy\n\n\n2\njbl\n\n\n3\ndmgrow\n\n\n4\nmaurorm\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nTo get the number of users who posted on January 1, 2014, we need only take the length of the DataFrame.\n\n\nCode\n# Number of users who posted stories or comments on January 1, 2014\nlen(union_result)\n\n\n2282\n\n\n\n\n4.1.10 Nested and Repeated data\nSo far, you’ve worked with many types of data, including numeric types (integers, floating point values), strings, and the DATETIME type. In this tutorial, you’ll learn how to query nested and repeated data. These are the most complex data types that you can find in BigQuery datasets!\nWe’ll work with the Google Analytics Sample dataset. It contains information tracking the behavior of visitors to the Google Merchandise store, an e-commerce website that sells Google branded items.\nWe begin by printing the first few rows of the ga_sessions_20170801 table. This table tracks visits to the website on August 1, 2017. The table has many nested fields from table preview:\n\n\nCode\n# Construct a reference to the \"google_analytics_sample\" dataset\ndataset_ref = client.dataset(\"google_analytics_sample\", project=\"bigquery-public-data\")\n\n# Construct a reference to the \"ga_sessions_20170801\" table\ntable_ref = dataset_ref.table(\"ga_sessions_20170801\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nvisitorId\nvisitNumber\nvisitId\nvisitStartTime\ndate\ntotals\ntrafficSource\ndevice\ngeoNetwork\ncustomDimensions\nhits\nfullVisitorId\nuserId\nclientId\nchannelGrouping\nsocialEngagementType\n\n\n\n\n0\n&lt;NA&gt;\n1\n1501591568\n1501591568\n20170801\n{'visits': 1, 'hits': 1, 'pageviews': 1, 'time...\n{'referralPath': None, 'campaign': '(not set)'...\n{'browser': 'Chrome', 'browserVersion': 'not a...\n{'continent': 'Europe', 'subContinent': 'South...\n[]\n[{'hitNumber': 1, 'time': 0, 'hour': 5, 'minut...\n3418334011779872055\nNone\nNone\nOrganic Search\nNot Socially Engaged\n\n\n1\n&lt;NA&gt;\n2\n1501589647\n1501589647\n20170801\n{'visits': 1, 'hits': 1, 'pageviews': 1, 'time...\n{'referralPath': '/analytics/web/', 'campaign'...\n{'browser': 'Chrome', 'browserVersion': 'not a...\n{'continent': 'Asia', 'subContinent': 'Souther...\n[{'index': 4, 'value': 'APAC'}]\n[{'hitNumber': 1, 'time': 0, 'hour': 5, 'minut...\n2474397855041322408\nNone\nNone\nReferral\nNot Socially Engaged\n\n\n2\n&lt;NA&gt;\n1\n1501616621\n1501616621\n20170801\n{'visits': 1, 'hits': 1, 'pageviews': 1, 'time...\n{'referralPath': '/analytics/web/', 'campaign'...\n{'browser': 'Chrome', 'browserVersion': 'not a...\n{'continent': 'Europe', 'subContinent': 'North...\n[{'index': 4, 'value': 'EMEA'}]\n[{'hitNumber': 1, 'time': 0, 'hour': 12, 'minu...\n5870462820713110108\nNone\nNone\nReferral\nNot Socially Engaged\n\n\n3\n&lt;NA&gt;\n1\n1501601200\n1501601200\n20170801\n{'visits': 1, 'hits': 1, 'pageviews': 1, 'time...\n{'referralPath': '/analytics/web/', 'campaign'...\n{'browser': 'Firefox', 'browserVersion': 'not ...\n{'continent': 'Americas', 'subContinent': 'Nor...\n[{'index': 4, 'value': 'North America'}]\n[{'hitNumber': 1, 'time': 0, 'hour': 8, 'minut...\n9397809171349480379\nNone\nNone\nReferral\nNot Socially Engaged\n\n\n4\n&lt;NA&gt;\n1\n1501615525\n1501615525\n20170801\n{'visits': 1, 'hits': 1, 'pageviews': 1, 'time...\n{'referralPath': '/analytics/web/', 'campaign'...\n{'browser': 'Chrome', 'browserVersion': 'not a...\n{'continent': 'Americas', 'subContinent': 'Nor...\n[{'index': 4, 'value': 'North America'}]\n[{'hitNumber': 1, 'time': 0, 'hour': 12, 'minu...\n6089902943184578335\nNone\nNone\nReferral\nNot Socially Engaged\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNow we’ll work with the hits column as an example of data that is both nested and repeated. Since:\n\nhits is a STRUCT (contains nested data) and is repeated,\nhitNumber, page, and type are all nested inside the hits column, and\npagePath is nested inside the page field,\n\nwe can query these fields with the following syntax:\n\n\nCode\n# Query to determine most popular landing point on the website\nquery = \"\"\"\n        SELECT hits.page.pagePath as path,\n            COUNT(hits.page.pagePath) as counts\n        FROM `bigquery-public-data.google_analytics_sample.ga_sessions_20170801`, \n            UNNEST(hits) as hits\n        WHERE hits.type=\"PAGE\" and hits.hitNumber=1\n        GROUP BY path\n        ORDER BY counts DESC\n        \"\"\"\n\n# Run the query, and return a pandas DataFrame\nresult = client.query(query).result().to_dataframe()\nresult.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\npath\ncounts\n\n\n\n\n0\n/home\n1257\n\n\n1\n/google+redesign/shop+by+brand/youtube\n587\n\n\n2\n/google+redesign/apparel/mens/mens+t+shirts\n117\n\n\n3\n/signin.html\n78\n\n\n4\n/basket.html\n35\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4.1.11 Analytic Function (Optional)\nYou can also define analytic functions, which also operate on a set of rows like aggregation function. However, unlike aggregate functions, analytic functions return a (potentially different) value for each row in the original table. Analytic functions allow us to perform complex calculations with relatively straightforward syntax. For instance, we can quickly calculate moving averages and running totals, among other quantities.\nWe’ll work with the San Francisco Open Data dataset.\n\n\nCode\n# Construct a reference to the \"san_francisco\" dataset\ndataset_ref = client.dataset(\"san_francisco\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"bikeshare_trips\" table\ntable_ref = dataset_ref.table(\"bikeshare_trips\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()\n\n\n\n  \n    \n      \n\n\n\n\n\n\ntrip_id\nduration_sec\nstart_date\nstart_station_name\nstart_station_id\nend_date\nend_station_name\nend_station_id\nbike_number\nzip_code\nsubscriber_type\n\n\n\n\n0\n1235850\n1540\n2016-06-11 08:19:00+00:00\nSan Jose Diridon Caltrain Station\n2\n2016-06-11 08:45:00+00:00\nSan Jose Diridon Caltrain Station\n2\n124\n15206\nCustomer\n\n\n1\n1219337\n6324\n2016-05-29 12:49:00+00:00\nSan Jose Diridon Caltrain Station\n2\n2016-05-29 14:34:00+00:00\nSan Jose Diridon Caltrain Station\n2\n174\n55416\nCustomer\n\n\n2\n793762\n115572\n2015-06-04 09:22:00+00:00\nSan Jose Diridon Caltrain Station\n2\n2015-06-05 17:28:00+00:00\nSan Jose Diridon Caltrain Station\n2\n190\n95391\nCustomer\n\n\n3\n453845\n54120\n2014-09-15 16:53:00+00:00\nSan Jose Diridon Caltrain Station\n2\n2014-09-16 07:55:00+00:00\nSan Jose Diridon Caltrain Station\n2\n127\n81\nCustomer\n\n\n4\n1245113\n5018\n2016-06-17 20:08:00+00:00\nSan Jose Diridon Caltrain Station\n2\n2016-06-17 21:32:00+00:00\nSan Jose Diridon Caltrain Station\n2\n153\n95070\nCustomer\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nEach row of the table corresponds to a different bike trip, and we can use an analytic function to calculate the cumulative number of trips for each date in 2015.\n\n\nCode\n# Query to count the (cumulative) number of trips per day\nnum_trips_query = \"\"\"\n                  WITH trips_by_day AS\n                  (\n                  SELECT DATE(start_date) AS trip_date,\n                      COUNT(*) as num_trips\n                  FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n                  WHERE EXTRACT(YEAR FROM start_date) = 2015\n                  GROUP BY trip_date\n                  )\n                  SELECT *,\n                      SUM(num_trips) \n                          OVER (\n                               ORDER BY trip_date\n                               ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n                               ) AS cumulative_trips\n                  FROM trips_by_day\n                  \"\"\"\n\n# Run the query, and return a pandas DataFrame\nnum_trips_result = client.query(num_trips_query).result().to_dataframe()\nnum_trips_result.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\ntrip_date\nnum_trips\ncumulative_trips\n\n\n\n\n0\n2015-01-13\n1368\n10709\n\n\n1\n2015-04-06\n1281\n91635\n\n\n2\n2015-04-25\n405\n111722\n\n\n3\n2015-06-18\n1352\n166602\n\n\n4\n2015-07-14\n1358\n192161\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe query uses a common table expression (CTE) to first calculate the daily number of trips. Then, we use SUM() as an aggregate function. - Since there is no PARTITION BY clause, the entire table is treated as a single partition. - The ORDER BY clause orders the rows by date, where earlier dates appear first. - By setting the window frame clause to ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW, we ensure that all rows up to and including the current date are used to calculate the (cumulative) sum. See https://cloud.google.com/bigquery/docs/reference/standard-sql/analytic-function-concepts#def_window_frame for more details.\nThe next query tracks the stations where each bike began (in start_station_id) and ended (in end_station_id) the day on October 25, 2015.\n\n\nCode\n# Query to track beginning and ending stations on October 25, 2015, for each bike\nstart_end_query = \"\"\"\n                  SELECT bike_number,\n                      TIME(start_date) AS trip_time,\n                      FIRST_VALUE(start_station_id)\n                          OVER (\n                               PARTITION BY bike_number\n                               ORDER BY start_date\n                               ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n                               ) AS first_station_id,\n                      LAST_VALUE(end_station_id)\n                          OVER (\n                               PARTITION BY bike_number\n                               ORDER BY start_date\n                               ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n                               ) AS last_station_id,\n                      start_station_id,\n                      end_station_id\n                  FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n                  WHERE DATE(start_date) = '2015-10-25' \n                  \"\"\"\n\n# Run the query, and return a pandas DataFrame\nstart_end_result = client.query(start_end_query).result().to_dataframe()\nstart_end_result.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nbike_number\ntrip_time\nfirst_station_id\nlast_station_id\nstart_station_id\nend_station_id\n\n\n\n\n0\n230\n22:41:00\n22\n22\n22\n22\n\n\n1\n601\n15:50:00\n68\n67\n68\n50\n\n\n2\n601\n23:27:00\n68\n67\n50\n67\n\n\n3\n604\n08:56:00\n70\n66\n70\n39\n\n\n4\n604\n12:34:00\n70\n66\n39\n67\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe query uses both FIRST_VALUE() and LAST_VALUE() as analytic functions. - The PARTITION BY clause breaks the data into partitions based on the bike_number column. Since this column holds unique identifiers for the bikes, this ensures the calculations are performed separately for each bike. - The ORDER BY clause puts the rows within each partition in chronological order. - Since the window frame clause is ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING, for each row, its entire partition is used to perform the calculation. (This ensures the calculated values for rows in the same partition are identical.)\nYou can check https://cloud.google.com/bigquery/docs/reference/standard-sql/introduction and https://googleapis.dev/python/bigquery/latest/index.html for more details."
  },
  {
    "objectID": "03_Relational_Database_and_data_wrangling.html#data-wrangling-with-pandas",
    "href": "03_Relational_Database_and_data_wrangling.html#data-wrangling-with-pandas",
    "title": "4  Relational Database and data wrangling",
    "section": "4.2 Data Wrangling with Pandas",
    "text": "4.2 Data Wrangling with Pandas\n\n4.2.1 Series objects\nThe Pandas library contains these useful data structures: * Series objects, that we will discuss now. A Series object is 1D array, similar to a column in a spreadsheet (with a column name and row labels). * DataFrame objects. This is a 2D table, similar to a spreadsheet (with column names and row labels).\n\n4.2.1.1 Creating a Series\nLet’s start by creating our first Series object!\n\n\nCode\ns = pd.Series([2,-1,3,5])\ns\n\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64\n\n\nArithmetic operations on Series are also possible, and they apply elementwise, just like for ndarrays in NumPy:\n\n\nCode\ns + [1000,2000,3000,4000]\n\n\n0    1002\n1    1999\n2    3003\n3    4005\ndtype: int64\n\n\n\n\nCode\ns + 1000 # Broadcasting\n\n\n0    1002\n1     999\n2    1003\n3    1005\ndtype: int64\n\n\n\n\n4.2.1.2 Index labels\nEach item in a Series object has a unique identifier called the index label. By default, it is simply the rank of the item in the Series (starting at 0) but you can also set the index labels manually:\n\n\nCode\ns2 = pd.Series([68, 83, 112, 68], index=[\"alice\", \"bob\", \"charles\", \"darwin\"])\ns2\n\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\nYou can then use the Series just like a dict:\n\n\nCode\ns2[\"bob\"]\n\n\n83\n\n\nYou can still access the items by integer location, like in a regular array:\n\n\nCode\ns2[1]\n\n\n83\n\n\nTo make it clear when you are accessing, it is recommended to always use the loc attribute when accessing by label, and the iloc attribute when accessing by integer location:\n\n\nCode\ns2.loc[\"bob\"]\n\n\n83\n\n\n\n\nCode\ns2.iloc[1]\n\n\n83\n\n\nSlicing a Series also slices the index labels:\n\n\nCode\ns2.iloc[1:3]\n\n\nbob         83\ncharles    112\ndtype: int64\n\n\n\n\n4.2.1.3 Initialize from dict\nYou can create a Series object from a dict. The keys will be used as index labels:\n\n\nCode\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\ns3 = pd.Series(weights)\ns3\n\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\nWhen an operation involves multiple Series objects, pandas automatically aligns items by matching index labels.\n\n\nCode\nprint(s2.keys())\nprint(s3.keys())\n\ns2 + s3\n\n\nIndex(['alice', 'bob', 'charles', 'darwin'], dtype='object')\nIndex(['alice', 'bob', 'colin', 'darwin'], dtype='object')\n\n\nalice      136.0\nbob        166.0\ncharles      NaN\ncolin        NaN\ndarwin     136.0\ndtype: float64\n\n\nThe resulting Series contains the union of index labels from s2 and s3. Since \"colin\" is missing from s2 and \"charles\" is missing from s3, these items have a NaN result value. (ie. Not-a-Number means missing).\nAutomatic alignment is very handy when working with data that may come from various sources with varying structure and missing items\n\n\n4.2.1.4 Initialize with a scalar\nYou can also initialize a Series object using a scalar and a list of index labels: all items will be set to the scalar.\n\n\nCode\nmeaning = pd.Series(42, [\"life\", \"universe\", \"everything\"])\nmeaning\n\n\nlife          42\nuniverse      42\neverything    42\ndtype: int64\n\n\nPandas makes it easy to plot Series data using matplotlib:\n\n\nCode\ntemperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5]\ns4 = pd.Series(temperatures, name=\"Temperature\")\ns4.plot()\nplt.show()\n\n\n\n\n\nYou can easily convert it to NumPy array by dicarding the index.\n\n\nCode\ns4.to_numpy()\n\n\narray([4.4, 5.1, 6.1, 6.2, 6.1, 6.1, 5.7, 5.2, 4.7, 4.1, 3.9, 3.5])\n\n\nThere are many options for plotting your data. It is not necessary to list them all here: if you need a particular type of plot (histograms, pie charts, etc.), just look for it in the excellent Visualization section of pandas’ documentation, and look at the example code.\n\n\n\n4.2.2 Handling time\nMany datasets have timestamps, and pandas is awesome at manipulating such data: * it can represent periods (such as 2016Q3) and frequencies (such as “monthly”) * it can convert periods to actual timestamps, and vice versa * it can resample data and aggregate values any way you like * it can handle timezones.\n\n4.2.2.1 Time range\nLet’s start by creating a time series using pd.date_range(). This returns a DatetimeIndex containing one datetime per hour for 12 hours starting on March 6th 2023 at 5:30pm.\n\n\nCode\ndates = pd.date_range('2023/03/06 5:30pm', periods=12, freq='H')\ndates\n\n\nDatetimeIndex(['2023-03-06 17:30:00', '2023-03-06 18:30:00',\n               '2023-03-06 19:30:00', '2023-03-06 20:30:00',\n               '2023-03-06 21:30:00', '2023-03-06 22:30:00',\n               '2023-03-06 23:30:00', '2023-03-07 00:30:00',\n               '2023-03-07 01:30:00', '2023-03-07 02:30:00',\n               '2023-03-07 03:30:00', '2023-03-07 04:30:00'],\n              dtype='datetime64[ns]', freq='H')\n\n\nThis DatetimeIndex may be used as an index in a Series:\n\n\nCode\ntemp_series = pd.Series(temperatures, dates)\ntemp_series\n\n\n2023-03-06 17:30:00    4.4\n2023-03-06 18:30:00    5.1\n2023-03-06 19:30:00    6.1\n2023-03-06 20:30:00    6.2\n2023-03-06 21:30:00    6.1\n2023-03-06 22:30:00    6.1\n2023-03-06 23:30:00    5.7\n2023-03-07 00:30:00    5.2\n2023-03-07 01:30:00    4.7\n2023-03-07 02:30:00    4.1\n2023-03-07 03:30:00    3.9\n2023-03-07 04:30:00    3.5\nFreq: H, dtype: float64\n\n\nLet’s plot this series:\n\n\nCode\ntemp_series.plot(kind=\"bar\")\n\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n4.2.3 Periods\nThe pd.period_range() function returns a PeriodIndex instead of a DatetimeIndex. For example, let’s get all quarters in 2022 and 2023:\n\n\nCode\nquarters = pd.period_range('2022Q1', periods=8, freq='Q')\nquarters\n\n\nPeriodIndex(['2022Q1', '2022Q2', '2022Q3', '2022Q4', '2023Q1', '2023Q2',\n             '2023Q3', '2023Q4'],\n            dtype='period[Q-DEC]')\n\n\nAdding a number N to a PeriodIndex shifts the periods by N times the PeriodIndex’s frequency:\n\n\nCode\nquarters + 3\n\n\nPeriodIndex(['2022Q4', '2023Q1', '2023Q2', '2023Q3', '2023Q4', '2024Q1',\n             '2024Q2', '2024Q3'],\n            dtype='period[Q-DEC]')\n\n\nPandas also provides many other time-related functions that we recommend you check out in the documentation\n\n\n4.2.4 DataFrame objects\nA DataFrame object represents a spreadsheet, with cell values, column names and row index labels. You can define expressions to compute columns based on other columns, create pivot-tables, group rows, draw graphs, etc. You can see DataFrame as dictionaries of Series.\n\n4.2.4.1 Creating a DataFrame\n\n\nCode\n#%unload_ext google.colab.data_table #cloab only\n\n\n\n\nCode\n#from google.colab import data_table\n#data_table.disable_dataframe_formatter()\n\n\nYou can create a DataFrame by passing a dictionary of Series objects:\n\n\nCode\npeople_dict = {\n    \"weight\": pd.Series([68, 83, 112], index=[\"alice\", \"bob\", \"charles\"]),\n    \"birthyear\": pd.Series([1984, 1985, 1992], index=[\"bob\", \"alice\", \"charles\"], name=\"year\"),\n    \"children\": pd.Series([0, 3], index=[\"charles\", \"bob\"]),\n    \"hobby\": pd.Series([\"Biking\", \"Dancing\"], index=[\"alice\", \"bob\"]),\n}\npeople = pd.DataFrame(people_dict)\npeople\n\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nbirthyear\nchildren\nhobby\n\n\n\n\nalice\n68\n1985\nNaN\nBiking\n\n\nbob\n83\n1984\n3.0\nDancing\n\n\ncharles\n112\n1992\n0.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nA few things to note: * the Series were automatically aligned based on their index, * missing values are represented as NaN, * Series names are ignored (the name \"year\" was dropped), * DataFrames are displayed nicely in Jupyter notebooks!\n\n\n4.2.4.2 Subsets - Accessing columns\nYou can access columns by using the column name or fancy indexing. They are returned as Series objects:\n\n\nCode\npeople[\"birthyear\"]\n\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\nYou can also get multiple columns at once:\n\n\nCode\npeople[[\"birthyear\", \"hobby\"]]\n\n\n\n  \n    \n      \n\n\n\n\n\n\nbirthyear\nhobby\n\n\n\n\nalice\n1985\nBiking\n\n\nbob\n1984\nDancing\n\n\ncharles\n1992\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAnother convenient way to create a DataFrame is to pass all the values to the constructor as an ndarray, or a list of lists, and specify the column names and row index labels separately:\n\n\nCode\nvalues = [\n            [1985, np.nan, \"Biking\",   68],\n            [1984, 3,      \"Dancing\",  83],\n            [1992, 0,      np.nan,    112]\n         ]\nd3 = pd.DataFrame(\n        values,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n\n  \n    \n      \n\n\n\n\n\n\nbirthyear\nchildren\nhobby\nweight\n\n\n\n\nalice\n1985\nNaN\nBiking\n68\n\n\nbob\n1984\n3.0\nDancing\n83\n\n\ncharles\n1992\n0.0\nNaN\n112\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4.2.4.3 Multi-index (optional)\nYou can also create multi-index datafram as follows:\n\n\nCode\ndf = pd.DataFrame(\n    {\n        \"a\" : [4 ,5, 6],\n        \"b\" : [7, 8, 9],\n        \"c\" : [10, 11, 12]\n     },\n    index = pd.MultiIndex.from_tuples(\n        [('d',1),('d',2),('e',2)], names=['n','v']\n    )\n)\ndf\n\n\n\n  \n    \n      \n\n\n\n\n\n\n\na\nb\nc\n\n\nn\nv\n\n\n\n\n\n\n\nd\n1\n4\n7\n10\n\n\n2\n5\n8\n11\n\n\ne\n2\n6\n9\n12\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nIf all columns are tuples of the same size, then they are understood as a multi-index. The same goes for row index labels. For example:\n\n\nCode\nd5 = pd.DataFrame(\n  {\n    (\"public\", \"birthyear\"):\n        {(\"Paris\",\"alice\"):1985, (\"Paris\",\"bob\"): 1984, (\"London\",\"charles\"): 1992},\n    (\"public\", \"hobby\"):\n        {(\"Paris\",\"alice\"):\"Biking\", (\"Paris\",\"bob\"): \"Dancing\"},\n    (\"private\", \"weight\"):\n        {(\"Paris\",\"alice\"):68, (\"Paris\",\"bob\"): 83, (\"London\",\"charles\"): 112},\n    (\"private\", \"children\"):\n        {(\"Paris\", \"alice\"):np.nan, (\"Paris\",\"bob\"): 3, (\"London\",\"charles\"): 0}\n  }\n)\nd5\n\n\n\n  \n    \n      \n\n\n\n\n\n\n\npublic\nprivate\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nParis\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\nLondon\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou can now get a DataFrame containing all the “public” columns very simply:\n\n\nCode\nd5[\"public\"]\n\n\n\n  \n    \n      \n\n\n\n\n\n\n\nbirthyear\nhobby\n\n\n\n\nParis\nalice\n1985\nBiking\n\n\nbob\n1984\nDancing\n\n\nLondon\ncharles\n1992\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nIt is noted that most methods return modified copies in pandas.\n\n\n4.2.4.4 Subsets - Accessing rows\nLet’s go back to the people DataFrame:\n\n\nCode\npeople\n\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nbirthyear\nchildren\nhobby\n\n\n\n\nalice\n68\n1985\nNaN\nBiking\n\n\nbob\n83\n1984\n3.0\nDancing\n\n\ncharles\n112\n1992\n0.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe loc attribute lets you access rows instead of columns. The result is a Series object in which the DataFrame’s column names are mapped to row index labels:\n\n\nCode\npeople.loc[\"charles\"]\n\n\nweight        112\nbirthyear    1992\nchildren      0.0\nhobby         NaN\nName: charles, dtype: object\n\n\nYou can also access rows by integer location using the iloc attribute:\n\n\nCode\npeople.iloc[2]\n\n\nweight        112\nbirthyear    1992\nchildren      0.0\nhobby         NaN\nName: charles, dtype: object\n\n\nYou can also get a slice of rows, and this returns a DataFrame object:\n\n\nCode\npeople.iloc[1:3]\n\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nbirthyear\nchildren\nhobby\n\n\n\n\nbob\n83\n1984\n3.0\nDancing\n\n\ncharles\n112\n1992\n0.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nFinally, you can pass a boolean array to get the matching rows. This is most useful when combined with boolean expressions:\n\n\nCode\npeople[people[\"birthyear\"] &lt; 1990]\n\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nbirthyear\nchildren\nhobby\n\n\n\n\nalice\n68\n1985\nNaN\nBiking\n\n\nbob\n83\n1984\n3.0\nDancing\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou can also accessing columns by specifiying the second axis:\n\n\nCode\npeople.iloc[:,2]\n\n\nalice      NaN\nbob        3.0\ncharles    0.0\nName: children, dtype: float64\n\n\n\n\n4.2.4.5 Adding and removing columns\nYou can generally treat DataFrame objects like dictionaries of Series, so the following work fine:\n\n\nCode\npeople\n\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nbirthyear\nchildren\nhobby\n\n\n\n\nalice\n68\n1985\nNaN\nBiking\n\n\nbob\n83\n1984\n3.0\nDancing\n\n\ncharles\n112\n1992\n0.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\npeople[\"age\"] = 2023 - people[\"birthyear\"]  # adds a new column \"age\"\npeople[\"over 30\"] = people[\"age\"] &gt; 30      # adds another column \"over 30\"\nbirthyears = people.pop(\"birthyear\")\npeople.drop(columns=['children'], inplace=True) # drop a column inplace\npeople\n\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nhobby\nage\nover 30\n\n\n\n\nalice\n68\nBiking\n38\nTrue\n\n\nbob\n83\nDancing\n39\nTrue\n\n\ncharles\n112\nNaN\n31\nTrue\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nbirthyears\n\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\nWhen you add a new column, it must have the same number of rows. Missing rows are filled with NaN, and extra rows are ignored:\n\n\nCode\npeople[\"pets\"] = pd.Series({\"bob\": 0, \"charles\": 5, \"eugene\":1})  # alice is missing, eugene is ignored\npeople\n\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nhobby\nage\nover 30\npets\n\n\n\n\nalice\n68\nBiking\n38\nTrue\nNaN\n\n\nbob\n83\nDancing\n39\nTrue\n0.0\n\n\ncharles\n112\nNaN\n31\nTrue\n5.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWhen adding a new column, it is added at the end (on the right) by default. You can also insert a column anywhere else using the insert() method:\n\n\nCode\npeople.insert(1, \"height\", [172, 181, 185])\npeople\n\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nheight\nhobby\nage\nover 30\npets\n\n\n\n\nalice\n68\n172\nBiking\n38\nTrue\nNaN\n\n\nbob\n83\n181\nDancing\n39\nTrue\n0.0\n\n\ncharles\n112\n185\nNaN\n31\nTrue\n5.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou can also create new columns by calling the assign() method. Note that this returns a new DataFrame object, the original is not modified\n\n\nCode\np2 = people.assign(\n    bmi = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n    has_pets = people[\"pets\"] &gt; 0\n)\np2\n\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nheight\nhobby\nage\nover 30\npets\nbmi\nhas_pets\n\n\n\n\nalice\n68\n172\nBiking\n38\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\n83\n181\nDancing\n39\nTrue\n0.0\n25.335002\nFalse\n\n\ncharles\n112\n185\nNaN\n31\nTrue\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou can also rename the column name:\n\n\nCode\np2.rename(columns={'bmi':'body_mass_index'})\n\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nheight\nhobby\nage\nover 30\npets\nbody_mass_index\nhas_pets\n\n\n\n\nalice\n68\n172\nBiking\n38\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\n83\n181\nDancing\n39\nTrue\n0.0\n25.335002\nFalse\n\n\ncharles\n112\n185\nNaN\n31\nTrue\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4.2.4.6 Querying a DataFrame\nThe query() method lets you filter a DataFrame based on a query expression:\n\n\nCode\npeople.query(\"age &gt; 30 and pets == 0\")\n\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nheight\nhobby\nage\nover 30\npets\n\n\n\n\nbob\n83\n181\nDancing\n39\nTrue\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4.2.4.7 Sorting a DataFrame\nYou can sort a DataFrame by calling its sort_index method. By default it sorts the rows by their index label, in ascending order, but let’s reverse the order:\n\n\nCode\npeople.sort_index(ascending=False)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nweight\nheight\nhobby\nage\nover 30\npets\n\n\n\n\ncharles\n112\n185\nNaN\n31\nTrue\n5.0\n\n\nbob\n83\n181\nDancing\n39\nTrue\n0.0\n\n\nalice\n68\n172\nBiking\n38\nTrue\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNote that sort_index returned a sorted copy of the DataFrame. To modify people directly, we can set the inplace argument to True. Also, we can sort the columns instead of the rows by setting axis=1:\n\n\nCode\npeople.sort_index(axis=1, inplace=True)\npeople\n\n\n\n  \n    \n      \n\n\n\n\n\n\nage\nheight\nhobby\nover 30\npets\nweight\n\n\n\n\nalice\n38\n172\nBiking\nTrue\nNaN\n68\n\n\nbob\n39\n181\nDancing\nTrue\n0.0\n83\n\n\ncharles\n31\n185\nNaN\nTrue\n5.0\n112\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nTo sort the DataFrame by the values instead of the labels, we can use sort_values and specify the column to sort by:\n\n\nCode\npeople.sort_values(by=\"age\", inplace=True)\npeople\n\n\n\n  \n    \n      \n\n\n\n\n\n\nage\nheight\nhobby\nover 30\npets\nweight\n\n\n\n\ncharles\n31\n185\nNaN\nTrue\n5.0\n112\n\n\nalice\n38\n172\nBiking\nTrue\nNaN\n68\n\n\nbob\n39\n181\nDancing\nTrue\n0.0\n83\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4.2.4.8 Plotting a DataFrame\nJust like for Series, pandas makes it easy to draw nice graphs based on a DataFrame.\nFor example, it is trivial to create a line plot from a DataFrame’s data by calling its plot method:\n\n\nCode\npeople = people.assign(\n    body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2\n)\npeople.plot(kind = \"line\", x = \"body_mass_index\", y = [\"height\", \"weight\"])\nplt.show()\n\n\n\n\n\nAgain, there are way too many options to list here: the best option is to scroll through the Visualization page in pandas’ documentation, find the plot you are interested in and look at the example code.\n\n\n4.2.4.9 Operations on DataFrames\nAlthough DataFrames do not try to mimick NumPy arrays, there are a few similarities. Let’s create a DataFrame to demonstrate this:\n\n\nCode\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"bob\",\"charles\",\"darwin\"])\ngrades\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n8\n8\n9\n\n\nbob\n10\n9\n9\n\n\ncharles\n4\n8\n2\n\n\ndarwin\n9\n10\n10\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou can apply NumPy mathematical functions on a DataFrame: the function is applied to all values:\n\n\nCode\nnp.sqrt(grades)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n2.828427\n2.828427\n3.000000\n\n\nbob\n3.162278\n3.000000\n3.000000\n\n\ncharles\n2.000000\n2.828427\n1.414214\n\n\ndarwin\n3.000000\n3.162278\n3.162278\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\ngrades + 1\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n9\n9\n10\n\n\nbob\n11\n10\n10\n\n\ncharles\n5\n9\n3\n\n\ndarwin\n10\n11\n11\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAggregation operations, such as computing the max(), the sum() or the mean() of a DataFrame, apply to each column, and you get back a Series object:\n\n\nCode\ngrades.mean()\n\n\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\n\n\nMost of these functions take an optional axis parameter which lets you specify along which axis of the DataFrame you want the operation executed. The default is axis=0, meaning that the operation is executed vertically (on each column). You can set axis=1 to execute the operation horizontally (on each row). For example, let’s find out which students had all grades greater than 5:\n\n\nCode\n(grades &gt; 5).all(axis = 1)\n\n\nalice       True\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nIf you add a Series object to a DataFrame (or execute any other binary operation), Pandas attempts to broadcast the operation to all rows in the DataFrame. This only works if the Series has the same size as the DataFrames rows. For example, let’s subtract the mean of the DataFrame (a Series object) from the DataFrame:\n\n\nCode\ngrades - grades.mean()  # equivalent to: grades - [7.75, 8.75, 7.50]\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n0.25\n-0.75\n1.5\n\n\nbob\n2.25\n0.25\n1.5\n\n\ncharles\n-3.75\n-0.75\n-5.5\n\n\ndarwin\n1.25\n1.25\n2.5\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nIf you want to subtract the global mean from every grade, here is one way to do it:\n\n\nCode\ngrades - grades.values.mean() # subtracts the global mean (8.00) from all grades\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsep\noct\nnov\n\n\n\n\nalice\n0.0\n0.0\n1.0\n\n\nbob\n2.0\n1.0\n1.0\n\n\ncharles\n-4.0\n0.0\n-6.0\n\n\ndarwin\n1.0\n2.0\n2.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe following shows the behavior of nan\n\n\nCode\nbonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]])\nbonus_points = pd.DataFrame(bonus_array, columns=[\"oct\", \"nov\", \"dec\"], index=[\"bob\",\"colin\", \"darwin\", \"charles\"])\nbonus_points\n\n\n\n  \n    \n      \n\n\n\n\n\n\noct\nnov\ndec\n\n\n\n\nbob\n0.0\nNaN\n2.0\n\n\ncolin\nNaN\n1.0\n0.0\n\n\ndarwin\n0.0\n1.0\n0.0\n\n\ncharles\n3.0\n3.0\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\ngrades + bonus_points\n\n\n\n  \n    \n      \n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nalice\nNaN\nNaN\nNaN\nNaN\n\n\nbob\nNaN\nNaN\n9.0\nNaN\n\n\ncharles\nNaN\n5.0\n11.0\nNaN\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\n\n\ndarwin\nNaN\n11.0\n10.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4.2.4.10 Handling missing data\nDealing with missing data is a frequent task when working with real life data. Pandas offers a few tools to handle missing data.\nLet’s try to fix the problem above. For example, we can decide that missing data should result in a zero, instead of NaN. We can replace all NaN values by a any value using the fillna() method:\n\n\nCode\n(grades + bonus_points).fillna(0)\n\n\n\n  \n    \n      \n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nalice\n0.0\n0.0\n0.0\n0.0\n\n\nbob\n0.0\n0.0\n9.0\n0.0\n\n\ncharles\n0.0\n5.0\n11.0\n0.0\n\n\ncolin\n0.0\n0.0\n0.0\n0.0\n\n\ndarwin\n0.0\n11.0\n10.0\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nfinal_grades = grades + bonus_points\nfinal_grades\n\n\n\n  \n    \n      \n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nalice\nNaN\nNaN\nNaN\nNaN\n\n\nbob\nNaN\nNaN\n9.0\nNaN\n\n\ncharles\nNaN\n5.0\n11.0\nNaN\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\n\n\ndarwin\nNaN\n11.0\n10.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe can call the dropna() method to get rid of rows that are full of NaNs:\n\n\nCode\nfinal_grades_clean = final_grades.dropna(how=\"all\")\nfinal_grades_clean\n\n\n\n  \n    \n      \n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\n\n\nbob\nNaN\nNaN\n9.0\nNaN\n\n\ncharles\nNaN\n5.0\n11.0\nNaN\n\n\ndarwin\nNaN\n11.0\n10.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNow let’s remove columns that are full of NaNs by setting the axis argument to 1:\n\n\nCode\nfinal_grades_clean = final_grades_clean.dropna(axis=1, how=\"all\")\nfinal_grades_clean\n\n\n\n  \n    \n      \n\n\n\n\n\n\nnov\noct\n\n\n\n\nbob\nNaN\n9.0\n\n\ncharles\n5.0\n11.0\n\n\ndarwin\n11.0\n10.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4.2.4.11 Aggregating with groupby\nSimilar to the SQL language, pandas allows grouping your data into groups to run calculations over each group.\nFirst, let’s add some extra data about each person so we can group them, and let’s go back to the final_grades DataFrame so we can see how NaN values are handled:\n\n\nCode\nfinal_grades[\"hobby\"] = [\"Biking\", \"Dancing\", np.nan, \"Dancing\", \"Biking\"]\nfinal_grades\n\n\n\n  \n    \n      \n\n\n\n\n\n\ndec\nnov\noct\nsep\nhobby\n\n\n\n\nalice\nNaN\nNaN\nNaN\nNaN\nBiking\n\n\nbob\nNaN\nNaN\n9.0\nNaN\nDancing\n\n\ncharles\nNaN\n5.0\n11.0\nNaN\nNaN\n\n\ncolin\nNaN\nNaN\nNaN\nNaN\nDancing\n\n\ndarwin\nNaN\n11.0\n10.0\nNaN\nBiking\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNow let’s group data in this DataFrame by hobby:\n\n\nCode\ngrouped_grades = final_grades.groupby(\"hobby\")\n\n\nWe are ready to compute the average grade per hobby:\n\n\nCode\ngrouped_grades.mean()\n\n\n\n  \n    \n      \n\n\n\n\n\n\ndec\nnov\noct\nsep\n\n\nhobby\n\n\n\n\n\n\n\n\nBiking\nNaN\n11.0\n10.0\nNaN\n\n\nDancing\nNaN\nNaN\n9.0\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThat was easy! Note that the NaN values have simply been skipped when computing the means.\n\n\n4.2.4.12 Pivot tables (Optional)\nPandas supports spreadsheet-like pivot tables that allow quick data summarization.\n\n\n4.2.4.13 Overview functions\nWhen dealing with large DataFrames, it is useful to get a quick overview of its content. Pandas offers a few functions for this. First, let’s create a large DataFrame with a mix of numeric values, missing values and text values. Notice how Jupyter displays only the corners of the DataFrame:\n\n\nCode\nmuch_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26))\nlarge_df = pd.DataFrame(much_data, columns=list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\nlarge_df[large_df % 16 == 0] = np.nan\nlarge_df.insert(3,\"some_text\", \"Blabla\")\nlarge_df\n\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nsome_text\nD\nE\nF\nG\nH\nI\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\n0\nNaN\n11.0\n44.0\nBlabla\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n...\n11.0\nNaN\n11.0\n44.0\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n\n\n1\n11.0\n22.0\n55.0\nBlabla\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n...\n22.0\n11.0\n22.0\n55.0\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n\n\n2\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n3\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n4\n44.0\n55.0\n88.0\nBlabla\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n...\n55.0\n44.0\n55.0\n88.0\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\nNaN\nNaN\n33.0\nBlabla\n88.0\n165.0\n77.0\n11.0\n154.0\n132.0\n...\nNaN\nNaN\nNaN\n33.0\n88.0\n165.0\n77.0\n11.0\n154.0\n132.0\n\n\n9996\nNaN\n11.0\n44.0\nBlabla\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n...\n11.0\nNaN\n11.0\n44.0\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n\n\n9997\n11.0\n22.0\n55.0\nBlabla\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n...\n22.0\n11.0\n22.0\n55.0\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n\n\n9998\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n9999\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n\n\n\n10000 rows × 27 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe head() method returns the top 5 rows:\n\n\nCode\nlarge_df.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nsome_text\nD\nE\nF\nG\nH\nI\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\n0\nNaN\n11.0\n44.0\nBlabla\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n...\n11.0\nNaN\n11.0\n44.0\n99.0\nNaN\n88.0\n22.0\n165.0\n143.0\n\n\n1\n11.0\n22.0\n55.0\nBlabla\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n...\n22.0\n11.0\n22.0\n55.0\n110.0\nNaN\n99.0\n33.0\nNaN\n154.0\n\n\n2\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n3\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n4\n44.0\n55.0\n88.0\nBlabla\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n...\n55.0\n44.0\n55.0\n88.0\n143.0\n33.0\n132.0\n66.0\n22.0\nNaN\n\n\n\n\n\n5 rows × 27 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nOf course there’s also a tail() function to view the bottom 5 rows. You can pass the number of rows you want:\n\n\nCode\nlarge_df.tail(n=2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nsome_text\nD\nE\nF\nG\nH\nI\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\n9998\n22.0\n33.0\n66.0\nBlabla\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n...\n33.0\n22.0\n33.0\n66.0\n121.0\n11.0\n110.0\n44.0\nNaN\n165.0\n\n\n9999\n33.0\n44.0\n77.0\nBlabla\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n...\n44.0\n33.0\n44.0\n77.0\n132.0\n22.0\n121.0\n55.0\n11.0\nNaN\n\n\n\n\n\n2 rows × 27 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe info() method prints out a summary of each columns contents:\n\n\nCode\nlarge_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 27 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   A          8823 non-null   float64\n 1   B          8824 non-null   float64\n 2   C          8824 non-null   float64\n 3   some_text  10000 non-null  object \n 4   D          8824 non-null   float64\n 5   E          8822 non-null   float64\n 6   F          8824 non-null   float64\n 7   G          8824 non-null   float64\n 8   H          8822 non-null   float64\n 9   I          8823 non-null   float64\n 10  J          8823 non-null   float64\n 11  K          8822 non-null   float64\n 12  L          8824 non-null   float64\n 13  M          8824 non-null   float64\n 14  N          8822 non-null   float64\n 15  O          8824 non-null   float64\n 16  P          8824 non-null   float64\n 17  Q          8824 non-null   float64\n 18  R          8823 non-null   float64\n 19  S          8824 non-null   float64\n 20  T          8824 non-null   float64\n 21  U          8824 non-null   float64\n 22  V          8822 non-null   float64\n 23  W          8824 non-null   float64\n 24  X          8824 non-null   float64\n 25  Y          8822 non-null   float64\n 26  Z          8823 non-null   float64\ndtypes: float64(26), object(1)\nmemory usage: 2.1+ MB\n\n\nFinally, the describe() method gives a nice overview of the main aggregated values over each column: * count: number of non-null (not NaN) values * mean: mean of non-null values * std: standard deviation of non-null values * min: minimum of non-null values * 25%, 50%, 75%: 25th, 50th and 75th percentile of non-null values * max: maximum of non-null values\n\n\nCode\nlarge_df.describe()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n...\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\ncount\n8823.000000\n8824.000000\n8824.000000\n8824.000000\n8822.000000\n8824.000000\n8824.000000\n8822.000000\n8823.000000\n8823.000000\n...\n8824.000000\n8823.000000\n8824.000000\n8824.000000\n8824.000000\n8822.000000\n8824.000000\n8824.000000\n8822.000000\n8823.000000\n\n\nmean\n87.977559\n87.972575\n87.987534\n88.012466\n87.983791\n88.007480\n87.977561\n88.000000\n88.022441\n88.022441\n...\n87.972575\n87.977559\n87.972575\n87.987534\n88.012466\n87.983791\n88.007480\n87.977561\n88.000000\n88.022441\n\n\nstd\n47.535911\n47.535523\n47.521679\n47.521679\n47.535001\n47.519371\n47.529755\n47.536879\n47.535911\n47.535911\n...\n47.535523\n47.535911\n47.535523\n47.521679\n47.521679\n47.535001\n47.519371\n47.529755\n47.536879\n47.535911\n\n\nmin\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n...\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n11.000000\n\n\n25%\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n...\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n44.000000\n\n\n50%\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n...\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n88.000000\n\n\n75%\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n...\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n132.000000\n\n\nmax\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n...\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n165.000000\n\n\n\n\n\n8 rows × 26 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4.2.4.14 Saving & loading\nPandas can save DataFrames to various backends, including file formats such as CSV, Excel, JSON, HTML and HDF5, or to a SQL database. Let’s create a DataFrame to demonstrate this:\n\n\nCode\nmy_df = pd.DataFrame(\n    [[\"Biking\", 68.5, 1985, np.nan], [\"Dancing\", 83.1, 1984, 3]], \n    columns=[\"hobby\",\"weight\",\"birthyear\",\"children\"],\n    index=[\"alice\", \"bob\"]\n)\nmy_df\n\n\n\n  \n    \n      \n\n\n\n\n\n\nhobby\nweight\nbirthyear\nchildren\n\n\n\n\nalice\nBiking\n68.5\n1985\nNaN\n\n\nbob\nDancing\n83.1\n1984\n3.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nLet’s save it to CSV, HTML and JSON:\n\n\nCode\nmy_df.to_csv(\"my_df.csv\")\nmy_df.to_html(\"my_df.html\")\nmy_df.to_json(\"my_df.json\")\n\n\nDone! Let’s take a peek at what was saved:\n\n\nCode\nfor filename in (\"my_df.csv\", \"my_df.html\", \"my_df.json\"):\n    print(\"#\", filename)\n    with open(filename, \"rt\") as f:\n        print(f.read())\n        print()\n\n\n# my_df.csv\n,hobby,weight,birthyear,children\nalice,Biking,68.5,1985,\nbob,Dancing,83.1,1984,3.0\n\n\n# my_df.html\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n  &lt;thead&gt;\n    &lt;tr style=\"text-align: right;\"&gt;\n      &lt;th&gt;&lt;/th&gt;\n      &lt;th&gt;hobby&lt;/th&gt;\n      &lt;th&gt;weight&lt;/th&gt;\n      &lt;th&gt;birthyear&lt;/th&gt;\n      &lt;th&gt;children&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr&gt;\n      &lt;th&gt;alice&lt;/th&gt;\n      &lt;td&gt;Biking&lt;/td&gt;\n      &lt;td&gt;68.5&lt;/td&gt;\n      &lt;td&gt;1985&lt;/td&gt;\n      &lt;td&gt;NaN&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;bob&lt;/th&gt;\n      &lt;td&gt;Dancing&lt;/td&gt;\n      &lt;td&gt;83.1&lt;/td&gt;\n      &lt;td&gt;1984&lt;/td&gt;\n      &lt;td&gt;3.0&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n\n# my_df.json\n{\"hobby\":{\"alice\":\"Biking\",\"bob\":\"Dancing\"},\"weight\":{\"alice\":68.5,\"bob\":83.1},\"birthyear\":{\"alice\":1985,\"bob\":1984},\"children\":{\"alice\":null,\"bob\":3.0}}\n\n\n\nNote that the index is saved as the first column (with no name) in a CSV file, as &lt;th&gt; tags in HTML and as keys in JSON.\nSaving to other formats works very similarly, but some formats require extra libraries to be installed. For example, saving to Excel requires the openpyxl library:\n\n\nCode\ntry:\n    my_df.to_excel(\"my_df.xlsx\", sheet_name='People')\nexcept ImportError as e:\n    print(e)\n\n\nNow let’s load our CSV file back into a DataFrame:\n\n\nCode\nmy_df_loaded = pd.read_csv(\"my_df.csv\", index_col=0)\nmy_df_loaded\n\n\n\n  \n    \n      \n\n\n\n\n\n\nhobby\nweight\nbirthyear\nchildren\n\n\n\n\nalice\nBiking\n68.5\n1985\nNaN\n\n\nbob\nDancing\n83.1\n1984\n3.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAs you might guess, there are similar read_json, read_html, read_excel functions as well. We can also read data straight from the Internet. For example, let’s load the top 1,000 U.S. cities from github:\n\n\nCode\nus_cities = None\ntry:\n    csv_url = \"https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv\"\n    us_cities = pd.read_csv(csv_url, index_col=0)\n    us_cities = us_cities.head()\nexcept IOError as e:\n    print(e)\nus_cities\n\n\n\n  \n    \n      \n\n\n\n\n\n\nState\nPopulation\nlat\nlon\n\n\nCity\n\n\n\n\n\n\n\n\nMarysville\nWashington\n63269\n48.051764\n-122.177082\n\n\nPerris\nCalifornia\n72326\n33.782519\n-117.228648\n\n\nCleveland\nOhio\n390113\n41.499320\n-81.694361\n\n\nWorcester\nMassachusetts\n182544\n42.262593\n-71.802293\n\n\nColumbia\nSouth Carolina\n133358\n34.000710\n-81.034814\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThere are more options available, in particular regarding datetime format. Check out the documentation for more details.\n\n\n4.2.4.15 Combining DataFrames\nOne powerful feature of Pandas is it’s ability to perform SQL-like joins on DataFrames. Various types of joins are supported: inner joins, left/right outer joins and full joins. To illustrate this, let’s start by creating a couple simple DataFrames:\n\n\nCode\ncity_loc = pd.DataFrame(\n    [\n        [\"CA\", \"San Francisco\", 37.781334, -122.416728],\n        [\"NY\", \"New York\", 40.705649, -74.008344],\n        [\"FL\", \"Miami\", 25.791100, -80.320733],\n        [\"OH\", \"Cleveland\", 41.473508, -81.739791],\n        [\"UT\", \"Salt Lake City\", 40.755851, -111.896657]\n    ], columns=[\"state\", \"city\", \"lat\", \"lng\"])\ncity_loc\n\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\ncity\nlat\nlng\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\ncity_pop = pd.DataFrame(\n    [\n        [808976, \"San Francisco\", \"California\"],\n        [8363710, \"New York\", \"New-York\"],\n        [413201, \"Miami\", \"Florida\"],\n        [2242193, \"Houston\", \"Texas\"]\n    ], index=[3,4,5,6], columns=[\"population\", \"city\", \"state\"])\ncity_pop\n\n\n\n  \n    \n      \n\n\n\n\n\n\npopulation\ncity\nstate\n\n\n\n\n3\n808976\nSan Francisco\nCalifornia\n\n\n4\n8363710\nNew York\nNew-York\n\n\n5\n413201\nMiami\nFlorida\n\n\n6\n2242193\nHouston\nTexas\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNow let’s join these DataFrames using the merge() function:\n\n\nCode\npd.merge(left=city_loc, right=city_pop, on=\"city\")\n\n\n\n  \n    \n      \n\n\n\n\n\n\nstate_x\ncity\nlat\nlng\npopulation\nstate_y\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n808976\nCalifornia\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n8363710\nNew-York\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n413201\nFlorida\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNote that both DataFrames have a column named state, so in the result they got renamed to state_x and state_y.\nAlso, note that Cleveland, Salt Lake City and Houston were dropped because they don’t exist in both DataFrames. This is the equivalent of a SQL INNER JOIN. If you want a FULL OUTER JOIN, where no city gets dropped and NaN values are added, you must specify how=\"outer\":\n\n\nCode\nall_cities = pd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"outer\")\nall_cities\n\n\n\n  \n    \n      \n\n\n\n\n\n\nstate_x\ncity\nlat\nlng\npopulation\nstate_y\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n808976.0\nCalifornia\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n8363710.0\nNew-York\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n413201.0\nFlorida\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\nNaN\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\nNaN\nNaN\n\n\n5\nNaN\nHouston\nNaN\nNaN\n2242193.0\nTexas\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nOf course LEFT OUTER JOIN is also available by setting how=\"left\": only the cities present in the left DataFrame end up in the result. Similarly, with how=\"right\" only cities in the right DataFrame appear in the result. For example:\n\n\nCode\npd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"right\")\n\n\n\n  \n    \n      \n\n\n\n\n\n\nstate_x\ncity\nlat\nlng\npopulation\nstate_y\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\n808976\nCalifornia\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\n8363710\nNew-York\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\n413201\nFlorida\n\n\n3\nNaN\nHouston\nNaN\nNaN\n2242193\nTexas\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4.2.4.16 Concatenation\nRather than joining DataFrames, we may just want to concatenate them. That’s what concat() is for:\n\n\nCode\nresult_concat = pd.concat([city_loc, city_pop])\nresult_concat\n\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\nNaN\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\nNaN\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\nNaN\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\nNaN\n\n\n3\nCalifornia\nSan Francisco\nNaN\nNaN\n808976.0\n\n\n4\nNew-York\nNew York\nNaN\nNaN\n8363710.0\n\n\n5\nFlorida\nMiami\nNaN\nNaN\n413201.0\n\n\n6\nTexas\nHouston\nNaN\nNaN\n2242193.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNote that this operation aligned the data horizontally (by columns) but not vertically (by rows). In this example, we end up with multiple rows having the same index (eg. 3). Pandas handles this rather gracefully:\n\n\nCode\nresult_concat.loc[3]\n\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\n\n\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\n\n\n3\nCalifornia\nSan Francisco\nNaN\nNaN\n808976.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nOr you can tell Pandas to just ignore the index:\n\n\nCode\npd.concat([city_loc, city_pop], ignore_index=True)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\ncity\nlat\nlng\npopulation\n\n\n\n\n0\nCA\nSan Francisco\n37.781334\n-122.416728\nNaN\n\n\n1\nNY\nNew York\n40.705649\n-74.008344\nNaN\n\n\n2\nFL\nMiami\n25.791100\n-80.320733\nNaN\n\n\n3\nOH\nCleveland\n41.473508\n-81.739791\nNaN\n\n\n4\nUT\nSalt Lake City\n40.755851\n-111.896657\nNaN\n\n\n5\nCalifornia\nSan Francisco\nNaN\nNaN\n808976.0\n\n\n6\nNew-York\nNew York\nNaN\nNaN\n8363710.0\n\n\n7\nFlorida\nMiami\nNaN\nNaN\n413201.0\n\n\n8\nTexas\nHouston\nNaN\nNaN\n2242193.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNotice that when a column does not exist in a DataFrame, it acts as if it was filled with NaN values. If we set join=\"inner\", then only columns that exist in both DataFrames are returned:\n\n\nCode\npd.concat([city_loc, city_pop], join=\"inner\")\n\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\ncity\n\n\n\n\n0\nCA\nSan Francisco\n\n\n1\nNY\nNew York\n\n\n2\nFL\nMiami\n\n\n3\nOH\nCleveland\n\n\n4\nUT\nSalt Lake City\n\n\n3\nCalifornia\nSan Francisco\n\n\n4\nNew-York\nNew York\n\n\n5\nFlorida\nMiami\n\n\n6\nTexas\nHouston\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4.2.4.17 Categories\nIt is quite frequent to have values that represent categories, for example 1 for female and 2 for male, or \"A\" for Good, \"B\" for Average, \"C\" for Bad. These categorical values can be hard to read and cumbersome to handle, but fortunately pandas makes it easy. To illustrate this, let’s take the city_pop DataFrame we created earlier, and add a column that represents a category:\n\n\nCode\ncity_eco = city_pop.copy()\ncity_eco[\"eco_code\"] = [17, 17, 34, 20]\ncity_eco\n\n\n\n  \n    \n      \n\n\n\n\n\n\npopulation\ncity\nstate\neco_code\n\n\n\n\n3\n808976\nSan Francisco\nCalifornia\n17\n\n\n4\n8363710\nNew York\nNew-York\n17\n\n\n5\n413201\nMiami\nFlorida\n34\n\n\n6\n2242193\nHouston\nTexas\n20\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nRight now the eco_code column is full of apparently meaningless codes. Let’s fix that. First, we will create a new categorical column based on the eco_codes:\n\n\nCode\ncity_eco[\"economy\"] = city_eco[\"eco_code\"].astype('category')\ncity_eco[\"economy\"].cat.categories\n\n\nInt64Index([17, 20, 34], dtype='int64')\n\n\nNow we can give each category a meaningful name:\n\n\nCode\ncity_eco[\"economy\"].cat.categories = [\"Finance\", \"Energy\", \"Tourism\"]\ncity_eco\n\n\n\n  \n    \n      \n\n\n\n\n\n\npopulation\ncity\nstate\neco_code\neconomy\n\n\n\n\n3\n808976\nSan Francisco\nCalifornia\n17\nFinance\n\n\n4\n8363710\nNew York\nNew-York\n17\nFinance\n\n\n5\n413201\nMiami\nFlorida\n34\nTourism\n\n\n6\n2242193\nHouston\nTexas\n20\nEnergy\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNote that categorical values are sorted according to their categorical order, not their alphabetical order:\n\n\nCode\ncity_eco.sort_values(by=\"economy\", ascending=False)\n\n\n\n  \n    \n      \n\n\n\n\n\n\npopulation\ncity\nstate\neco_code\neconomy\n\n\n\n\n5\n413201\nMiami\nFlorida\n34\nTourism\n\n\n6\n2242193\nHouston\nTexas\n20\nEnergy\n\n\n3\n808976\nSan Francisco\nCalifornia\n17\nFinance\n\n\n4\n8363710\nNew York\nNew-York\n17\nFinance"
  },
  {
    "objectID": "03_Relational_Database_and_data_wrangling.html#what-next",
    "href": "03_Relational_Database_and_data_wrangling.html#what-next",
    "title": "4  Relational Database and data wrangling",
    "section": "4.3 What next?",
    "text": "4.3 What next?\nAs you probably noticed by now, pandas is quite a large library with many features. Although we went through the most important features, there is still a lot to discover. Probably the best way to learn more is to get your hands dirty with some real-life data. It is also a good idea to go through pandas’ excellent documentation, in particular the Cookbook.\nYou can also work with BigQuery in Pandas. Check out https://pandas.pydata.org/docs/reference/api/pandas.read_gbq.html and https://pandas-gbq.readthedocs.io/en/latest/ for more details."
  },
  {
    "objectID": "03_Relational_Database_and_data_wrangling.html#references",
    "href": "03_Relational_Database_and_data_wrangling.html#references",
    "title": "4  Relational Database and data wrangling",
    "section": "4.4 References",
    "text": "4.4 References\n\nhttps://www.kaggle.com/learn/\nhttps://github.com/ageron/handson-ml3\nhttps://github.com/ageron/handson-ml3"
  },
  {
    "objectID": "04_Clean_feature_engineering.html",
    "href": "04_Clean_feature_engineering.html",
    "title": "5  Data cleaning and feature engineering",
    "section": "",
    "text": "6 References"
  },
  {
    "objectID": "04_Clean_feature_engineering.html#setup",
    "href": "04_Clean_feature_engineering.html#setup",
    "title": "5  Data cleaning and feature engineering",
    "section": "5.1 Setup",
    "text": "5.1 Setup\n\n\nCode\n!pip install fancyimpute -q\n!pip install thefuzz -q\n!pip install --upgrade xlrd -q\n!pip install category_encoders -q\n!pip install cleanlab -q\n\n\n\n\nCode\n# Scientific computing\nimport numpy as np\nfrom numpy.random import multivariate_normal\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# Modeling\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\n# Imputing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer # To use IterativeImputer, you need to explicitly import enable_iterative_imputer.\nfrom sklearn.impute import IterativeImputer\nfrom fancyimpute import SoftImpute\n\n# helpful character encoding module\nimport chardet\nfrom thefuzz import fuzz\nfrom thefuzz import process\n\n# Normalization\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import QuantileTransformer\n\n# Encoding\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom category_encoders import MEstimateEncoder\n\n# Fix labels\nimport cleanlab\nfrom cleanlab.classification import CleanLearning\nfrom cleanlab.benchmarking import noise_generation\n\n\n\n\nCode\n## Make synthetic dataset for cleanlab\n\nSEED = 0\n\ndef make_data(\n    means=[[3, 2], [7, 7], [0, 8], [0, 10]],\n    covs=[\n        [[5, -1.5], [-1.5, 1]],\n        [[1, 0.5], [0.5, 4]],\n        [[5, 1], [1, 5]],\n        [[3, 1], [1, 1]],\n    ],\n    sizes=[100, 50, 50, 50],\n    avg_trace=0.8,\n    seed=SEED,  # set to None for non-reproducible randomness\n):\n    np.random.seed(seed=SEED)\n\n    K = len(means)  # number of classes\n    data = []\n    labels = []\n    test_data = []\n    test_labels = []\n\n    for idx in range(K):\n        data.append(\n            np.random.multivariate_normal(\n                mean=means[idx], cov=covs[idx], size=sizes[idx]\n            )\n        )\n        test_data.append(\n            np.random.multivariate_normal(\n                mean=means[idx], cov=covs[idx], size=sizes[idx]\n            )\n        )\n        labels.append(np.array([idx for i in range(sizes[idx])]))\n        test_labels.append(np.array([idx for i in range(sizes[idx])]))\n    X_train = np.vstack(data)\n    y_train = np.hstack(labels)\n    X_test = np.vstack(test_data)\n    y_test = np.hstack(test_labels)\n\n    # Compute p(y=k) the prior distribution over true labels.\n    py_true = np.bincount(y_train) / float(len(y_train))\n\n    noise_matrix_true = noise_generation.generate_noise_matrix_from_trace(\n        K,\n        trace=avg_trace * K,\n        py=py_true,\n        valid_noise_matrix=True,\n        seed=SEED,\n    )\n\n    # Generate our noisy labels using the noise_marix.\n    s = noise_generation.generate_noisy_labels(y_train, noise_matrix_true)\n    s_test = noise_generation.generate_noisy_labels(y_test, noise_matrix_true)\n    ps = np.bincount(s) / float(len(s))  # Prior distribution over noisy labels\n\n    return {\n        \"data\": X_train,\n        \"true_labels\": y_train,  # You never get to see these perfect labels.\n        \"labels\": s,  # Instead, you have these labels, which have some errors.\n        \"test_data\": X_test,\n        \"test_labels\": y_test,  # Perfect labels used for \"true\" measure of model's performance during deployment.\n        \"noisy_test_labels\": s_test,  # With IID train/test split, you'd have these labels, which also have some errors.\n        \"ps\": ps,\n        \"py_true\": py_true,\n        \"noise_matrix_true\": noise_matrix_true,\n        \"class_names\": [\"purple\", \"blue\", \"seafoam green\", \"yellow\"],\n    }\n\n\n# Display dataset visually using matplotlib\ndef plot_data(data, circles, title, alpha=1.0):\n    plt.figure(figsize=(14, 5))\n    plt.scatter(data[:, 0], data[:, 1], c=labels, s=60)\n    for i in circles:\n        plt.plot(\n            data[i][0],\n            data[i][1],\n            \"o\",\n            markerfacecolor=\"none\",\n            markeredgecolor=\"red\",\n            markersize=14,\n            markeredgewidth=2.5,\n            alpha=alpha\n        )\n    _ = plt.title(title, fontsize=25)"
  },
  {
    "objectID": "04_Clean_feature_engineering.html#exploratory-data-analysis",
    "href": "04_Clean_feature_engineering.html#exploratory-data-analysis",
    "title": "5  Data cleaning and feature engineering",
    "section": "5.2 Exploratory Data Analysis",
    "text": "5.2 Exploratory Data Analysis\nYou can checkout some of useful EDA tools pandas-profiling, dataprep, lux or dtale"
  },
  {
    "objectID": "04_Clean_feature_engineering.html#handling-missing-value",
    "href": "04_Clean_feature_engineering.html#handling-missing-value",
    "title": "5  Data cleaning and feature engineering",
    "section": "5.3 Handling missing value",
    "text": "5.3 Handling missing value\nIn this section, you’ll learn why you’ve run into the data cleaning problems and, more importantly, how to fix them! Specifically, you’ll learn how to tackle some of the most common data cleaning problems so you can get to actually analyzing your data faster.\n\n5.3.1 Take a first look at the data\nFor demonstration, we’ll use a dataset of events that occured in American Football games. You’ll apply your new skills to a dataset of building permits issued in San Francisco. The dataset that we will use was made available by Kaggle. You can download the original dataset from https://www.kaggle.com/datasets/maxhorowitz/nflplaybyplay2009to2016.\nBut you can also use Kaggle API. First, you need to create a Kaggle API key and download it to your local machine. Just navigate to the Kaggle website in a web browser, log in, and go to the My Account page. In your account settings, you’ll find an API section. Clicking the Create New API Token button will generate a kaggle.json key file and will download it to your machine.\n\n\n\n\n\nCode\n# Upload the API’s key JSON file to your Colab\n# session by running the following code in a notebook cell:\nfrom google.colab import files\nfiles.upload()\n\n\n\n\nCode\n!mkdir ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n\n\n\nIn some dataset, when the first time you try to download the data, you may get a “403 Forbidden” error. That’s because you need to accept the terms associated with the dataset before you download it and click the “I Understand and Accept button”. You only need to do this once.\n\n\n\nCode\n!kaggle datasets download -d maxhorowitz/nflplaybyplay2009to2016\n\n\n\n\nCode\n!unzip -qq nflplaybyplay2009to2016\n\n\n\n\nCode\n# read in all our data\nnfl_data = pd.read_csv(\"NFL Play by Play 2009-2017 (v4).csv\")\n\n# set seed for reproducibility\nnp.random.seed(0) \n\n\nThe first thing to do when you get a new dataset is take a look at some of it. This lets you see that it all read in correctly and gives an idea of what’s going on with the data. In this case, let’s see if there are any missing values, which will be reprsented with NaN or None.\n\n\nCode\n# look at the first five rows of the nfl_data file. \n# I can see a handful of missing data already!\nnfl_data.head()\n\n\n\n\nCode\nnfl_data.shape\n\n\n(407688, 102)\n\n\n\n\n5.3.2 How many missing data points do we have?\nOk, now we know that we do have some missing values. Let’s see how many we have in each column.\n\n\nCode\n# get the number of missing data points per column\nmissing_values_count = nfl_data.isnull().sum()\n\n# look at the # of missing points in the first ten columns\nmissing_values_count[0:10]\n\n\nDate                0\nGameID              0\nDrive               0\nqtr                 0\ndown            61154\ntime              224\nTimeUnder           0\nTimeSecs          224\nPlayTimeDiff      444\nSideofField       528\ndtype: int64\n\n\n\n\nCode\n# how many total missing values do we have?\ntotal_cells = np.product(nfl_data.shape)\ntotal_missing = missing_values_count.sum()\n\n# percent of data that is missing\npercent_missing = (total_missing/total_cells) * 100\nprint(percent_missing)\n\n\n24.87214126835169\n\n\nAlmost a quarter of the cells in this dataset are empty! In the next step, we’re going to take a closer look at some of the columns with missing values and try to figure out what might be going on with them.\nLooking at the number of missing values in the nfl_data dataframe, we notice that the column TimesSecs has missing values in it. By looking at the documentation, we can see that this column has information on the number of seconds left in the game when the play was made. This means that these values are probably missing because they were not recorded, rather than because they don’t exist. So, it would make sense for us to try and guess what they should be rather than just leaving them as NA’s.\nOn the other hand, there are other fields, like PenalizedTeam that also have lot of missing fields. In this case, though, the field is missing because if there was no penalty then it doesn’t make sense to say which team was penalized. For this column, it would make more sense to either leave it empty or to add a third value like “neither” and use that to replace the NA’s.\nWe’ll cover some “quick and dirty” techniques that can help you with missing values but will probably also end up removing some useful information or adding some noise to your data.\n\n\n5.3.3 Drop missing values\nIf you’re sure you want to drop rows with missing values, pandas does have a handy function, dropna() to help you do this. Let’s try it out on our NFL dataset!\n\n\nCode\n# remove all the rows that contain a missing value results in empty dataset\n# This is because every row in our dataset had at least one missing value. \n# We might have better luck removing all the columns that have at least one missing value instead.\nnfl_data.dropna()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nDate\nGameID\nDrive\nqtr\ndown\ntime\nTimeUnder\nTimeSecs\nPlayTimeDiff\nSideofField\n...\nyacEPA\nHome_WP_pre\nAway_WP_pre\nHome_WP_post\nAway_WP_post\nWin_Prob\nWPA\nairWPA\nyacWPA\nSeason\n\n\n\n\n\n\n\n0 rows × 102 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n# remove all columns with at least one missing value\ncolumns_with_na_dropped = nfl_data.dropna(axis=1)\ncolumns_with_na_dropped.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nDate\nGameID\nDrive\nqtr\nTimeUnder\nydstogo\nydsnet\nPlayAttempted\nYards.Gained\nsp\n...\nTimeout_Indicator\nTimeout_Team\nposteam_timeouts_pre\nHomeTimeouts_Remaining_Pre\nAwayTimeouts_Remaining_Pre\nHomeTimeouts_Remaining_Post\nAwayTimeouts_Remaining_Post\nExPoint_Prob\nTwoPoint_Prob\nSeason\n\n\n\n\n0\n2009-09-10\n2009091000\n1\n1\n15\n0\n0\n1\n39\n0\n...\n0\nNone\n3\n3\n3\n3\n3\n0.0\n0.0\n2009\n\n\n1\n2009-09-10\n2009091000\n1\n1\n15\n10\n5\n1\n5\n0\n...\n0\nNone\n3\n3\n3\n3\n3\n0.0\n0.0\n2009\n\n\n2\n2009-09-10\n2009091000\n1\n1\n15\n5\n2\n1\n-3\n0\n...\n0\nNone\n3\n3\n3\n3\n3\n0.0\n0.0\n2009\n\n\n3\n2009-09-10\n2009091000\n1\n1\n14\n8\n2\n1\n0\n0\n...\n0\nNone\n3\n3\n3\n3\n3\n0.0\n0.0\n2009\n\n\n4\n2009-09-10\n2009091000\n1\n1\n14\n8\n2\n1\n0\n0\n...\n0\nNone\n3\n3\n3\n3\n3\n0.0\n0.0\n2009\n\n\n\n\n\n5 rows × 41 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n# just how much data did we lose?\nprint(\"Columns in original dataset: %d \\n\" % nfl_data.shape[1])\nprint(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])\n\n\nColumns in original dataset: 102 \n\nColumns with na's dropped: 41\n\n\nNotice that this can drop a lot of data that you might want to keep, particularly in smaller datasets. What if you just want to drop rows or columns that contain several or even just all null values? You specify those setting in dropna with the how and thresh parameters.\nBy default, how='any'. You could alternatively specify how='all' so as to drop only rows or columns that contain all null values. The threshparameter gives you finer-grained control: you set the number of non-null values that a row or column needs to have in order to be kept.\n\n\nCode\ndf1 = pd.DataFrame([[ 1, np.nan, 7], \n                    [ 2,  5,  8], \n                    [ np.nan, 6, 9]])\ndf1[3] = np.nan\ndf1\n\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.0\nNaN\n7\nNaN\n\n\n1\n2.0\n5.0\n8\nNaN\n\n\n2\nNaN\n6.0\n9\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n# the `thresh` parameter specifies the minimum number of non-null values required in a row or column for it to be retained.\ndf1.dropna(thresh=3)\n\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n1\n2.0\n5.0\n8\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nHere, the first and last row have been dropped, because they contain only two non-null values.\n\n\n5.3.4 Filling in missing values automatically\nDepending on your dataset, it can sometimes make more sense to fill null values with valid ones rather than drop them. Pandas provides fillna(), which returns a copy of the Series or DataFrame with the missing values replaced with one of your choosing.\n\n\nCode\n# You can fill all of the null entries with a single value, such as -9999:\ndf1.fillna(-9999)\n\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.0\n-9999.0\n7\n-9999.0\n\n\n1\n2.0\n5.0\n8\n-9999.0\n\n\n2\n-9999.0\n6.0\n9\n-9999.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe could also replace missing values with whatever value comes directly after/before it in the same column. (This makes a lot of sense for datasets where the observations have some sort of logical order to them.)\nYou can forward-fill null values, which is to use the last valid value to fill a null:\n\n\nCode\ndf1.fillna(method='ffill', axis=0)\n\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.0\nNaN\n7\nNaN\n\n\n1\n2.0\n5.0\n8\nNaN\n\n\n2\n2.0\n6.0\n9\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nBackward-fill to propagate the next valid value backward to fill a null:\n\n\nCode\ndf1.fillna(method='bfill', axis=0)\n\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.0\n5.0\n7\nNaN\n\n\n1\n2.0\n5.0\n8\nNaN\n\n\n2\nNaN\n6.0\n9\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNotice that when a previous/next value is not available for forward/backward-filling, the null value remains.\n\n\n5.3.5 Imputation of missing value\n\n5.3.5.1 Univariate feature imputation\nThe SimpleImputer class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.\n\n\nCode\ndf2 = pd.DataFrame([[1, 2], [np.nan, 3], [7, 6]])\ndf2\n\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1.0\n2\n\n\n1\nNaN\n3\n\n\n2\n7.0\n6\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean value of the columns that contain the missing values:\n\n\nCode\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp.fit_transform(df2)\n\n\narray([[1., 2.],\n       [4., 3.],\n       [7., 6.]])\n\n\nThe SimpleImputer class also supports categorical data represented as string values or pandas categoricals when using the most_frequent or constant strategy:\n\n\nCode\ndf2 = pd.DataFrame([[\"a\", \"x\"],\n                    [np.nan, \"y\"],\n                    [\"a\", np.nan],\n                    [\"b\", \"y\"]], \n                   dtype=\"category\")\ndf2\n\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n\n\n\n\n0\na\nx\n\n\n1\nNaN\ny\n\n\n2\na\nNaN\n\n\n3\nb\ny\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nimp = SimpleImputer(strategy=\"most_frequent\")\nprint(imp.fit_transform(df2))\n\n\n[['a' 'x']\n ['a' 'y']\n ['a' 'y']\n ['b' 'y']]\n\n\n\n\n5.3.5.2 Multivariate feature imputation\nA more sophisticated approach is to use the IterativeImputer class, which models each feature with missing values as a function of other features, and uses that estimate for imputation.\n\n\nCode\nimp = IterativeImputer(max_iter=10, random_state=0)\n# the model learns that the second feature is double of the first\nimp.fit_transform([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n\n\narray([[ 1.        ,  2.        ],\n       [ 3.        ,  6.        ],\n       [ 4.        ,  8.        ],\n       [ 1.50004509,  3.        ],\n       [ 7.        , 14.00004135]])\n\n\n\n\nCode\n # You can also use other regressor as well (default is BayesianRidge())\n est = ExtraTreesRegressor(n_estimators=10, random_state=0)\n imp = IterativeImputer(max_iter=10, random_state=0, estimator=est)\n imp.fit_transform([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n\n\narray([[1. , 2. ],\n       [3. , 6. ],\n       [4. , 8. ],\n       [1.6, 3. ],\n       [7. , 8. ]])\n\n\nThe KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors. Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor.\nThe following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean feature value of the two nearest neighbors of samples with missing values:\n\n\nCode\nX = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\nimputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\nimputer.fit_transform(X)\n\n\narray([[1. , 2. , 4. ],\n       [3. , 4. , 3. ],\n       [5.5, 6. , 5. ],\n       [8. , 8. , 7. ]])\n\n\nif you wishes to apply matrix completion to your data, you can use functions from fancyimpute\n\n\nCode\nSoftImpute().fit_transform(X)\n\n\n[SoftImpute] Max Singular Value of X_init = 16.044670\n[SoftImpute] Iter 1: observed MAE=0.129498 rank=3\n[SoftImpute] Iter 2: observed MAE=0.129346 rank=3\n[SoftImpute] Iter 3: observed MAE=0.129795 rank=3\n[SoftImpute] Iter 4: observed MAE=0.131896 rank=3\n[SoftImpute] Iter 5: observed MAE=0.134509 rank=3\n[SoftImpute] Iter 6: observed MAE=0.137663 rank=3\n[SoftImpute] Iter 7: observed MAE=0.141068 rank=3\n[SoftImpute] Iter 8: observed MAE=0.143794 rank=3\n[SoftImpute] Iter 9: observed MAE=0.145304 rank=3\n[SoftImpute] Iter 10: observed MAE=0.145850 rank=3\n[SoftImpute] Iter 11: observed MAE=0.145866 rank=3\n[SoftImpute] Iter 12: observed MAE=0.145914 rank=3\n[SoftImpute] Iter 13: observed MAE=0.146068 rank=3\n[SoftImpute] Iter 14: observed MAE=0.146126 rank=2\n[SoftImpute] Iter 15: observed MAE=0.146125 rank=2\n[SoftImpute] Iter 16: observed MAE=0.146132 rank=2\n[SoftImpute] Iter 17: observed MAE=0.146126 rank=2\n[SoftImpute] Iter 18: observed MAE=0.146092 rank=2\n[SoftImpute] Iter 19: observed MAE=0.146022 rank=2\n[SoftImpute] Iter 20: observed MAE=0.145907 rank=2\n[SoftImpute] Iter 21: observed MAE=0.145740 rank=2\n[SoftImpute] Iter 22: observed MAE=0.145510 rank=2\n[SoftImpute] Iter 23: observed MAE=0.145209 rank=2\n[SoftImpute] Iter 24: observed MAE=0.144824 rank=2\n[SoftImpute] Iter 25: observed MAE=0.144345 rank=2\n[SoftImpute] Iter 26: observed MAE=0.143761 rank=2\n[SoftImpute] Iter 27: observed MAE=0.143059 rank=2\n[SoftImpute] Iter 28: observed MAE=0.142233 rank=2\n[SoftImpute] Iter 29: observed MAE=0.141275 rank=2\n[SoftImpute] Iter 30: observed MAE=0.140185 rank=2\n[SoftImpute] Iter 31: observed MAE=0.138969 rank=2\n[SoftImpute] Iter 32: observed MAE=0.137638 rank=2\n[SoftImpute] Iter 33: observed MAE=0.136213 rank=2\n[SoftImpute] Iter 34: observed MAE=0.134720 rank=2\n[SoftImpute] Iter 35: observed MAE=0.133194 rank=2\n[SoftImpute] Iter 36: observed MAE=0.131669 rank=2\n[SoftImpute] Iter 37: observed MAE=0.130180 rank=2\n[SoftImpute] Iter 38: observed MAE=0.129421 rank=2\n[SoftImpute] Iter 39: observed MAE=0.128890 rank=2\n[SoftImpute] Iter 40: observed MAE=0.128397 rank=2\n[SoftImpute] Iter 41: observed MAE=0.127946 rank=2\n[SoftImpute] Iter 42: observed MAE=0.127542 rank=2\n[SoftImpute] Iter 43: observed MAE=0.127185 rank=2\n[SoftImpute] Iter 44: observed MAE=0.126874 rank=2\n[SoftImpute] Iter 45: observed MAE=0.126605 rank=2\n[SoftImpute] Iter 46: observed MAE=0.126375 rank=2\n[SoftImpute] Iter 47: observed MAE=0.126180 rank=2\n[SoftImpute] Iter 48: observed MAE=0.126016 rank=2\n[SoftImpute] Iter 49: observed MAE=0.125878 rank=2\n[SoftImpute] Iter 50: observed MAE=0.125763 rank=2\n[SoftImpute] Iter 51: observed MAE=0.125668 rank=2\n[SoftImpute] Stopped after iteration 51 for lambda=0.320893\n\n\narray([[1.        , 2.        , 1.29115131],\n       [3.        , 4.        , 3.        ],\n       [5.10495139, 6.        , 5.        ],\n       [8.        , 8.        , 7.        ]])\n\n\nFor more information, please refer to https://github.com/iskandr/fancyimpute or https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute."
  },
  {
    "objectID": "04_Clean_feature_engineering.html#other-data-cleaning-problem",
    "href": "04_Clean_feature_engineering.html#other-data-cleaning-problem",
    "title": "5  Data cleaning and feature engineering",
    "section": "5.4 Other data cleaning problem",
    "text": "5.4 Other data cleaning problem\n\n5.4.1 Duplicate data entry\nIn addition to missing data, you will often encounter duplicated data in real-world datasets. Fortunately, pandas provides an easy means of detecting and removing duplicate entries.\nYou can easily spot duplicate values using the duplicated() method in pandas, which returns a Boolean mask indicating whether an entry in a DataFrame is a duplicate of an earlier one. Let’s create another example DataFrame to see this in action.\n\n\nCode\ndf3 = pd.DataFrame({'letters': ['A','B'] * 2 + ['B'],\n                    'numbers': [1, 2, 1, 3, 3]})\ndf3\n\n\n\n  \n    \n      \n\n\n\n\n\n\nletters\nnumbers\n\n\n\n\n0\nA\n1\n\n\n1\nB\n2\n\n\n2\nA\n1\n\n\n3\nB\n3\n\n\n4\nB\n3\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\ndf3.duplicated()\n\n\n0    False\n1    False\n2     True\n3    False\n4     True\ndtype: bool\n\n\ndrop_duplicates() will simply returns a copy of the data for which all of the duplicated values are False:\n\n\nCode\ndf3.drop_duplicates()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nletters\nnumbers\n\n\n\n\n0\nA\n1\n\n\n1\nB\n2\n\n\n3\nB\n3\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nBoth duplicated() and drop_duplicates() default to consider all columns but you can specify that they examine only a subset of columns in your DataFrame:\n\n\nCode\ndf3.drop_duplicates(['letters'])\n\n\n\n  \n    \n      \n\n\n\n\n\n\nletters\nnumbers\n\n\n\n\n0\nA\n1\n\n\n1\nB\n2\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n5.4.2 Inconsistent data entry\nWe will use the dataset that is modified from https://www.kaggle.com/datasets/zusmani/pakistanintellectualcapitalcs.\n\n\nCode\n!kaggle datasets download -d alexisbcook/pakistan-intellectual-capital\n!unzip -qq pakistan-intellectual-capital.zip\n\n\nDownloading pakistan-intellectual-capital.zip to /content\n  0% 0.00/47.8k [00:00&lt;?, ?B/s]\n100% 47.8k/47.8k [00:00&lt;00:00, 25.0MB/s]\n\n\n\n\nCode\n# read in all our data\nprofessors = pd.read_csv(\"pakistan_intellectual_capital.csv\")\n\n\n\n\nCode\nprofessors.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nUnnamed: 0\nS#\nTeacher Name\nUniversity Currently Teaching\nDepartment\nProvince University Located\nDesignation\nTerminal Degree\nGraduated from\nCountry\nYear\nArea of Specialization/Research Interests\nOther Information\n\n\n\n\n0\n2\n3\nDr. Abdul Basit\nUniversity of Balochistan\nComputer Science & IT\nBalochistan\nAssistant Professor\nPhD\nAsian Institute of Technology\nThailand\nNaN\nSoftware Engineering & DBMS\nNaN\n\n\n1\n4\n5\nDr. Waheed Noor\nUniversity of Balochistan\nComputer Science & IT\nBalochistan\nAssistant Professor\nPhD\nAsian Institute of Technology\nThailand\nNaN\nDBMS\nNaN\n\n\n2\n5\n6\nDr. Junaid Baber\nUniversity of Balochistan\nComputer Science & IT\nBalochistan\nAssistant Professor\nPhD\nAsian Institute of Technology\nThailand\nNaN\nInformation processing, Multimedia mining\nNaN\n\n\n3\n6\n7\nDr. Maheen Bakhtyar\nUniversity of Balochistan\nComputer Science & IT\nBalochistan\nAssistant Professor\nPhD\nAsian Institute of Technology\nThailand\nNaN\nNLP, Information Retrieval, Question Answering...\nNaN\n\n\n4\n24\n25\nSamina Azim\nSardar Bahadur Khan Women's University\nComputer Science\nBalochistan\nLecturer\nBS\nBalochistan University of Information Technolo...\nPakistan\n2005.0\nVLSI Electronics DLD Database\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nSay we’re interested in cleaning up the Country column to make sure there’s no data entry inconsistencies in it. We could go through and check each row by hand, of course, and hand-correct inconsistencies when we find them. There’s a more efficient way to do this, though!\n\n\nCode\n# get all the unique values in the 'Country' column\ncountries = professors['Country'].unique()\n\n# sort them and then take a closer look\ncountries.sort()\ncountries\n\n\narray([' Germany', ' New Zealand', ' Sweden', ' USA', 'Australia',\n       'Austria', 'Canada', 'China', 'Finland', 'France', 'Greece',\n       'HongKong', 'Ireland', 'Italy', 'Japan', 'Macau', 'Malaysia',\n       'Mauritius', 'Netherland', 'New Zealand', 'Norway', 'Pakistan',\n       'Portugal', 'Russian Federation', 'Saudi Arabia', 'Scotland',\n       'Singapore', 'South Korea', 'SouthKorea', 'Spain', 'Sweden',\n       'Thailand', 'Turkey', 'UK', 'USA', 'USofA', 'Urbana', 'germany'],\n      dtype=object)\n\n\nJust looking at this, we can see some problems due to inconsistent data entry: ’ Germany’, and ‘germany’, for example, or ’ New Zealand’ (start wirh whitespace) and ‘New Zealand’.\nThe first thing we are going to do is make everything lower case (we can change it back at the end if we like) and remove any white spaces at the beginning and end of cells. Inconsistencies in capitalizations and trailing white spaces are very common in text data and you can fix a good 80% of your text data entry inconsistencies by doing this.\n\n\nCode\n# convert to lower case\nprofessors['Country'] = professors['Country'].str.lower()\n# remove trailing white spaces\nprofessors['Country'] = professors['Country'].str.strip()\n\n\nNote that .str() provide vectorized method for columns. See https://realpython.com/python-data-cleaning-numpy-pandas/#tidying-up-fields-in-the-data for more details.\n\n5.4.2.1 Use fuzzy matching to correct inconsistent data entry\nAlright, let’s take another look at the Country column and see if there’s any more data cleaning we need to do\n\n\nCode\n# get all the unique values in the 'Country' column\ncountries = professors['Country'].unique()\n\n# sort them and then take a closer look\ncountries.sort()\ncountries\n\n\narray(['australia', 'austria', 'canada', 'china', 'finland', 'france',\n       'germany', 'greece', 'hongkong', 'ireland', 'italy', 'japan',\n       'macau', 'malaysia', 'mauritius', 'netherland', 'new zealand',\n       'norway', 'pakistan', 'portugal', 'russian federation',\n       'saudi arabia', 'scotland', 'singapore', 'south korea',\n       'southkorea', 'spain', 'sweden', 'thailand', 'turkey', 'uk',\n       'urbana', 'usa', 'usofa'], dtype=object)\n\n\nIt does look like there is another inconsistency: ‘southkorea’ and ‘south korea’ should be the same. We’re going to use the thefuzz package to help identify which strings are closest to each other. This dataset is small enough that we could probably correct errors by hand, but that approach doesn’t scale well. (Would you want to correct a thousand errors by hand? What about ten thousand? Automating things as early as possible is generally a good idea!)\nthefuzz returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. Here, we’re going to get the ten strings from our list of cities that have the closest distance to “south korea”\n\n\nCode\n# get the top 10 closest matches to \"south korea\"\nmatches = process.extract(\"south korea\", countries, limit=10)\n\n# take a look at them\nmatches\n\n\n[('south korea', 100),\n ('southkorea', 95),\n ('ireland', 50),\n ('norway', 50),\n ('uk', 45),\n ('austria', 44),\n ('saudi arabia', 43),\n ('scotland', 42),\n ('australia', 40),\n ('france', 40)]\n\n\nWe can see that two of the items in the cities are very close to “south korea”: “south korea” and “southkorea”. Let’s replace all rows in our “Country” column that have a score &gt; 90 with “south korea”.\nTo do this, we are going to write a function.\n\n\nCode\n# function to replace rows in the provided column of the provided dataframe\n# that match the provided string above the provided ratio with the provided string\ndef replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n    # get a list of unique strings\n    strings = df[column].unique()\n    \n    # get the top 10 closest matches to our input string\n    matches = process.extract(string_to_match, strings, limit=10)\n\n    # only get matches with a ratio &gt; 90\n    close_matches = [matches[0] for matches in matches if matches[1] &gt;= min_ratio]\n\n    # get the rows of all the close matches in our dataframe\n    rows_with_matches = df[column].isin(close_matches)\n\n    # replace all rows with close matches with the input matches \n    df.loc[rows_with_matches, column] = string_to_match\n    \n    # let us know the function's done\n    print(\"All done!\")\n\n\nNow that we have a function, we can put it to the test!\n\n\nCode\n# use the function we just wrote to replace close matches to \"south korea\" with \"south korea\"\nreplace_matches_in_column(df=professors, column='Country', string_to_match=\"south korea\")\n\n\nAll done!\n\n\nAnd now let’s check the unique values in our “Country” column again and make sure we’ve tidied up “south korea” correctly.\n\n\nCode\n# get all the unique values in the 'Country' column\ncountries = professors['Country'].unique()\n\n# sort them alphabetically and then take a closer look\ncountries.sort()\ncountries\n\n\narray(['australia', 'austria', 'canada', 'china', 'finland', 'france',\n       'germany', 'greece', 'hongkong', 'ireland', 'italy', 'japan',\n       'macau', 'malaysia', 'mauritius', 'netherland', 'new zealand',\n       'norway', 'pakistan', 'portugal', 'russian federation',\n       'saudi arabia', 'scotland', 'singapore', 'south korea', 'spain',\n       'sweden', 'thailand', 'turkey', 'uk', 'urbana', 'usa', 'usofa'],\n      dtype=object)\n\n\nNow we only have “south korea” in our dataframe and we didn’t have to change anything by hand.\n\n\n\n5.4.3 Character encoding\nThere are two main data types you’ll encounter when working with text in Python 3. One is is the string, which is what text is by default.\n\n\nCode\n# start with a string\nbefore = \"This is the euro symbol: €\"\n\n# check to see what datatype it is\ntype(before)\n\n\nstr\n\n\nThe other data is the bytes data type, which is a sequence of integers. You can convert a string into bytes by specifying which encoding it’s in:\n\n\nCode\n# encode it to a different encoding, replacing characters that raise errors\nafter = before.encode(\"utf-8\", errors=\"replace\")\n\n# check the type\ntype(after)\n\n\nbytes\n\n\nIf you look at a bytes object, you’ll see that it has a b in front of it, and then maybe some text after. That’s because bytes are printed out as if they were characters encoded in UTF-8. Here you can see that our euro symbol has been replaced with some mojibake that looks like “” when it’s printed as if it were an UTF-8 string\n\n\nCode\n# take a look at what the bytes look like\nafter\n\n\nb'This is the euro symbol: \\xe2\\x82\\xac'\n\n\nWhen we convert our bytes back to a string with the correct encoding, we can see that our text is all there correctly, which is great! :)\n\n\nCode\n# convert it back to utf-8\nprint(after.decode(\"utf-8\"))\n\n\nThis is the euro symbol: €\n\n\nHowever, when we try to use a different encoding to map our bytes into a string, we get an error. This is because the encoding we’re trying to use doesn’t know what to do with the bytes we’re trying to pass it. You need to tell Python the encoding that the byte string is actually supposed to be in\n\n\nCode\n# try to decode our bytes with the ascii encoding\nprint(after.decode(\"ascii\"))\n\n\nUnicodeDecodeError: ignored\n\n\nThe best time to convert non UTF-8 input into UTF-8 is when you read in files, which we’ll talk about next.\n\n\n5.4.4 Reading in files with encoding problems\nMost files you’ll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won’t run into problems. However, sometimes you’ll get an error like this:\n\n\nCode\n!kaggle datasets download -d kemical/kickstarter-projects\n!unzip -qq kickstarter-projects.zip\n\n\nDownloading kickstarter-projects.zip to /content\n 68% 25.0M/36.8M [00:00&lt;00:00, 128MB/s] \n100% 36.8M/36.8M [00:00&lt;00:00, 142MB/s]\n\n\n\n\nCode\n# try to read in a file not in UTF-8\nkickstarter_2016 = pd.read_csv(\"ks-projects-201612.csv\")\n\n\nUnicodeDecodeError: ignored\n\n\nNotice that we get the same UnicodeDecodeError we got when we tried to decode UTF-8 bytes! This tells us that this file isn’t actually UTF-8. We don’t know what encoding it actually is though. One way to figure it out is to try and test a bunch of different character encodings and see if any of them work. A better way, though, is to use the chardet module to try and automatically guess what the right encoding is. It’s not 100% guaranteed to be right, but it’s usually faster than just trying to guess.\nWe are going to just look at the first ten thousand bytes of this file. This is usually enough for a good guess about what the encoding is and is much faster than trying to look at the whole file. (Especially with a large file this can be very slow.)\n\n\nCode\n# look at the first ten thousand bytes to guess the character encoding\nwith open(\"ks-projects-201612.csv\", 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(10000))\n\n# check what the character encoding might be\nprint(result)\n\n\n{'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n\n\nSo chardet is 73% confidence that the right encoding is “Windows-1252”. Let’s see if that’s correct:\n\n\nCode\n# read in the file with the encoding detected by chardet\nkickstarter_2016 = pd.read_csv(\"ks-projects-201612.csv\", encoding='Windows-1252')\n\n# look at the first few lines\nkickstarter_2016.head()\n\n\n/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (13,14,15) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nID\nname\ncategory\nmain_category\ncurrency\ndeadline\ngoal\nlaunched\npledged\nstate\nbackers\ncountry\nusd pledged\nUnnamed: 13\nUnnamed: 14\nUnnamed: 15\nUnnamed: 16\n\n\n\n\n0\n1000002330\nThe Songs of Adelaide & Abullah\nPoetry\nPublishing\nGBP\n2015-10-09 11:36:00\n1000\n2015-08-11 12:12:28\n0\nfailed\n0\nGB\n0\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1000004038\nWhere is Hank?\nNarrative Film\nFilm & Video\nUSD\n2013-02-26 00:20:50\n45000\n2013-01-12 00:20:50\n220\nfailed\n3\nUS\n220\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1000007540\nToshiCapital Rekordz Needs Help to Complete Album\nMusic\nMusic\nUSD\n2012-04-16 04:24:11\n5000\n2012-03-17 03:24:11\n1\nfailed\n1\nUS\n1\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1000011046\nCommunity Film Project: The Art of Neighborhoo...\nFilm & Video\nFilm & Video\nUSD\n2015-08-29 01:00:00\n19500\n2015-07-04 08:35:03\n1283\ncanceled\n14\nUS\n1283\nNaN\nNaN\nNaN\nNaN\n\n\n4\n1000014025\nMonarch Espresso Bar\nRestaurants\nFood\nUSD\n2016-04-01 13:38:27\n50000\n2016-02-26 13:38:27\n52375\nsuccessful\n224\nUS\n52375\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYep, looks like chardet was right! The file reads in with no problem (although we do get a warning about datatypes) and when we look at the first few rows it seems to be fine.\nWhat if the encoding chardet guesses isn’t right? Since chardet is basically just a fancy guesser, sometimes it will guess the wrong encoding. One thing you can try is looking at more or less of the file and seeing if you get a different result and then try that.\n\n\n5.4.5 Saving your files with UTF-8 encoding\nFinally, once you’ve gone through all the trouble of getting your file into UTF-8, you’ll probably want to keep it that way. The easiest way to do that is to save your files with UTF-8 encoding. The good news is, since UTF-8 is the standard encoding in Python, when you save a file it will be saved as UTF-8 by default:\n\n\nCode\n# save our file (will be saved as UTF-8 by default!)\nkickstarter_2016.to_csv(\"ks-projects-201612-utf8.csv\")\n\n\n\n\nCode\n# try to read in a file not in UTF-8\nkickstarter_2016 = pd.read_csv(\"ks-projects-201612-utf8.csv\")\nkickstarter_2016.head()\n\n\n/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (14,15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nUnnamed: 0\nID\nname\ncategory\nmain_category\ncurrency\ndeadline\ngoal\nlaunched\npledged\nstate\nbackers\ncountry\nusd pledged\nUnnamed: 13\nUnnamed: 14\nUnnamed: 15\nUnnamed: 16\n\n\n\n\n0\n0\n1000002330\nThe Songs of Adelaide & Abullah\nPoetry\nPublishing\nGBP\n2015-10-09 11:36:00\n1000\n2015-08-11 12:12:28\n0\nfailed\n0\nGB\n0\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1\n1000004038\nWhere is Hank?\nNarrative Film\nFilm & Video\nUSD\n2013-02-26 00:20:50\n45000\n2013-01-12 00:20:50\n220\nfailed\n3\nUS\n220\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2\n1000007540\nToshiCapital Rekordz Needs Help to Complete Album\nMusic\nMusic\nUSD\n2012-04-16 04:24:11\n5000\n2012-03-17 03:24:11\n1\nfailed\n1\nUS\n1\nNaN\nNaN\nNaN\nNaN\n\n\n3\n3\n1000011046\nCommunity Film Project: The Art of Neighborhoo...\nFilm & Video\nFilm & Video\nUSD\n2015-08-29 01:00:00\n19500\n2015-07-04 08:35:03\n1283\ncanceled\n14\nUS\n1283\nNaN\nNaN\nNaN\nNaN\n\n\n4\n4\n1000014025\nMonarch Espresso Bar\nRestaurants\nFood\nUSD\n2016-04-01 13:38:27\n50000\n2016-02-26 13:38:27\n52375\nsuccessful\n224\nUS\n52375\nNaN\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "04_Clean_feature_engineering.html#scaling-and-normalization",
    "href": "04_Clean_feature_engineering.html#scaling-and-normalization",
    "title": "5  Data cleaning and feature engineering",
    "section": "5.5 Scaling and normalization",
    "text": "5.5 Scaling and normalization\n\n5.5.1 Standardization\nBy scaling your variables, you can help compare different variables on equal footing. The preprocessing module provides the StandardScaler utility class, which is a quick and easy way to perform the following operation on an array-like dataset.\n\n\nCode\n# Uncpmment below if you are using Kaggle\n#!pip install gdown\n\n\n\n\nCode\n!gdown --fuzzy https://drive.google.com/file/d/1HEcKRMe_bpgmQH3vFlRZVDF_gAzYGETE/view?usp=sharing # use --fuzzy so that it directly accept link from google drive \n!gdown --fuzzy https://drive.google.com/file/d/1GQ1z0-aRPCzMQTcc1ckDgwkiF-GNP_t3/view?usp=sharing\n\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1HEcKRMe_bpgmQH3vFlRZVDF_gAzYGETE\nTo: /content/train_preprocessed.csv\n100% 1.14M/1.14M [00:00&lt;00:00, 101MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1GQ1z0-aRPCzMQTcc1ckDgwkiF-GNP_t3\nTo: /content/test_preprocessed.csv\n100% 1.12M/1.12M [00:00&lt;00:00, 112MB/s]\n\n\n\n\nCode\ntrain = pd.read_csv('train_preprocessed.csv')\ntrain_x = train.drop(['target'], axis=1)\ntrain_y = train['target']\ntest_x = pd.read_csv('test_preprocessed.csv')\ntrain_x_saved = train_x.copy()\ntest_x_saved = test_x.copy()\n\ndef load_data():\n    train_x, test_x = train_x_saved.copy(), test_x_saved.copy()\n    return train_x, test_x\n\ntrain\n\n\n\n  \n    \n      \n\n\n\n\n\n\nage\nsex\nheight\nweight\nproduct\namount\nmedical_info_a1\nmedical_info_a2\nmedical_info_a3\nmedical_info_b1\n...\nmedical_keyword_6\nmedical_keyword_7\nmedical_keyword_8\nmedical_keyword_9\nmedical_keyword_10\nyear\nmonth\nday\nyearmonth\ntarget\n\n\n\n\n0\n50\n1\n166.445608\n65.016732\n9\n7000000\n134\n202\n1\n11\n...\n1\n0\n1\n0\n0\n2015\n2\n3\n24182\n0\n\n\n1\n68\n0\n164.334615\n56.544217\n0\n7000000\n438\n263\n3\n14\n...\n0\n1\n1\n0\n0\n2015\n5\n9\n24185\n0\n\n\n2\n77\n1\n167.462917\n54.242267\n2\n6000000\n313\n325\n1\n18\n...\n1\n0\n1\n0\n0\n2016\n2\n13\n24194\n1\n\n\n3\n17\n1\n177.097725\n71.147762\n3\n8000000\n342\n213\n2\n11\n...\n0\n0\n1\n0\n0\n2015\n7\n6\n24187\n0\n\n\n4\n62\n0\n158.165788\n65.240697\n1\n9000000\n327\n102\n0\n14\n...\n0\n1\n1\n1\n0\n2016\n9\n17\n24201\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\n61\n1\n182.729800\n73.393777\n1\n2000000\n189\n232\n7\n17\n...\n0\n0\n1\n1\n0\n2015\n10\n21\n24190\n0\n\n\n9996\n33\n0\n167.701136\n75.006529\n8\n9000\n426\n202\n3\n19\n...\n0\n0\n1\n1\n0\n2015\n5\n28\n24185\n0\n\n\n9997\n44\n0\n145.609998\n47.739397\n8\n1000\n370\n274\n1\n11\n...\n0\n0\n1\n0\n1\n2016\n2\n29\n24194\n0\n\n\n9998\n34\n0\n165.796017\n57.567695\n6\n5000\n291\n105\n1\n13\n...\n1\n1\n1\n1\n0\n2016\n2\n27\n24194\n0\n\n\n9999\n31\n1\n180.301762\n71.425135\n4\n1000000\n288\n454\n4\n13\n...\n1\n0\n1\n0\n0\n2015\n7\n1\n24187\n0\n\n\n\n\n\n10000 rows × 29 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nnum_cols = ['age', 'height', 'weight', 'amount',\n            'medical_info_a1', 'medical_info_a2', 'medical_info_a3', 'medical_info_b1'] # some numerical columns\n\n\n\n\nCode\ntrain_x, test_x = load_data()\nscaler = StandardScaler()\nscaler.fit(train_x[num_cols])\n\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScalerStandardScaler()\n\n\nNotice that you should apply the same transfrom on both training and testing dataset.\n\n\nCode\ntrain_x[num_cols] = scaler.transform(train_x[num_cols])\ntest_x[num_cols] = scaler.transform(test_x[num_cols])\n\n\n\n\nCode\nscaler.mean_,  scaler.scale_\n\n\n(array([4.20265000e+01, 1.65892951e+02, 6.08570495e+01, 3.84084370e+06,\n        2.99101200e+02, 2.49454700e+02, 1.98780000e+00, 1.44192000e+01]),\n array([2.16749209e+01, 9.40817216e+00, 1.07177883e+01, 3.45926743e+06,\n        1.04491031e+02, 1.03710381e+02, 1.41733947e+00, 2.87131875e+00]))\n\n\nScaled data has zero mean and unit variance:\n\n\nCode\ntrain_x[num_cols].mean(axis=0)\n\n\nage                6.679102e-17\nheight            -2.836842e-15\nweight            -2.337686e-16\namount            -3.588241e-17\nmedical_info_a1   -5.684342e-17\nmedical_info_a2   -2.344791e-17\nmedical_info_a3    1.563194e-17\nmedical_info_b1   -3.410605e-17\ndtype: float64\n\n\n\n\nCode\ntrain_x[num_cols].std(axis=0)\n\n\nage                1.00005\nheight             1.00005\nweight             1.00005\namount             1.00005\nmedical_info_a1    1.00005\nmedical_info_a2    1.00005\nmedical_info_a3    1.00005\nmedical_info_b1    1.00005\ndtype: float64\n\n\nNote that it is possible to disable either centering or scaling by either passing with_mean=False or with_std=False to the constructor of StandardScaler.\n\n\n5.5.2 Scaling\nAn alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one.\n\n\nCode\nscaler = MinMaxScaler()\nscaler.fit(train_x[num_cols])\n\n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\n\nCode\ntrain_x[num_cols] = scaler.transform(train_x[num_cols])\ntest_x[num_cols] = scaler.transform(test_x[num_cols])\n\n\n\n\nCode\ntrain_x[num_cols].describe()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nage\nheight\nweight\namount\nmedical_info_a1\nmedical_info_a2\nmedical_info_a3\nmedical_info_b1\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n0.500358\n0.524152\n0.380388\n0.384023\n0.470184\n0.487333\n0.220867\n0.491022\n\n\nstd\n0.292919\n0.143889\n0.125162\n0.345979\n0.136063\n0.130789\n0.157490\n0.319051\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.243243\n0.427145\n0.292879\n0.000800\n0.376302\n0.399748\n0.111111\n0.222222\n\n\n50%\n0.513514\n0.527079\n0.376458\n0.299930\n0.470703\n0.488020\n0.222222\n0.444444\n\n\n75%\n0.756757\n0.624006\n0.461614\n0.699970\n0.563802\n0.576293\n0.333333\n0.777778\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNote that you can scale to any range by specifying feature_range=(min, max).\n\n\n5.5.3 Logarithm transform and binning\n\n\nCode\nx = np.array([1.0, 10.0, 100.0, 1000.0, 10000.0])\n\n\n\n\nCode\nnp.log(x)\n\n\narray([0.        , 2.30258509, 4.60517019, 6.90775528, 9.21034037])\n\n\n\n\nCode\n# If your data contains zero value, try to plus one first\nnp.log1p(x)\n\n\narray([0.69314718, 2.39789527, 4.61512052, 6.90875478, 9.21044037])\n\n\nBinning allows you to transform numerical variable to categorical variable\n\n\nCode\nx = [1, 7, 5, 4, 6, 3]\n\n\n\n\nCode\nbin_edges = [-float('inf'), 3.0, 5.0, float('inf')]\nbinned = pd.cut(x, bin_edges, labels=False, retbins=True)\nprint(binned)\n\n\n(array([0, 2, 1, 1, 2, 0]), array([-inf,   3.,   5.,  inf]))\n\n\n\n\nCode\nbinned \n\n\n(array([0, 2, 1, 1, 2, 0]), array([-inf,   3.,   5.,  inf]))\n\n\n\n\n5.5.4 Power transfrom\n\n\nCode\ntrain_x, test_x = load_data()\n\n\nBox-cox transform only works for postive data\n\n\nCode\npos_cols = [c for c in num_cols if (train_x[c] &gt; 0.0).all() and (test_x[c] &gt; 0.0).all()] # List comprehension which is similar to set-builder notation\n\n\n\n\nCode\npt = PowerTransformer(method='box-cox')\npt.fit(train_x[pos_cols])\n\n\nPowerTransformer(method='box-cox')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PowerTransformerPowerTransformer(method='box-cox')\n\n\n\n\nCode\ntrain_x[pos_cols] = pt.transform(train_x[pos_cols])\ntest_x[pos_cols] = pt.transform(test_x[pos_cols])\n\n\n\n\nCode\nrng = np.random.RandomState(304)\noriginal_data = rng.lognormal(size=(1000,1))\npt = PowerTransformer(method='box-cox')\ntransformed_data = pt.fit_transform(original_data)\n\nfig, ax=plt.subplots(1, 2, figsize=(15, 3))\nsns.histplot(original_data, ax=ax[0], kde=True, legend=False)\nax[0].set_title(\"Original Data\")\nsns.histplot(transformed_data, ax=ax[1], kde=True, legend=False)\nax[1].set_title(\"Transformed data\")\nplt.show()\n\n\n\n\n\n\n\nCode\ntrain_x, test_x = load_data()\n\n\n\n\nCode\npt = PowerTransformer(method='yeo-johnson')\npt.fit(train_x[num_cols])\n\n\nPowerTransformer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PowerTransformerPowerTransformer()\n\n\n\n\nCode\ntrain_x[num_cols] = pt.transform(train_x[num_cols])\ntest_x[num_cols] = pt.transform(test_x[num_cols])\n\n\n\n\nCode\nrng = np.random.RandomState(304)\noriginal_data = rng.lognormal(size=(1000,1))\npt = PowerTransformer(method='yeo-johnson')\ntransformed_data = pt.fit_transform(original_data)\n\nfig, ax=plt.subplots(1, 2, figsize=(15, 3))\nsns.histplot(original_data, ax=ax[0], kde=True, legend=False)\nax[0].set_title(\"Original Data\")\nsns.histplot(transformed_data, ax=ax[1], kde=True, legend=False)\nax[1].set_title(\"Transformed data\")\nplt.show()\n\n\n\n\n\n\n\n5.5.5 Quantile transfrom\n\n\nCode\ntransformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\ntransformer.fit(train_x[num_cols])\n\n\nQuantileTransformer(n_quantiles=100, output_distribution='normal',\n                    random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.QuantileTransformerQuantileTransformer(n_quantiles=100, output_distribution='normal',\n                    random_state=0)\n\n\n\n\nCode\nrng = np.random.RandomState(304)\noriginal_data = rng.lognormal(size=(1000,1))\ntransformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\ntransformed_data = transformer.fit_transform(original_data)\n\nfig, ax=plt.subplots(1, 2, figsize=(15, 3))\nsns.histplot(original_data, ax=ax[0], kde=True, legend=False)\nax[0].set_title(\"Original Data\")\nsns.histplot(transformed_data, ax=ax[1], kde=True, legend=False)\nax[1].set_title(\"Transformed data\")\nplt.show()"
  },
  {
    "objectID": "04_Clean_feature_engineering.html#encoding",
    "href": "04_Clean_feature_engineering.html#encoding",
    "title": "5  Data cleaning and feature engineering",
    "section": "5.6 Encoding",
    "text": "5.6 Encoding\n\n5.6.1 One-hot encoding\nOne possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the OneHotEncoder, which transforms each categorical feature with n_categories possible values into n_categories binary features, with one of them 1, and all others 0.\n\n\nCode\nenc = OneHotEncoder(drop=None, handle_unknown='ignore')\n\n\n\n\nCode\nX = pd.DataFrame(np.array([['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox'], ['female', 'from Europe', 'uses Chrome']]), columns=['gender', 'locations', 'browsers'])\nX.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\ngender\nlocations\nbrowsers\n\n\n\n\n0\nmale\nfrom US\nuses Safari\n\n\n1\nfemale\nfrom Europe\nuses Firefox\n\n\n2\nfemale\nfrom Europe\nuses Chrome\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nonehot_encoded = enc.fit_transform(X).toarray()\npd.DataFrame(onehot_encoded, columns=enc.get_feature_names_out())\n\n\n\n  \n    \n      \n\n\n\n\n\n\ngender_female\ngender_male\nlocations_from Europe\nlocations_from US\nbrowsers_uses Chrome\nbrowsers_uses Firefox\nbrowsers_uses Safari\n\n\n\n\n0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n1\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n2\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWhen an unknown category is encountered during transform(), the resulting one-hot encoded columns for this feature will be all zeros:\n\n\nCode\nX_test = pd.DataFrame(np.array([['male', 'from US', 'uses Firefox'], ['female', 'from Europe', 'uses IE']]), columns=['gender', 'locations', 'browsers'])\nonehot_encoded = enc.transform(X_test).toarray()\npd.DataFrame(onehot_encoded, columns=enc.get_feature_names_out())\n\n\n\n  \n    \n      \n\n\n\n\n\n\ngender_female\ngender_male\nlocations_from Europe\nlocations_from US\nbrowsers_uses Chrome\nbrowsers_uses Firefox\nbrowsers_uses Safari\n\n\n\n\n0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n\n\n1\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n5.6.2 Ordinal encoding\n\n\nCode\nenc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value= -1)\n\n\n\n\nCode\nX = pd.DataFrame(np.array([['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox'], ['female', 'from Europe', 'uses Chrome']]), columns=['gender', 'locations', 'browsers'])\nX.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\ngender\nlocations\nbrowsers\n\n\n\n\n0\nmale\nfrom US\nuses Safari\n\n\n1\nfemale\nfrom Europe\nuses Firefox\n\n\n2\nfemale\nfrom Europe\nuses Chrome\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nordinal_encoded = enc.fit_transform(X)\npd.DataFrame(ordinal_encoded, columns=enc.get_feature_names_out())\n\n\n\n  \n    \n      \n\n\n\n\n\n\ngender\nlocations\nbrowsers\n\n\n\n\n0\n1.0\n1.0\n2.0\n\n\n1\n0.0\n0.0\n1.0\n\n\n2\n0.0\n0.0\n0.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nX_test = pd.DataFrame(np.array([['male', 'from US', 'uses Firefox'], ['female', 'from Europe', 'uses IE']]), columns=['gender', 'locations', 'browsers'])\nordinal_encoded = enc.transform(X_test)\npd.DataFrame(ordinal_encoded, columns=enc.get_feature_names_out())\n\n\n\n  \n    \n      \n\n\n\n\n\n\ngender\nlocations\nbrowsers\n\n\n\n\n0\n1.0\n1.0\n1.0\n\n\n1\n0.0\n0.0\n-1.0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n5.6.3 Target encoding\nThe MovieLens1M dataset contains one-million movie ratings by users of the MovieLens website, with features describing each user and movie. This hidden cell sets everything up:\n\n\nCode\n!kaggle datasets download -d odedgolden/movielens-1m-dataset\n\n\nDownloading movielens-1m-dataset.zip to /content\n  0% 0.00/5.83M [00:00&lt;?, ?B/s] 86% 5.00M/5.83M [00:00&lt;00:00, 46.2MB/s]\n100% 5.83M/5.83M [00:00&lt;00:00, 51.8MB/s]\n\n\n\n\nCode\n!unzip -qq movielens-1m-dataset.zip\n\n\n\n\nCode\nratings = pd.read_csv('ratings.dat',sep='::', header=None, names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"])\nusers = pd.read_csv('users.dat',sep='::', header=None, names=[\"UserID\", \"Gender\", \"Age\", \"Occupation\", \"Zip-code\"])\n\n\n/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators &gt; 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n  return func(*args, **kwargs)\n\n\n\n\nCode\ndf = pd.merge(left=ratings, right=users, how='inner', on='UserID')\n\n\n\n\nCode\ndf = df.astype(np.uint8, errors='ignore') # reduce memory footprint\nprint(\"Number of Unique Zipcodes: {}\".format(df[\"Zip-code\"].nunique()))\n\n\nNumber of Unique Zipcodes: 3439\n\n\n\n\nCode\ndf\n\n\n\n  \n    \n      \n\n\n\n\n\n\nUserID\nMovieID\nRating\nTimestamp\nGender\nAge\nOccupation\nZip-code\n\n\n\n\n0\n1\n169\n5\n88\nF\n1\n10\n48067\n\n\n1\n1\n149\n3\n157\nF\n1\n10\n48067\n\n\n2\n1\n146\n3\n16\nF\n1\n10\n48067\n\n\n3\n1\n80\n4\n115\nF\n1\n10\n48067\n\n\n4\n1\n51\n5\n99\nF\n1\n10\n48067\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1000204\n152\n67\n1\n253\nM\n25\n6\n11106\n\n\n1000205\n152\n70\n5\n119\nM\n25\n6\n11106\n\n\n1000206\n152\n50\n5\n234\nM\n25\n6\n11106\n\n\n1000207\n152\n72\n4\n128\nM\n25\n6\n11106\n\n\n1000208\n152\n73\n4\n49\nM\n25\n6\n11106\n\n\n\n\n\n1000209 rows × 8 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWith over 3000 categories, the Zipcode feature makes a good candidate for target encoding, and the size of this dataset (over one-million rows) means we can spare some data to create the encoding.\nWe’ll start by creating a 25% split to train the target encoder.\n\n\nCode\nX = df.copy()\ny = X.pop('Rating')\n\nX_encode = X.sample(frac=0.25)\ny_encode = y[X_encode.index]\nX_pretrain = X.drop(X_encode.index)\ny_train = y[X_pretrain.index]\n\n\nThe category_encoders package in scikit-learn-contrib implements an MEstimateEncoder, which we’ll use to encode our Zipcode feature.\n\n\nCode\n# Create the encoder instance. Choose m to control noise.\nencoder = MEstimateEncoder(cols=[\"Zip-code\"], m=5.0)\n\n# Fit the encoder on the encoding split.\nencoder.fit(X_encode, y_encode)\n\n# Encode the Zipcode column to create the final training data\nX_train = encoder.transform(X_pretrain)\n\n\nLet’s compare the encoded values to the target to see how informative our encoding might be.\n\n\nCode\nplt.figure(dpi=90)\nax = sns.distplot(y, kde=False, norm_hist=True)\nax = sns.kdeplot(X_train[\"Zip-code\"], color='r', ax=ax)\nax.set_xlabel(\"Rating\")\nax.legend(labels=['Zipcode', 'Rating']);\n\n\n/usr/local/lib/python3.9/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\nThe distribution of the encoded Zipcode feature roughly follows the distribution of the actual ratings, meaning that movie-watchers differed enough in their ratings from zipcode to zipcode that our target encoding was able to capture useful information.\nIf you would like to use KFold encoding, take a look at http://contrib.scikit-learn.org/category_encoders/wrapper.html#category_encoders.wrapper.NestedCVWrapper"
  },
  {
    "objectID": "04_Clean_feature_engineering.html#feature-engineering",
    "href": "04_Clean_feature_engineering.html#feature-engineering",
    "title": "5  Data cleaning and feature engineering",
    "section": "5.7 Feature Engineering",
    "text": "5.7 Feature Engineering\nWe’ll see how adding a few synthetic features to a dataset can improve the predictive performance of a random forest model.\nThe Concrete dataset contains a variety of concrete formulations and the resulting product’s compressive strength, which is a measure of how much load that kind of concrete can bear. The task for this dataset is to predict a concrete’s compressive strength given its formulation.\n\n\nCode\n!kaggle datasets download -d sinamhd9/concrete-comprehensive-strength\n!unzip -qq concrete-comprehensive-strength.zip\n\n\nDownloading concrete-comprehensive-strength.zip to /content\n  0% 0.00/32.9k [00:00&lt;?, ?B/s]\n100% 32.9k/32.9k [00:00&lt;00:00, 16.2MB/s]\n\n\n\n\nCode\ndf = pd.read_excel(\"Concrete_Data.xls\")\ndf.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nCement (component 1)(kg in a m^3 mixture)\nBlast Furnace Slag (component 2)(kg in a m^3 mixture)\nFly Ash (component 3)(kg in a m^3 mixture)\nWater (component 4)(kg in a m^3 mixture)\nSuperplasticizer (component 5)(kg in a m^3 mixture)\nCoarse Aggregate (component 6)(kg in a m^3 mixture)\nFine Aggregate (component 7)(kg in a m^3 mixture)\nAge (day)\nConcrete compressive strength(MPa, megapascals)\n\n\n\n\n0\n540.0\n0.0\n0.0\n162.0\n2.5\n1040.0\n676.0\n28\n79.986111\n\n\n1\n540.0\n0.0\n0.0\n162.0\n2.5\n1055.0\n676.0\n28\n61.887366\n\n\n2\n332.5\n142.5\n0.0\n228.0\n0.0\n932.0\n594.0\n270\n40.269535\n\n\n3\n332.5\n142.5\n0.0\n228.0\n0.0\n932.0\n594.0\n365\n41.052780\n\n\n4\n198.6\n132.4\n0.0\n192.0\n0.0\n978.4\n825.5\n360\n44.296075\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nYou can see here the various ingredients going into each variety of concrete. We’ll see in a moment how adding some additional synthetic features derived from these can help a model to learn important relationships among them. We’ll first establish a baseline by training the model on the un-augmented dataset. This will help us determine whether our new features are actually useful.\nEstablishing baselines like this is good practice at the start of the feature engineering process. A baseline score can help you decide whether your new features are worth keeping, or whether you should discard them and possibly try something else.\n\n\nCode\ndf.columns\n\n\nIndex(['Cement (component 1)(kg in a m^3 mixture)',\n       'Blast Furnace Slag (component 2)(kg in a m^3 mixture)',\n       'Fly Ash (component 3)(kg in a m^3 mixture)',\n       'Water  (component 4)(kg in a m^3 mixture)',\n       'Superplasticizer (component 5)(kg in a m^3 mixture)',\n       'Coarse Aggregate  (component 6)(kg in a m^3 mixture)',\n       'Fine Aggregate (component 7)(kg in a m^3 mixture)', 'Age (day)',\n       'Concrete compressive strength(MPa, megapascals) '],\n      dtype='object')\n\n\n\n\nCode\nX = df.copy()\ny = X.pop(df.columns[-1])\n\n# Train and score baseline model\nbaseline = RandomForestRegressor(criterion=\"absolute_error\", random_state=0)\nbaseline_score = cross_val_score(\n    baseline, X, y, cv=5, scoring=\"neg_mean_absolute_error\"\n)\nbaseline_score = -1 * baseline_score.mean()\n\nprint(f\"MAE Baseline Score: {baseline_score:.4}\")\n\n\nMAE Baseline Score: 8.397\n\n\nIf you ever cook at home, you might know that the ratio of ingredients in a recipe is usually a better predictor of how the recipe turns out than their absolute amounts. We might reason then that ratios of the features above would be a good predictor of CompressiveStrength. The cell below adds three new ratio features to the dataset.\n\n\nCode\nX = df.copy()\ny = X.pop(df.columns[-1])\n\n# Create synthetic features\nX[\"FCRatio\"] = X[df.columns[-2]] / X[df.columns[-3]]\nX[\"AggCmtRatio\"] = (X[df.columns[-3]] + X[df.columns[-2]]) / X[df.columns[0]]\nX[\"WtrCmtRatio\"] = X[df.columns[3]] / X[df.columns[0]]\n\n# Train and score model on dataset with additional ratio features\nmodel = RandomForestRegressor(criterion=\"absolute_error\", random_state=0)\nscore = cross_val_score(\n    model, X, y, cv=5, scoring=\"neg_mean_absolute_error\"\n)\nscore = -1 * score.mean()\n\nprint(f\"MAE Score with Ratio Features: {score:.4}\")\n\n\nMAE Score with Ratio Features: 7.732\n\n\nAnd sure enough, performance improved! This is evidence that these new ratio features exposed important information to the model that it wasn’t detecting before.\n\n5.7.1 Mathematical Transforms\nWe’ll use four datasets that having a range of feature types: US Traffic Accidents, 1985 Automobiles, and Customer Lifetime Value. The following hidden cell loads them up.\n\n\nCode\n!kaggle datasets download -d sobhanmoosavi/us-accidents\n!kaggle datasets download -d toramky/automobile-dataset\n!kaggle datasets download -d pankajjsh06/ibm-watson-marketing-customer-value-data\n\n\nDownloading us-accidents.zip to /content\n 96% 257M/269M [00:01&lt;00:00, 123MB/s]\n100% 269M/269M [00:01&lt;00:00, 147MB/s]\nDownloading automobile-dataset.zip to /content\n  0% 0.00/4.87k [00:00&lt;?, ?B/s]\n100% 4.87k/4.87k [00:00&lt;00:00, 4.82MB/s]\nDownloading ibm-watson-marketing-customer-value-data.zip to /content\n  0% 0.00/345k [00:00&lt;?, ?B/s]\n100% 345k/345k [00:00&lt;00:00, 87.6MB/s]\n\n\n\n\nCode\n!unzip -qq us-accidents.zip\n!unzip -qq automobile-dataset.zip\n!unzip -qq ibm-watson-marketing-customer-value-data.zip\n\n\n\n\nCode\naccidents = pd.read_csv(\"US_Accidents_Dec21_updated.csv\")\nautos = pd.read_csv(\"Automobile_data.csv\")\nconcrete = pd.read_excel(\"Concrete_Data.xls\")\ncustomer = pd.read_csv(\"WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv\")\n\n\nParserError: ignored\n\n\nRelationships among numerical features are often expressed through mathematical formulas, which you’ll frequently come across as part of your domain research. In Pandas, you can apply arithmetic operations to columns just as if they were ordinary numbers.\nIn the Automobile dataset are features describing a car’s engine. Research yields a variety of formulas for creating potentially useful new features. The “stroke ratio”, for instance, is a measure of how efficient an engine is versus how performant:\n\n\nCode\nautos.replace(\"?\", np.nan, inplace = True)\navg_bore=autos['bore'].astype('float').mean(axis=0)\nautos[\"bore\"].replace(np.nan, avg_bore, inplace=True)\navg_stroke = autos[\"stroke\"].astype(\"float\").mean(axis=0)\nautos[\"stroke\"].replace(np.nan, avg_stroke, inplace=True)\n\n\n\n\nCode\nautos[[\"bore\", \"stroke\"]] = autos[[\"bore\", \"stroke\"]].astype(\"float\")\nautos[\"stroke_ratio\"] = autos.stroke/ autos.bore\nautos[[\"stroke\", \"bore\", \"stroke_ratio\"]].head()\n\n\nData visualization can suggest transformations, often a “reshaping” of a feature through powers or logarithms. The distribution of WindSpeed in US Accidents is highly skewed, for instance. In this case the logarithm is effective at normalizing it:\n\n\nCode\naccidents.columns\n\n\n\n\nCode\naccidents[\"Wind_Speed(mph)\"].describe()\n\n\n\n\nCode\n# If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log\naccidents[\"LogWindSpeed\"] = accidents[\"Wind_Speed(mph)\"].apply(np.log1p)\n\n# Plot a comparison\nfig, axs = plt.subplots(1, 2, figsize=(8, 4))\nsns.kdeplot(accidents[\"Wind_Speed(mph)\"], shade=True, ax=axs[0])\nsns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1]);\n\n\n\n\n5.7.2 Counts\nFeatures describing the presence or absence of something often come in sets, the set of risk factors for a disease, say. You can aggregate such features by creating a count. These features will be binary (1 for Present, 0 for Absent) or boolean (True or False). In Python, booleans can be added up just as if they were integers.\nIn Traffic Accidents are several features indicating whether some roadway object was near the accident. This will create a count of the total number of roadway features nearby using the sum() method:\n\n\nCode\nroadway_features = [\"Amenity\", \"Bump\", \"Crossing\", \"Give_Way\",\n    \"Junction\", \"No_Exit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\",\n    \"Traffic_Calming\", \"Traffic_Signal\"]\naccidents[\"RoadwayFeatures\"] = accidents[roadway_features].sum(axis=1)\n\naccidents[roadway_features + [\"RoadwayFeatures\"]].head(20)\n\n\n\n\n5.7.3 Group Transforms\nFinally we have Group transforms, which aggregate information across multiple rows grouped by some category. With a group transform you can create features like: “the average income of a person’s state of residence,” or “the proportion of movies released on a weekday, by genre.” If you had discovered a category interaction, a group transform over that categry could be something good to investigate.\nUsing an aggregation function, a group transform combines two features: a categorical feature that provides the grouping and another feature whose values you wish to aggregate. For an “average income by state”, you would choose State for the grouping feature, mean() for the aggregation function, and Income for the aggregated feature. To compute this in Pandas, we use the groupby() and transform() methods:\n\n\nCode\ncustomer[\"AverageIncome\"] = (\n    customer.groupby(\"State\")  # for each state\n    [\"Income\"]                 # select the income\n    .transform(\"mean\")         # and compute its mean\n)\n\ncustomer[[\"State\", \"Income\", \"AverageIncome\"]].head(10)\n\n\nThe mean() function is a built-in dataframe method, which means we can pass it as a string to transform(). Other handy methods include max(), min(), median(), var(), std(), and count(). Here’s how you could calculate the frequency with which each state occurs in the dataset:\n\n\nCode\ncustomer[\"StateFreq\"] = (\n    customer.groupby(\"State\")\n    [\"State\"]\n    .transform(\"count\")\n    / customer.State.count()\n)\n\ncustomer[[\"State\", \"StateFreq\"]].head(10)"
  },
  {
    "objectID": "04_Clean_feature_engineering.html#data-centric-ai-with-cleanlab",
    "href": "04_Clean_feature_engineering.html#data-centric-ai-with-cleanlab",
    "title": "5  Data cleaning and feature engineering",
    "section": "5.8 Data-centric AI with CleanLab",
    "text": "5.8 Data-centric AI with CleanLab\ncleanlab automatically finds and fixes errors in any ML dataset. This data-centric AI package facilitates machine learning with messy, real-world data by providing clean labels during training.\n\n\nCode\ndata_dict = make_data()\nfor key, val in data_dict.items():  # Map data_dict to variables in namespace\n    print(key)\n    exec(key + \"=val\")\n\n\ndata\ntrue_labels\nlabels\ntest_data\ntest_labels\nnoisy_test_labels\nps\npy_true\nnoise_matrix_true\nclass_names\n\n\n\n\nCode\ntrue_errors = np.where(true_labels != labels)[0]\nplot_data(data, circles=true_errors, title=\"A realistic, messy dataset with 4 classes\", alpha=0.3)\n\n\n\n\n\nThe figure above represents a toy dataset we’ll use to demonstrate various cleanlab functionality. In this data, the features X are 2-dimensional and examples are colored according to their given label above. The given label happens to be incorrect for some of the examples (circled in red) in this dataset!\n\n5.8.1 Use CleanLearning() for everything\n\n\nCode\n# For comparison, this is how you would have trained your model normally (without Cleanlab)\nyourFavoriteModel = LogisticRegression(random_state=SEED)\nyourFavoriteModel.fit(data, labels)\nprint(f\"Accuracy using yourFavoriteModel: {yourFavoriteModel.score(test_data, test_labels):.0%}\")\n\n\nAccuracy using yourFavoriteModel: 83%\n\n\n\n\nCode\nyourFavoriteModel = LogisticRegression(random_state=SEED)\n\n# CleanLearning: Machine Learning with cleaned data (given messy, real-world data)\ncl = cleanlab.classification.CleanLearning(yourFavoriteModel, seed=SEED)\n\n# Fit model to messy, real-world data, automatically training on cleaned data.\n_ = cl.fit(data, labels)\n\n\n\n\nCode\n# But CleanLearning can do anything yourFavoriteModel can do, but enhanced.\n# For example, CleanLearning gives you predictions (just like yourFavoriteModel)\n# but the magic is that CleanLearning was trained as if your data did not have label errors.\nprint(f\"Accuracy using yourFavoriteModel (+ CleanLearning): {cl.score(test_data, test_labels):.0%}\")\n\n\nAccuracy using yourFavoriteModel (+ CleanLearning): 86%\n\n\nNote! Accuracy refers to the accuracy with respect to the true error-free labels of a test set., i.e. what we actually care about in practice because that’s what real-world model performance is based on!\n\n\n5.8.2 Use CleanLearning() to find_label_issues() in one line of code\n\n\nCode\n# One line of code. Literally.\nissues = CleanLearning(yourFavoriteModel, seed=SEED).find_label_issues(data, labels)\nissues.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nis_label_issue\nlabel_quality\ngiven_label\npredicted_label\n\n\n\n\n0\nFalse\n0.695174\n0\n0\n\n\n1\nFalse\n0.522929\n0\n0\n\n\n2\nTrue\n0.013722\n3\n0\n\n\n3\nFalse\n0.675606\n0\n0\n\n\n4\nFalse\n0.646438\n0\n0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nVisualize the twenty examples with lowest label quality to see if Cleanlab works.\n\n\nCode\nlowest_quality_labels = issues[\"label_quality\"].argsort()[:20]\nplot_data(data, circles=lowest_quality_labels, title=\"The 20 lowest label quality examples\")\n\n\n\n\n\nAbove, the top 20 label issues circled in red are found automatically using cleanlab (no true labels given).\n\n\n5.8.3 Use cleanlab to find dataset-level and class-level issues\n\nDid you notice that the yellow and seafoam green class above are overlapping?\nHow can a model ever know (or learn) what’s ground truth inside the yellow distribution?\nIf these two classes were merged, the model can learn more accurately from 3 classes (versus 4).\n\ncleanlab automatically finds data-set level issues like this, in one line of code. Check this out!\n\n\nCode\ncleanlab.dataset.find_overlapping_classes(\n    labels=labels,\n    confident_joint=cl.confident_joint,  # cleanlab uses the confident_joint internally to quantify label noise (see cleanlab.count.compute_confident_joint)\n    class_names=class_names,\n)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nClass Name A\nClass Name B\nClass Index A\nClass Index B\nNum Overlapping Examples\nJoint Probability\n\n\n\n\n0\nseafoam green\nyellow\n2\n3\n26\n0.104\n\n\n1\npurple\nseafoam green\n0\n2\n23\n0.092\n\n\n2\npurple\nyellow\n0\n3\n10\n0.040\n\n\n3\nblue\nseafoam green\n1\n2\n6\n0.024\n\n\n4\npurple\nblue\n0\n1\n5\n0.020\n\n\n5\nblue\nyellow\n1\n3\n1\n0.004\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThere are two things being happening here:\n\nDistribution Overlap: The green distribution has huge variance and overlaps with other distributions. Cleanlab handles this for you: read the theory behind cleanlab for overlapping classes here.\nLabel Issues: A ton of examples (which actually belong to the purple class) have been mislabeled as “green” in our dataset.\n\nNow, let’s see what happens if we merge classes “seafoam green” and “yellow”\n\n\nCode\nyourFavoriteModel1 = LogisticRegression(random_state=SEED)\nyourFavoriteModel1.fit(data, labels)\nprint(f\"[Original classes] Accuracy of yourFavoriteModel: {yourFavoriteModel1.score(test_data, test_labels):.0%}\")\n\nmerged_labels, merged_test_labels = np.array(labels), np.array(test_labels)\n\n# Merge classes: map all yellow-labeled examples to seafoam green\nmerged_labels[merged_labels == 3] = 2\nmerged_test_labels[merged_test_labels == 3] = 2\n\n# Re-run our comparison. Re-run your model on the newly labeled dataset.\nyourFavoriteModel2 = LogisticRegression(random_state=SEED)\nyourFavoriteModel2.fit(data, merged_labels)\nprint(f\"[Modified classes] Accuracy of yourFavoriteModel: {yourFavoriteModel2.score(test_data, merged_test_labels):.0%}\")\n\n# Re-run CleanLearning as well.\nyourFavoriteModel3 = LogisticRegression(random_state=SEED)\ncl3 = cleanlab.classification.CleanLearning(yourFavoriteModel, seed=SEED)\ncl3.fit(data, merged_labels)\nprint(f\"[Modified classes] Accuracy of yourFavoriteModel (+ CleanLearning): {cl3.score(test_data, merged_test_labels):.0%}\")\n\n\n[Original classes] Accuracy of yourFavoriteModel: 83%\n[Modified classes] Accuracy of yourFavoriteModel: 94%\n[Modified classes] Accuracy of yourFavoriteModel (+ CleanLearning): 96%\n\n\nWhile on one hand that’s a huge improvement, it’s important to remember that choosing among three classes is an easier task than choosing among four classes, so it’s not fair to directly compare these numbers. Instead, the big takeaway is… if you get to choose your classes, combining overlapping classes can make the learning task easier for your model!\n\n\n5.8.4 Clean your test set too if you’re doing ML with noisy labels!\nIf your test and training data were randomly split, then be aware that your test labels are likely noisy too! It is thus important to fix label issues in them before we can trust measures like test accuracy.\n\n\nCode\n# Fit your model on noisily labeled train data\nyourFavoriteModel = LogisticRegression(random_state=SEED)\nyourFavoriteModel.fit(data, labels)\n\n# Get predicted probabilities for test data (these are out-of-sample)\nmy_test_pred_probs = yourFavoriteModel.predict_proba(test_data)\nmy_test_preds = my_test_pred_probs.argmax(axis=1)  # predicted labels\n\n# Find label issues in the test data\nissues_test = CleanLearning(yourFavoriteModel, seed=SEED).find_label_issues(\n    labels=noisy_test_labels, pred_probs=my_test_pred_probs)\n\nissues_test.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nis_label_issue\nlabel_quality\ngiven_label\npredicted_label\n\n\n\n\n0\nFalse\n0.702970\n0\n0\n\n\n1\nTrue\n0.009418\n1\n0\n\n\n2\nFalse\n0.788533\n0\n0\n\n\n3\nTrue\n0.207881\n2\n0\n\n\n4\nFalse\n0.713672\n0\n0\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n# You should inspect issues_test and fix issues to ensure high-quality test data labels.\ncorrected_test_labels = test_labels  # Here we'll pretend you have done this perfectly :)\n\n# Fit more robust version of model on noisily labeled training data\ncl = CleanLearning(yourFavoriteModel, seed=SEED).fit(data, labels)\ncl_test_preds = cl.predict(test_data)\n\nprint(f\" Noisy Test Accuracy (on given test labels) using yourFavoriteModel: {accuracy_score(noisy_test_labels, my_test_preds):.0%}\")\nprint(f\" Noisy Test Accuracy (on given test labels) using yourFavoriteModel (+ CleanLearning): {accuracy_score(noisy_test_labels, cl_test_preds):.0%}\")\nprint(f\"Actual Test Accuracy (on corrected test labels) using yourFavoriteModel: {accuracy_score(corrected_test_labels, my_test_preds):.0%}\")\nprint(f\"Actual Test Accuracy (on corrected test labels) using yourFavoriteModel (+ CleanLearning): {accuracy_score(corrected_test_labels, cl_test_preds):.0%}\")\n\n\n Noisy Test Accuracy (on given test labels) using yourFavoriteModel: 69%\n Noisy Test Accuracy (on given test labels) using yourFavoriteModel (+ CleanLearning): 71%\nActual Test Accuracy (on corrected test labels) using yourFavoriteModel: 83%\nActual Test Accuracy (on corrected test labels) using yourFavoriteModel (+ CleanLearning): 86%\n\n\n\n\n5.8.5 One score to rule them all – use cleanlab’s overall dataset health score\nThis score can be fairly compared across datasets or across versions of a dataset to track overall dataset quality (a.k.a. dataset health) over time.\n\n\nCode\n# One line of code.\nhealth = cleanlab.dataset.health_summary(\n    labels, confident_joint=cl.confident_joint\n    # cleanlab uses the confident_joint internally to quantify label noise (see cleanlab.count.compute_confident_joint)\n)\n\n\n--------------------------------------------------------\n|  Generating a Cleanlab Dataset Health Summary        |\n|   for your dataset with 250 examples and 4 classes.  |\n|  Note, Cleanlab is not a medical doctor... yet.      |\n--------------------------------------------------------\n\nOverall Class Quality and Noise across your dataset (below)\n------------------------------------------------------------ \n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nClass Index\nLabel Issues\nInverse Label Issues\nLabel Noise\nInverse Label Noise\nLabel Quality Score\n\n\n\n\n0\n2\n32\n23\n0.507937\n0.425926\n0.492063\n\n\n1\n3\n15\n22\n0.306122\n0.392857\n0.693878\n\n\n2\n0\n16\n22\n0.190476\n0.244444\n0.809524\n\n\n3\n1\n8\n4\n0.148148\n0.080000\n0.851852\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nClass Overlap. In some cases, you may want to merge classes in the top rows (below)\n-----------------------------------------------------------------------------------\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nClass Index A\nClass Index B\nNum Overlapping Examples\nJoint Probability\n\n\n\n\n0\n2\n3\n26\n0.104\n\n\n1\n0\n2\n23\n0.092\n\n\n2\n0\n3\n10\n0.040\n\n\n3\n1\n2\n6\n0.024\n\n\n4\n0\n1\n5\n0.020\n\n\n5\n1\n3\n1\n0.004\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n * Overall, about 28% (71 of the 250) labels in your dataset have potential issues.\n ** The overall label health score for this dataset is: 0.72.\n\nGenerated with &lt;3 from Cleanlab.\n\n\n\n\n\nCode\n# One line of code.\nhealth = cleanlab.dataset.overall_label_health_score(\n    labels, confident_joint=cl.confident_joint\n    # cleanlab uses the confident_joint internally to quantify label noise (see cleanlab.count.compute_confident_joint)\n)\n\n\n * Overall, about 28% (71 of the 250) labels in your dataset have potential issues.\n ** The overall label health score for this dataset is: 0.72.\n\n\nBecause we know the true labels (we created this toy dataset), we can compare with ground truth.\n\n\nCode\nlabel_acc = sum(labels != true_labels) / len(labels)\nprint(f\"Percentage of label issues guessed by cleanlab {1 - health:.0%}\")\nprint(f\"Percentage of (ground truth) label errors): {label_acc:.0%}\")\n\noffset = (1 - label_acc) - health\noffset\n\n\ncleanlab seems to be overestimating. Since data points that fall in between two overlapping distributions are often impossible to label and are counted as issues.\nFor more details, see https://docs.cleanlab.ai/stable/tutorials/indepth_overview.html#"
  },
  {
    "objectID": "05_Feature_selection_extraction.html#setup",
    "href": "05_Feature_selection_extraction.html#setup",
    "title": "6  Feature selection and extraction",
    "section": "6.1 Setup",
    "text": "6.1 Setup\n\n\nCode\n!pip install Boruta -qq\n!pip install opentsne -qq\n!pip install umap-learn -qq\n\n\n\n\nCode\n# Scientific computing\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# Preprocessing and datasets\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_digits\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\n# Modeling\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\n\n# Feature selection\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.inspection import permutation_importance\n\nfrom boruta import BorutaPy\n\n# Feature extraction\nfrom sklearn.decomposition import PCA\nfrom openTSNE import TSNE as oTSNE\nimport umap\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import FeatureAgglomeration"
  },
  {
    "objectID": "05_Feature_selection_extraction.html#feature-selection",
    "href": "05_Feature_selection_extraction.html#feature-selection",
    "title": "6  Feature selection and extraction",
    "section": "6.2 Feature selection",
    "text": "6.2 Feature selection\nThe classes in the sklearn.feature_selection module can be used for feature selection/extraction methods on datasets, either to improve estimators’ accuracy scores or to boost their performance on very high-dimensional datasets.\n\n6.2.1 Removing low variance features\nSuppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is 0.8 * (1 - 0.8)\n\n\nCode\nX = pd.DataFrame([[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]])\nX\n\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0\n0\n1\n\n\n1\n0\n1\n0\n\n\n2\n1\n0\n0\n\n\n3\n0\n1\n1\n\n\n4\n0\n1\n0\n\n\n5\n0\n1\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nsel = VarianceThreshold(threshold=(.8 * (1 - .8))).set_output(transform=\"pandas\")\nsel.fit_transform(X)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nx1\nx2\n\n\n\n\n0\n0\n1\n\n\n1\n1\n0\n\n\n2\n0\n0\n\n\n3\n1\n1\n\n\n4\n1\n0\n\n\n5\n1\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAs expected, VarianceThreshold() has removed the first column, which has a probability of containing a zero.\n\n\n6.2.2 Univariate feature selection\nScikit-learn exposes feature selection routines as objects that implement the transform() method. For instance, we can perform a \\(\\chi^2\\) test to the samples to retrieve only the two best features as follows:\n\n\nCode\nX, y = load_iris(return_X_y=True, as_frame=True) # Load the iris data set\nX\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n\n150 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nselecter = SelectKBest(chi2, k=2).set_output(transform=\"pandas\")\nX_new = selecter.fit_transform(X, y)\nX_new\n\n\n\n  \n    \n      \n\n\n\n\n\n\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n1.4\n0.2\n\n\n1\n1.4\n0.2\n\n\n2\n1.3\n0.2\n\n\n3\n1.5\n0.2\n\n\n4\n1.4\n0.2\n\n\n...\n...\n...\n\n\n145\n5.2\n2.3\n\n\n146\n5.0\n1.9\n\n\n147\n5.2\n2.0\n\n\n148\n5.4\n2.3\n\n\n149\n5.1\n1.8\n\n\n\n\n\n150 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nplt.scatter(X_new.iloc[:,0], X_new.iloc[:,1], c=y)\nplt.show()\n\n\n\n\n\nThese objects take as input a scoring function that returns univariate scores/p-values (or only scores for SelectKBest() and SelectPercentile()):\n\nFor regression: r_regression, f_regression, mutual_info_regression\nFor classification: chi2, f_classif, mutual_info_classif\n\nThe methods based on F-test estimate the degree of linear dependency between two random variables. For example, we create a dataset with two informative features among a hundred. To simplify our example, we do not include either redundant or repeated features. In addition, We will explicitly not shuffle the dataset to ensure that the informative features will correspond to the three two columns of X.\n\n\nCode\n# Generate synthetic dataset\n\ndata, target = make_classification(\n    n_samples=5000,\n    n_features=100,\n    n_informative=2,\n    n_redundant=0,\n    n_repeated=0,\n    random_state=0,\n    shuffle = False  # So that we know the informative freatures are X[:, n_informative + n_redundant + n_repeated:].\n)\n\n\nWe will create two machine learning pipelines.\n\nThe former will be a random forest that will use all available features.\nThe latter will also be a random forest, but we will add a feature selection step to train this classifier.\n\n\n\nCode\n# Let’s create the model without any feature selection\nmodel_without_selection = RandomForestClassifier(n_jobs=2)\n\n\n\n\nCode\n# Then, let’s create a pipeline where the first stage will make the feature selection processing.\nmodel_with_selection = make_pipeline(\n    SelectKBest(score_func=f_classif, k=2), # Feature slection\n    RandomForestClassifier(n_jobs=2),       # Model\n)\n\n\nWe will measure the average time spent to train each pipeline and make it predict. Besides, we will compute the testing score of the model. We will collect these results via cross-validation.\n\n\nCode\n# Let’s start with the random forest without feature selection. We will store the results into a dataframe.\ncv_results_without_selection = cross_validate(model_without_selection, data, target)\ncv_results_without_selection = pd.DataFrame(cv_results_without_selection)\n\n\n\n\nCode\n# Now, we will repeat the process for the pipeline incorporating the feature selection.\ncv_results_with_selection = cross_validate(model_with_selection, data, target, return_estimator=True)\ncv_results_with_selection = pd.DataFrame(cv_results_with_selection)\n\n\nTo analyze the results, we will merge the results from the two pipeline in a single pandas dataframe.\n\n\nCode\ncv_results = pd.concat(\n    [cv_results_without_selection, cv_results_with_selection],\n    axis=1,\n    keys=[\"Without feature selection\", \"With feature selection\"],\n)\n\n# swap the level of the multi-index of the columns\ncv_results = cv_results.swaplevel(axis=\"columns\")\n\n\nLet’s first analyze the train and score time for each pipeline.\n\n\nCode\ncolor = {\"whiskers\": \"black\", \"medians\": \"black\", \"caps\": \"black\"}\ncv_results[\"fit_time\"].plot.box(color=color, vert=False)\nplt.xlabel(\"Elapsed time (s)\")\n_ = plt.title(\"Time to fit the model\")\n\n\n\n\n\n\n\nCode\ncv_results[\"score_time\"].plot.box(color=color, vert=False)\nplt.xlabel(\"Elapsed time (s)\")\n_ = plt.title(\"Time to make prediction\")\n\n\n\n\n\nWe can draw the same conclusions for both training and scoring elapsed time: selecting the most informative features speed-up our pipeline. Of course, such speed-up is beneficial only if the generalization performance in terms of metrics remain the same. Let’s check the testing score.\n\n\nCode\ncv_results[\"test_score\"].plot.box(color=color, vert=False)\nplt.xlabel(\"Accuracy score\")\n_ = plt.title(\"Test score via cross-validation\")\n\n\n\n\n\nWe can observe that the model’s generalization performance selecting a subset of features decreases compared with the model using all available features. Since we generated the dataset, we can infer that the decrease is because of the selection. The feature selection algorithm did not choose the two informative features.\n\n\nCode\nfor idx, pipeline in enumerate(cv_results_with_selection[\"estimator\"]):\n    print(\n        f\"Fold #{idx} - features selected are: \"\n        f\"{np.argsort(pipeline[0].scores_)[-2:]}\"\n    )\n\n\nFold #0 - features selected are: [30  1]\nFold #1 - features selected are: [10  1]\nFold #2 - features selected are: [10  1]\nFold #3 - features selected are: [30  1]\nFold #4 - features selected are: [61  1]\n\n\nWe see that the feature 1 is always selected while the other feature varies depending on the cross-validation fold.\nIf we would like to keep our score with similar generalization performance, we could choose another metric to perform the test or select more features. For instance, we could select the number of features based on a specific percentile of the highest scores.\n\n6.2.2.1 Mutual information\nThe Automobile dataset consists of 193 cars from the 1985 model year. The goal for this dataset is to predict a car’s price (the target) from 23 of the car’s features, such as make, body_style, and horsepower. In this example, we’ll rank the features with mutual information and investigate the results by data visualization. (The original dataset requires data cleaning, you could refer to https://skill-lync.com/student-projects/project-1-1299 for more details)\n\n\nCode\n# Uncpmment below if you are using Kaggle\n#!pip install gdown\n\n\n\n\nCode\n!gdown --fuzzy https://drive.google.com/file/d/1FoCRK2LQBo1hlPWK2fDDcOjjy-DnPiJw/view?usp=sharing\n\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1FoCRK2LQBo1hlPWK2fDDcOjjy-DnPiJw\nTo: /content/autos.csv\n  0% 0.00/21.6k [00:00&lt;?, ?B/s]100% 21.6k/21.6k [00:00&lt;00:00, 31.6MB/s]\n\n\n\n\nCode\ndf = pd.read_csv(\"autos.csv\") # Clean version\ndf.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsymboling\nmake\nfuel_type\naspiration\nnum_of_doors\nbody_style\ndrive_wheels\nengine_location\nwheel_base\nlength\n...\nengine_size\nfuel_system\nbore\nstroke\ncompression_ratio\nhorsepower\npeak_rpm\ncity_mpg\nhighway_mpg\nprice\n\n\n\n\n0\n3\nalfa-romero\ngas\nstd\n2\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9\n111\n5000\n21\n27\n13495\n\n\n1\n3\nalfa-romero\ngas\nstd\n2\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9\n111\n5000\n21\n27\n16500\n\n\n2\n1\nalfa-romero\ngas\nstd\n2\nhatchback\nrwd\nfront\n94.5\n171.2\n...\n152\nmpfi\n2.68\n3.47\n9\n154\n5000\n19\n26\n16500\n\n\n3\n2\naudi\ngas\nstd\n4\nsedan\nfwd\nfront\n99.8\n176.6\n...\n109\nmpfi\n3.19\n3.40\n10\n102\n5500\n24\n30\n13950\n\n\n4\n2\naudi\ngas\nstd\n4\nsedan\n4wd\nfront\n99.4\n176.6\n...\n136\nmpfi\n3.19\n3.40\n8\n115\n5500\n18\n22\n17450\n\n\n\n\n\n5 rows × 25 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe scikit-learn algorithm for MI treats discrete features differently from continuous features. Consequently, you need to tell it which are which. As a rule of thumb, anything that have a float dtype is not discrete. Categoricals (object or categorial dtype) can be treated as discrete by giving them a label encoding\n\n\nCode\nX = df.copy()\ny = X.pop(\"price\")\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n# A way to perfrom label encoding see https://pandas.pydata.org/docs/reference/api/pandas.factorize.html\n    X[colname], _ = X[colname].factorize() \n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int\n\n\nScikit-learn has two mutual information metrics in its feature_selection module: one for real-valued targets (mutual_info_regression()) and one for categorical targets (mutual_info_classif()). Our target, price, is real-valued. The next cell computes the MI scores for our features and wraps them up in a nice dataframe.\n\n\nCode\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nmi_scores[::3]  # show a few features with their MI scores\n\n\ncurb_weight          1.436041\nhighway_mpg          0.948495\nlength               0.615407\nbore                 0.497058\nstroke               0.385846\nnum_of_cylinders     0.331445\ncompression_ratio    0.132048\nfuel_type            0.047279\nName: MI Scores, dtype: float64\n\n\n\n\nCode\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)\n\n\n\n\n\n\n\nCode\n# As we might expect, the high-scoring `curb_weight` feature exhibits a strong relationship with `price`, the target.\nsns.relplot(x=\"curb_weight\", y=\"price\", data=df)\n\n\n\n\n\nThe fuel_type feature has a fairly low MI score, but as we can see from the figure below, it clearly separates two price populations with different trends within the horsepower feature. This indicates that fuel_type contributes an interaction effect and might not be unimportant after all. Before deciding a feature is unimportant from its MI score, it’s good to investigate any possible interaction effects – domain knowledge can offer a lot of guidance here.\n\n\nCode\nsns.lmplot(x=\"horsepower\", y=\"price\", hue=\"fuel_type\", data=df)\n\n\n\n\n\n\n\n\n6.2.3 Sequential feature selection\nSequential Feature Selection is available in the SequentialFeatureSelector transformer. SFS can be either forward or backward:\n\nForward-SFS is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. Concretely, we initially start with zero feature and find the one feature that maximizes a cross-validated score when an estimator is trained on this single feature. Once that first feature is selected, we repeat the procedure by adding a new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the n_features_to_select parameter.\nBackward-SFS follows the same idea but works in the opposite direction: instead of starting with no feature and greedily adding features, we start with all the features and greedily remove features from the set. The direction parameter controls whether forward or backward SFS is used.\n\n\nIn general, forward and backward selection do not yield equivalent results. Also, one may be much faster than the other depending on the requested number of selected features: if we have 10 features and ask for 7 selected features, forward selection would need to perform 7 iterations while backward selection would only need to perform 3.\n\n\n\nCode\nX, y = load_iris(return_X_y=True, as_frame=True)\nknn = KNeighborsClassifier(n_neighbors=3)\n\n\n\n\nCode\nsfs = SequentialFeatureSelector(knn, n_features_to_select=2, direction='forward').set_output(transform=\"pandas\")\nsfs.fit(X, y)\n\n\nSequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),\n                          n_features_to_select=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SequentialFeatureSelectorSequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),\n                          n_features_to_select=2)estimator: KNeighborsClassifierKNeighborsClassifier(n_neighbors=3)KNeighborsClassifierKNeighborsClassifier(n_neighbors=3)\n\n\n\n\nCode\n df = sfs.transform(X)\n df\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsepal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n0.2\n\n\n1\n4.9\n0.2\n\n\n2\n4.7\n0.2\n\n\n3\n4.6\n0.2\n\n\n4\n5.0\n0.2\n\n\n...\n...\n...\n\n\n145\n6.7\n2.3\n\n\n146\n6.3\n1.9\n\n\n147\n6.5\n2.0\n\n\n148\n6.2\n2.3\n\n\n149\n5.9\n1.8\n\n\n\n\n\n150 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nplt.scatter(df.iloc[:,0], df.iloc[:,1],c=y)\nplt.show()\n\n\n\n\n\n\n\n6.2.4 Feature selection from model\nSelectFromModel is a meta-transformer that can be used alongside any estimator that assigns importance to each feature through a specific attribute (such as coef_, feature_importances_) or via an importance_getter callable after fitting. The features are considered unimportant and removed if the corresponding importance of the feature values are below the provided threshold parameter.\nApart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. **Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”. In combination with the threshold criteria, one can use the max_features parameter to set a limit on the number of features to select.**\n\n\nCode\nX, y = load_iris(return_X_y=True, as_frame=True)\n\n\n\n\nCode\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False)\nmodel = SelectFromModel(lsvc).set_output(transform=\"pandas\")\n\n\n\n\nCode\nX_new = model.fit_transform(X,y) # We use threshold instead or max_features here\nX_new\n\n\n/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n\n\n1\n4.9\n3.0\n1.4\n\n\n2\n4.7\n3.2\n1.3\n\n\n3\n4.6\n3.1\n1.5\n\n\n4\n5.0\n3.6\n1.4\n\n\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n\n\n146\n6.3\n2.5\n5.0\n\n\n147\n6.5\n3.0\n5.2\n\n\n148\n6.2\n3.4\n5.4\n\n\n149\n5.9\n3.0\n5.1\n\n\n\n\n\n150 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n6.2.5 A Concret example\nThe following dataset is our old friend which is a record of neighborhoods in California district, predicting the median house value (target) given some information about the neighborhoods, as the average number of rooms, the latitude, the longitude or the median income of people in the neighborhoods (block).\n\n\nCode\nX, y = fetch_california_housing(as_frame=True, return_X_y=True)\n\n\n\n\nCode\n# To speed up the computation, we take the first 10,000 samples\nX = X[:10000]\ny = y[:10000]\nX.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe feature reads as follow:\n\nMedInc: median income in block\nHouseAge: median house age in block\nAveRooms: average number of rooms\nAveBedrms: average number of bedrooms\nPopulation: block population\nAveOccup: average house occupancy\nLatitude: house block latitude\nLongitude: house block longitude\nMedHouseVal: Median house value in 100k$ (target)\n\nTo assert the quality of our inspection technique, let’s add some random feature that won’t help the prediction (un-informative feature)\n\n\nCode\n# Adding random features\nrng = np.random.RandomState(0)\nbin_var = pd.Series(rng.randint(0, 2, X.shape[0]), name='rnd_bin')\nnum_var = pd.Series(np.arange(X.shape[0]), name='rnd_num')\nX_with_rnd_feat = pd.concat((X, bin_var, num_var), axis=1)\nX_with_rnd_feat\n\n\n\n  \n    \n      \n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nrnd_bin\nrnd_num\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n0\n0\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n1\n1\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n1\n2\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n0\n3\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n1\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\n4.0775\n10.0\n6.140900\n1.025440\n1275.0\n2.495108\n39.14\n-121.03\n0\n9995\n\n\n9996\n4.0848\n8.0\n6.350394\n1.091864\n1977.0\n2.594488\n39.13\n-121.07\n1\n9996\n\n\n9997\n3.6333\n7.0\n7.243455\n1.107330\n1143.0\n2.992147\n39.11\n-121.05\n0\n9997\n\n\n9998\n3.4630\n8.0\n6.363636\n1.166297\n1307.0\n2.898004\n39.08\n-121.04\n1\n9998\n\n\n9999\n3.0781\n7.0\n5.487500\n1.050000\n246.0\n3.075000\n39.09\n-121.00\n0\n9999\n\n\n\n\n\n10000 rows × 10 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X_with_rnd_feat, y, random_state=42)\n\n\nIn linear models, the target value is modeled as a linear combination of the features.\n\n\nCode\nmodel = RidgeCV()\n\nmodel.fit(X_train, y_train)\n\nprint(f'model score on training data: {model.score(X_train, y_train)}')\nprint(f'model score on testing data: {model.score(X_test, y_test)}')\n\n\nmodel score on training data: 0.6049524592207427\nmodel score on testing data: 0.5863921053581754\n\n\nOur linear model obtains a score of .60, so it explains a significant part of the target. Its coefficient should be somehow relevant. Let’s look at the coefficient learnt\n\n\nCode\ncoefs = pd.DataFrame(\n   model.coef_,\n   columns=['Coefficients'], index=X_train.columns\n)\n\ncoefs.plot(kind='barh', figsize=(9, 7))\nplt.title('Ridge model')\nplt.axvline(x=0, color='.5')\nplt.subplots_adjust(left=.3)\n\n\n\n\n\nThe AveBedrms have the higher coefficient. However, we can’t compare the magnitude of these coefficients directly, since they are not scaled. Indeed, Population is an integer which can be thousands, while AveBedrms is around 4 and Latitude is in degree.\nSo the Population coefficient is expressed in “100k$/habitant” while the AveBedrms is expressed in “100k$/nb of bedrooms” and the Latitude coefficient in “100k$/degree”. We see that changing population by one does not change the outcome, while as we go south (latitude increase) the price becomes cheaper. Also, adding a bedroom (keeping all other feature constant) shall rise the price of the house by 80k$.\nSo looking at the coefficient plot to gauge feature importance can be misleading as some of them vary on a small scale, while others vary a lot more, several decades. So before any interpretation, we need to scale each column (removing the mean and scaling the variance to 1).\n\n\nCode\nmodel = make_pipeline(StandardScaler(), RidgeCV())\n\nmodel.fit(X_train, y_train)\n\nprint(f'model score on training data: {model.score(X_train, y_train)}')\nprint(f'model score on testing data: {model.score(X_test, y_test)}')\n\n\nmodel score on training data: 0.6049222473801685\nmodel score on testing data: 0.586090835494786\n\n\n\n\nCode\ncoefs = pd.DataFrame(\n   model[1].coef_,\n   columns=['Coefficients'], index=X_train.columns\n)\n\ncoefs.plot(kind='barh', figsize=(9, 7))\nplt.title('Ridge model')\nplt.axvline(x=0, color='.5')\nplt.subplots_adjust(left=.3)\n\n\n\n\n\nNow that the coefficients have been scaled, we can safely compare them. The MedInc feature, with longitude and latitude are the three variables that most influence the model.\nThe plot above tells us about dependencies between a specific feature and the target when all other features remain constant, i.e., conditional dependencies. An increase of the HouseAge will induce an increase of the price when all other features remain constant. On the contrary, an increase of the AveRooms will induce an decrease of the price when all other features remain constant.\nWe can check the coefficient variability through cross-validation: it is a form of data perturbation.\n\n\nCode\ncv_model = cross_validate(\n   model, X_with_rnd_feat, y, cv=RepeatedKFold(n_splits=5, n_repeats=5),\n   return_estimator=True, n_jobs=2\n)\ncoefs = pd.DataFrame(\n   [model[1].coef_\n    for model in cv_model['estimator']],\n   columns=X_with_rnd_feat.columns\n)\nplt.figure(figsize=(9, 7))\nsns.boxplot(data=coefs, orient='h', color='cyan', saturation=0.5)\nplt.axvline(x=0, color='.5')\nplt.xlabel('Coefficient importance')\nplt.title('Coefficient importance and its variability')\nplt.subplots_adjust(left=.3)\n\n\n\n\n\nNow if we want to select the four features which are the most important according to the coefficients. The SelectFromModel() is meant just for that. SelectFromModel() accepts a threshold parameter and will select the features whose importance (defined by the coefficients) are above this threshold.\n\n\nCode\nmodel\n\n\nPipeline(steps=[('standardscaler', StandardScaler()), ('ridgecv', RidgeCV())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()), ('ridgecv', RidgeCV())])StandardScalerStandardScaler()RidgeCVRidgeCV()\n\n\n\n\nCode\nimportance = np.abs(model[1].coef_)\nthreshold = np.sort(importance)[-5] + 0.01\n\n\n\n\nCode\nsfm = SelectFromModel(model[1], threshold=threshold).fit(X, y)\nprint(f\"Features selected by SelectFromModel: {sfm.get_feature_names_out()}\")\n\n\nFeatures selected by SelectFromModel: ['MedInc' 'AveBedrms' 'Latitude' 'Longitude']\n\n\n\n6.2.5.1 Linear models with sparse coefficients (Lasso)\nIn it important to keep in mind that the associations extracted depend on the model. To illustrate this point we consider a Lasso model, that performs feature selection with a L1 penalty. Let us fit a Lasso model with a strong regularization parameters alpha\n\n\nCode\nmodel = make_pipeline(StandardScaler(), Lasso(alpha=.015))\n\nmodel.fit(X_train, y_train)\n\nprint(f'model score on training data: {model.score(X_train, y_train)}')\nprint(f'model score on testing data: {model.score(X_test, y_test)}')\n\n\nmodel score on training data: 0.5933235371761756\nmodel score on testing data: 0.5673786563118284\n\n\n\n\nCode\ncoefs = pd.DataFrame(\n   model[1].coef_,\n   columns=['Coefficients'], index=X_train.columns\n)\n\ncoefs.plot(kind='barh', figsize=(9, 7))\nplt.title('Lasso model, strong regularization')\nplt.axvline(x=0, color='.5')\nplt.subplots_adjust(left=.3)\n\n\n\n\n\nHere the model score is a bit lower, because of the strong regularization. However, it has zeroed out 3 coefficients, selecting a small number of variables to make its prediction.\n\n\n6.2.5.2 Randomforest with feature importance\nOn some algorithms, there are some feature importance methods, inherently built within the model. It is the case in RandomForest models. Let’s investigate the built-in feature_importances_ attribute.\n\n\nCode\nmodel = RandomForestRegressor()\n\nmodel.fit(X_train, y_train)\n\nprint(f'model score on training data: {model.score(X_train, y_train)}')\nprint(f'model score on testing data: {model.score(X_test, y_test)}')\n\n\nmodel score on training data: 0.9796643318943656\nmodel score on testing data: 0.8429137479202747\n\n\n\n\nCode\nimportances = model.feature_importances_\nindices = np.argsort(importances)\n\nfig, ax = plt.subplots()\nax.barh(range(len(importances)), importances[indices])\nax.set_yticks(range(len(importances)))\n_ = ax.set_yticklabels(np.array(X_train.columns)[indices])\n\n\n\n\n\nMedInc is still the most important feature. It also has a small bias toward high cardinality features, such as the noisy feature rnd_num, which are here predicted having 0.1 importance, more than HouseAge (which has low cardinality).\n\n\n6.2.5.3 Feature importance by permutation\nWe introduce here a new technique to evaluate the feature importance of any given fitted model. It basically shuffles a feature and sees how the model changes its prediction. Thus, the change in prediction will correspond to the feature importance.\n\n\nCode\n# Any model could be used here\n\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\nprint(f'model score on training data: {model.score(X_train, y_train)}')\nprint(f'model score on testing data: {model.score(X_test, y_test)}')\n\n\nmodel score on training data: 0.9795458070557226\nmodel score on testing data: 0.8448700648905965\n\n\n\n\nCode\nr = permutation_importance(model, X_test, y_test, n_repeats=30, random_state=42) # Notice that the model are already fitted\n\n\n\n\nCode\nfig, ax = plt.subplots()\n\nindices = r.importances_mean.argsort()\nplt.barh(range(len(indices)), r.importances_mean[indices], xerr=r.importances_std[indices])\n\nax.set_yticks(range(len(indices)))\n_ = ax.set_yticklabels(np.array(X_train.columns)[indices])\n\n\n\n\n\nWe see again that the feature MedInc, Latitude and Longitude are important for the model. We note that our random variable rnd_num is now less important than Latitude. Indeed, the feature importance built-in in RandomForest has bias for continuous data, such as AveOccup and rnd_num.\n\n\n6.2.5.4 Feature rejection using Boruta\n\n\nCode\n# define Boruta feature selection method\nmodel = RandomForestRegressor()\nfeat_selector = BorutaPy(model, n_estimators='auto', verbose=2, random_state=1)\n\n\n\n\nCode\n# find all relevant features \nfeat_selector.fit(X_train.to_numpy(), y_train.to_numpy())\n\n\nIteration:  1 / 100\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  2 / 100\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  3 / 100\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  4 / 100\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  5 / 100\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  6 / 100\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  7 / 100\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  8 / 100\nConfirmed:  9\nTentative:  0\nRejected:   1\n\n\nBorutaPy finished running.\n\nIteration:  9 / 100\nConfirmed:  9\nTentative:  0\nRejected:   1\n\n\nBorutaPy(estimator=RandomForestRegressor(n_estimators=44,\n                                         random_state=RandomState(MT19937) at 0x7F60C3A0E640),\n         n_estimators='auto',\n         random_state=RandomState(MT19937) at 0x7F60C3A0E640, verbose=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BorutaPyBorutaPy(estimator=RandomForestRegressor(n_estimators=44,\n                                         random_state=RandomState(MT19937) at 0x7F60C3A0E640),\n         n_estimators='auto',\n         random_state=RandomState(MT19937) at 0x7F60C3A0E640, verbose=2)estimator: RandomForestRegressorRandomForestRegressor(n_estimators=44,\n                      random_state=RandomState(MT19937) at 0x7F60C3A0E640)RandomForestRegressorRandomForestRegressor(n_estimators=44,\n                      random_state=RandomState(MT19937) at 0x7F60C3A0E640)\n\n\n\n\nCode\nX_train\n\n\n\n  \n    \n      \n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nrnd_bin\nrnd_num\n\n\n\n\n4901\n1.3287\n43.0\n4.036723\n1.090395\n1412.0\n3.988701\n34.01\n-118.25\n0\n4901\n\n\n4375\n2.3421\n48.0\n3.425532\n1.046809\n633.0\n2.693617\n34.10\n-118.28\n0\n4375\n\n\n6698\n3.6572\n26.0\n4.160797\n1.093023\n3001.0\n1.994020\n34.14\n-118.10\n0\n6698\n\n\n9805\n3.2750\n52.0\n8.357827\n1.543131\n582.0\n1.859425\n36.55\n-121.92\n1\n9805\n\n\n1101\n3.5189\n15.0\n5.489011\n1.027473\n1786.0\n2.453297\n39.82\n-121.68\n0\n1101\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5734\n5.1718\n31.0\n5.676417\n1.063985\n1359.0\n2.484461\n34.16\n-118.23\n0\n5734\n\n\n5191\n1.5256\n36.0\n4.897778\n1.097778\n702.0\n3.120000\n33.93\n-118.26\n1\n5191\n\n\n5390\n2.9344\n36.0\n3.986717\n1.079696\n1756.0\n3.332068\n34.03\n-118.38\n0\n5390\n\n\n860\n5.7192\n15.0\n6.395349\n1.067979\n1777.0\n3.178891\n37.58\n-121.96\n0\n860\n\n\n7270\n2.3900\n25.0\n3.928287\n1.235060\n1439.0\n5.733068\n33.98\n-118.23\n1\n7270\n\n\n\n\n\n7500 rows × 10 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n# check selected features \nnp.array(X_train.columns)[feat_selector.support_]\n\n\narray(['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population',\n       'AveOccup', 'Latitude', 'Longitude', 'rnd_num'], dtype=object)\n\n\n\n\nCode\n# check ranking of features\nfeat_selector.ranking_\n\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 2, 1])\n\n\n\n\nCode\n# call transform() on X to filter it down to selected features\nX_filtered = feat_selector.transform(X_train.to_numpy())\nX_filtered.shape\n\n\n(7500, 9)"
  },
  {
    "objectID": "05_Feature_selection_extraction.html#dimensional-reduction",
    "href": "05_Feature_selection_extraction.html#dimensional-reduction",
    "title": "6  Feature selection and extraction",
    "section": "6.3 Dimensional reduction",
    "text": "6.3 Dimensional reduction\nWe now looked at our model-based method for feature engineering: principal component analysis (PCA). You could think of PCA as a partitioning of the variation in the data. PCA is a great tool to help you discover important relationships in the data and can also be used to create more informative features.\nThere are two ways you could use PCA for feature engineering.\n\nThe first way is to use it as a descriptive technique. Since the components tell you about the variation, you could compute the MI scores for the components and see what kind of variation is most predictive of your target. That could give you ideas for kinds of features to create – a product of 'Height' and 'Diameter' if 'Size' is important, say, or a ratio of 'Height' and 'Diameter' if Shape is important. You could even try clustering on one or more of the high-scoring components. Biplot will be useful in this case.\nThe second way is to use the components themselves as features. Because the components expose the variational structure of the data directly, they can often be more informative than the original features. Here are some use-cases:\n\n\nDimensionality reduction: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.\nAnomaly detection: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.\nNoise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.\nDecorrelation: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.\n\nPCA basically gives you direct access to the correlational structure of your data. You’ll no doubt come up with applications of your own!\n\n\nCode\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\n\n\n\nCode\ndf = pd.read_csv(\"autos.csv\")\ndf.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsymboling\nmake\nfuel_type\naspiration\nnum_of_doors\nbody_style\ndrive_wheels\nengine_location\nwheel_base\nlength\n...\nengine_size\nfuel_system\nbore\nstroke\ncompression_ratio\nhorsepower\npeak_rpm\ncity_mpg\nhighway_mpg\nprice\n\n\n\n\n0\n3\nalfa-romero\ngas\nstd\n2\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9\n111\n5000\n21\n27\n13495\n\n\n1\n3\nalfa-romero\ngas\nstd\n2\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9\n111\n5000\n21\n27\n16500\n\n\n2\n1\nalfa-romero\ngas\nstd\n2\nhatchback\nrwd\nfront\n94.5\n171.2\n...\n152\nmpfi\n2.68\n3.47\n9\n154\n5000\n19\n26\n16500\n\n\n3\n2\naudi\ngas\nstd\n4\nsedan\nfwd\nfront\n99.8\n176.6\n...\n109\nmpfi\n3.19\n3.40\n10\n102\n5500\n24\n30\n13950\n\n\n4\n2\naudi\ngas\nstd\n4\nsedan\n4wd\nfront\n99.4\n176.6\n...\n136\nmpfi\n3.19\n3.40\n8\n115\n5500\n18\n22\n17450\n\n\n\n\n\n5 rows × 25 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe’ve selected four features that cover a range of properties. Each of these features also has a high MI score with the target, price. We’ll standardize the data since these features aren’t naturally on the same scale.\n\n\nCode\nfeatures = [\"highway_mpg\", \"engine_size\", \"horsepower\", \"curb_weight\"]\n\nX = df.copy()\ny = X.pop('price')\nX = X.loc[:, features]\n\n# Standardize\nX_scaled = (X - X.mean(axis=0)) / X.std(axis=0)\n\n\nNow we can fit scikit-learn’s PCA estimator and create the principal components. You can see here the first few rows of the transformed dataset.\n\n\nCode\n# Create principal components\npca = PCA()\nX_pca = pca.fit_transform(X_scaled)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca, columns=component_names)\n\nX_pca.head(10)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\n\n\n\n\n0\n0.382486\n-0.400222\n0.124122\n0.169539\n\n\n1\n0.382486\n-0.400222\n0.124122\n0.169539\n\n\n2\n1.550890\n-0.107175\n0.598361\n-0.256081\n\n\n3\n-0.408859\n-0.425947\n0.243335\n0.013920\n\n\n4\n1.132749\n-0.814565\n-0.202885\n0.224138\n\n\n5\n0.547265\n-0.545141\n0.139969\n0.424955\n\n\n6\n0.869268\n-0.472834\n-0.294073\n0.090174\n\n\n7\n0.974373\n-0.449233\n-0.435749\n-0.019102\n\n\n8\n1.796553\n-1.050783\n-0.081821\n-0.296071\n\n\n9\n-0.306514\n-0.542020\n0.138605\n0.012612\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nAfter fitting, the PCA instance contains the loadings in its components_ attribute. We’ll wrap the loadings up in a dataframe.\n\n\nCode\nloadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=X.columns,  # and the rows are the original features\n)\nloadings\n\n\n\n  \n    \n      \n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\n\n\n\n\nhighway_mpg\n-0.492347\n0.770892\n0.070142\n-0.397996\n\n\nengine_size\n0.503859\n0.626709\n0.019960\n0.594107\n\n\nhorsepower\n0.500448\n0.013788\n0.731093\n-0.463534\n\n\ncurb_weight\n0.503262\n0.113008\n-0.678369\n-0.523232\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n# biplot \nxvector = pca.components_[0] \nyvector = -pca.components_[1]\n\nxs = X_pca.to_numpy()[:,0] \nys = -X_pca.to_numpy()[:,1]\n\nfig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(10, 9))\n\n# First Plot : Principal Component Scores \nx_min, x_max = -8, 8\ny_min, y_max = -3, 3\nax1.set_xlim(x_min, x_max)\nax1.set_ylim(y_min, y_max)\n\nfor i in range(len(xs)):\n    plt.plot(xs[i], ys[i], 'bo')\n\nax1.set_xlabel(\"1'st Principal Component Scores\")\nax1.set_ylabel(\"2'nd Principal Component Scores\")\n\n# Plot reference lines\nax1.hlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\nax1.vlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\n\n# Second Plot : Principal Component Loadings 'PCs' \nx_min, x_max = -1, 1          \ny_min, y_max = -1, 1\n\nax2 = ax1.twinx().twiny()\nax2.set_xlim(x_min, x_max)\nax2.set_ylim(y_min, y_max)\n\nfor i in range(len(xvector)):\n    ax2.arrow(0, 0, xvector[i],  yvector[i], color='red', width=0.005, head_width=0.02)\n    ax2.annotate(X.columns[i], (xvector[i]*1.05, yvector[i]*1.05), color='red', size=14)  \n\n    \nax2.set_xlabel(\"1'st Principal Component (Loading)\", color='red')\nax2.set_ylabel(\"2'nd Principal Component (Loading)\", color='red')\n\n\nText(0, 0.5, \"2'nd Principal Component (Loading)\")\n\n\n\n\n\nRecall that the signs and magnitudes of a component’s loadings tell us what kind of variation it’s captured. The first component (PC1) shows a contrast between large, powerful vehicles with poor gas milage, and smaller, more economical vehicles with good gas milage. We might call this the “Luxury/Economy” axis. The next figure shows that our four chosen features mostly vary along the Luxury/Economy axis.\n\n\nCode\n# Look at explained variance\nplot_variance(pca)\n\n\narray([&lt;Axes: title={'center': '% Explained Variance'}, xlabel='Component'&gt;,\n       &lt;Axes: title={'center': '% Cumulative Variance'}, xlabel='Component'&gt;],\n      dtype=object)\n\n\n\n\n\nLet’s also look at the MI scores of the components. Not surprisingly, PC1 is highly informative, though the remaining components, despite their small variance, still have a significant relationship with price. Examining those components could be worthwhile to find relationships not captured by the main Luxury/Economy axis.\n\n\nCode\nmi_scores = make_mi_scores(X_pca, y, discrete_features=False)\nmi_scores\n\n\nPC1    1.013739\nPC2    0.379649\nPC3    0.306207\nPC4    0.204905\nName: MI Scores, dtype: float64\n\n\nThe third component shows a contrast between horsepower and curb_weight – sports cars vs. wagons, it seems.\n\n\nCode\n# Show dataframe sorted by PC3\nidx = X_pca[\"PC3\"].sort_values(ascending=False).index\ncols = [\"make\", \"body_style\", \"horsepower\", \"curb_weight\"]\ndf.loc[idx, cols]\n\n\n\n  \n    \n      \n\n\n\n\n\n\nmake\nbody_style\nhorsepower\ncurb_weight\n\n\n\n\n118\nporsche\nhardtop\n207\n2756\n\n\n117\nporsche\nhardtop\n207\n2756\n\n\n119\nporsche\nconvertible\n207\n2800\n\n\n45\njaguar\nsedan\n262\n3950\n\n\n96\nnissan\nhatchback\n200\n3139\n\n\n...\n...\n...\n...\n...\n\n\n59\nmercedes-benz\nwagon\n123\n3750\n\n\n61\nmercedes-benz\nsedan\n123\n3770\n\n\n101\npeugot\nwagon\n95\n3430\n\n\n105\npeugot\nwagon\n95\n3485\n\n\n143\ntoyota\nwagon\n62\n3110\n\n\n\n\n\n193 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nTo express this contrast, let’s create a new ratio feature:\n\n\nCode\ndf[\"sports_or_wagon\"] = X.curb_weight / X.horsepower\nsns.regplot(x=\"sports_or_wagon\", y='price', data=df, order=2);"
  },
  {
    "objectID": "05_Feature_selection_extraction.html#manifold-learning",
    "href": "05_Feature_selection_extraction.html#manifold-learning",
    "title": "6  Feature selection and extraction",
    "section": "6.4 Manifold learning",
    "text": "6.4 Manifold learning\n\n6.4.0.1 t-SNE\n\n\nCode\ndigits = load_digits()\n\n\n\n\nCode\nX = digits.images.reshape(-1, digits.images.shape[1]*digits.images.shape[2])\nX.shape\n\n\n(1797, 64)\n\n\n\n\nCode\nfig, ax_array = plt.subplots(5, 5)\naxes = ax_array.flatten()\nfor i, ax in enumerate(axes):\n    ax.imshow(digits.images[i], cmap='gray_r')\nplt.setp(axes, xticks=[], yticks=[], frame_on=False)\nplt.tight_layout(h_pad=0.5, w_pad=0.01)\n\n\n\n\n\n\n\nCode\notsne = oTSNE(\n    n_components=2,\n    perplexity=30,\n    initialization='pca', \n    n_jobs=2,\n    random_state=0,\n    negative_gradient_method='auto', \n    verbose=True,\n)\n\n\n\n\nCode\nembedding = otsne.fit(X)\n\n\n--------------------------------------------------------------------------------\nTSNE(early_exaggeration=12, n_jobs=2, random_state=0, verbose=True)\n--------------------------------------------------------------------------------\n===&gt; Finding 90 nearest neighbors using Annoy approximate search using euclidean distance...\n   --&gt; Time elapsed: 0.79 seconds\n===&gt; Calculating affinity matrix...\n   --&gt; Time elapsed: 0.08 seconds\n===&gt; Calculating PCA-based initialization...\n   --&gt; Time elapsed: 0.01 seconds\n===&gt; Running optimization with exaggeration=12.00, lr=149.75 for 250 iterations...\nIteration   50, KL divergence 2.6162, 50 iterations in 0.5615 sec\nIteration  100, KL divergence 2.6249, 50 iterations in 0.4249 sec\nIteration  150, KL divergence 2.6110, 50 iterations in 0.5087 sec\nIteration  200, KL divergence 2.6054, 50 iterations in 0.4346 sec\nIteration  250, KL divergence 2.6028, 50 iterations in 0.4644 sec\n   --&gt; Time elapsed: 2.40 seconds\n===&gt; Running optimization with exaggeration=1.00, lr=1797.00 for 500 iterations...\nIteration   50, KL divergence 0.9203, 50 iterations in 0.4606 sec\nIteration  100, KL divergence 0.8272, 50 iterations in 0.4424 sec\nIteration  150, KL divergence 0.7951, 50 iterations in 0.4422 sec\nIteration  200, KL divergence 0.7783, 50 iterations in 0.4284 sec\nIteration  250, KL divergence 0.7685, 50 iterations in 0.4518 sec\nIteration  300, KL divergence 0.7625, 50 iterations in 1.4672 sec\nIteration  350, KL divergence 0.7580, 50 iterations in 1.2427 sec\nIteration  400, KL divergence 0.7539, 50 iterations in 0.4364 sec\nIteration  450, KL divergence 0.7514, 50 iterations in 0.4209 sec\nIteration  500, KL divergence 0.7492, 50 iterations in 1.7576 sec\n   --&gt; Time elapsed: 7.56 seconds\n\n\n\n\nCode\nplt.figure(figsize=(10, 8))\nplt.scatter(embedding[:, 0], embedding[:, 1], c=digits.target, cmap='Spectral', s=5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\nplt.title('tSNE of the Digits dataset', fontsize=24)\n\n\nText(0.5, 1.0, 'tSNE of the Digits dataset')\n\n\n\n\n\n\n\n6.4.0.2 UMAP\nUMAP is useful for generating visualisations, but if you want to make use of UMAP more generally for machine learning tasks it is important to be be able to train a model and then later pass new data to the model and have it transform that data into the learned space. For example if we use UMAP to learn a latent space and then train a classifier on data transformed into the latent space then the classifier is only useful for prediction if we can transform data for which we want a prediction into the latent space the classifier uses.\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, stratify=digits.target, random_state=42)\n\n\nNow to get a benchmark idea of what we are looking at let’s train a couple of different classifiers and then see how well they score on the test set. For this example let’s try a support vector classifier and a KNN classifier.\n\n\nCode\nsvc = SVC(gamma='auto').fit(X_train, y_train)\nknn = KNeighborsClassifier().fit(X_train, y_train)\nsvc.score(X_test, y_test), knn.score(X_test, y_test)\n\n\n(0.62, 0.9844444444444445)\n\n\nThe goal now is to make use of UMAP as a preprocessing step that one could potentially fit into a pipeline.\n\n\nCode\ntrans = umap.UMAP(n_neighbors=5, random_state=42).fit(X_train)\n\n\n\n\nCode\nplt.figure(figsize=(10, 8))\nplt.scatter(trans.embedding_[:, 0], trans.embedding_[:, 1], c=y_train, cmap='Spectral', s=5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\nplt.title('Umap of the Digits dataset', fontsize=24)\n\n\nText(0.5, 1.0, 'Umap of the Digits dataset')\n\n\n\n\n\nThis looks very promising! Most of the classes got very cleanly separated, and that gives us some hope that it could help with classifier performance. We can now train some new models (again an SVC and a KNN classifier) on the embedded training data. This looks exactly as before but now we pass it the embedded data.\n\n\nCode\nsvc = SVC(gamma='auto').fit(trans.embedding_, y_train)\nknn = KNeighborsClassifier().fit(trans.embedding_, y_train)\n\n\n\n\nCode\ntest_embedding = trans.transform(X_test)\n\n\nThe next important question is what the transform did to our test data. In principle we have a new two dimensional representation of the test-set, and ideally this should be based on the existing embedding of the training set\n\n\nCode\nplt.figure(figsize=(10, 8))\nplt.scatter(test_embedding[:, 0], test_embedding[:, 1], c=y_test, cmap='Spectral', s=5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\nplt.title('Umap of the Digits dataset', fontsize=24)\n\n\nText(0.5, 1.0, 'Umap of the Digits dataset')\n\n\n\n\n\nThe results look like what we should expect; the test data has been embedded into two dimensions in exactly the locations we should expect (by class) given the embedding of the training data visualised above. This means we can now try out models that were trained on the embedded training data by handing them the newly transformed test set.\n\n\nCode\ntrans.transform(X_test)\n\n\narray([[ 4.73177004e+00,  2.46851373e+00],\n       [ 1.91173019e+01,  3.47619963e+00],\n       [ 7.86290264e+00,  1.09972172e+01],\n       [-8.19267273e+00,  3.35794115e+00],\n       [ 7.09956217e+00,  1.42389803e+01],\n       [ 1.32900066e+01,  1.82322578e+01],\n       [ 2.65943050e-01, -8.95168245e-01],\n       [ 8.13252068e+00,  1.00992136e+01],\n       [ 5.73924875e+00,  2.07148552e+00],\n       [ 5.40985489e+00,  1.51770325e+01],\n       [ 6.47868681e+00,  1.46163197e+01],\n       [ 7.92969179e+00,  9.91152573e+00],\n       [ 1.17540598e+01, -5.85403776e+00],\n       [ 1.21744642e+01, -6.17784595e+00],\n       [ 6.87150419e-01, -2.17636895e+00],\n       [ 1.28335676e+01,  1.65336361e+01],\n       [ 6.92930698e+00,  1.49666691e+01],\n       [ 1.00950708e+01,  6.95953965e-01],\n       [ 3.05954218e+00,  4.18441391e+00],\n       [ 1.22419624e+01,  1.81537476e+01],\n       [ 8.41183376e+00,  1.01441069e+01],\n       [ 9.57836246e+00,  2.75005311e-01],\n       [ 1.93220310e+01,  2.32694030e+00],\n       [ 1.28659058e+01,  1.85482330e+01],\n       [ 1.96019783e+01,  2.01119971e+00],\n       [ 4.99640083e+00,  1.86033607e+00],\n       [ 1.13867264e+01, -6.41329575e+00],\n       [ 2.00881138e+01,  3.05871224e+00],\n       [ 3.00027013e+00, -8.01352322e-01],\n       [ 2.92265922e-01, -2.01319861e+00],\n       [ 1.41106904e+00,  9.47769833e+00],\n       [ 2.43231082e+00,  1.04742994e+01],\n       [ 4.24076748e+00,  3.97745585e+00],\n       [-6.14816606e-01, -1.45310473e+00],\n       [ 1.03815241e+01,  1.65336680e+00],\n       [ 1.33825254e+01,  1.69268723e+01],\n       [ 7.61123228e+00,  1.40565615e+01],\n       [ 5.55537558e+00,  2.05418849e+00],\n       [ 6.94975233e+00,  1.43994637e+01],\n       [ 8.03399944e+00,  1.41689272e+01],\n       [ 1.11499023e+01,  1.69359303e+00],\n       [ 1.93824215e+01,  1.83013022e+00],\n       [ 4.15738869e+00,  5.18572140e+00],\n       [ 3.60030508e+00,  5.01691294e+00],\n       [ 8.05832386e+00,  1.41819515e+01],\n       [ 3.83167720e+00,  4.41925764e+00],\n       [ 1.04558792e+01,  1.46102712e-01],\n       [ 1.96752357e+00,  9.15024090e+00],\n       [ 1.96036167e+01,  2.88023281e+00],\n       [ 4.75290745e-01, -3.02901536e-01],\n       [ 1.28543739e+01, -6.69067383e+00],\n       [ 7.68392706e+00,  1.35354586e+01],\n       [ 5.70900965e+00,  2.04589939e+00],\n       [ 1.27460394e+01, -7.56970310e+00],\n       [-8.09807491e+00,  3.45251179e+00],\n       [-8.20304966e+00,  3.34776187e+00],\n       [ 1.01384201e+01,  6.57227516e-01],\n       [ 1.05179510e+01,  1.68638420e+00],\n       [ 7.96865273e+00,  1.05273275e+01],\n       [ 4.90999174e+00,  1.58829975e+01],\n       [ 8.63133049e+00, -4.62174177e+00],\n       [ 7.42704582e+00,  1.51531487e+01],\n       [ 1.93333473e+01,  1.75126565e+00],\n       [ 7.81681252e+00,  1.01148701e+01],\n       [ 1.88073101e+01,  2.74154377e+00],\n       [ 1.29166088e+01, -6.52322578e+00],\n       [ 9.41298866e+00,  1.08080494e+00],\n       [ 6.16237783e+00, -9.58264709e-01],\n       [ 7.35584676e-01, -1.40368450e+00],\n       [ 7.49761915e+00,  9.53435230e+00],\n       [ 1.26329851e+01,  1.74112072e+01],\n       [ 6.09630585e+00, -2.83607483e-01],\n       [ 4.56159449e+00,  4.63957405e+00],\n       [ 5.21145630e+00,  1.55418911e+01],\n       [ 8.52060318e+00, -5.29649198e-01],\n       [ 1.76717103e+00,  9.06908798e+00],\n       [ 1.12594156e+01,  1.60219014e+00],\n       [ 1.06761303e+01,  5.61923206e-01],\n       [ 1.52136588e+00,  9.05504322e+00],\n       [ 1.22102089e+01,  1.73202934e+01],\n       [ 6.19346762e+00, -2.01016366e-01],\n       [ 1.95389805e+01,  2.73830152e+00],\n       [ 6.07237339e+00, -7.12067902e-01],\n       [ 1.18179951e+01, -7.24027014e+00],\n       [ 2.35402918e+00,  9.97527313e+00],\n       [ 8.05538595e-01, -1.55245471e+00],\n       [-5.48529863e-01, -1.44640124e+00],\n       [ 8.46719360e+00, -4.49677277e+00],\n       [ 2.63080096e+00, -4.21153259e+00],\n       [ 1.34470768e+01,  1.80865421e+01],\n       [ 7.93155432e+00,  1.32082472e+01],\n       [ 2.90374517e+00,  5.25399637e+00],\n       [ 1.21058340e+01, -5.75693417e+00],\n       [ 1.18059082e+01,  1.69832764e+01],\n       [-2.30677761e-02, -2.63910270e+00],\n       [ 7.50664759e+00,  1.45911045e+01],\n       [ 2.61786890e+00, -4.19906235e+00],\n       [ 6.76057696e-01, -2.03193378e+00],\n       [ 1.87298660e+01,  2.62258267e+00],\n       [ 7.26300764e+00,  1.50124683e+01],\n       [ 1.90552788e+01,  3.07881856e+00],\n       [ 6.09509850e+00, -7.56763577e-01],\n       [ 6.17576075e+00, -7.26652086e-01],\n       [ 7.98868608e+00,  9.83880329e+00],\n       [ 1.21262283e+01, -7.42956352e+00],\n       [ 9.36670208e+00,  9.97476220e-01],\n       [ 1.25175686e+01,  1.71377945e+01],\n       [ 6.26237488e+00,  1.43485527e+01],\n       [ 5.53839064e+00,  1.20771849e+00],\n       [ 1.95191860e+01,  3.09513426e+00],\n       [ 1.93319950e+01,  2.04565048e+00],\n       [ 7.90213203e+00,  1.01868477e+01],\n       [ 4.40916538e+00,  4.91170883e+00],\n       [ 1.59102690e+00,  5.91555834e+00],\n       [ 1.06758175e+01,  7.85464406e-01],\n       [ 8.50780201e+00, -4.53833485e+00],\n       [ 3.33708262e+00,  4.26565790e+00],\n       [ 1.03685141e+01,  1.33268273e+00],\n       [ 5.50983715e+00,  1.92333841e+00],\n       [ 9.89408302e+00,  7.60242999e-01],\n       [ 1.92521572e+01,  3.28466463e+00],\n       [ 1.93915577e+01,  1.80793393e+00],\n       [-9.59185779e-01, -1.43418705e+00],\n       [ 5.38180447e+00,  1.55832243e+01],\n       [-2.10883573e-01, -1.04512537e+00],\n       [ 6.69982815e+00, -7.31428787e-02],\n       [ 2.64970207e+00,  1.04629345e+01],\n       [ 2.70043421e+00, -4.25971699e+00],\n       [ 1.29440527e+01, -6.61230564e+00],\n       [ 9.27611649e-01, -9.55748022e-01],\n       [ 1.18965158e+01, -6.58824778e+00],\n       [ 1.23818903e+01,  1.64546566e+01],\n       [ 2.48696733e+00, -4.27661419e+00],\n       [ 9.49136913e-01, -8.76113117e-01],\n       [ 1.91355953e+01,  2.56769919e+00],\n       [ 2.03352299e+01,  3.30455041e+00],\n       [ 5.40721941e+00,  1.76240933e+00],\n       [ 1.20430918e+01,  1.79942932e+01],\n       [ 1.66636074e+00,  8.91286182e+00],\n       [ 6.12154388e+00,  1.49706059e+01],\n       [ 5.76479673e+00,  2.05096912e+00],\n       [ 1.26848288e+01,  1.63744678e+01],\n       [ 1.91420193e+01,  3.13646555e+00],\n       [ 7.81396389e+00,  1.00339603e+01],\n       [ 1.26516829e+01,  1.84274235e+01],\n       [ 3.53068799e-01, -9.54452991e-01],\n       [ 1.02022676e+01,  1.75975370e+00],\n       [ 4.81706905e+00,  2.32386041e+00],\n       [ 1.26537113e+01,  1.69497089e+01],\n       [ 1.23623486e+01,  1.78191795e+01],\n       [ 8.54515076e-01, -1.01536226e+00],\n       [ 7.05518866e+00,  1.40135460e+01],\n       [ 2.21469665e+00,  5.26067495e+00],\n       [ 2.56542659e+00,  5.00661182e+00],\n       [ 1.80082548e+00,  9.62335300e+00],\n       [ 1.07883806e+01,  1.61723030e+00],\n       [ 9.88454342e+00, -2.33417273e-01],\n       [ 8.17566204e+00,  9.67235184e+00],\n       [ 1.88418846e+01,  2.49431753e+00],\n       [ 1.28272562e+01, -6.51219225e+00],\n       [ 8.06352520e+00,  1.05998135e+01],\n       [ 1.97371750e+01,  3.02595925e+00],\n       [ 8.25599575e+00,  1.07834330e+01],\n       [ 1.17708435e+01,  1.67840595e+01],\n       [ 1.96602249e+01,  2.35182858e+00],\n       [-4.44723725e-01, -1.08014917e+00],\n       [ 1.22830667e+01,  1.63258533e+01],\n       [ 1.00864286e+01,  1.41382837e+00],\n       [ 9.63279915e+00,  1.22358000e+00],\n       [ 1.17074785e+01,  1.73078613e+01],\n       [ 4.91881818e-01, -1.86782694e+00],\n       [ 9.27524626e-01, -3.64588648e-01],\n       [ 1.03519487e+01,  1.71525097e+00],\n       [ 1.21878176e+01,  1.66502571e+01],\n       [ 2.54178166e+00, -4.13714075e+00],\n       [-7.94874549e-01, -1.32138753e+00],\n       [ 8.94740105e+00, -7.02543706e-02],\n       [ 1.60824668e+00,  9.57526588e+00],\n       [ 7.69076777e+00,  1.38979092e+01],\n       [ 1.19308853e+01, -5.87281370e+00],\n       [ 1.96669064e+01,  2.94271183e+00],\n       [ 4.00215197e+00,  5.47322416e+00],\n       [ 1.04111280e+01,  6.09881997e-01],\n       [ 7.43757534e+00,  1.38516932e+01],\n       [ 4.26444292e+00,  4.87076139e+00],\n       [ 2.90101218e+00,  8.00403118e+00],\n       [ 7.60269547e+00,  1.41803226e+01],\n       [ 1.09815300e+00, -4.20335007e+00],\n       [ 3.72613168e+00,  5.09122086e+00],\n       [ 2.86066222e+00,  1.06173820e+01],\n       [ 2.42815518e+00, -4.00461817e+00],\n       [ 2.61127090e+00,  5.14484406e+00],\n       [ 1.18339100e+01,  1.67339821e+01],\n       [ 1.22553959e+01, -6.08764076e+00],\n       [ 4.20773602e+00,  4.65283203e+00],\n       [ 8.62094879e+00, -4.78529274e-01],\n       [ 1.29443531e+01, -6.50885916e+00],\n       [ 5.15094805e+00,  3.08955741e+00],\n       [ 1.90184188e+00, -4.20560122e+00],\n       [ 7.78751183e+00,  1.03105097e+01],\n       [ 4.66492891e+00,  4.62675238e+00],\n       [ 7.20657921e+00,  1.00898695e+01],\n       [ 1.23361588e+01, -7.76199770e+00],\n       [ 7.19676876e+00,  1.45420494e+01],\n       [-8.28833485e+00,  3.26244545e+00],\n       [ 1.93086472e+01,  2.02614713e+00],\n       [ 8.56536388e+00,  1.08816519e+01],\n       [ 1.56364882e+00,  9.02052307e+00],\n       [ 1.93720913e+01,  3.13724089e+00],\n       [ 1.03784208e+01,  1.45391178e+00],\n       [ 1.01358051e+01, -4.34333980e-01],\n       [ 7.30056190e+00,  1.39617376e+01],\n       [ 7.31627369e+00,  1.48641815e+01],\n       [ 2.33456850e+00,  8.97941303e+00],\n       [ 2.09031320e+00,  9.15919304e+00],\n       [ 1.35915003e+01,  1.69954529e+01],\n       [ 2.61211252e+00,  9.67325783e+00],\n       [ 5.70830059e+00,  2.07967734e+00],\n       [ 2.03164749e+01,  2.60851431e+00],\n       [ 1.23774891e+01,  1.72216015e+01],\n       [ 9.85767186e-01, -8.03058326e-01],\n       [ 6.18093252e+00, -2.71137953e-01],\n       [ 1.33752537e+01,  1.74806652e+01],\n       [ 7.87491941e+00,  1.15665522e+01],\n       [ 1.19647760e+01,  1.65779037e+01],\n       [ 1.58233976e+00,  8.97871685e+00],\n       [ 1.11217403e+01, -7.53401709e+00],\n       [ 2.77137256e+00,  1.07307339e+01],\n       [ 1.98257885e+01,  3.34123826e+00],\n       [ 9.88493085e-01, -6.36084318e-01],\n       [-5.42631149e-01, -1.49713326e+00],\n       [ 6.00486565e+00, -1.04312032e-01],\n       [ 5.50934029e+00,  1.54812260e+01],\n       [ 6.79059935e+00,  1.43414955e+01],\n       [ 8.20319176e+00,  1.14464417e+01],\n       [ 1.51997113e+00,  9.93821526e+00],\n       [ 9.26871109e+00,  8.75526607e-01],\n       [ 1.15349522e+01, -7.81715679e+00],\n       [ 2.84447312e+00,  4.09177399e+00],\n       [ 3.01040769e+00,  1.07021465e+01],\n       [ 5.26517034e-01,  3.56633514e-01],\n       [ 2.23956490e+00,  1.05628624e+01],\n       [ 7.40757895e+00,  1.05235777e+01],\n       [ 7.57580614e+00,  1.39888668e+01],\n       [ 1.19394760e+01, -5.87033081e+00],\n       [ 1.19141283e+01, -7.70306826e+00],\n       [ 1.45862615e+00,  9.67681885e+00],\n       [ 5.70191717e+00,  1.50980463e+01],\n       [ 1.27075195e+01,  1.71650524e+01],\n       [ 1.90146370e+01,  2.91956520e+00],\n       [ 1.00382977e+01,  1.05415595e+00],\n       [ 1.20603743e+01, -6.73856401e+00],\n       [ 1.21463318e+01, -5.93729019e+00],\n       [ 2.98728800e+00,  5.01034117e+00],\n       [ 4.37886333e+00,  5.13170195e+00],\n       [ 5.71549463e+00,  9.21205699e-01],\n       [ 1.97396123e+00,  8.89696503e+00],\n       [ 1.93121815e+01,  3.62117839e+00],\n       [ 1.83073807e+00, -4.12922049e+00],\n       [ 1.35204191e+01,  1.69217930e+01],\n       [ 3.89947939e+00,  4.44158363e+00],\n       [ 2.01038971e+01,  3.17173958e+00],\n       [ 1.76283216e+00,  8.74818230e+00],\n       [ 1.39133549e+00,  9.86009979e+00],\n       [ 4.96045542e+00,  2.09638572e+00],\n       [ 5.37798691e+00,  2.55944920e+00],\n       [ 1.00153885e+01, -4.30421829e-01],\n       [ 2.58003759e+00,  5.65518475e+00],\n       [ 3.10385394e+00,  4.31488323e+00],\n       [ 8.06058502e+00,  1.08455706e+01],\n       [ 5.15572214e+00,  2.80767560e+00],\n       [ 7.04855144e-01, -2.10322872e-01],\n       [ 1.32493429e+01,  1.70429554e+01],\n       [ 1.25905075e+01, -6.49386692e+00],\n       [-8.13499737e+00,  3.41613173e+00],\n       [ 1.97236271e+01,  3.78322840e+00],\n       [ 8.44617176e+00, -4.47045088e+00],\n       [ 2.22543931e+00,  8.94437122e+00],\n       [ 7.39531803e+00,  9.88773346e+00],\n       [ 4.95809615e-01, -1.17228663e+00],\n       [ 3.56299803e-02, -2.51918101e+00],\n       [ 9.97978687e-01, -9.73300874e-01],\n       [ 7.88709497e+00,  1.02189322e+01],\n       [ 1.19830122e+01, -6.20376873e+00],\n       [ 7.27090883e+00,  1.48665085e+01],\n       [ 7.90857124e+00,  1.04754467e+01],\n       [ 7.78450203e+00,  1.34945431e+01],\n       [ 9.20992315e-01, -3.84849370e-01],\n       [ 2.06664467e+00,  8.91980267e+00],\n       [ 6.82216120e+00,  1.47176342e+01],\n       [ 7.96708155e+00,  9.88164520e+00],\n       [ 9.99245048e-01, -1.10285509e+00],\n       [ 4.92387438e+00,  3.11497521e+00],\n       [ 4.15050173e+00,  5.49935293e+00],\n       [ 1.85929432e+01,  2.93218923e+00],\n       [ 1.05673771e+01,  1.58918750e+00],\n       [ 1.17832613e+01, -6.88497448e+00],\n       [ 2.05436015e+00,  8.92279243e+00],\n       [-8.32482147e+00,  3.22605968e+00],\n       [-6.68644428e-01, -1.44689846e+00],\n       [ 1.26672497e+01, -6.95247126e+00],\n       [ 4.02511883e+00,  4.68911886e+00],\n       [ 1.83233964e+00,  1.02191486e+01],\n       [ 1.24769630e+01,  1.83115978e+01],\n       [ 6.54460144e+00,  8.17592907e+00],\n       [ 4.99710226e+00,  2.14675093e+00],\n       [ 8.82683563e+00, -3.46012086e-01],\n       [ 5.66530418e+00,  2.03878736e+00],\n       [ 1.12690344e+01,  1.65654433e+00],\n       [ 1.94616604e+01,  1.83572781e+00],\n       [ 4.22863197e+00,  4.24069071e+00],\n       [ 1.32883120e+01,  1.72658978e+01],\n       [ 2.04569340e+00,  9.79823208e+00],\n       [ 1.16519632e+01, -5.80133963e+00],\n       [ 2.73238510e-01, -1.97288191e+00],\n       [ 1.37230902e+01,  1.69941349e+01],\n       [ 2.34973192e+00,  9.78479385e+00],\n       [-8.32408810e+00,  3.22686195e+00],\n       [ 5.00070572e+00,  2.14611840e+00],\n       [ 1.94834633e+01,  3.31213975e+00],\n       [ 5.13502932e+00,  2.53799510e+00],\n       [ 3.51085663e+00,  4.86806011e+00],\n       [ 1.60497224e+00,  1.02030182e+01],\n       [ 1.77237749e+00,  9.68281555e+00],\n       [ 1.01137390e+01,  6.01305068e-01],\n       [ 1.24101944e+01,  1.69083271e+01],\n       [ 9.99769878e+00, -4.69175726e-01],\n       [ 1.92375374e+01,  2.82835078e+00],\n       [ 1.29148035e+01, -6.90968513e+00],\n       [ 4.79194498e+00,  1.79446292e+00],\n       [ 1.09801130e+01, -7.14352322e+00],\n       [ 4.43158436e+00,  4.53871965e+00],\n       [ 1.19143171e+01, -5.24801064e+00],\n       [ 8.03468132e+00,  1.08030205e+01],\n       [ 1.25794535e+01,  1.83805733e+01],\n       [ 1.23168669e+01,  1.66269989e+01],\n       [ 1.23458223e+01,  1.77778034e+01],\n       [ 9.14830303e+00,  2.24295601e-01],\n       [ 2.66571760e+00, -4.23472404e+00],\n       [ 5.79700947e+00,  2.10118103e+00],\n       [ 4.93734884e+00,  2.10275912e+00],\n       [ 8.59681225e+00, -6.13303065e-01],\n       [ 4.52130413e+00,  4.89938354e+00],\n       [ 1.25310907e+01,  1.77465248e+01],\n       [ 4.70883727e-01, -2.58479118e-01],\n       [ 7.51077795e+00,  1.44778681e+01],\n       [ 3.78150916e+00,  4.56379843e+00],\n       [ 5.95912027e+00,  3.97240400e-01],\n       [ 3.72255659e+00,  5.11259413e+00],\n       [ 1.19461737e+01,  1.67762375e+01],\n       [ 9.08414245e-01, -7.92203903e-01],\n       [ 2.96357393e+00,  1.00844154e+01],\n       [ 1.17556305e+01, -6.66913843e+00],\n       [ 2.02050533e+01,  3.61629868e+00],\n       [ 1.10305328e+01, -7.52728462e+00],\n       [ 1.96918893e+00,  9.84622478e+00],\n       [ 4.19275188e+00,  5.32430649e+00],\n       [ 7.87172747e+00,  1.34470758e+01],\n       [ 1.98914948e+01,  2.00160146e+00],\n       [ 8.35500717e+00,  9.87718010e+00],\n       [ 9.96109867e+00,  8.08443546e-01],\n       [ 1.18106031e+01, -5.74526215e+00],\n       [ 5.16102409e+00,  1.80734324e+00],\n       [ 1.06605225e+01,  5.83091080e-01],\n       [ 5.54406452e+00,  1.55050116e+01],\n       [-8.23605418e-01, -1.30636895e+00],\n       [ 1.74808240e+00, -4.07517624e+00],\n       [ 9.26349831e+00,  8.88854027e-01],\n       [ 1.09849749e+01, -7.48004770e+00],\n       [ 1.20971031e+01,  1.70955334e+01],\n       [ 7.80687809e+00,  1.46189804e+01],\n       [ 7.74820900e+00,  1.09906073e+01],\n       [ 2.86658764e+00,  8.10734367e+00],\n       [ 5.48915911e+00,  1.53481274e+01],\n       [ 3.66877079e+00,  4.72051382e+00],\n       [ 9.71720600e+00,  1.88182616e+00],\n       [ 1.29751072e+01, -6.48790073e+00],\n       [ 4.74673939e+00,  2.45048618e+00],\n       [ 6.65666580e+00,  1.45971031e+01],\n       [ 8.40965176e+00,  1.14738111e+01],\n       [-8.10725212e+00,  3.44368553e+00],\n       [ 7.50969601e+00,  1.05435114e+01],\n       [ 2.85458922e+00,  1.05950384e+01],\n       [ 9.97610331e-01, -4.11940002e+00],\n       [ 1.61350918e+00,  9.61839390e+00],\n       [ 7.60637164e-01, -2.08404779e+00],\n       [ 1.08878446e+00, -4.20332289e+00],\n       [ 1.71200943e+00,  1.01663733e+01],\n       [ 1.68586135e+00,  9.51459694e+00],\n       [ 9.96205997e+00, -4.65066135e-01],\n       [ 2.30960464e+00,  9.90717316e+00],\n       [ 7.35355377e-01, -1.99536717e+00],\n       [ 6.53025198e+00,  8.15707302e+00],\n       [ 2.84418404e-01, -4.09361899e-01],\n       [ 1.47742963e+00,  8.79598904e+00],\n       [ 1.94371700e+01,  2.68893600e+00],\n       [ 6.51380682e+00,  1.46814518e+01],\n       [ 1.20655746e+01, -6.61174297e+00],\n       [ 2.61204028e+00, -4.18903923e+00],\n       [ 1.27581778e+01,  1.74870110e+01],\n       [ 7.28155994e+00,  1.42150888e+01],\n       [ 7.64708996e+00,  1.37616825e+01],\n       [ 1.20456152e+01, -5.99717808e+00],\n       [ 5.90163994e+00, -5.59923708e-01],\n       [ 9.67985809e-01, -1.10858858e+00],\n       [ 1.89800892e+01,  3.22926378e+00],\n       [ 1.28227119e+01, -6.45035076e+00],\n       [ 2.41951394e+00,  1.07949047e+01],\n       [ 1.95769138e+01,  3.33323169e+00],\n       [ 1.95406742e+01,  3.64278507e+00],\n       [ 4.94027615e+00,  3.08518648e+00],\n       [ 1.16152763e+01, -7.42098475e+00],\n       [ 1.89538881e-01, -2.01126289e+00],\n       [ 3.81815052e+00,  4.59767532e+00],\n       [ 2.12944388e+00,  1.01413727e+01],\n       [ 5.31503677e+00,  1.56595840e+01],\n       [ 7.29801846e+00,  1.01420183e+01],\n       [ 2.09905934e+00,  5.28698969e+00],\n       [ 2.07446499e+01,  3.95429325e+00],\n       [ 8.20926762e+00,  1.09102173e+01],\n       [ 1.08266563e+01,  7.70246744e-01],\n       [ 6.07540846e+00, -7.29724586e-01],\n       [ 1.33210297e+01,  1.75505371e+01],\n       [ 1.24755239e+01,  1.77963142e+01],\n       [ 8.17131042e+00,  9.57447624e+00],\n       [ 1.85363064e+01,  2.59018946e+00],\n       [ 1.92491379e+01,  1.91278136e+00],\n       [ 1.18102551e+01,  1.75860405e+01],\n       [ 1.96883640e+01,  2.18472767e+00],\n       [ 9.52509582e-01, -1.43717289e+00],\n       [ 6.90265989e+00,  1.45218258e+01],\n       [ 5.67792988e+00,  1.57095594e+01],\n       [ 4.19719028e+00,  4.65386629e+00],\n       [ 1.18064594e+01,  1.68005753e+01],\n       [ 4.59421349e+00,  4.83855247e+00],\n       [ 1.28239794e+01, -6.72075176e+00],\n       [-1.79536175e-02, -1.28720593e+00],\n       [ 1.02263308e+01,  8.62763345e-01],\n       [ 7.40937901e+00,  1.00602980e+01],\n       [ 6.83069372e+00,  1.49400043e+01],\n       [-1.60397947e-01, -1.87085092e+00],\n       [ 1.31290855e+01,  1.73606892e+01],\n       [-8.06395912e+00,  3.48659539e+00],\n       [ 3.60813916e-01, -2.49946070e+00],\n       [ 3.95752883e+00,  5.46306419e+00],\n       [ 3.21658778e+00,  5.64122772e+00],\n       [ 1.89867115e+01,  2.76880908e+00],\n       [ 1.43908429e+00,  9.82541084e+00],\n       [ 8.73421192e+00, -4.49178368e-01],\n       [ 4.20121336e+00,  4.74164057e+00]], dtype=float32)\n\n\n\n\nCode\nsvc.score(trans.transform(X_test), y_test), knn.score(trans.transform(X_test), y_test)\n\n\n(0.98, 0.98)\n\n\nThe results are pretty good. While the accuracy of the KNN classifier did not improve there was not a lot of scope for improvement given the data. On the other hand the SVC has improved to have equal accuracy to the KNN classifier!\nFor more interesting datasets the larger dimensional embedding might have been a significant gain – it is certainly worth exploring as one of the parameters in a grid search across a pipeline that includes UMAP."
  },
  {
    "objectID": "05_Feature_selection_extraction.html#clustering",
    "href": "05_Feature_selection_extraction.html#clustering",
    "title": "6  Feature selection and extraction",
    "section": "6.5 Clustering",
    "text": "6.5 Clustering\nWhen used for feature engineering, we could attempt to discover groups of customers representing a market segment, for instance, or geographic areas that share similar weather patterns. Adding a feature of cluster labels can help machine learning models untangle complicated relationships of space or proximity.\n\n6.5.1 Cluster Labels as a feature\nApplied to a single real-valued feature, clustering acts like a traditional “binning” or “discretization” transform. On multiple features, it’s like “multi-dimensional binning” (sometimes called vector quantization).\nIt’s important to remember that this Cluster feature is categorical. Here, it’s shown with a label encoding (that is, as a sequence of integers) as a typical clustering algorithm would produce; depending on your model, a one-hot encoding may be more appropriate. The motivating idea for adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks. Our model can then just learn the simpler chunks one-by-one instead having to learn the complicated whole all at once. It’s a “divide and conquer” strategy.\nAs spatial features, California Housing’s 'Latitude' and 'Longitude' make natural candidates for k-means clustering. In this example we’ll cluster these with 'MedInc' (median income) to create economic segments in different regions of California. Since k-means clustering is sensitive to scale, it can be a good idea rescale or normalize data with extreme values. Our features are already roughly on the same scale, so we’ll leave them as-is.\n\n\nCode\ndf = fetch_california_housing(as_frame=True)['frame']\n\n\n\n\nCode\nX = df.loc[:, [\"MedInc\", \"Latitude\", \"Longitude\"]]\nX.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nMedInc\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n37.88\n-122.23\n\n\n1\n8.3014\n37.86\n-122.22\n\n\n2\n7.2574\n37.85\n-122.24\n\n\n3\n5.6431\n37.85\n-122.25\n\n\n4\n3.8462\n37.85\n-122.25\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n# Create cluster feature\nkmeans = KMeans(n_clusters = 6, n_init = 10)\nX[\"Cluster\"] = kmeans.fit_predict(X)\nX[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")\n\nX.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nMedInc\nLatitude\nLongitude\nCluster\n\n\n\n\n0\n8.3252\n37.88\n-122.23\n5\n\n\n1\n8.3014\n37.86\n-122.22\n5\n\n\n2\n7.2574\n37.85\n-122.24\n5\n\n\n3\n5.6431\n37.85\n-122.25\n5\n\n\n4\n3.8462\n37.85\n-122.25\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNotice the differnece between predict() and transform() in the KMeans. predict() will predict the closest cluster each sample in X belongs to. transform() will transform data to a cluster-distance space where each dimension is the distance to the cluster centers.\nNow let’s look at a couple plots to see how effective this was. First, a scatter plot that shows the geographic distribution of the clusters. It seems like the algorithm has created separate segments for higher-income areas on the coasts.\n\n\nCode\nsns.relplot(\n    x=\"Longitude\", y=\"Latitude\", hue=\"Cluster\", data=X, height=6,\n);\n\n\n\n\n\nThe target in this dataset is MedHouseVal (median house value). These box-plots show the distribution of the target within each cluster. If the clustering is informative, these distributions should, for the most part, separate across MedHouseVal, which is indeed what we see.\n\n\nCode\nX[\"MedHouseVal\"] = df[\"MedHouseVal\"]\nsns.catplot(x=\"MedHouseVal\", y=\"Cluster\", data=X, kind=\"boxen\", height=6);\n\n\n\n\n\n\n\n6.5.2 Cluster distance as a feature\n\n\nCode\nX_digits, y_digits = load_digits(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)\n\n\nNow let’s fit a Logistic Regression model and evaluate it on the test set:\n\n\nCode\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\nlog_reg.fit(X_train, y_train)\n\n\nLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)\n\n\n\n\nCode\nlog_reg_score = log_reg.score(X_test, y_test)\nlog_reg_score\n\n\n0.9688888888888889\n\n\nOkay, that’s our baseline: 96.89% accuracy. Let’s see if we can do better by using K-Means as a preprocessing step. We will create a pipeline that will first cluster the training set into 30 clusters and replace the images with their distances to the 30 clusters, then apply a logistic regression model:\n\n\nCode\npipeline = Pipeline([\n    (\"kmeans\", KMeans(n_clusters=30, random_state=42, n_init=10)),\n    (\"log_reg\", LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)),\n])\npipeline.fit(X_train, y_train)\n\n\nPipeline(steps=[('kmeans', KMeans(n_clusters=30, n_init=10, random_state=42)),\n                ('log_reg',\n                 LogisticRegression(max_iter=5000, multi_class='ovr',\n                                    random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('kmeans', KMeans(n_clusters=30, n_init=10, random_state=42)),\n                ('log_reg',\n                 LogisticRegression(max_iter=5000, multi_class='ovr',\n                                    random_state=42))])KMeansKMeans(n_clusters=30, n_init=10, random_state=42)LogisticRegressionLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)\n\n\n\n\nCode\nX_train.shape\n\n\n(1347, 64)\n\n\n\n\nCode\npipeline_score = pipeline.score(X_test, y_test)\npipeline_score\n\n\n0.9733333333333334\n\n\nHow much did the error rate drop?\n\n\nCode\n1 - (1 - pipeline_score) / (1 - log_reg_score)\n\n\n0.1428571428571439\n\n\nWe reduced the error rate by over 14%!"
  },
  {
    "objectID": "05_Feature_selection_extraction.html#guideline-to-determine-the-optimal-number-of-features-or-threshold",
    "href": "05_Feature_selection_extraction.html#guideline-to-determine-the-optimal-number-of-features-or-threshold",
    "title": "6  Feature selection and extraction",
    "section": "6.6 Guideline to determine the optimal number of features or threshold?",
    "text": "6.6 Guideline to determine the optimal number of features or threshold?\nTo determine the optimal hyperparameter, we can use cross validation. For instance, in the above example, we chose the number of clusters k completely arbitrarily. However, we can surely do better. Since K-Means is just a preprocessing step in a classification pipeline, finding a good value for k is the best value of k is simply the one that results in the best classification performance.\n\n\nCode\n# The following cell may take close to 20 minutes to run, or more depending on your hardware!\nparam_grid = dict(kmeans__n_clusters=range(2, 60))\ngrid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)\ngrid_clf.fit(X_train, y_train)\n\n\n\n\nCode\ngrid_clf.best_params_\n\n\n\n\nCode\ngrid_clf.score(X_test, y_test)\n\n\nIn the same way, you can also use cross-validation to evaluate model performance with different numbers of top-ranked features or different numbers of features and choose the optimal number based on the performance metric (e.g., highest accuracy or lowest error).\n\n6.6.1 Using Clustering for Semi-Supervised Learning\nAnother use case for clustering is in semi-supervised learning, when we have plenty of unlabeled instances and very few labeled instances.\n\n\nCode\nX_digits, y_digits = load_digits(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)\n\n\nLet’s look at the performance of a logistic regression model when we only have 50 labeled instances:\n\n\nCode\nn_labeled = 50\n\n\n\n\nCode\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\nlog_reg.fit(X_train[:n_labeled], y_train[:n_labeled])\nlog_reg.score(X_test, y_test)\n\n\n0.8333333333333334\n\n\nThe model’s accuracy is just 83.33%. It’s much less than earlier of course. Let’s see how we can do better. First, let’s cluster the training set into 50 clusters, then for each cluster let’s find the image closest to the centroid. We will call these images the representative images:\n\n\nCode\nk = 50\nkmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\nX_digits_dist = kmeans.fit_transform(X_train)\nrepresentative_digit_idx = np.argmin(X_digits_dist, axis=0)\nX_representative_digits = X_train[representative_digit_idx]\n\n\nNow let’s plot these representative images and label them manually:\n\n\nCode\nplt.figure(figsize=(8, 2))\nfor index, X_representative_digit in enumerate(X_representative_digits):\n    plt.subplot(k // 10, 10, index + 1)\n    plt.imshow(X_representative_digit.reshape(8, 8), cmap=\"binary\", interpolation=\"bilinear\")\n    plt.axis('off')\nplt.show()\n\n\n\n\n\n\n\nCode\n# Assuming we manually label these digits\ny_train[representative_digit_idx]\n\n\narray([4, 8, 0, 6, 8, 3, 7, 7, 9, 2, 5, 5, 8, 5, 2, 1, 2, 9, 6, 1, 1, 6,\n       9, 0, 8, 3, 0, 7, 4, 1, 6, 5, 2, 4, 1, 8, 6, 3, 9, 2, 4, 2, 9, 4,\n       7, 6, 2, 3, 1, 1])\n\n\n\n\nCode\ny_representative_digits = y_train[representative_digit_idx]\n\n\nNow we have a dataset with just 50 labeled instances, but instead of being completely random instances, each of them is a representative image of its cluster. Let’s see if the performance is any better:\n\n\nCode\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\nlog_reg.fit(X_representative_digits, y_representative_digits)\nlog_reg.score(X_test, y_test)\n\n\n0.9222222222222223\n\n\nWe jumped from 83.3% accuracy to 92.2%, although we are still only training the model on 50 instances. Since it’s often costly and painful to label instances, especially when it has to be done manually by experts, it’s a good idea to make them label representative instances rather than just random instances.\nBut perhaps we can go one step further: what if we propagated the labels to all the other instances in the same cluster?\n\n\nCode\ny_train_propagated = np.empty(len(X_train), dtype=np.int32)\nfor i in range(k):\n    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]\n\n\n\n\nCode\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\nlog_reg.fit(X_train, y_train_propagated)\n\n\nLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)\n\n\n\n\nCode\nlog_reg.score(X_test, y_test)\n\n\n0.9333333333333333\n\n\nWe got a tiny little accuracy boost. Better than nothing, but we should probably have propagated the labels only to the instances closest to the centroid, because by propagating to the full cluster, we have certainly included some outliers. Let’s only propagate the labels to the 75th percentile closest to the centroid:\n\n\nCode\npercentile_closest = 75\n\nX_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\nfor i in range(k):\n    in_cluster = (kmeans.labels_ == i)\n    cluster_dist = X_cluster_dist[in_cluster]\n    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n    above_cutoff = (X_cluster_dist &gt; cutoff_distance)\n    X_cluster_dist[in_cluster & above_cutoff] = -1\n\n\n\n\nCode\npartially_propagated = (X_cluster_dist != -1)\nX_train_partially_propagated = X_train[partially_propagated]\ny_train_partially_propagated = y_train_propagated[partially_propagated]\n\n\n\n\nCode\nX_train.shape, X_train_partially_propagated.shape\n\n\n((1347, 64), (1003, 64))\n\n\n\n\nCode\nlog_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\nlog_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n\n\nLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=5000, multi_class='ovr', random_state=42)\n\n\n\n\nCode\nlog_reg.score(X_test, y_test)\n\n\n0.9355555555555556\n\n\nA bit better. With just 50 labeled instances (just 5 examples per class on average!), we got 93.5% performance, which is getting closer to the performance of logistic regression on the fully labeled digits dataset.\nOur propagated labels are actually pretty good: their accuracy is about 97.5%:\n\n\nCode\nnp.mean(y_train_partially_propagated == y_train[partially_propagated])\n\n\n0.9750747756729811\n\n\nYou could also do a few iterations of active learning:\n\nManually label the instances that the classifier is least sure about, if possible by picking them in distinct clusters.\nTrain a new model with these additional labels.\n\n\n\n6.6.2 Feature agglomeration\ncluster.FeatureAgglomeration applies Hierarchical clustering to group together features that behave similarly.\n\n\nCode\nX, y = load_iris(return_X_y=True)\n\n\n\n\nCode\n#set n_clusters to 2, the output will be two columns of agglomerated features (iris has 4 features)\nagglo = FeatureAgglomeration(n_clusters=2).fit_transform(X)\n\n\n\n\nCode\nplt.scatter(agglo[:,0], agglo[:,1],c=y)\nplt.show()"
  },
  {
    "objectID": "05_Feature_selection_extraction.html#references",
    "href": "05_Feature_selection_extraction.html#references",
    "title": "6  Feature selection and extraction",
    "section": "6.7 References",
    "text": "6.7 References\n\nhttps://www.kaggle.com/learn/feature-engineering\nhttps://inria.github.io/scikit-learn-mooc/python_scripts/dev_features_importance.html#\nhttps://scikit-learn.org/stable/modules/feature_selection.html\nhttps://scikit-learn.org/stable/modules/preprocessing.html#\nhttps://scikit-learn.org/stable/modules/unsupervised_reduction.html\nhttps://github.com/ageron/handson-ml3/blob/main/09_unsupervised_learning.ipynb"
  },
  {
    "objectID": "06_XAI.html#setup",
    "href": "06_XAI.html#setup",
    "title": "7  Explainable AI",
    "section": "7.1 Setup",
    "text": "7.1 Setup\n\n\nCode\n!pip install --upgrade mlxtend -qq\n!pip install imodels -qq\n!pip install dtreeviz -qq\n!pip install lime -qq\n!pip install shap -qq\n\n\n\n\nCode\n# Scientific computing\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline\n\n# Preprocessing and datasets\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import fetch_openml\n\n# Modeling\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree, DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_absolute_error\nimport tensorflow as tf\n\n# Interpretable Models\nfrom imodels import RuleFitRegressor\nfrom imodels import OneRClassifier, BayesianRuleListClassifier, FIGSClassifier, HSTreeClassifierCV\nfrom imodels.discretization import ExtraBasicDiscretizer\nfrom imodels.tree.viz_utils import extract_sklearn_tree_from_figs\n\n# Model-Agnostic Methods\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.inspection import partial_dependence\n\n# Local methods\nimport lime\nimport lime.lime_tabular\nfrom lime import lime_image\nimport shap  # package used to calculate Shap values\n\n# Helper functions\nfrom skimage.io import imread\nfrom skimage.segmentation import mark_boundaries\nfrom scipy.io.arff import loadarff\nimport graphviz\nimport dtreeviz\n\nimport logging\nimport warnings\nlogging.getLogger('matplotlib.font_manager').setLevel(level=logging.CRITICAL) # For dtreeviz\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) # For shap\n\n\n\n\nCode\nif not tf.config.list_physical_devices('GPU'):\n    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n    if \"google.colab\" in sys.modules:\n        print(\"Go to Runtime &gt; Change runtime and select a GPU hardware \"\n              \"accelerator.\")\n    if \"kaggle_secrets\" in sys.modules:\n        print(\"Go to Settings &gt; Accelerator and select GPU.\")\n\n\n\n\nCode\n!gdown --fuzzy https://drive.google.com/file/d/1GzAuz0gkk5arJPYC7NgnCrkVWS4rvTtW/view?usp=sharing\n!gdown --fuzzy https://drive.google.com/file/d/1GvSaZ1E45e45ns5IH8Y6EoVu8C6Vhffy/view?usp=sharing"
  },
  {
    "objectID": "06_XAI.html#decsion-rule-based-modeld-by-imodels",
    "href": "06_XAI.html#decsion-rule-based-modeld-by-imodels",
    "title": "7  Explainable AI",
    "section": "7.2 Decsion-Rule based modeld by imodels",
    "text": "7.2 Decsion-Rule based modeld by imodels\nimodels provides a simple interface for fitting and using state-of-the-art interpretable models, all compatible with scikit-learn. These models can often replace black-box models (e.g. random forests) with simpler models (e.g. rule lists) while improving interpretability and computational efficiency, all without sacrificing predictive accuracy!\n\n\nCode\nnp.random.seed(13)\n\ndef get_ames_data():\n    try:\n        housing = fetch_openml(name=\"house_prices\", as_frame=True, parser='auto')\n    except:\n        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\n    housing_target = housing['target'].values\n    housing_data_numeric = housing['data'].select_dtypes('number').drop(columns=['Id']).dropna(axis=1)\n    feature_names = housing_data_numeric.columns.values\n    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n        housing_data_numeric.values, housing_target, test_size=0.75)\n    return X_train_reg, X_test_reg, y_train_reg, y_test_reg, feature_names\n\ndef get_diabetes_data():\n    '''load (classification) data on diabetes\n    '''\n    data = loadarff(\"diabetes.arff\")\n    data_np = np.array(list(map(lambda x: np.array(list(x)), data[0])))\n    X = data_np[:, :-1].astype('float32')\n    y_text = data_np[:, -1].astype('str')\n    y = (y_text == 'tested_positive').astype(int)  # labels 0-1\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75)  # split\n    feature_names = [\"#Pregnant\", \"Glucose concentration test\", \"Blood pressure(mmHg)\",\n                     \"Triceps skin fold thickness(mm)\",\n                     \"2-Hour serum insulin (mu U/ml)\", \"Body mass index\", \"Diabetes pedigree function\", \"Age (years)\"]\n    return X_train, X_test, y_train, y_test, feature_names\n\ndef viz_classification_preds(probs, y_test):\n    '''look at prediction breakdown\n    '''\n    plt.subplot(121)\n    plt.hist(probs[:, 1][y_test == 0], label='Class 0')\n    plt.hist(probs[:, 1][y_test == 1], label='Class 1', alpha=0.8)\n    plt.ylabel('Count')\n    plt.xlabel('Predicted probability of class 1')\n    plt.legend()\n\n    plt.subplot(122)\n    preds = np.argmax(probs, axis=1)\n    plt.title('ROC curve')\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, preds)\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.plot(fpr, tpr)\n    plt.tight_layout()\n    plt.show()\n\n\nThe Ames dataset is a housing dataset that use seveal conditions to predict the housing price. The diabetes dataset has a binary-valued variable. We would like to investigated whether the patient shows signs of diabetes according to World Health Organization criteria.\n\n\nCode\nX_train_reg, X_test_reg, y_train_reg, y_test_reg, feat_names_reg = get_ames_data()\nX_train, X_test, y_train, y_test, feat_names = get_diabetes_data()\n\n\n\n\nCode\npd.DataFrame(X_train_reg, columns=feat_names_reg)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nMSSubClass\nLotArea\nOverallQual\nOverallCond\nYearBuilt\nYearRemodAdd\nBsmtFinSF1\nBsmtFinSF2\nBsmtUnfSF\nTotalBsmtSF\n...\nGarageArea\nWoodDeckSF\nOpenPorchSF\nEnclosedPorch\n3SsnPorch\nScreenPorch\nPoolArea\nMiscVal\nMoSold\nYrSold\n\n\n\n\n0\n50\n6435\n6\n5\n1939\n1950\n0\n0\n972\n972\n...\n312\n0\n0\n0\n0\n0\n0\n0\n10\n2006\n\n\n1\n20\n10200\n5\n7\n1954\n2003\n320\n362\n404\n1086\n...\n490\n0\n0\n0\n0\n0\n0\n0\n5\n2010\n\n\n2\n20\n9503\n5\n5\n1958\n1983\n457\n374\n193\n1024\n...\n484\n316\n28\n0\n0\n0\n0\n0\n6\n2007\n\n\n3\n60\n9000\n8\n5\n2008\n2008\n0\n0\n768\n768\n...\n676\n0\n30\n0\n0\n0\n0\n0\n6\n2009\n\n\n4\n80\n19690\n6\n7\n1966\n1966\n0\n0\n697\n697\n...\n432\n586\n236\n0\n0\n0\n738\n0\n8\n2006\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n360\n20\n10656\n8\n5\n2006\n2007\n0\n0\n1638\n1638\n...\n870\n192\n80\n0\n0\n0\n0\n0\n11\n2007\n\n\n361\n20\n8450\n7\n5\n2000\n2001\n0\n0\n1349\n1349\n...\n539\n120\n55\n0\n0\n0\n0\n0\n12\n2007\n\n\n362\n50\n5790\n3\n6\n1915\n1950\n0\n0\n840\n840\n...\n379\n0\n0\n202\n0\n0\n0\n0\n5\n2010\n\n\n363\n60\n10029\n6\n5\n1988\n1989\n831\n0\n320\n1151\n...\n521\n0\n228\n0\n0\n192\n0\n0\n9\n2007\n\n\n364\n20\n14145\n7\n7\n1984\n1998\n213\n0\n995\n1208\n...\n440\n108\n45\n0\n0\n0\n0\n400\n5\n2006\n\n\n\n\n\n365 rows × 33 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\ny_train_reg[:10]\n\n\narray([140200, 144900, 144000, 210000, 274970, 218000, 167500, 195400,\n        76000, 246578])\n\n\n\n\nCode\npd.DataFrame(X_train, columns=feat_names)\n\n\n\n  \n    \n      \n\n\n\n\n\n\n#Pregnant\nGlucose concentration test\nBlood pressure(mmHg)\nTriceps skin fold thickness(mm)\n2-Hour serum insulin (mu U/ml)\nBody mass index\nDiabetes pedigree function\nAge (years)\n\n\n\n\n0\n3.0\n158.0\n76.0\n36.0\n245.0\n31.600000\n0.851\n28.0\n\n\n1\n8.0\n186.0\n90.0\n35.0\n225.0\n34.500000\n0.423\n37.0\n\n\n2\n2.0\n85.0\n65.0\n0.0\n0.0\n39.599998\n0.930\n27.0\n\n\n3\n3.0\n187.0\n70.0\n22.0\n200.0\n36.400002\n0.408\n36.0\n\n\n4\n6.0\n93.0\n50.0\n30.0\n64.0\n28.700001\n0.356\n23.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n187\n0.0\n165.0\n76.0\n43.0\n255.0\n47.900002\n0.259\n26.0\n\n\n188\n8.0\n181.0\n68.0\n36.0\n495.0\n30.100000\n0.615\n60.0\n\n\n189\n0.0\n111.0\n65.0\n0.0\n0.0\n24.600000\n0.660\n31.0\n\n\n190\n3.0\n129.0\n92.0\n49.0\n155.0\n36.400002\n0.968\n32.0\n\n\n191\n1.0\n109.0\n56.0\n21.0\n135.0\n25.200001\n0.833\n23.0\n\n\n\n\n\n192 rows × 8 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\ny_train[:10]\n\n\narray([1, 1, 0, 1, 0, 0, 0, 0, 1, 0])\n\n\nWe will now show how to fit different models. All models support the fit() and predict() method (classifiers also support predict_proba()).\nThe simplest way to visualize a fitted model m is usually just to call str(m) or print(m). Some models have custom methods that allow you to visualize them further. To pass feature names into a model for visualization, you can usually (i) pass in the feature_names argument to the fit() function or (ii) pass in a pandas dataframe with the feature names as column names.\n\n7.2.1 Rule lists\nRule list is nonoverlapping\n\n7.2.1.1 oneR\nFits a rule list restricted to use only one feature\n\n\nCode\n# fit a oneR model\nmodel = OneRClassifier()\nmodel.fit(X_train, y=y_train, feature_names=feat_names) # stores into m.rules_\nprobs = model.predict_proba(X_test)\npreds = model.predict(X_test)\n\n# print the rule list\nprint(\"Classifier Accuracy:\", np.mean(y_test == preds), \"\\n Learned interpretable model:\\n\", model)\n\n# look at prediction breakdown\nviz_classification_preds(probs, y_test)\n\n\nClassifier Accuracy: 0.6649305555555556 \n Learned interpretable model:\n &gt; ------------------------------\n&gt; Greedy Rule List\n&gt; ------------------------------\n↓\n24.11% risk (192 pts)\n    if #Pregnant ==&gt; 60.8% risk (51 pts)\n↓\n19.51% risk (141 pts)\n    if #Pregnant ==&gt; 30.5% risk (59 pts)\n↓\n15.38% risk (82 pts)\n    if ~#Pregnant ==&gt; 26.700000000000003% risk (30 pts)\n↓\n12.5% risk (52 pts)\n    if #Pregnant ==&gt; 20.0% risk (20 pts)\n\n\n\n\n\n\n\n\nCode\nmodel.rules_\n\n\n[{'col': '#Pregnant',\n  'index_col': 0,\n  'cutoff': 6.5,\n  'val': 0.24113475177304963,\n  'flip': False,\n  'val_right': 0.6078431372549019,\n  'num_pts': 192,\n  'num_pts_right': 51},\n {'col': '#Pregnant',\n  'index_col': 0,\n  'cutoff': 2.5,\n  'val': 0.1951219512195122,\n  'flip': False,\n  'val_right': 0.3050847457627119,\n  'num_pts': 141,\n  'num_pts_right': 59},\n {'col': '#Pregnant',\n  'index_col': 0,\n  'cutoff': 0.5,\n  'val': 0.15384615384615385,\n  'flip': True,\n  'val_right': 0.26666666666666666,\n  'num_pts': 82,\n  'num_pts_right': 30},\n {'col': '#Pregnant',\n  'index_col': 0,\n  'cutoff': 1.5,\n  'val': 0.125,\n  'flip': False,\n  'val_right': 0.2,\n  'num_pts': 52,\n  'num_pts_right': 20}]\n\n\n\n\n7.2.1.2 Bayesian rule lists\n\n\nCode\n# train classifier (allow more iterations for better accuracy; use BigDataRuleListClassifier for large datasets)\n# All numeric features must be discretized prior to fitting!\ndisc = ExtraBasicDiscretizer(feat_names, n_bins=3, strategy='uniform')\nX_train_disc = disc.fit_transform(pd.DataFrame(X_train, columns=feat_names))\nX_test_disc = disc.transform(pd.DataFrame(X_test, columns=feat_names))\n\nX_train_disc\n\n\n/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n\n\n\n  \n    \n      \n\n\n\n\n\n\n#Pregnant_0.0_to_4.666666666666667\n#Pregnant_4.666666666666667_to_9.333333333333334\n#Pregnant_9.333333333333334_to_14.0\nGlucose concentration test_44.0_to_95.66666666666666\nGlucose concentration test_95.66666666666666_to_147.33333333333331\nGlucose concentration test_147.33333333333331_to_199.0\nBlood pressure(mmHg)_0.0_to_40.666666666666664\nBlood pressure(mmHg)_40.666666666666664_to_81.33333333333333\nBlood pressure(mmHg)_81.33333333333333_to_122.0\nTriceps skin fold thickness(mm)_0.0_to_21.0\n...\n2-Hour serum insulin (mu U/ml)_330.0_to_495.0\nBody mass index_0.0_to_19.8000005086263\nBody mass index_19.8000005086263_to_39.6000010172526\nBody mass index_39.6000010172526_to_59.400001525878906\nDiabetes pedigree function_0.10199999809265137_to_0.874666690826416\nDiabetes pedigree function_0.874666690826416_to_1.6473333835601807\nDiabetes pedigree function_1.6473333835601807_to_2.4200000762939453\nAge (years)_21.0_to_36.333333333333336\nAge (years)_36.333333333333336_to_51.66666666666667\nAge (years)_51.66666666666667_to_67.0\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n1\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n2\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n3\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n4\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n187\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n188\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n189\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n190\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n191\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n192 rows × 24 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nmodel = BayesianRuleListClassifier(max_iter=3000, class1label=\"diabetes\", verbose=False)\nmodel.fit(X_train_disc.to_numpy(), y_train, feature_names=X_train_disc.columns)\nprobs = model.predict_proba(X_test_disc)\npreds = model.predict(X_test_disc.to_numpy(), threshold=0.5)\n\nprint(\"RuleListClassifier Accuracy:\", np.mean(y_test == preds), \"\\n Learned interpretable model:\\n\", model)\n\nviz_classification_preds(probs, y_test)\n\n\nRuleListClassifier Accuracy: 0.7309027777777778 \n Learned interpretable model:\n Trained RuleListClassifier for detecting diabetes\n==================================================\nIF Body mass index_39.6000010172526_to_59.400001525878906 &gt; 0.5 THEN probability of diabetes: 50.0% (30.6%-69.4%)\nELSE IF Glucose concentration test_147.33333333333331_to_199.0 &gt; 0.5 THEN probability of diabetes: 69.7% (53.3%-83.9%)\nELSE IF Triceps skin fold thickness(mm)_42.0_to_63.0 &gt; 0.5 THEN probability of diabetes: 38.5% (15.2%-65.1%)\nELSE IF #Pregnant_0.0_to_4.666666666666667 &gt; 0.5 THEN probability of diabetes: 11.0% (5.2%-18.5%)\nELSE IF 2-Hour serum insulin (mu U/ml)_0.0_to_165.0 &gt; 0.5 THEN probability of diabetes: 34.9% (21.6%-49.5%)\nELSE probability of diabetes: 77.8% (47.3%-96.8%)\n=================================================\n\n\n\n\n\n\n\n\n\n7.2.2 Rule sets\nRule sets are models that create a set of (potentially overlapping) rules.\n\n7.2.2.1 Rulefit\nIt fits a sparse linear model on rules extracted from decision trees\n\n\nCode\n# fit a rulefit model\nmodel = RuleFitRegressor(max_rules=10)\nmodel.fit(X_train_reg, y_train_reg, feature_names=feat_names_reg)\n\n# get test performance\npreds = model.predict(X_test_reg)\nprint(f'test mse: {metrics.mean_squared_error(y_test_reg, preds):0.2f}')\nprint(f'test r2: {metrics.r2_score(y_test_reg, preds):0.2f}')\n\n\n# inspect and print the rules\n#rules = model._get_rules()\n#rules = rules[rules.coef != 0].sort_values(\"support\", ascending=False)\n# 'rule' is how the feature is constructed\n# 'coef' is its weight in the final linear model\n# 'support' is the fraction of points it applies to\n#rules[['rule', 'coef', 'support']].style.background_gradient(cmap='viridis')\nmodel\n\n\ntest mse: 2224531388.26\ntest r2: 0.65\n\n\n&gt; ------------------------------\n&gt; RuleFit:\n&gt;    Predictions are made by summing the coefficients of each rule\n&gt; ------------------------------\n                                         rule      coef\n                                  OverallQual  17096.64\n                                    GrLivArea     30.09\n                                   GarageArea     21.71\n OverallQual &lt;= 7.5 and TotalBsmtSF &lt;= 1201.0  -2144.37\nGrLivArea &lt;= 1934.0 and TotalBsmtSF &lt;= 1199.0 -11512.20\n  GrLivArea &lt;= 1790.0 and YearBuilt &lt;= 1994.5  -5185.18\n   GrLivArea &gt; 1415.0 and TotalBsmtSF &gt; 984.0  13188.14\n     GrLivArea &gt; 1821.0 and OverallQual &gt; 6.5    185.65\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RuleFitRegressor&gt; ------------------------------\n&gt; RuleFit:\n&gt;    Predictions are made by summing the coefficients of each rule\n&gt; ------------------------------\n                                         rule      coef\n                                  OverallQual  17096.64\n                                    GrLivArea     30.09\n                                   GarageArea     21.71\n OverallQual &lt;= 7.5 and TotalBsmtSF &lt;= 1201.0  -2144.37\nGrLivArea &lt;= 1934.0 and TotalBsmtSF &lt;= 1199.0 -11512.20\n  GrLivArea &lt;= 1790.0 and YearBuilt &lt;= 1994.5  -5185.18\n   GrLivArea &gt; 1415.0 and TotalBsmtSF &gt; 984.0  13188.14\n     GrLivArea &gt; 1821.0 and OverallQual &gt; 6.5    185.65\n\n\n\n\n\nCode\nmodel._get_rules()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nrule\ntype\ncoef\nsupport\nimportance\n\n\n\n\n0\nMSSubClass\nlinear\n-0.000000\n1.000000\n0.000000\n\n\n1\nLotArea\nlinear\n0.000000\n1.000000\n0.000000\n\n\n2\nOverallQual\nlinear\n17096.637090\n1.000000\n21985.927425\n\n\n3\nOverallCond\nlinear\n0.000000\n1.000000\n0.000000\n\n\n4\nYearBuilt\nlinear\n0.000000\n1.000000\n0.000000\n\n\n5\nYearRemodAdd\nlinear\n0.000000\n1.000000\n0.000000\n\n\n6\nBsmtFinSF1\nlinear\n0.000000\n1.000000\n0.000000\n\n\n7\nBsmtFinSF2\nlinear\n-0.000000\n1.000000\n0.000000\n\n\n8\nBsmtUnfSF\nlinear\n0.000000\n1.000000\n0.000000\n\n\n9\nTotalBsmtSF\nlinear\n0.000000\n1.000000\n0.000000\n\n\n10\n1stFlrSF\nlinear\n0.000000\n1.000000\n0.000000\n\n\n11\n2ndFlrSF\nlinear\n0.000000\n1.000000\n0.000000\n\n\n12\nLowQualFinSF\nlinear\n0.000000\n1.000000\n0.000000\n\n\n13\nGrLivArea\nlinear\n30.088170\n1.000000\n13783.356137\n\n\n14\nBsmtFullBath\nlinear\n0.000000\n1.000000\n0.000000\n\n\n15\nBsmtHalfBath\nlinear\n-0.000000\n1.000000\n0.000000\n\n\n16\nFullBath\nlinear\n0.000000\n1.000000\n0.000000\n\n\n17\nHalfBath\nlinear\n0.000000\n1.000000\n0.000000\n\n\n18\nBedroomAbvGr\nlinear\n0.000000\n1.000000\n0.000000\n\n\n19\nKitchenAbvGr\nlinear\n-0.000000\n1.000000\n0.000000\n\n\n20\nTotRmsAbvGrd\nlinear\n0.000000\n1.000000\n0.000000\n\n\n21\nFireplaces\nlinear\n0.000000\n1.000000\n0.000000\n\n\n22\nGarageCars\nlinear\n0.000000\n1.000000\n0.000000\n\n\n23\nGarageArea\nlinear\n21.705852\n1.000000\n4319.756022\n\n\n24\nWoodDeckSF\nlinear\n0.000000\n1.000000\n0.000000\n\n\n25\nOpenPorchSF\nlinear\n0.000000\n1.000000\n0.000000\n\n\n26\nEnclosedPorch\nlinear\n-0.000000\n1.000000\n0.000000\n\n\n27\n3SsnPorch\nlinear\n0.000000\n1.000000\n0.000000\n\n\n28\nScreenPorch\nlinear\n0.000000\n1.000000\n0.000000\n\n\n29\nPoolArea\nlinear\n0.000000\n1.000000\n0.000000\n\n\n30\nMiscVal\nlinear\n0.000000\n1.000000\n0.000000\n\n\n31\nMoSold\nlinear\n0.000000\n1.000000\n0.000000\n\n\n32\nYrSold\nlinear\n-0.000000\n1.000000\n0.000000\n\n\n33\nGrLivArea &lt;= 1790.0 and YearBuilt &lt;= 1994.5\nrule\n-5185.184696\n0.550685\n2579.237407\n\n\n34\nGrLivArea &lt;= 1934.0 and TotalBsmtSF &lt;= 1199.0\nrule\n-11512.201674\n0.583562\n5675.147066\n\n\n35\nOverallQual &lt;= 7.5 and TotalBsmtSF &lt;= 1201.0\nrule\n-2144.369052\n0.641096\n1028.608817\n\n\n36\nGrLivArea &gt; 1415.0 and TotalBsmtSF &gt; 984.0\nrule\n13188.139108\n0.312329\n6111.952092\n\n\n37\nGrLivArea &gt; 1821.0 and OverallQual &gt; 6.5\nrule\n185.651378\n0.156164\n67.393514\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nrules = model._get_rules()\nrules = rules[rules.coef != 0].sort_values(\"support\", ascending=False)\n# 'rule' is how the feature is constructed\n# 'coef' is its weight in the final linear model\n# 'support' is the fraction of points it applies to\nrules[['rule', 'coef', 'support']].style.background_gradient(cmap='viridis')\n\n\n\n\n\n\n\n \nrule\ncoef\nsupport\n\n\n\n\n2\nOverallQual\n17096.637090\n1.000000\n\n\n13\nGrLivArea\n30.088170\n1.000000\n\n\n23\nGarageArea\n21.705852\n1.000000\n\n\n35\nOverallQual &lt;= 7.5 and TotalBsmtSF &lt;= 1201.0\n-2144.369052\n0.641096\n\n\n34\nGrLivArea &lt;= 1934.0 and TotalBsmtSF &lt;= 1199.0\n-11512.201674\n0.583562\n\n\n33\nGrLivArea &lt;= 1790.0 and YearBuilt &lt;= 1994.5\n-5185.184696\n0.550685\n\n\n36\nGrLivArea &gt; 1415.0 and TotalBsmtSF &gt; 984.0\n13188.139108\n0.312329\n\n\n37\nGrLivArea &gt; 1821.0 and OverallQual &gt; 6.5\n185.651378\n0.156164\n\n\n\n\n\n\n\n\n7.2.3 Ruletree\n\n7.2.3.1 FIGSClassifier\n\n\nCode\n# specify a decision tree with a maximum depth\nfigs = FIGSClassifier(max_rules=7)\nfigs.fit(X_train, y_train, feature_names=feat_names)\n\n# calculate mse on the training data\nprobs = figs.predict_proba(X_test)\npreds = figs.predict(X_test)\n\nprint(\"Classifier Accuracy:\", np.mean(y_test == preds), \"\\n Learned interpretable model:\\n\", figs)\n\nviz_classification_preds(probs, y_test)\n\n\nClassifier Accuracy: 0.7152777777777778 \n Learned interpretable model:\n &gt; ------------------------------\n&gt; FIGS-Fast Interpretable Greedy-Tree Sums:\n&gt;   Predictions are made by summing the \"Val\" reached by traversing each tree.\n&gt;   For classifiers, a sigmoid function is then applied to the sum.\n&gt; ------------------------------\nGlucose concentration test &lt;= 99.500 (Tree #0 root)\n    Val: 0.068 (leaf)\n    Glucose concentration test &lt;= 168.500 (split)\n        #Pregnant &lt;= 6.500 (split)\n            Body mass index &lt;= 30.850 (split)\n                Val: 0.065 (leaf)\n                Blood pressure(mmHg) &lt;= 67.000 (split)\n                    Val: 0.705 (leaf)\n                    Val: 0.303 (leaf)\n            Val: 0.639 (leaf)\n        Blood pressure(mmHg) &lt;= 93.000 (split)\n            Val: 0.860 (leaf)\n            Val: -0.009 (leaf)\n\n    +\nDiabetes pedigree function &lt;= 0.404 (Tree #1 root)\n    Val: -0.088 (leaf)\n    Val: 0.106 (leaf)\n\n\n\n\n\n\n\n\nCode\nprint('Alternative visualization:')\nfigs.plot()\n\n\nAlternative visualization:\n\n\n\n\n\n\n\nCode\ndt = extract_sklearn_tree_from_figs(figs, tree_num=0, n_classes=2) # tree_num =  0 or 1\nviz_model = dtreeviz.model(dt,\n                           X_train=X_train, y_train=y_train,\n                           feature_names=feat_names,\n                           target_name='y', class_names=[0, 1])\n\n\n\n\nCode\nviz_model.view(scale=1.5)\n\n\n\n\n\n\n\nCode\nviz_model.view(orientation=\"LR\", scale=1.5)\n\n\n\n\n\n\n\nCode\nx_example = X_train[13]\nviz_model.view(x=x_example)\n\n\n\n\n\nSee https://github.com/parrt/dtreeviz/blob/master/notebooks/dtreeviz_sklearn_visualisations.ipynb for more information.\n\n\n7.2.3.2 HSTreeClassifier\n\n\nCode\n# specify a decision tree with a maximum depth\ndt = HSTreeClassifierCV(max_leaf_nodes=7)\ndt.fit(X_train, y_train, feature_names=feat_names)\n\n# calculate mse on the training data\nprobs = dt.predict_proba(X_test)\npreds = dt.predict(X_test)\n\nprint(\"Classifier Accuracy:\", np.mean(y_test == preds), \"\\n Learned interpretable model:\\n\", dt)\n\nviz_classification_preds(probs, y_test)\n\n\nClassifier Accuracy: 0.7291666666666666 \n Learned interpretable model:\n &gt; ------------------------------\n&gt; Decision Tree with Hierarchical Shrinkage\n&gt;   Prediction is made by looking at the value in the appropriate leaf of the tree\n&gt; ------------------------------\n|--- feature_1 &lt;= 99.50\n|   |--- weights: [0.84, 0.16] class: 0.0\n|--- feature_1 &gt;  99.50\n|   |--- feature_1 &lt;= 168.50\n|   |   |--- feature_0 &lt;= 6.50\n|   |   |   |--- feature_5 &lt;= 30.85\n|   |   |   |   |--- weights: [0.77, 0.23] class: 0.0\n|   |   |   |--- feature_5 &gt;  30.85\n|   |   |   |   |--- feature_2 &lt;= 67.00\n|   |   |   |   |   |--- weights: [0.53, 0.47] class: 0.0\n|   |   |   |   |--- feature_2 &gt;  67.00\n|   |   |   |   |   |--- weights: [0.66, 0.34] class: 0.0\n|   |   |--- feature_0 &gt;  6.50\n|   |   |   |--- feature_6 &lt;= 0.26\n|   |   |   |   |--- weights: [0.57, 0.43] class: 0.0\n|   |   |   |--- feature_6 &gt;  0.26\n|   |   |   |   |--- weights: [0.45, 0.55] class: 1.0\n|   |--- feature_1 &gt;  168.50\n|   |   |--- weights: [0.38, 0.62] class: 1.0\n\n\n\n\n\n\n\n\nCode\nprint('Alternative visualization:')\nfig = plt.figure(figsize=(15, 15))\nplot_tree(dt.estimator_, feature_names=feat_names)\nplt.show()\n\n\nAlternative visualization:"
  },
  {
    "objectID": "06_XAI.html#partial-depedency-plot-and-individual-conditional-expectation-plots",
    "href": "06_XAI.html#partial-depedency-plot-and-individual-conditional-expectation-plots",
    "title": "7  Explainable AI",
    "section": "7.3 Partial Depedency Plot and Individual Conditional Expectation plots",
    "text": "7.3 Partial Depedency Plot and Individual Conditional Expectation plots\nPartial dependence plots (PDP) and individual conditional expectation (ICE) plots can be used to visualize and analyze interaction between the target response 1 and a set of input features of interest.\nBoth PDPs and ICEs assume that the input features of interest are independent from the complement features, and this assumption is often violated in practice. Thus, in the case of correlated features, we will create absurd data points to compute the PDP/ICE.\n\n7.3.1 Partial dependence plots\nPartial dependence plots (PDP) show the dependence between the target response and a set of input features of interest, marginalizing over the values of all other input features (the ‘complement’ features). Intuitively, we can interpret the partial dependence as the expected target response as a function of the input features of interest.\nDue to the limits of human perception the size of the set of input feature of interest must be small (usually, one or two) thus the input features of interest are usually chosen among the most important features.\n\n\nCode\ncal_housing = fetch_california_housing()\nX = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\ny = cal_housing.target\n\ny -= y.mean()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n\n\n\n7.3.1.1 1-way partial dependence with different models\nNote that it is important to check that the model is accurate enough on a test set before plotting the partial dependence since there would be little use in explaining the impact of a given feature on the prediction function of a poor model.\n\n\nCode\nest = HistGradientBoostingRegressor(random_state=0) # Similar to lightgbm\nest.fit(X_train, y_train)\nprint(f\"Test R2 score: {est.score(X_test, y_test):.2f}\")\n\n\nTest R2 score: 0.85\n\n\nThe sklearn.inspection module provides a convenience function from_estimator to create one-way and two-way partial dependence plots.\n\n\nCode\nfeatures = [\"MedInc\", \"AveOccup\", \"HouseAge\", \"AveRooms\"]\n_, ax = plt.subplots(ncols=4, figsize=(15, 7))\ndisplay = PartialDependenceDisplay.from_estimator(\n    est,\n    X_train,\n    features,\n    grid_resolution=20,\n    ax = ax,\n    random_state=0,\n    pd_line_kw={\"color\": \"tab:orange\", \"linestyle\": \"--\"}\n)\ndisplay.figure_.suptitle(\n    \"Partial dependence of house value on non-location features\\n\"\n    \"for the California housing dataset, with HistGradientBoostingRegressor\"\n)\ndisplay.figure_.subplots_adjust(hspace=2)\n\n\n\n\n\nWe can clearly see on the PDPs (dashed orange line) that the median house price shows a linear relationship with the median income (left) and that the house price drops when the average occupants per household increases (middle). The right plots show that the house age in a district does not have a strong influence on the (median) house price; so does the average rooms per household.\nOne-way PDPs tell us about the interaction between the target response and an input feature of interest feature (e.g. linear, non-linear).\n\n\n7.3.1.2 2D Partial Dependence Plots\nPDPs with two features of interest enable us to visualize interactions among them. Another consideration is linked to the performance to compute the PDPs. With the tree-based algorithm, when only PDPs are requested, they can be computed on an efficient way using the ‘recursion’ method.\n\n\nCode\nfeatures = [\"AveOccup\", \"HouseAge\", (\"AveOccup\", \"HouseAge\")]\n_, ax = plt.subplots(ncols=3, figsize=(13, 6))\ndisplay = PartialDependenceDisplay.from_estimator(\n    est,\n    X_train,\n    features,\n    kind=\"average\",\n    grid_resolution=10,\n    ax=ax,\n)\n\ndisplay.figure_.suptitle(\n    \"Partial dependence of house value on non-location features\\n\"\n    \"for the California housing dataset, with Gradient Boosting\"\n)\ndisplay.figure_.subplots_adjust(wspace=0.4, hspace=0.3)\n\n\n\n\n\nThe left plot in the above figure shows the effect of the average occupancy on the median house price; we can clearly see a linear relationship among them when the average occupancy is inferior to 3 persons. Similarly, we could analyze the effect of the house age on the median house price (middle plot). Thus, these interpretations are marginal, considering a feature at a time.\nThe two-way partial dependence plot shows the dependence of median house price on joint values of house age and average occupants per household. We can clearly see an interaction between the two features: for an average occupancy greater than two, the house price is nearly independent of the house age, whereas for values less than two there is a strong dependence on age.\nIf you need the raw values of the partial dependence function rather than the plots, you can use the sklearn.inspection.partial_dependence() function.\n\n\n7.3.1.3 Another example\nLike permutation importance, partial dependence plots are calculated after a model has been fit. The model is fit on real data that has not been artificially manipulated in any way. Our example will use a model that predicts whether a soccer/football team will have the “Man of the Game” winner based on the team’s statistics. The “Man of the Game” award is given to the best player in the game.\nTeams may differ in many ways. How many passes they made, shots they took, goals they scored, etc. At first glance, it seems difficult to disentangle the effect of these features. To see how partial plots separate out the effect of each feature, we start by considering a single row of data. For example, that row of data might represent a team that had the ball 50% of the time, made 100 passes, took 10 shots and scored 1 goal.\nWe will use the fitted model to predict our outcome (probability their player won “man of the match”). But we repeatedly alter the value for one variable to make a series of predictions. We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time. Then predict again for 60%. And so on. We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis).\nIn this description, we used only a single row of data. Interactions between features may cause the plot for a single row to be atypical. So, we repeat that mental experiment with multiple rows from the original dataset, and we plot the average predicted outcome on the vertical axis.\n\n\nCode\ndata = pd.read_csv('FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\nX = data[feature_names]\nX\n\n\n\n  \n    \n      \n\n\n\n\n\n\nGoal Scored\nBall Possession %\nAttempts\nOn-Target\nOff-Target\nBlocked\nCorners\nOffsides\nFree Kicks\nSaves\nPass Accuracy %\nPasses\nDistance Covered (Kms)\nFouls Committed\nYellow Card\nYellow & Red\nRed\nGoals in PSO\n\n\n\n\n0\n5\n40\n13\n7\n3\n3\n6\n3\n11\n0\n78\n306\n118\n22\n0\n0\n0\n0\n\n\n1\n0\n60\n6\n0\n3\n3\n2\n1\n25\n2\n86\n511\n105\n10\n0\n0\n0\n0\n\n\n2\n0\n43\n8\n3\n3\n2\n0\n1\n7\n3\n78\n395\n112\n12\n2\n0\n0\n0\n\n\n3\n1\n57\n14\n4\n6\n4\n5\n1\n13\n3\n86\n589\n111\n6\n0\n0\n0\n0\n\n\n4\n0\n64\n13\n3\n6\n4\n5\n0\n14\n2\n86\n433\n101\n22\n1\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n123\n1\n46\n11\n1\n6\n4\n4\n3\n24\n5\n79\n479\n148\n14\n1\n0\n0\n0\n\n\n124\n2\n43\n12\n4\n3\n5\n4\n1\n5\n5\n88\n510\n108\n11\n1\n0\n0\n0\n\n\n125\n0\n57\n15\n5\n7\n3\n5\n0\n12\n2\n92\n698\n110\n5\n2\n0\n0\n0\n\n\n126\n4\n39\n8\n6\n1\n1\n2\n1\n14\n1\n75\n271\n99\n14\n2\n0\n0\n0\n\n\n127\n2\n61\n15\n3\n8\n4\n6\n1\n15\n3\n83\n547\n100\n13\n1\n0\n0\n0\n\n\n\n\n\n128 rows × 18 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\ntree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)\n\n\nOur first example uses a decision tree, which you can see below. In practice, you’ll use more sophistated models for real-world applications.\n\n\nCode\ntree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names)\ngraphviz.Source(tree_graph)\n\n\n\n\n\n\n\nCode\nfeature_names\n\n\n['Goal Scored',\n 'Ball Possession %',\n 'Attempts',\n 'On-Target',\n 'Off-Target',\n 'Blocked',\n 'Corners',\n 'Offsides',\n 'Free Kicks',\n 'Saves',\n 'Pass Accuracy %',\n 'Passes',\n 'Distance Covered (Kms)',\n 'Fouls Committed',\n 'Yellow Card',\n 'Yellow & Red',\n 'Red',\n 'Goals in PSO']\n\n\n\n\nCode\nPartialDependenceDisplay.from_estimator(tree_model, val_X, features=['Goal Scored'], feature_names=feature_names)\nplt.ylim(0,1)\n\n\n(0.0, 1.0)\n\n\n\n\n\nA few items are worth pointing out as you interpret this plot\n\nThe y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\nA blue shaded area indicates level of confidence\n\nFrom this particular graph, we see that scoring a goal substantially increases your chances of winning “Man of The Match.” But extra goals beyond that appear to have little impact on predictions.\nHere is another example plot:\n\n\nCode\nPartialDependenceDisplay.from_estimator(tree_model, val_X, features=['Distance Covered (Kms)'], feature_names=feature_names)\nplt.ylim(0,2)\n\n\n(0.0, 2.0)\n\n\n\n\n\nThis graph seems too simple to represent reality. But that’s because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model’s structure.\nYou can easily compare the structure or implications of different models. Here is the same plot with a Random Forest model.\n\n\nCode\n# Build Random Forest model\nrf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n\nPartialDependenceDisplay.from_estimator(rf_model, val_X, features=['Distance Covered (Kms)'], feature_names=feature_names)\n\n\n&lt;sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x7f495beb38e0&gt;\n\n\n\n\n\nThis model thinks you are more likely to win Man of the Match if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.\nIn general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model. Though this dataset is small enough that we would be careful in how we interpret any model.\nIf you are curious about interactions between features, 2D partial dependence plots are also useful. An example may clarify this.\nWe will again use the Decision Tree model for this graph. It will create an extremely simple plot, but you should be able to match what you see in the plot to the tree itself.\n\n\nCode\n# Similar to previous PDP plot\nPartialDependenceDisplay.from_estimator(tree_model, val_X, features=[('Goal Scored', 'Distance Covered (Kms)')], feature_names=feature_names)\n\n\n&lt;sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x7f495be2df70&gt;\n\n\n\n\n\nThis graph shows predictions for any combination of Goals Scored and Distance covered.\nFor example, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km. If they score 0 goals, distance covered doesn’t matter. Can you see this by tracing through the decision tree with 0 goals?\nBut distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot. Can you see this pattern in the decision tree too?\n\n\n\n7.3.2 Individual conditional expectation (ICE) plot\nDue to the limits of human perception, only one input feature of interest is supported for ICE plots.\nWhile the PDPs are good at showing the average effect of the target features, they can obscure a heterogeneous relationship created by interactions. When interactions are present the ICE plot will provide many more insights. For example, we could observe a linear relationship between the median income and the house price in the PD line. However, the ICE lines show that there are some exceptions, where the house price remains constant in some ranges of the median income. We will plot the partial dependence, both individual (ICE) and averaged one (PDP). We limit to only 50 ICE curves to not overcrowd the plot.\nThe sklearn.inspection module’s PartialDependenceDisplay.from_estimatorconvenience function can be used to create ICE plots by setting kind='individual'. But in ICE plots it might not be easy to see the average effect of the input feature of interest. Hence, it is recommended to use ICE plots alongside PDPs. They can be plotted together with kind='both'.\n\n\nCode\nfeatures = [\"MedInc\", \"AveOccup\", \"HouseAge\", \"AveRooms\"]\n_, ax = plt.subplots(ncols=4, figsize=(13, 6))\ndisplay = PartialDependenceDisplay.from_estimator(\n    est,\n    X_train,\n    features,\n    kind=\"both\",\n    subsample=50,\n    n_jobs=3,\n    grid_resolution=20,\n    random_state=0,\n    ax = ax, \n    ice_lines_kw={\"color\": \"tab:blue\", \"alpha\": 0.2, \"linewidth\": 0.5},\n    pd_line_kw={\"color\": \"tab:orange\", \"linestyle\": \"--\"},\n)\ndisplay.figure_.suptitle(\n    \"Partial dependence of house value on non-location features\\n\"\n    \"for the California housing dataset, with Gradient Boosting\"\n)\ndisplay.figure_.subplots_adjust(wspace=0.4, hspace=0.3)\n\n\n\n\n\nThe ICE curves (light blue lines) complement the analysis: we can see that there are some exceptions, where the house price remain constant with median income and average occupants. On the other hand, while the house age (top right) does not have a strong influence on the median house price on average, there seems to be a number of exceptions where the house price increase when between the ages 15-25. Similar exceptions can be observed for the average number of rooms (bottom left). Therefore, ICE plots show some individual effect which are attenuated by taking the averages.\nCheckout more information at https://scikit-learn.org/stable/modules/partial_dependence.html# or https://github.com/SauceCat/PDPbox"
  },
  {
    "objectID": "06_XAI.html#lime",
    "href": "06_XAI.html#lime",
    "title": "7  Explainable AI",
    "section": "7.4 LIME",
    "text": "7.4 LIME\nWe’ll use the Iris dataset, and we’ll train a random forest.\n\n\nCode\nnp.random.seed(1)\niris = load_iris()\ntrain, test, labels_train, labels_test = train_test_split(iris.data, iris.target, train_size=0.8)\n\n\n\n\nCode\nrf = RandomForestClassifier(n_estimators=500)\nrf.fit(train, labels_train)\nmetrics.accuracy_score(labels_test, rf.predict(test))\n\n\n0.9666666666666667\n\n\n\n7.4.1 Tabular data\n\n7.4.1.1 Create the explainer\nTabular explainers need a training set. The reason for this is because we compute statistics on each feature (column). If the feature is numerical, we compute the mean and std, and discretize it into quartiles. If the feature is categorical, we compute the frequency of each value. For this part, we’ll only look at numerical features.\nWe use these computed statistics for two things:\n\nTo scale the data, so that we can meaningfully compute distances when the attributes are not on the same scale\nTo sample perturbed instances - which we do by sampling from a Normal(0,1), multiplying by the std and adding back the mean.\n\n\n\nCode\nexplainer = lime.lime_tabular.LimeTabularExplainer(train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)\n\n\n\n\n7.4.1.2 Explaining an instance\nSince this is a multi-class classification problem, we set the top_labels parameter, so that we only explain the top class.\n\n\nCode\ni = np.random.randint(0, test.shape[0])\nexp = explainer.explain_instance(test[i], rf.predict_proba, num_features=2, top_labels=1)\n\n\nWe now explain a single instance:\n\n\nCode\nexp.show_in_notebook(show_table=True, show_all=True)\n\n\n\n\n        \n        \n        \n        \n        \n        \n\n\n\n\nCode\nfig = exp.as_pyplot_figure(label=0)\n\n\n\n\n\nNow, there is a lot going on here. First, note that the row we are explained is displayed on the right side, in table format. Since we had the show_all parameter set to false, only the features used in the explanation are displayed. The value column displays the original value for each feature.\nNote that LIME has discretized the features in the explanation. This is because we let discretize_continuous=True in the constructor (this is the default). Discretized features make for more intuitive explanations.\n\n\n\n7.4.2 Image data\n\n\nCode\ninet_model = tf.keras.applications.inception_v3.InceptionV3()\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n96112376/96112376 [==============================] - 0s 0us/step\n\n\n\n\nCode\n!wget https://raw.githubusercontent.com/marcotcr/lime/master/doc/notebooks/data/cat_mouse.jpg\n\n\n--2023-03-27 03:50:28--  https://raw.githubusercontent.com/marcotcr/lime/master/doc/notebooks/data/cat_mouse.jpg\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 248960 (243K) [image/jpeg]\nSaving to: ‘cat_mouse.jpg’\n\ncat_mouse.jpg         0%[                    ]       0  --.-KB/s               cat_mouse.jpg       100%[===================&gt;] 243.12K  --.-KB/s    in 0.003s  \n\n2023-03-27 03:50:28 (91.5 MB/s) - ‘cat_mouse.jpg’ saved [248960/248960]\n\n\n\n\n\nCode\ndef transform_img_fn(path_list):\n    out = []\n    for img_path in path_list:\n        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(299, 299))\n        x = tf.keras.preprocessing.image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n        x = tf.keras.applications.inception_v3.preprocess_input(x)\n        out.append(x)\n    return np.vstack(out)\n\n\n\n\nCode\nimages = transform_img_fn(['cat_mouse.jpg'])\n# I'm dividing by 2 and adding 0.5 because of how this Inception represents images\nplt.imshow(images[0] / 2 + 0.5)\npreds = inet_model.predict(images)\nfor x in tf.keras.applications.imagenet_utils.decode_predictions(preds)[0]:\n    print(x)\n\n\n1/1 [==============================] - 12s 12s/step\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n35363/35363 [==============================] - 0s 0us/step\n('n02133161', 'American_black_bear', 0.6371615)\n('n02105056', 'groenendael', 0.03181786)\n('n02104365', 'schipperke', 0.02994415)\n('n01883070', 'wombat', 0.028509395)\n('n01877812', 'wallaby', 0.025093386)\n\n\n\n\n\n\n7.4.2.1 Explanation\nNow let’s get an explanation\n\n\nCode\nexplainer = lime_image.LimeImageExplainer()\n\n\n\n\nCode\n# Hide color is the color for a superpixel turned OFF. Alternatively, if it is NONE, the superpixel will be replaced by the average of its pixels\nexplanation = explainer.explain_instance(images[0].astype('double'), inet_model.predict, top_labels=5, hide_color=0, num_samples=1000)\n\n\n\n\n\n1/1 [==============================] - 1s 1s/step\n1/1 [==============================] - 0s 54ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 44ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 46ms/step\n1/1 [==============================] - 0s 46ms/step\n1/1 [==============================] - 0s 44ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 48ms/step\n1/1 [==============================] - 0s 46ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 45ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 51ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 52ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 48ms/step\n1/1 [==============================] - 0s 46ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 46ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 45ms/step\n1/1 [==============================] - 0s 46ms/step\n1/1 [==============================] - 0s 47ms/step\n1/1 [==============================] - 0s 53ms/step\n1/1 [==============================] - 0s 52ms/step\n1/1 [==============================] - 0s 51ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 57ms/step\n1/1 [==============================] - 0s 54ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 57ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 49ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 58ms/step\n1/1 [==============================] - 0s 53ms/step\n\n\n\n\n7.4.2.2 Now let’s see the explanation for the classes\n\n\nCode\ntemp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=10, hide_rest=True)\nplt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n\n\n&lt;matplotlib.image.AxesImage at 0x7f4944ff6220&gt;\n\n\n\n\n\nWe can also see the ‘pros and cons’ (pros in green, cons in red)\n\n\nCode\ntemp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\nplt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n\n\n&lt;matplotlib.image.AxesImage at 0x7f495d2fac10&gt;\n\n\n\n\n\nAlternatively, we can also plot explanation weights onto a heatmap visualization. The colorbar shows the values of the weights.\n\n\nCode\n#Select the same class explained on the figures above.\nind =  explanation.top_labels[0]\n\n#Map each explanation weight to the corresponding superpixel\ndict_heatmap = dict(explanation.local_exp[ind])\nheatmap = np.vectorize(dict_heatmap.get)(explanation.segments) \n\n#Plot. The visualization makes more sense if a symmetrical colorbar is used.\nplt.imshow(heatmap, cmap = 'RdBu', vmin  = -heatmap.max(), vmax = heatmap.max())\nplt.colorbar()\n\n\n&lt;matplotlib.colorbar.Colorbar at 0x7f495feeaca0&gt;\n\n\n\n\n\n\n\nCode\ndict_heatmap\n\n\n{25: 0.1266119733320075,\n 26: 0.12625454084766954,\n 20: 0.12550891124563643,\n 14: 0.11200991992818318,\n 17: 0.09093842626197812,\n 10: -0.05383741765004169,\n 34: -0.04666506465080401,\n 11: 0.03839457072387601,\n 33: 0.03631442598802239,\n 24: 0.03619821434663484,\n 45: 0.02873184783414671,\n 22: 0.02866525965253672,\n 12: 0.02705639125595404,\n 8: 0.025929842390449525,\n 27: -0.02401695011845878,\n 54: -0.022137392638820155,\n 29: -0.020931992891644612,\n 6: 0.02065014310337862,\n 31: 0.020590037005144224,\n 5: 0.020036001632855294,\n 41: -0.019769696586538182,\n 3: 0.01919291308718661,\n 32: 0.016896245705981867,\n 51: 0.015878670165086477,\n 35: -0.014445619649604592,\n 13: -0.014122936684619367,\n 0: 0.013491125214863297,\n 39: 0.013017245875456086,\n 38: -0.012823059854158792,\n 4: -0.01278698007178597,\n 37: 0.012529557102235748,\n 19: 0.011597288332063638,\n 52: 0.010744092391949366,\n 53: -0.010660751286737019,\n 15: 0.010452156377416876,\n 18: 0.007091472431529134,\n 36: -0.006765111561901239,\n 7: -0.006490874832987173,\n 23: -0.0059956691966935515,\n 50: -0.0057907960843224925,\n 21: 0.005479746317145224,\n 28: 0.004983914391121539,\n 48: 0.004792603255925444,\n 42: -0.004650885399392036,\n 46: -0.004480095737890147,\n 40: -0.004215050771676743,\n 1: 0.003401562283486891,\n 43: -0.00306510230290569,\n 44: -0.0027089154172046694,\n 30: -0.0025121327258090156,\n 49: -0.0021503267934872327,\n 16: -0.0014023601941624687,\n 2: -0.0011691877595369229,\n 9: -0.0008138065988610777,\n 47: 0.0005239851503219061}\n\n\nLet’s see the explanation for the wombat\n\n\nCode\ntemp, mask = explanation.get_image_and_mask(explanation.top_labels[3], positive_only=False, num_features=10, hide_rest=False)\nplt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n\n\n&lt;matplotlib.image.AxesImage at 0x7f495c4c7bb0&gt;\n\n\n\n\n\nFor more information, please refer to https://github.com/marcotcr/lime"
  },
  {
    "objectID": "06_XAI.html#shap",
    "href": "06_XAI.html#shap",
    "title": "7  Explainable AI",
    "section": "7.5 SHAP",
    "text": "7.5 SHAP\n\n7.5.1 The force plot\nAn example is helpful, and we’ll continue the soccer/football example from the partial dependence plots. In these part, we predicted whether a team would have a player win the Man of the Match award.\nWe could ask: * How much was a prediction driven by the fact that the team scored 3 goals?\nBut it’s easier to give a concrete, numeric answer if we restate this as: * How much was a prediction driven by the fact that the team scored 3 goals, instead of some baseline number of goals.\nOf course, each team has many features. So if we answer this question for number of goals, we could repeat the process for all other features.\n\n\nCode\ndata = pd.read_csv('FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n\n\nWe will look at SHAP values for a single row of the dataset (we arbitrarily chose row 5). For context, we’ll look at the raw predictions before looking at the SHAP values.\n\n\nCode\nrow_to_show = 5\ndata_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n#data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\nmy_model.predict_proba(data_for_prediction.values.reshape(1, -1))\n\n\n/usr/local/lib/python3.9/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n\n\narray([[0.29, 0.71]])\n\n\nThe team is 71% likely to have a player win the award. Now, we’ll move onto the code to get SHAP values for that single prediction.\n\n\nCode\n# Create object that can calculate shap values\nexplainer = shap.Explainer(my_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\n\n\nThe shap_values object above is a list with two arrays. The first array is the SHAP values for a negative outcome (don’t win the award), and the second array is the list of SHAP values for the positive outcome (wins the award). We typically think about predictions in terms of the prediction of a positive outcome, so we’ll pull out SHAP values for positive outcomes (pulling out shap_values[1]).\nIt’s cumbersome to review raw arrays, but the shap package has a nice way to visualize the results.\n\n\nCode\nshap.initjs()\nshap.plots.force(explainer.expected_value[1], shap_values[1], data_for_prediction) # You can use view output in full screen\n\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\nIf you look carefully at the code where we created the SHAP values, you’ll notice we reference Trees in shap.TreeExplainer(my_model). But the SHAP package has explainers for every type of model.\n\nshap.DeepExplainer works with Deep Learning models.\nshap.KernelExplainer works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.\n\n\n\n7.5.2 Summary Plots\nIn addition to this nice breakdown for each prediction, the Shap library offers great visualizations of groups of Shap values. We will focus on two of these visualizations. These visualizations have conceptual similarities to permutation importance and partial dependence plots.\n\n\nCode\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\nshap_values = explainer.shap_values(val_X)\n\n# Make plot. Index of [1] is explained in text below.\nshap.summary_plot(shap_values[1], val_X)\n\n\n/usr/local/lib/python3.9/dist-packages/shap/plots/_beeswarm.py:664: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n\n\n\n\n\nThe code isn’t too complex. But there are a few caveats.\n\nWhen plotting, we call shap_values[1]. For classification problems, there is a separate array of SHAP values for each possible outcome. In this case, we index in to get the SHAP values for the prediction of “True”.\nCalculating SHAP values can be slow. It isn’t a problem here, because this dataset is small. But you’ll want to be careful when running these to plot with reasonably sized datasets. The exception is when using an xgboost model, which SHAP has some optimizations for and which is thus much faster.\n\nThis provides a great overview of the model, but we might want to delve into a single feature. That’s where SHAP dependence contribution plots come into play.\n\n\n7.5.3 Dependence Contribution Plots\n\n\nCode\n# calculate shap values. This is what we will plot.\nshap_values = explainer.shap_values(X)\n\n# make plot.\nshap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index=\"Goal Scored\")\n\n\n\n\n\n\n\n7.5.4 Image data\nDeep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with DeepLIFT. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT.\n\n\nCode\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\ninput_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(tf.keras.layers.Dropout(0.25))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11490434/11490434 [==============================] - 0s 0us/step\nx_train shape: (60000, 28, 28, 1)\n60000 train samples\n10000 test samples\nEpoch 1/12\n469/469 [==============================] - 9s 10ms/step - loss: 0.2362 - accuracy: 0.9283 - val_loss: 0.0500 - val_accuracy: 0.9838\nEpoch 2/12\n469/469 [==============================] - 4s 9ms/step - loss: 0.0817 - accuracy: 0.9759 - val_loss: 0.0368 - val_accuracy: 0.9873\nEpoch 3/12\n469/469 [==============================] - 5s 10ms/step - loss: 0.0629 - accuracy: 0.9811 - val_loss: 0.0299 - val_accuracy: 0.9902\nEpoch 4/12\n469/469 [==============================] - 4s 9ms/step - loss: 0.0507 - accuracy: 0.9843 - val_loss: 0.0303 - val_accuracy: 0.9904\nEpoch 5/12\n469/469 [==============================] - 5s 10ms/step - loss: 0.0440 - accuracy: 0.9867 - val_loss: 0.0287 - val_accuracy: 0.9911\nEpoch 6/12\n469/469 [==============================] - 5s 10ms/step - loss: 0.0384 - accuracy: 0.9879 - val_loss: 0.0266 - val_accuracy: 0.9908\nEpoch 7/12\n469/469 [==============================] - 4s 9ms/step - loss: 0.0344 - accuracy: 0.9890 - val_loss: 0.0265 - val_accuracy: 0.9922\nEpoch 8/12\n469/469 [==============================] - 5s 10ms/step - loss: 0.0296 - accuracy: 0.9906 - val_loss: 0.0265 - val_accuracy: 0.9912\nEpoch 9/12\n469/469 [==============================] - 5s 10ms/step - loss: 0.0261 - accuracy: 0.9913 - val_loss: 0.0296 - val_accuracy: 0.9903\nEpoch 10/12\n469/469 [==============================] - 4s 9ms/step - loss: 0.0250 - accuracy: 0.9919 - val_loss: 0.0272 - val_accuracy: 0.9914\nEpoch 11/12\n469/469 [==============================] - 5s 10ms/step - loss: 0.0210 - accuracy: 0.9929 - val_loss: 0.0289 - val_accuracy: 0.9916\nEpoch 12/12\n469/469 [==============================] - 4s 9ms/step - loss: 0.0211 - accuracy: 0.9932 - val_loss: 0.0265 - val_accuracy: 0.9931\nTest loss: 0.02652600035071373\nTest accuracy: 0.9930999875068665\n\n\n\n\nCode\n# select a set of background examples to take an expectation over\nbackground = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]\n\n# explain predictions of the model on three images\ne = shap.DeepExplainer(model, background)\n# ...or pass tensors directly\n# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)\nshap_values = e.shap_values(x_test[1:5])\n\n\n/usr/local/lib/python3.9/dist-packages/shap/explainers/_deep/deep_tf.py:95: UserWarning: keras is no longer supported, please use tf.keras instead.\n/usr/local/lib/python3.9/dist-packages/shap/explainers/_deep/deep_tf.py:100: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n/usr/local/lib/python3.9/dist-packages/keras/backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\nWARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\nInstructions for updating:\nLambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n\n\n\n\nCode\nlen(shap_values)\n\n\n10\n\n\n\n\nCode\n# plot the feature attributions\n# 10 classes, thus we have 10 plots!\nshap.image_plot(shap_values, -x_test[1:5])\n\n\n\n\n\nThe plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model’s output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the ‘zero’ image the blank middle is important, while for the ‘four’ image the lack of a connection on top makes it a four instead of a nine."
  },
  {
    "objectID": "06_XAI.html#protodash-using-axi360",
    "href": "06_XAI.html#protodash-using-axi360",
    "title": "7  Explainable AI",
    "section": "7.6 Protodash using AXI360",
    "text": "7.6 Protodash using AXI360\nYou can find more examples here"
  },
  {
    "objectID": "06_XAI.html#counterfactual-instances",
    "href": "06_XAI.html#counterfactual-instances",
    "title": "7  Explainable AI",
    "section": "7.7 Counterfactual instances",
    "text": "7.7 Counterfactual instances\nYou can find more informations here and here"
  },
  {
    "objectID": "06_XAI.html#using-interpretable-features-for-model-debugging",
    "href": "06_XAI.html#using-interpretable-features-for-model-debugging",
    "title": "7  Explainable AI",
    "section": "7.8 Using Interpretable Features for Model Debugging",
    "text": "7.8 Using Interpretable Features for Model Debugging\nYou can find more informations here"
  },
  {
    "objectID": "06_XAI.html#references",
    "href": "06_XAI.html#references",
    "title": "7  Explainable AI",
    "section": "7.9 References",
    "text": "7.9 References\n\nhttps://www.kaggle.com/learn/machine-learning-explainability\nhttps://scikit-learn.org/stable/modules/partial_dependence.html#\nhttps://github.com/csinva/imodels\nhttps://github.com/marcotcr/lime\nhttps://github.com/slundberg/shap"
  },
  {
    "objectID": "07_Deploy.html#setup",
    "href": "07_Deploy.html#setup",
    "title": "8  Deploy and monitoring",
    "section": "8.1 Setup",
    "text": "8.1 Setup\n\n\nCode\n!pip install bentoml -qq\n!pip install pyngrok -qq\n!pip install PyYAML -U -qq\n!pip install streamlit -qq\n!pip install gradio -qq\n!pip install evidently -qq\n\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 968.6/968.6 KB 19.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.6/200.6 KB 15.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.9/50.9 KB 3.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 KB 3.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.5/94.5 KB 6.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.3/135.3 KB 7.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.4/66.4 KB 4.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.8/57.8 KB 2.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 KB 3.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 17.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 8.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.5/66.5 KB 6.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 KB 12.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.8/158.8 KB 21.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 KB 10.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.6/264.6 KB 30.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 KB 8.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 761.3/761.3 KB 26.6 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n  Building wheel for pyngrok (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/9.6 MB 41.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.1/82.1 KB 10.4 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 75.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164.8/164.8 KB 20.4 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.3/184.3 KB 26.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 KB 9.2 MB/s eta 0:00:00\n  Building wheel for validators (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.8/15.8 MB 86.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.8/199.8 KB 28.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.5/106.5 KB 16.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.5/71.5 KB 10.2 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 144.1/144.1 KB 991.0 kB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 KB 5.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.1/57.1 KB 7.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.9/66.9 KB 9.1 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.6/69.6 KB 10.5 MB/s eta 0:00:00\n  Building wheel for ffmpy (setup.py) ... done\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbentoml 1.0.16 requires starlette&lt;0.26, but you have starlette 0.26.1 which is incompatible.\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 103.4 MB/s eta 0:00:00\n\n\nNotice that you may need to restart the kernel after the above installations.\n\n\nCode\n# Scientific computing\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline\n\n# Modeling\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn import ensemble\nfrom sklearn import datasets\n\n# Deploy\nimport bentoml\nimport gradio as gr\n\n# Monitoring\nfrom evidently import ColumnMapping\nfrom evidently.report import Report\nfrom evidently.metric_preset import DataDriftPreset, TargetDriftPreset, RegressionPreset\n\n# Helper library\nfrom pyngrok import ngrok, conf\nimport getpass\n\n# Other system library\nfrom pathlib import Path\nimport requests\nimport os\nimport json\nimport sys\n\nimport zipfile\nimport io\nfrom datetime import datetime, time\n\n\nHere are some tips for this notebook https://amitness.com/2020/06/google-colaboratory-tips/ and https://stackoverflow.com/questions/59741453/is-there-a-general-way-to-run-web-applications-on-google-colab.\nngrok is a reverse proxy tool that opens secure tunnels from public URLs to localhost, perfect for exposing local web servers, building webhook integrations, enabling SSH access, testing chatbots, demoing from your own machine, and more. In this lab, we will use use https://pyngrok.readthedocs.io/en/latest/integrations.html. However, for production environment, it is recommended to use cloud service such as AWS, GCP or Azure, see here or https://pycaret.gitbook.io/docs/get-started/functions/deploy#deploy_model for more details.\n\n\nCode\nprint(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\nconf.get_default().auth_token = getpass.getpass()\n\n\nEnter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\n··········\n\n\n\n\nCode\n# Setup a tunnel to the streamlit port 8050\npublic_url = ngrok.connect(8050)\n\n\n\n\n\n\n\nCode\npublic_url\n\n\n&lt;NgrokTunnel: \"http://faf8-34-147-81-41.ngrok.io\" -&gt; \"http://localhost:8050\"&gt;\n\n\n\n\nCode\nif not tf.config.list_physical_devices('GPU'):\n    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n    if \"google.colab\" in sys.modules:\n        print(\"Go to Runtime &gt; Change runtime and select a GPU hardware \"\n              \"accelerator.\")\n    if \"kaggle_secrets\" in sys.modules:\n        print(\"Go to Settings &gt; Accelerator and select GPU.\")"
  },
  {
    "objectID": "07_Deploy.html#deploying-tensorflow-models-to-tensorflow-serving-tfs",
    "href": "07_Deploy.html#deploying-tensorflow-models-to-tensorflow-serving-tfs",
    "title": "8  Deploy and monitoring",
    "section": "8.2 Deploying TensorFlow models to TensorFlow Serving (TFS)",
    "text": "8.2 Deploying TensorFlow models to TensorFlow Serving (TFS)\nYou could create your own microservice using any technology you want (e.g., using the Flask library), but why reinvent the wheel when you can just use TF Serving?\n\n8.2.1 Exporting SavedModels\nTensorFlow provides a simple tf.saved_model.save() function to export models to the SavedModel format. All you need to do is give it the model, specifying its name and version number, and the function will save the model’s computation graph and its weights:\n\n\nCode\n# Load and split the MNIST dataset\nmnist = tf.keras.datasets.mnist.load_data()\n(X_train_full, y_train_full), (X_test, y_test) = mnist\nX_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11490434/11490434 [==============================] - 1s 0us/step\n\n\nIt’s usually a good idea to include all the preprocessing layers in the final model you export so that it can ingest data in its natural form once it is deployed to production. This avoids having to take care of preprocessing separately within the application that uses the model. Bundling the preprocessing steps within the model also makes it simpler to update them later on and limits the risk of mismatch between a model and the preprocessing steps it requires!\n\n\nCode\n# Build & train an MNIST model (also handles image preprocessing)\n\ntf.random.set_seed(42)\ntf.keras.backend.clear_session()\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n    tf.keras.layers.Rescaling(scale=1 / 255),\n    tf.keras.layers.Dense(100, activation=\"relu\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n              metrics=[\"accuracy\"])\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n\n\nEpoch 1/10\n1719/1719 [==============================] - 12s 4ms/step - loss: 0.7007 - accuracy: 0.8216 - val_loss: 0.3758 - val_accuracy: 0.8980\nEpoch 2/10\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.3560 - accuracy: 0.9020 - val_loss: 0.3034 - val_accuracy: 0.9140\nEpoch 3/10\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.3056 - accuracy: 0.9142 - val_loss: 0.2692 - val_accuracy: 0.9246\nEpoch 4/10\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.2756 - accuracy: 0.9229 - val_loss: 0.2475 - val_accuracy: 0.9298\nEpoch 5/10\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.2528 - accuracy: 0.9296 - val_loss: 0.2286 - val_accuracy: 0.9322\nEpoch 6/10\n1719/1719 [==============================] - 6s 4ms/step - loss: 0.2343 - accuracy: 0.9341 - val_loss: 0.2148 - val_accuracy: 0.9392\nEpoch 7/10\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.2183 - accuracy: 0.9391 - val_loss: 0.1998 - val_accuracy: 0.9444\nEpoch 8/10\n1719/1719 [==============================] - 12s 7ms/step - loss: 0.2045 - accuracy: 0.9424 - val_loss: 0.1895 - val_accuracy: 0.9474\nEpoch 9/10\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.1922 - accuracy: 0.9462 - val_loss: 0.1797 - val_accuracy: 0.9502\nEpoch 10/10\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.1817 - accuracy: 0.9489 - val_loss: 0.1703 - val_accuracy: 0.9514\n\n\n&lt;keras.callbacks.History at 0x7fccb46c1850&gt;\n\n\n\n\nCode\nX_new = X_test[:3]  # pretend we have 3 new digit images to classify\nnp.round(model.predict(X_new), 2)\n\n\n1/1 [==============================] - 0s 326ms/step\n\n\narray([[0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99, 0.  , 0.  ],\n       [0.  , 0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n      dtype=float32)\n\n\nNow to version the model, you just need to create a subdirectory for each model version:\n\n\nCode\nmodel_name = \"my_mnist_model\"\nmodel_version = \"0001\"\nmodel_path = Path(model_name) / model_version\n#model.save(model_path, save_format=\"tf\")\n\n\n\n\nCode\ntf.keras.models.save_model(\n    model,\n    model_path,\n    overwrite=True,\n    include_optimizer=True,\n    save_format=\"tf\",\n    signatures=None,\n    options=None\n)\n\n\nA SavedModel represents a version of your model. It is stored as a directory containing a saved_model.pb file, which defines the computation graph (represented as a serialized protocol buffer), and a variables subdirectory containing the variable values. For models containing a large number of weights, these variable values may be split across multiple files. A SavedModel also includes an assets subdirectory that may contain additional data, such as vocabulary files, class names, or some example instances for this model.\n\n\nCode\nfor root, dirs, files in os.walk(model_name):\n    indent = '    ' * root.count(os.sep)\n    print('{}{}/'.format(indent, os.path.basename(root)))\n    for filename in files:\n        print('{}{}'.format(indent + '    ', filename))\n\n\nmy_mnist_model/\n    0001/\n        keras_metadata.pb\n        saved_model.pb\n        fingerprint.pb\n        assets/\n        variables/\n            variables.index\n            variables.data-00000-of-00001\n\n\nAs you might expect, you can load a SavedModel using the tf.keras.models.load_model() function.\n\n\nCode\nsaved_model = tf.keras.models.load_model(model_path)\nnp.round(saved_model.predict(X_new), 2)\n\n\n1/1 [==============================] - 0s 107ms/step\n\n\narray([[0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99, 0.  , 0.  ],\n       [0.  , 0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n      dtype=float32)\n\n\nTensorFlow also comes with a small saved_model_cli command-line tool to inspect SavedModels:\n\n\nCode\n!saved_model_cli show --dir {model_path} --all\n\n\n2023-03-28 11:23:38.548893: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n2023-03-28 11:23:38.549018: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n2023-03-28 11:23:38.549041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['flatten_input'] tensor_info:\n        dtype: DT_UINT8\n        shape: (-1, 28, 28)\n        name: serving_default_flatten_input:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['dense_1'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 10)\n        name: StatefulPartitionedCall:0\n  Method name is: tensorflow/serving/predict\n2023-03-28 11:23:41.439161: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n\nConcrete Functions:\n  Function Name: '__call__'\n    Option #1\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='inputs')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #2\n      Callable with:\n        Argument #1\n          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #3\n      Callable with:\n        Argument #1\n          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #4\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='inputs')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n\n  Function Name: '_default_save_signature'\n    Option #1\n      Callable with:\n        Argument #1\n          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n\n  Function Name: 'call_and_return_all_conditional_losses'\n    Option #1\n      Callable with:\n        Argument #1\n          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #2\n      Callable with:\n        Argument #1\n          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #3\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='inputs')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #4\n      Callable with:\n        Argument #1\n          inputs: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='inputs')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n\n\nA SavedModel contains one or more metagraphs. When you pass a tf.keras model, by default the function saves a simple SavedModel: it saves a single metagraph tagged “serve”, which contains two signature definitions, an initialization function (called _saved_model_init_op) and a default serving function (called serving_default). When saving a tf.keras model, the default serving function corresponds to the model’s call() function, which of course makes predictions.\n\n\n8.2.2 Serve your model with TensorFlow Serving (Server side)\nThere are many ways to install TF Serving: using the system’s package manager, using a Docker image, installing from source, and more. Since Colab/Kaggle runs on Ubuntu, we can use Ubuntu’s apt package manager like this:\n\n\nCode\nif \"google.colab\" in sys.modules or \"kaggle_secrets\" in sys.modules:\n    url = \"https://storage.googleapis.com/tensorflow-serving-apt\"\n    src = \"stable tensorflow-model-server tensorflow-model-server-universal\"\n    !echo 'deb {url} {src}' &gt; /etc/apt/sources.list.d/tensorflow-serving.list\n    !curl '{url}/tensorflow-serving.release.pub.gpg' | apt-key add -\n    !apt update -q && apt-get install -y tensorflow-model-server\n    %pip install -q -U tensorflow-serving-api\n\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  2943  100  2943    0     0  17011      0 --:--:-- --:--:-- --:--:-- 17011\nOK\nGet:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\nGet:2 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\nGet:3 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\nHit:4 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\nGet:5 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB]\nHit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\nHit:7 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\nGet:8 https://storage.googleapis.com/tensorflow-serving-apt stable InRelease [3,026 B]\nGet:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,407 kB]\nGet:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,140 kB]\nHit:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\nGet:12 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,025 kB]\nGet:13 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2,060 kB]\nGet:14 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,587 kB]\nGet:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [29.5 kB]\nGet:16 https://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server-universal amd64 Packages [348 B]\nHit:17 http://archive.ubuntu.com/ubuntu focal InRelease\nGet:18 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\nGet:19 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\nGet:20 https://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 Packages [340 B]\nGet:21 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,322 kB]\nGet:22 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,198 kB]\nGet:23 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,065 kB]\nFetched 16.2 MB in 31s (526 kB/s)\nReading package lists...\nBuilding dependency tree...\nReading state information...\n33 packages can be upgraded. Run 'apt list --upgradable' to see them.\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following NEW packages will be installed:\n  tensorflow-model-server\n0 upgraded, 1 newly installed, 0 to remove and 33 not upgraded.\nNeed to get 414 MB of archives.\nAfter this operation, 0 B of additional disk space will be used.\nGet:1 https://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 tensorflow-model-server all 2.11.1 [414 MB]\nFetched 414 MB in 13s (31.4 MB/s)\nSelecting previously unselected package tensorflow-model-server.\n(Reading database ... 128285 files and directories currently installed.)\nPreparing to unpack .../tensorflow-model-server_2.11.1_all.deb ...\nUnpacking tensorflow-model-server (2.11.1) ...\nSetting up tensorflow-model-server (2.11.1) ...\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 585.9/585.9 MB 2.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.8/77.8 KB 7.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 588.3/588.3 MB 2.5 MB/s eta 0:00:00\n\n\n\nThe code above starts by adding TensorFlow’s package repository to Ubuntu’s list of package sources. Then it downloads TensorFlow’s public GPG key and adds it to the package manager’s key list so it can verify TensorFlow’s package signatures. Next, it uses apt to install the tensorflow-model-server package. Lastly, it installs the tensorflow-serving-api library, which we will need to communicate with the server.\n\nIf tensorflow_model_server is installed (e.g., if you are running this notebook in Colab/Kaggle), then the following 2 cells will start the server. If your OS is Windows, you may need to run the tensorflow_model_server command in a terminal, and replace ${MODEL_DIR} with the full path to the my_mnist_model directory. This is where we start running TensorFlow Serving and load our model. After it loads we can start making inference requests using REST. There are some important parameters:\n\nport: The port that you’ll use for gRPC requests.\nrest_api_port: The port that you’ll use for REST requests.\nmodel_name: You’ll use this in the URL of REST requests. It can be anything.\nmodel_base_path: This is the path to the directory where you’ve saved your model.\n\n\n\nCode\nos.environ[\"MODEL_DIR\"] = str(model_path.parent.absolute())\n\n\n\n\nCode\n%%bash --bg\nnohup tensorflow_model_server \\\n     --port=8500 \\\n     --rest_api_port=8050 \\\n     --model_name=my_mnist_model \\\n     --model_base_path=\"${MODEL_DIR}\" &gt; server.log 2&gt;&1\n\n\nThe %%bash  --bg magic command executes the cell as a bash script, running it in the background. The &gt;my_server.log  2&gt;&1 part redirects the standard output and standard error to the server.log file. And that’s it! TF Serving is now running in the background, and its logs are saved to server.log.\n\n\nCode\n!tail server.log\n\n\n[warn] getaddrinfo: address family for nodename not supported\n[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n\n\n\n\n8.2.3 Querying TF Serving through the REST API (client side)\nLet’s start by creating the query. It must contain the name of the function signature you want to call, and of course the input data. Since the request must use the JSON format, we have to convert the input images from a NumPy array to a Python list:\n\n\nCode\ninput_data_json = json.dumps({\n    \"signature_name\": \"serving_default\",\n    \"instances\": X_new.tolist(),\n})\n\n\nNote that the JSON format is 100% text-based:\n\n\nCode\nrepr(input_data_json)[:1500] + \"...\"\n\n\n'\\'{\"signature_name\": \"serving_default\", \"instances\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 84, 185, 159, 151, 60, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 222, 254, 254, 254, 254, 241, 198, 198, 198, 198, 198, 198, 198, 198, 170, 52, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 67, 114, 72, 114, 163, 227, 254, 225, 254, 254, 254, 250, 229, 254, 254, 140, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 66, 14, 67, 67, 67, 59, 21, 236, 254, 106, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 253, 209, 18, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 233, 255, 83, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 129, 254, 238, 44, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 249, 254, 62, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...'\n\n\nNow let’s send the input data to TF Serving by sending an HTTP POST request. This can be done easily using the requests library:\n\n\nCode\nSERVER_URL = 'http://localhost:8050/v1/models/my_mnist_model:predict'\nresponse = requests.post(SERVER_URL, data=input_data_json)\nresponse.raise_for_status() # raise an exception in case of error\nresponse = response.json()\n\n\nThe response is a dictionary containing a single “predictions” key. The corresponding value is the list of predictions. This list is a Python list, so let’s convert it to a NumPy array and round the floats it contains to the second decimal:\n\n\nCode\ny_proba = np.array(response[\"predictions\"])\ny_proba.round(2)\n\n\narray([[0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99, 0.  , 0.  ],\n       [0.  , 0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])\n\n\nFor more information, please refer to https://github.com/tensorflow/serving which include the usuage of gRPC.\n\n\n8.2.4 Deploying a new model version\nNow let’s create a new model version and export a SavedModel to the my_mnist_model/0002 directory, just like earlier:\n\n\nCode\n# Change the architecture\n\nnp.random.seed(42)\ntf.random.set_seed(42)\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n    tf.keras.layers.Rescaling(scale=1 / 255),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n              metrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train, epochs=10,\n                    validation_data=(X_valid, y_valid))\n\n\nEpoch 1/10\n1719/1719 [==============================] - 11s 6ms/step - loss: 0.7267 - accuracy: 0.8018 - val_loss: 0.3377 - val_accuracy: 0.9046\nEpoch 2/10\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.3219 - accuracy: 0.9068 - val_loss: 0.2651 - val_accuracy: 0.9244\nEpoch 3/10\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.2697 - accuracy: 0.9216 - val_loss: 0.2269 - val_accuracy: 0.9356\nEpoch 4/10\n1719/1719 [==============================] - 12s 7ms/step - loss: 0.2374 - accuracy: 0.9313 - val_loss: 0.2069 - val_accuracy: 0.9388\nEpoch 5/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.2142 - accuracy: 0.9382 - val_loss: 0.1877 - val_accuracy: 0.9462\nEpoch 6/10\n1719/1719 [==============================] - 11s 7ms/step - loss: 0.1953 - accuracy: 0.9429 - val_loss: 0.1765 - val_accuracy: 0.9508\nEpoch 7/10\n1719/1719 [==============================] - 14s 8ms/step - loss: 0.1796 - accuracy: 0.9475 - val_loss: 0.1634 - val_accuracy: 0.9540\nEpoch 8/10\n1719/1719 [==============================] - 9s 5ms/step - loss: 0.1662 - accuracy: 0.9517 - val_loss: 0.1581 - val_accuracy: 0.9574\nEpoch 9/10\n1719/1719 [==============================] - 7s 4ms/step - loss: 0.1547 - accuracy: 0.9551 - val_loss: 0.1477 - val_accuracy: 0.9572\nEpoch 10/10\n1719/1719 [==============================] - 10s 6ms/step - loss: 0.1449 - accuracy: 0.9581 - val_loss: 0.1399 - val_accuracy: 0.9596\n\n\n\n\nCode\nmodel_version = \"0002\"\nmodel_name = \"my_mnist_model\"\nmodel_path = os.path.join(model_name, model_version)\n\ntf.keras.models.save_model(\n    model,\n    model_path,\n    overwrite=True,\n    include_optimizer=True,\n    save_format=\"tf\",\n    signatures=None,\n    options=None\n)\n\n\n\n\nCode\nfor root, dirs, files in os.walk(model_name):\n    indent = '    ' * root.count(os.sep)\n    print('{}{}/'.format(indent, os.path.basename(root)))\n    for filename in files:\n        print('{}{}'.format(indent + '    ', filename))\n\n\nmy_mnist_model/\n    0001/\n        keras_metadata.pb\n        saved_model.pb\n        fingerprint.pb\n        assets/\n        variables/\n            variables.index\n            variables.data-00000-of-00001\n    0002/\n        keras_metadata.pb\n        saved_model.pb\n        fingerprint.pb\n        assets/\n        variables/\n            variables.index\n            variables.data-00000-of-00001\n\n\nAt regular intervals (the delay is configurable), TensorFlow Serving checks for new model versions. If it finds one, it will automatically handle the transition gracefully: by default, it will answer pending requests (if any) with the previous model version, while handling new requests with the new version. As soon as every pending request has been answered, the previous model version is unloaded. You can see this at work in the TF Serving logs:\n\n\nCode\n!tail server.log\n\n\n[warn] getaddrinfo: address family for nodename not supported\n[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n\n\n\n\nCode\nSERVER_URL = 'http://localhost:8050/v1/models/my_mnist_model:predict'\n            \nresponse = requests.post(SERVER_URL, data=input_data_json)\nresponse.raise_for_status()\nresponse = response.json()\n\n\n\n\nCode\ny_proba = np.array(response[\"predictions\"])\ny_proba.round(2)\n\n\narray([[0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99, 0.  , 0.  ],\n       [0.  , 0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.98, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])\n\n\n\n\nCode\n!pgrep tensorflow\n\n\n12064\n\n\n\n\nCode\n!kill 12064\n\n\nAs you can see, TF Serving makes it quite simple to deploy new models. Moreover, if you discover that version 2 does not work as well as you expected, then rolling back to version 1 is as simple as removing the my_mnist_model/0002 directory.\nYou can also refer to https://github.com/microsoft/ML-For-Beginners/blob/main/3-Web-App/1-Web-App/README.md or https://github.com/rodrigo-arenas/fast-ml-deploy which use Flask and FastAPI that may have more flexibility.\nIf you would like to deploy to GCP vertex AI, checkout here."
  },
  {
    "objectID": "07_Deploy.html#deploy-a-rest-api-server-using-bentoml",
    "href": "07_Deploy.html#deploy-a-rest-api-server-using-bentoml",
    "title": "8  Deploy and monitoring",
    "section": "8.3 Deploy a REST API server using BentoML",
    "text": "8.3 Deploy a REST API server using BentoML\nTo begin with BentoML, you will need to save your trained models with BentoML API in its model store (a local directory managed by BentoML). The model store is used for managing all your trained models locally as well as accessing them for serving.\n\n8.3.1 Train a classifier model using the iris dataset\n\n\nCode\n# Load training data\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Train the model\nclf = svm.SVC(gamma='scale')\nclf.fit(X_train, y_train)\n\n\nModel saved: Model(tag=\"iris_clf:ip4lyign2s6ucasc\")\n\n\n\n\nCode\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test,y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       0.83      0.83      0.83         6\n           2       0.91      0.91      0.91        11\n\n    accuracy                           0.93        30\n   macro avg       0.91      0.91      0.91        30\nweighted avg       0.93      0.93      0.93        30\n\n\n\nSave the clf model with BentoML. We begin by saving a trained model instance to BentoML’s local model store. The local model store is used to version your models as well as control which models are packaged with your bento. It is noted that there are a wide range of models can be saved via BentoML.\n\nIt is possible to use pre-trained models directly with BentoML or import existing trained model files to BentoML. Learn more about it from Preparing Models.\n\n\n\nCode\n# Save model to the BentoML local model store\nsaved_model = bentoml.sklearn.save_model(\"iris_clf\", clf)\nprint(f\"Model saved: {saved_model}\")\n\n\nModel(tag=\"iris_clf:wewrqnwn2s6ucasc\", path=\"/root/bentoml/models/iris_clf/wewrqnwn2s6ucasc/\")\n\n\nModels saved can be accessed via bentoml models CLI command:\n\n\nCode\n!bentoml models list\n\n\n Tag                        Module           Size      Creation Time       \n iris_clf:wewrqnwn2s6ucasc  bentoml.sklearn  5.39 KiB  2023-03-29 01:54:50 \n iris_clf:ip4lyign2s6ucasc  bentoml.sklearn  5.39 KiB  2023-03-29 01:51:47 \n\n\nTo verify that the saved model can be loaded correctly:\n\n\nCode\nloaded_model = bentoml.sklearn.load_model(\"iris_clf:latest\")\n# model = bentoml.sklearn.load_model(\"iris_clf:wewrqnwn2s6ucasc\") #we can instead load specific version of model\nloaded_model.predict([[5.9, 3.0, 5.1, 1.8]])\n\n\narray([2])\n\n\nIn BentoML, the recommended way of running ML model inference in serving is via Runner:\n\n\nCode\n# Create a Runner instance:\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n\n# Runner#init_local initializes the model in current process, this is meant for development and testing only:\niris_clf_runner.init_local()\n\n# This should yield the same result as the loaded model:\niris_clf_runner.predict.run([[5.9, 3.0, 5.1, 1.8]])\n\n\nWARNING:bentoml._internal.runner.runner:'Runner.init_local' is for debugging and testing only. Make sure to remove it before deploying to production.\n\n\narray([2])\n\n\nIn this example, bentoml.sklearn.get() creates a reference to the saved model in the model store, and to_runner() creates a Runner instance from the model. The Runner abstraction gives BentoServer more flexibility in terms of how to schedule the inference computation, how to dynamically batch inference calls and better take advantage of all hardware resource available.\n\n\n8.3.2 Create a BentoML Service for serving the model\nServices are the core components of BentoML, where the serving logic is defined. With the model saved in the model store, we can define the service by creating a Python file service.py with the following contents:\n\n\nCode\n%%writefile service.py\nimport numpy as np\nimport bentoml\nfrom bentoml.io import NumpyNdarray\n\n# Load the runner for the latest ScikitLearn model we just saved\niris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n\n# Create the iris_classifier service with the ScikitLearn runner\n# Multiple runners may be specified if needed in the runners array\n# When packaged as a bento, the runners here will included\nsvc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n\n# Create API function with pre- and post- processing logic with your new \"svc\" annotation\n@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\ndef classify(input_series: np.ndarray) -&gt; np.ndarray:\n    # Define pre-processing logic\n    result = iris_clf_runner.predict.run(input_series)\n    # Define post-processing logic\n    return result\n\n\nWriting service.py\n\n\nIn this example, we defined the input and output type to be numpy.ndarray. More options, such as pandas.DataFrame, JSON and PIL.image are also supported. The svc.api decorator adds a function to the bentoml.Service object’s APIs list. The input and output parameter takes an IO Descriptor object, which specifies the API function’s expected input/output types, and is used for generating HTTP endpoints. Inside the API function, users can define any business logic, feature fetching, and feature transformation code. Model inference calls are made directly through runner objects, that are passed into bentoml.Service(name=.., runners=[..]) call when creating the service object.\n\nBentoML Server runs the Service API in an ASGI web serving layer and puts Runners in a separate worker process pool managed by BentoML. The ASGI web serving layer will expose REST endpoints for inference APIs, such as POST /predict and common infrastructure APIs, such as GET /metrics for monitoring. You can use other ASGI app like FastAPI or WSGI app like Flask, see here.\n\nWe now have everything we need to serve our first request. Launch the server in debug mode by running the bentoml serve command in the current working directory. Using the --reload option allows the server to reflect any changes made to the service.py module without restarting:\n\n\nCode\n!nohup bentoml serve ./service.py:svc --reload --port 8050 &\n\n\nnohup: appending output to 'nohup.out'\n\n\nWe can then send requests to the newly started service with any HTTP client:\n\n\nCode\nrequests.post(\n    \"http://127.0.0.1:8050/classify\",\n    headers={\"content-type\": \"application/json\"},\n    data=\"[[5.9, 3, 5.1, 1.8]]\"\n    ).text\n\n\n'[2]'\n\n\n\n\nCode\n!pgrep bentoml\n\n\n7124\n\n\n\n\nCode\n!kill 7124\n\n\n\n\n8.3.3 Build and Deploy Bentos 🍱\nOnce we are happy with the service definition, we can build the model and service into a bento. Bento is the distribution format for a service. It is a self-contained archive that contains all the source code, model files and dependency specifications required to run the service. Checkout Building Bentos for more details.\nTo build a Bento, first create a file named bentofile.yaml in your project directory:\n\n\nCode\n%%writefile bentofile.yaml\nservice: \"service.py:svc\"  # A convention for locating your service: &lt;YOUR_SERVICE_PY&gt;:&lt;YOUR_SERVICE_ANNOTATION&gt;\ndescription: \"file: ./README.md\"\nlabels:\n    owner: nsysu-math608\n    stage: demo\ninclude:\n - \"*.py\"  # A pattern for matching which files to include in the bento\npython:\n  packages:\n   - scikit-learn  # Additional libraries to be included in the bento\n   - pandas\n  lock_packages: False\n\n\nOverwriting bentofile.yaml\n\n\n\n\nCode\n%%writefile README.md\nThis is a iris classifier build in math608\n\n\nOverwriting README.md\n\n\nNext, use the bentoml build CLI command in the same directory to build a bento.\n\n\nCode\n!bentoml build\n\n\nBuilding BentoML service \"iris_classifier:mgpj7bgn3c4jyasc\" from build context \"/content\".\nPacking model \"iris_clf:wewrqnwn2s6ucasc\"\n\n██████╗░███████╗███╗░░██╗████████╗░█████╗░███╗░░░███╗██╗░░░░░\n██╔══██╗██╔════╝████╗░██║╚══██╔══╝██╔══██╗████╗░████║██║░░░░░\n██████╦╝█████╗░░██╔██╗██║░░░██║░░░██║░░██║██╔████╔██║██║░░░░░\n██╔══██╗██╔══╝░░██║╚████║░░░██║░░░██║░░██║██║╚██╔╝██║██║░░░░░\n██████╦╝███████╗██║░╚███║░░░██║░░░╚█████╔╝██║░╚═╝░██║███████╗\n╚═════╝░╚══════╝╚═╝░░╚══╝░░░╚═╝░░░░╚════╝░╚═╝░░░░░╚═╝╚══════╝\n\nSuccessfully built Bento(tag=\"iris_classifier:mgpj7bgn3c4jyasc\").\n\nPossible next steps:\n\n * Containerize your Bento with `bentoml containerize`:\n    $ bentoml containerize iris_classifier:mgpj7bgn3c4jyasc\n\n * Push to BentoCloud with `bentoml push`:\n    $ bentoml push iris_classifier:mgpj7bgn3c4jyasc\n\n\nBentos built will be saved in the local bento store, which you can view using the bentoml list CLI command.\n\n\nCode\n!bentoml list\n\n\n Tag                     Size       Creation Time        Path                   \n iris_classifier:mgpj7…  18.38 KiB  2023-03-29 02:21:15  ~/bentoml/bentos/iris… \n iris_classifier:7bb5l…  18.38 KiB  2023-03-29 02:18:19  ~/bentoml/bentos/iris… \n iris_classifier:m56it…  18.38 KiB  2023-03-29 02:14:16  ~/bentoml/bentos/iris… \n\n\nWe can serve bentos from the bento store using the bentoml serve --production CLI command. Using the --production option will serve the bento in production mode.\n\n\nCode\n%%bash --bg\nnohup bentoml serve iris_classifier:latest \\\n     --production \\\n     --port 8050 &gt; bentoml.log 2&gt;&1\n\n\nThis is another way to query the server\n\n\nCode\n!curl \\\n  -X POST \\\n  -H \"content-type: application/json\" \\\n  --data \"[[5.9,3,5.1,1.8]]\" \\\n  http://127.0.0.1:8050/classify\n\n\n[2]\n\n\nThe Bento directory contains all code, files, models and configs required for running this service. BentoML standarlizes this file structure which enables serving runtimes and deployment tools to be built on top of it. By default, Bentos are managed under the ~/bentoml/bentos directory:\n\n\nCode\npath =\"/root/bentoml/bentos/iris_classifier/\"\n\n\n\n\nCode\nfor root, dirs, files in os.walk(path):\n    indent = ' ' * root.count(os.sep)\n    print('{}{}/'.format(indent, os.path.basename(root)))\n    for filename in files:\n        print('{}{}'.format(indent + ' ', filename))\n\n\n     /\n      latest\n     7bb5lgwn26xiyasc/\n      README.md\n      bento.yaml\n      models/\n       iris_clf/\n        latest\n        wewrqnwn2s6ucasc/\n         model.yaml\n         saved_model.pkl\n      apis/\n       openapi.yaml\n      env/\n       docker/\n        entrypoint.sh\n        Dockerfile\n       python/\n        version.txt\n        install.sh\n        requirements.txt\n      src/\n       service.py\n     mgpj7bgn3c4jyasc/\n      README.md\n      bento.yaml\n      models/\n       iris_clf/\n        latest\n        wewrqnwn2s6ucasc/\n         model.yaml\n         saved_model.pkl\n      apis/\n       openapi.yaml\n      env/\n       docker/\n        entrypoint.sh\n        Dockerfile\n       python/\n        version.txt\n        install.sh\n        requirements.txt\n      src/\n       service.py\n       __pycache__/\n        service.cpython-39.pyc\n     m56it3wn26e2gasc/\n      README.md\n      bento.yaml\n      models/\n       iris_clf/\n        latest\n        wewrqnwn2s6ucasc/\n         model.yaml\n         saved_model.pkl\n      apis/\n       openapi.yaml\n      env/\n       docker/\n        entrypoint.sh\n        Dockerfile\n       python/\n        version.txt\n        install.sh\n        requirements.txt\n      src/\n       service.py\n\n\n\n\nCode\n!pgrep bentoml\n\n\n12003\n\n\n\n\nCode\n!kill 12003\n\n\nFor more information, please refer to https://docs.bentoml.org/en/latest/index.html"
  },
  {
    "objectID": "07_Deploy.html#deploy-web-base-application-using-streamit",
    "href": "07_Deploy.html#deploy-web-base-application-using-streamit",
    "title": "8  Deploy and monitoring",
    "section": "8.4 Deploy web base application using streamit",
    "text": "8.4 Deploy web base application using streamit\nStreamlit’s simple and focused API lets you build incredibly rich and powerful tools. It contains a large number of elements and components that you can use.\nThere are a few ways to display data (tables, arrays, data frames) in Streamlit apps. Below, st.write() can be used to write anything from text, plots to tables. In addition, when you’ve got the data or model into the state that you want to explore, you can add in widgets like st.slider(), st.button() or st.selectbox(). Finally, Streamlit makes it easy to organize your widgets in a left panel sidebar with st.sidebar. Each element that’s passed to st.sidebar is pinned to the left, allowing users to focus on the content in your app while still having access to UI controls. For example, if you want to add a selectbox and a slider to a sidebar, use st.sidebar.slider and st.sidebar.selectbox instead of st.slider and st.selectbox:\n\n\nCode\n%%writefile iris-app.py\nimport streamlit as st\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestClassifier\n\nst.write(\"\"\"\n# Simple Iris Flower Prediction App\n\nThis app predicts the **Iris flower** type!\n\"\"\")\n\nst.sidebar.header('User Input Parameters')\n\ndef user_input_features():\n    sepal_length = st.sidebar.slider('Sepal length', 4.3, 7.9, 5.4)\n    sepal_width = st.sidebar.slider('Sepal width', 2.0, 4.4, 3.4)\n    petal_length = st.sidebar.slider('Petal length', 1.0, 6.9, 1.3)\n    petal_width = st.sidebar.slider('Petal width', 0.1, 2.5, 0.2)\n    data = {'sepal_length': sepal_length,\n            'sepal_width': sepal_width,\n            'petal_length': petal_length,\n            'petal_width': petal_width}\n    features = pd.DataFrame(data, index=[0])\n    return features\n\ndf = user_input_features()\n\nst.subheader('User Input parameters')\nst.write(df)\n\niris = datasets.load_iris()\nX = iris.data\nY = iris.target\n\nclf = RandomForestClassifier()\nclf.fit(X, Y)\n\nprediction = clf.predict(df)\nprediction_proba = clf.predict_proba(df)\n\nst.subheader('Class labels and their corresponding index number')\nst.write(iris.target_names)\n\nst.subheader('Prediction')\nst.write(iris.target_names[prediction])\n\nst.subheader('Prediction Probability')\nst.write(prediction_proba)\n\n\nWriting iris-app.py\n\n\n\n\nCode\n%%bash --bg \nstreamlit run iris-ml-app.py --server.port 8050 &gt; debug.log 2&gt;&1\n\n\nAs soon as you run the script as shown above, a local Streamlit server will spin up and your app will open in a new tab in your default web browser. The app is your canvas, where you’ll draw charts, text, widgets, tables, and more.\n\n\nCode\n!tail debug.log\n\n\n\n\nCode\npublic_url\n\n\n&lt;NgrokTunnel: \"http://faf8-34-147-81-41.ngrok.io\" -&gt; \"http://localhost:8050\"&gt;\n\n\nTry to click the above link to access the web app. For more information, please refer to https://github.com/streamlit/streamlit\n\n\nCode\n!pgrep streamlit\n\n\n16598\n\n\n\n\nCode\n!kill 16598"
  },
  {
    "objectID": "07_Deploy.html#deploy-web-base-application-using-gradio",
    "href": "07_Deploy.html#deploy-web-base-application-using-gradio",
    "title": "8  Deploy and monitoring",
    "section": "8.5 Deploy web base application using Gradio",
    "text": "8.5 Deploy web base application using Gradio\nUI models are perfect to use with Gradio’s image input component, so in this section we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this.\n\n8.5.1 Setting up the Image Classification Model\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from Keras. You can use a different pretrained model or train your own.\n\n\nCode\n!wget https://hf.space/embed/abidlabs/keras-image-classifier/file/banana.jpg\n!wget https://hf.space/embed/abidlabs/keras-image-classifier/file/car.jpg\n\n\n--2023-03-29 02:57:18--  https://hf.space/embed/abidlabs/keras-image-classifier/file/banana.jpg\nResolving hf.space (hf.space)... 54.81.27.42, 54.81.158.24, 54.91.138.213, ...\nConnecting to hf.space (hf.space)|54.81.27.42|:443... connected.\nHTTP request sent, awaiting response... 307 Temporary Redirect\nLocation: https://abidlabs-keras-image-classifier.hf.space/file/banana.jpg [following]\n--2023-03-29 02:57:18--  https://abidlabs-keras-image-classifier.hf.space/file/banana.jpg\nResolving abidlabs-keras-image-classifier.hf.space (abidlabs-keras-image-classifier.hf.space)... 18.211.254.225, 54.156.168.251, 34.195.4.197\nConnecting to abidlabs-keras-image-classifier.hf.space (abidlabs-keras-image-classifier.hf.space)|18.211.254.225|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 28437 (28K) [image/jpeg]\nSaving to: ‘banana.jpg’\n\nbanana.jpg          100%[===================&gt;]  27.77K  --.-KB/s    in 0.08s   \n\n2023-03-29 02:57:19 (327 KB/s) - ‘banana.jpg’ saved [28437/28437]\n\n--2023-03-29 02:57:19--  https://hf.space/embed/abidlabs/keras-image-classifier/file/car.jpg\nResolving hf.space (hf.space)... 54.81.27.42, 54.81.158.24, 54.91.138.213, ...\nConnecting to hf.space (hf.space)|54.81.27.42|:443... connected.\nHTTP request sent, awaiting response... 307 Temporary Redirect\nLocation: https://abidlabs-keras-image-classifier.hf.space/file/car.jpg [following]\n--2023-03-29 02:57:19--  https://abidlabs-keras-image-classifier.hf.space/file/car.jpg\nResolving abidlabs-keras-image-classifier.hf.space (abidlabs-keras-image-classifier.hf.space)... 18.211.254.225, 54.156.168.251, 34.195.4.197\nConnecting to abidlabs-keras-image-classifier.hf.space (abidlabs-keras-image-classifier.hf.space)|18.211.254.225|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 79626 (78K) [image/jpeg]\nSaving to: ‘car.jpg’\n\ncar.jpg             100%[===================&gt;]  77.76K   454KB/s    in 0.2s    \n\n2023-03-29 02:57:20 (454 KB/s) - ‘car.jpg’ saved [79626/79626]\n\n\n\n\n\nCode\ninception_net = tf.keras.applications.MobileNetV2()\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n14536120/14536120 [==============================] - 1s 0us/step\n\n\n\n\n8.5.2 Defining a predict function\nNext, we will need to define a function that takes in the user input, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this text file.\nIn the case of our pretrained model, it will look like this:\n\n\nCode\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef classify_image(inp):\n    inp = inp.reshape((-1, 224, 224, 3))\n    inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n    prediction = inception_net.predict(inp).flatten()\n    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n    return confidences\n\n\nLet’s break this down. The function takes one parameter: * inp: the input image as a NumPy array\nThen, the function adds a batch dimension, passes it through the model, and returns: * confidences: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n\n8.5.3 Creating a Gradio Interface\nNow that we have our predictive function set up, we can create a Gradio Interface around it. In this case, the input component is a drag-and-drop image component. To create this input, we can use the gradio.inputs.Image class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.\nThe output component will be a “label”, which displays the top labels in a nice form. Since we don’t want to show all 1,000 class labels, we will customize it to show only the top 3 images.\nFinally, we’ll add one more parameter, the examples, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n\nCode\ngr.Interface(fn=classify_image, \n             inputs=gr.Image(shape=(224, 224), label=\"Input image\"),\n             outputs=gr.Label(num_top_classes=3, label=\"Predition Probabilities\"),\n             examples=[\"banana.jpg\", \"car.jpg\"],\n             description=\"Please upload an image\",\n             title=\"Classification using MobileNet\",\n             ).launch(server_port=8050)\n\n\nColab notebook detected. To show errors in colab notebook, set debug=True in launch()\nNote: opening Chrome Inspector may crash demo inside Colab notebooks.\n\nTo create a public link, set `share=True` in `launch()`.\n\n\n\n\n\n\n\n\nGradio automatically produces sharable links with others, but you can also access the web app with our port as follows:\n\n\nCode\npublic_url\n\n\n&lt;NgrokTunnel: \"http://faf8-34-147-81-41.ngrok.io\" -&gt; \"http://localhost:8050\"&gt;\n\n\n\n\nCode\ngr.close_all()\n\n\nClosing server running on port: 8050\n\n\nFor more information, please refer to https://github.com/gradio-app/gradio"
  },
  {
    "objectID": "07_Deploy.html#deploy-web-base-applocation-using-tensorflow.js",
    "href": "07_Deploy.html#deploy-web-base-applocation-using-tensorflow.js",
    "title": "8  Deploy and monitoring",
    "section": "8.6 Deploy web base applocation using Tensorflow.js",
    "text": "8.6 Deploy web base applocation using Tensorflow.js\nTensorflow.js is a WebGL accelerated JavaScript library for training and deploying ML models. The TensorFlow.js project includes a tensorflowjs_converter tool that can convert a TensorFlow SavedModel or a Keras model file to the TensorFlow.js Layers format: this is a directory containing a set of sharded weight files in binary format and a model.json file that describes the model’s architecture and links to the weight files. This format is optimized to be downloaded efficiently on the web.\nUsers can then download the model and run predictions in the browser using the TensorFlow.js library. Here is a code snippet to give you an idea of what the JavaScript API looks like:\nimport * as tf from '@tensorflow/tfjs';\nconst model = await tf.loadLayersModel('https://example.com/tfjs/model.json');\nconst image = tf.fromPixels(webcamElement);\nconst prediction = model.predict(image);\nFor more information, please refer to https://github.com/tensorflow/tfjs."
  },
  {
    "objectID": "07_Deploy.html#deploy-mobile-applocation-using-tensorflow-lite",
    "href": "07_Deploy.html#deploy-mobile-applocation-using-tensorflow-lite",
    "title": "8  Deploy and monitoring",
    "section": "8.7 Deploy mobile applocation using Tensorflow Lite",
    "text": "8.7 Deploy mobile applocation using Tensorflow Lite\nOnce again, doing justice to this topic would require a whole book. If you want to learn more about TensorFlow Lite, check out the O’Reilly book Practical Deep Learning for Cloud, Mobile, and Edge or refer to https://www.tensorflow.org/lite."
  },
  {
    "objectID": "07_Deploy.html#monitoring-shift-with-evidently",
    "href": "07_Deploy.html#monitoring-shift-with-evidently",
    "title": "8  Deploy and monitoring",
    "section": "8.8 Monitoring shift with evidently",
    "text": "8.8 Monitoring shift with evidently\n\n8.8.1 The task at hand: bike demand forecasting\nWe took a Kaggle dataset on Bike Sharing Demand. Our goal is to predict the volume of bike rentals on an hourly basis. To do that, we have some data about the season, weather, and day of the week.\n\n\nCode\ncontent = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\").content\nwith zipfile.ZipFile(io.BytesIO(content)) as arc:\n    raw_data = pd.read_csv(arc.open(\"hour.csv\"), header=0, sep=',', parse_dates=['dteday'], index_col='dteday')\n\n\n\n\nCode\nraw_data.index = raw_data.apply(\n    lambda row: datetime.combine(row.name, time(hour=int(row['hr']))), axis = 1)\nraw_data.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\ninstant\nseason\nyr\nmnth\nhr\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n2011-01-01 00:00:00\n1\n1\n0\n1\n0\n0\n6\n0\n1\n0.24\n0.2879\n0.81\n0.0\n3\n13\n16\n\n\n2011-01-01 01:00:00\n2\n1\n0\n1\n1\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n8\n32\n40\n\n\n2011-01-01 02:00:00\n3\n1\n0\n1\n2\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n5\n27\n32\n\n\n2011-01-01 03:00:00\n4\n1\n0\n1\n3\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n3\n10\n13\n\n\n2011-01-01 04:00:00\n5\n1\n0\n1\n4\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n0\n1\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n8.8.2 Train a model\nWe trained a random forest model using data for the four weeks from January. Let’s imagine that in practice, we just started the data collection, and that was all the data available. The performance of the trained model looked acceptable, so we decided to give it a go.\nWe further assume that we only learn the ground truth (the actual demand) at the end of each week. That is a realistic assumption in real-world machine learning. Integrating and updating different data sources is not always straightforward. Even after the actual event has occurred! Maybe the daily usage data is stored locally and is only sent and merged in the database once per week.\nTo run it, we prepare our performance data as a Pandas DataFrame. It should include: * Model application logs—features that went into the model and corresponding prediction; and * Ground truth data—the actual number of bikes rented each hour as our “target.”\nOnce we train the model, we can take our training dataset and generated predictions and specify it as the “Reference” data. We can select this period directly from the DataFrame since it has datetime as an index:\n\n\nCode\nreference = raw_data.loc['2011-01-01 00:00:00':'2011-01-28 23:00:00']\n\ntarget = 'cnt'\nprediction = 'prediction'\nnumerical_features = ['temp', 'atemp', 'hum', 'windspeed', 'hr', 'weekday']\ncategorical_features = ['season', 'holiday', 'workingday']\n\n\n\n\nCode\nreference.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\ninstant\nseason\nyr\nmnth\nhr\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n2011-01-01 00:00:00\n1\n1\n0\n1\n0\n0\n6\n0\n1\n0.24\n0.2879\n0.81\n0.0\n3\n13\n16\n\n\n2011-01-01 01:00:00\n2\n1\n0\n1\n1\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n8\n32\n40\n\n\n2011-01-01 02:00:00\n3\n1\n0\n1\n2\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n5\n27\n32\n\n\n2011-01-01 03:00:00\n4\n1\n0\n1\n3\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n3\n10\n13\n\n\n2011-01-01 04:00:00\n5\n1\n0\n1\n4\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n0\n1\n1\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nreference[numerical_features + categorical_features].shape\n\n\n(618, 9)\n\n\n\n\nCode\nregressor = ensemble.RandomForestRegressor(random_state = 0, n_estimators = 50)\n\n\n\n\nCode\nregressor.fit(reference[numerical_features + categorical_features], reference[target])\n\n\nRandomForestRegressor(n_estimators=50, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(n_estimators=50, random_state=0)\n\n\n\n\nCode\nref_prediction = regressor.predict(reference[numerical_features + categorical_features])\n\n\n\n\nCode\nreference['prediction'] = ref_prediction\n\n\nWe also map the columns to show Evidently what each column contains and perform a correct analysis:\n\n\nCode\ncolumn_mapping = ColumnMapping()\n\ncolumn_mapping.target = target\ncolumn_mapping.prediction = prediction\ncolumn_mapping.numerical_features = numerical_features\ncolumn_mapping.categorical_features = categorical_features\n\n\nBy default, Evidently uses the index as an x-axis in plots. In this case, it is datetime, so we do not need to add anything else explicitly. Otherwise, we would have to specify it in our column mapping.\nNext, we call a corresponding report for regression models.\n\n\nCode\nregression_perfomance = Report(metrics=[RegressionPreset()])\nregression_perfomance.run(current_data=reference, reference_data=None, column_mapping=column_mapping)\n\n\n\n\nCode\n# You can also specify the metrics see https://docs.evidentlyai.com/reference/all-metrics\n#the_report = Report(metrics=[\n#    RegressionQualityMetric(),\n#    RegressionErrorPlot(),\n#    RegressionErrorDistribution(),\n#    DataDriftPreset(stattest=anderson_stat_test, stattest_threshold=0.9),\n#])\n\n\nAnd display the results right in the Jupyter notebook.\n\n\nCode\nregression_perfomance.show()\n\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nWe also save it as a .html file to be able to share it easily.\n\n\nCode\n!mkdir reports\nregression_perfomance.save_html('reports/regression_performance_at_training.html')\n\n\nWe can see that the model has a fine quality given that we only trained on four weeks of data! The error is symmetric and distributed around zero. There is no obvious under- or over-estimation.\nWe will continue treating this dataset from model performance in training as our “reference.” It gives us a good feel of the quality we can expect from our model in production use. So, we can contrast the future performance against this benchmark.\n\n\n8.8.3 The first week in production\nObserving the model in production has straightforward goals. We want to detect if something goes wrong. Ideally, in advance. We also want to diagnose the root cause and get a quick understanding of how to address it. Maybe, the model degrades too fast, and we need to retrain it more often? Perhaps, the error is too high, and we need to adapt the model and rebuild it? Which new patterns are emerging?\nIn our case, we simply start by checking how well the model performs outside the training data. Our first week becomes what would have otherwise been a holdout dataset.\nFor demonstration purposes, we generated all predictions for several weeks ahead in a single batch. In reality, we would run the model sequentially as the data comes in.\nLet’s start by comparing the performance in the first week to what we have seen in training. The first 28 days are our Reference dataset; the next 7 are the Production.\n\n\nCode\ncurrent = raw_data.loc['2011-01-29 00:00:00':'2011-02-28 23:00:00']\ncurrent_prediction = regressor.predict(current[numerical_features + categorical_features])\ncurrent['prediction'] = current_prediction\n\n\n\n\nCode\nregression_perfomance = Report(metrics=[RegressionPreset()])\nregression_perfomance.run(current_data=current.loc['2011-01-29 00:00:00':'2011-02-07 23:00:00'], \n                          reference_data=reference,\n                          column_mapping=column_mapping)\n\nregression_perfomance.show()\n\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nThe error has slightly increased and is leaning towards underestimation. Let’s check if there is any statistical change in our target. To do that, we will generate the Target Drift report.\n\n\nCode\ntarget_drift = Report(metrics=[TargetDriftPreset()])\ntarget_drift.run(current_data=current.loc['2011-01-29 00:00:00':'2011-02-07 23:00:00'],\n                 reference_data=reference,\n                 column_mapping=column_mapping)\n\ntarget_drift.show()\n\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nWe can see that the distribution of the actual number of bikes rented remains sufficiently similar. To be more precise, the similarity hypothesis is not rejected. No drift is detected. The distributions of our predictions did not change much either.\nDespite this, a rational decision is to update your model by including the new week’s data. This way, the model can continue to learn, and we can probably improve the error. For the sake of demonstration, we’ll stick to see how fast things go really wrong.\n\n\n8.8.4 The second week: failing to keep up\nOnce again, we benchmark our new week against the reference dataset.\n\n\nCode\nregression_perfomance = Report(metrics=[RegressionPreset()])\nregression_perfomance.run(current_data=current.loc['2011-02-07 00:00:00':'2011-02-14 23:00:00'], \n                          reference_data=reference,\n                          column_mapping=column_mapping)\n\nregression_perfomance.show()\n\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nAt first glance, the model performance in the second week does not differ much. MAE remains almost the same. But, the skew towards under-estimation continues to grow. It seems that the error is not random! To know more, we move to the plots. We can see that the model catches overall daily trends just fine. So it learned something useful! But, at peak hours, the actual demand tends to be higher than predicted.\nIn the error distribution plot, we can see how it became “wider,” as we have more predictions with a high error. The shift to the left is visible, too. In some extreme instances, we have errors between 80 and 40 bikes that were unseen previously.\nLet’s check our target as well.\n\n\nCode\ntarget_drift = Report(metrics=[TargetDriftPreset()])\ntarget_drift.run(current_data=current.loc['2011-02-07 00:00:00':'2011-02-14 23:00:00'],\n                 reference_data=reference,\n                 column_mapping=column_mapping)\n\ntarget_drift.show()\n\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nThings are getting interesting!\nWe can see that the target distribution is now different: the similarity hypothesis is rejected. Literally, people are renting more bikes. And this is a statistically different change from our training period.\nBut, the distribution of our predictions does not keep up! That is an obvious example of model decay. Something new happens in the world, but it misses the patterns.\nIt is tempting to investigate further. Is there anything in the data that can explain this change? If there is some new signal, retraining would likely help the model to keep up. The Target Drift report has a section to help us explore the relationship between the features and the target (or model predictions). ‍When browsing through the individual features, we can inspect if we notice any new patterns. We know that predictions did not change, so we only look at the relations with the target. For example, there is a shift towards higher temperatures (measured in Celsius) with a corresponding increase in rented bikes.\nMaybe, it would pick up these patterns in retraining. But for now, we simply move on to the next week without any updates.\n\n\n8.8.5 Week 3: when things go south\n\n\nCode\nregression_perfomance = Report(metrics=[RegressionPreset()])\nregression_perfomance.run(current_data=current.loc['2011-02-15 00:00:00':'2011-02-21 23:00:00'], \n                          reference_data=reference,\n                          column_mapping=column_mapping)\n\nregression_perfomance.show()\n\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nOkay, now things do look bad. On week 3, we face a major quality drop. Both absolute and percentage error grew significantly. If we look at the plots, the model predictions are visibly scattered. We also face a new data segment with high demand that the model fails to predict. But even within the known range of target value, the model now makes errors. Things did change since the training. We can see that the model does not extrapolate well. The predicted demand stays within the same known range, while actual values are peaking.\nIf we zoom in on specific days, we might suggest that the error is higher on specific (active) hours of the day. We are doing just fine from 10 pm to 6 am!\nIn our example, we particularly want to understand the segment where the model underestimates the target function. The Error Bias table gives up more details. We sort it by the \"Range%\" field. If the values of a specific feature are significantly different in the group where the model under- or over-estimates, this feature will rank high. In our case, we can see that the extreme errors are dependent on the “temp” (temperature) and “atemp” (feels-like temperature) features.\nAfter this quick analysis, we have a more specific idea about model performance and its weaknesses. The model faces new, unusually high demand. Given how it was trained, it tends to underestimate it. On top of it, these errors are not at all random. At the very least, they are related to the temperature we observe. The higher it is, the larger the underestimation. It suggests new patterns that are related to the weather that the model could not learn before. Days got warmer, and the model went rogue.\nIf we run a target drift report, we will also see a relevant change in the linear correlations between the feature and the target. Temperature and humidity stand out.\n\n\nCode\ntarget_drift = Report(metrics=[TargetDriftPreset()])\ntarget_drift.run(current_data=current.loc['2011-02-15 00:00:00':'2011-02-21 23:00:00'],\n                 reference_data=reference,\n                 column_mapping=column_mapping)\n\ntarget_drift.show()\n\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nWe should retrain as soon as possible and do this often until we learn all the patterns. If we are not comfortable with frequent retraining, we might choose an algorithm that is more suitable for time series or is better in extrapolation.\n\n\n8.8.6 Data Drift\nIn practice, once we receive the ground truth, we can indeed course-correct quickly. Had we retrained the model after week one, it would have likely ended less dramatically. But what if we do not have the ground truth available? Can we catch such decay in advance?\nIn this case, we can analyze the data drift. We do not need actuals to calculate the error. Instead, our goal is to see if the input data has changed.\nOnce again, let’s compare the first week of production to our data in training. We can, of course, look at all our features. But we can also conclude that categorical features (like “season,” “holiday” and “workingday”) are not likely to change. Let’s look at numerical features only!\nWe specify these features so that the tool applies the correct statistical test. It would be Kolmogorov-Smirnov in this case.\n\n\nCode\ncolumn_mapping = ColumnMapping()\n\ncolumn_mapping.numerical_features = numerical_features\n\n\n\n\nCode\ndata_drift = Report(metrics = [DataDriftPreset()])\ndata_drift.run(current_data = current.loc['2011-01-29 00:00:00':'2011-02-07 23:00:00'],\n               reference_data = reference,\n               column_mapping=column_mapping)\n\ndata_drift.show()\n\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\nThe data drift report compares the distributions of each feature in the two datasets. It automatically picks an appropriate statistical test or metric based on the feature type and volume. It then returns p-values or distances and visually plots the distributions. You can also adjust the drift detection method or thresholds, or pass your own.\n\nOnce we show the report, it returns an answer. We can see already during the first week there is a statistical change in feature distributions.\nLet’s zoom in on our usual suspect—temperature. The report gives us two views on how the feature distributions evolve with time. We can notice how the observed temperature becomes higher day by day. The values clearly drift out of our green corridor (one standard deviation from the mean) that we saw in training. Looking at the steady growth, we can suspect an upward trend.\nAs we checked earlier, we did not detect drift in the model predictions after week one. Given that our model is not good at extrapolating, we should not really expect it. Such prediction drift might still happen and signal about things like broken input data. Otherwise, we would observe it if we had a more sensitive model. Regardless of this, the data drift alone provides excellent early monitoring to detect the change and react to it.\nFor more information please refer to https://github.com/evidentlyai/evidently, https://github.com/SeldonIO/alibi-detect, https://github.com/great-expectations/great_expectations or https://github.com/whylabs/whylogs."
  },
  {
    "objectID": "07_Deploy.html#references",
    "href": "07_Deploy.html#references",
    "title": "8  Deploy and monitoring",
    "section": "8.9 References",
    "text": "8.9 References\n\nhttps://github.com/ageron/handson-ml2/blob/master/19_training_and_deploying_at_scale.ipynb\nhttps://github.com/bentoml/BentoML\nhttps://github.com/streamlit/streamlit\nhttps://raw.githubusercontent.com/dataprofessor/code/master/streamlit/part2/iris-ml-app.py\nhttps://gradio.app/image-classification-in-tensorflow/\nhttps://evidentlyai.com/blog/tutorial-1-model-analytics-in-production"
  }
]