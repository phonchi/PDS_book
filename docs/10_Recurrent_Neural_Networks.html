<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.306">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="phonchi">
<meta name="dcterms.date" content="2023-05-08">

<title>Practical and Innovative Analytics in Data Science - 10&nbsp; Sequence Processing with RNNs and Attention - Tensforflow</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./11_Transfer_learning.html" rel="next">
<link href="./09_Convolutional_NeuralNetworks_tensorflow.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10_Recurrent_Neural_Networks.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sequence Processing with RNNs and Attention - Tensforflow</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Practical and Innovative Analytics in Data Science</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_end_to_end_machine_learning_project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">End-to-end Machine Learning project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_Dataset.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Framing the problem and constructing the dataset</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_Relational_Database_and_data_wrangling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Relational Database and data wrangling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_Clean_feature_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Data cleaning and feature engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_Feature_selection_extraction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Feature selection and extraction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_XAI.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Explainable AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_Deploy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Deploy and monitoring</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_neural_nets_with_tensorflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Introduction to Artificial Neural Networks - Tensorflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_Convolutional_NeuralNetworks_tensorflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Image processing with Convolutional Neural Networks - Tensorflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_Recurrent_Neural_Networks.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sequence Processing with RNNs and Attention - Tensforflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_Transfer_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Transfer learning and self-supervised learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_Representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Representation learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_Hyperparameter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./NumPy_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Numpy - multidimensional data arrays for python</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Colab_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Introduction to Colab</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./kaggle-explore.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Introduction to Kaggle</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_neural_nets_with_pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Introduction to Artificial Neural Networks - Pytorch</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_Convolutional_NeuralNetworks_pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Image processing with Convolutional Neural Networks - Pytorch</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#setup" id="toc-setup" class="nav-link active" data-scroll-target="#setup"><span class="header-section-number">10.1</span> Setup</a></li>
  <li><a href="#basic-rnns-for-forecasting-times-series" id="toc-basic-rnns-for-forecasting-times-series" class="nav-link" data-scroll-target="#basic-rnns-for-forecasting-times-series"><span class="header-section-number">10.2</span> Basic RNNs for forecasting times series</a>
  <ul class="collapse">
  <li><a href="#get-the-dataset" id="toc-get-the-dataset" class="nav-link" data-scroll-target="#get-the-dataset"><span class="header-section-number">10.2.1</span> Get the Dataset</a></li>
  <li><a href="#computing-some-baselines" id="toc-computing-some-baselines" class="nav-link" data-scroll-target="#computing-some-baselines"><span class="header-section-number">10.2.2</span> Computing Some Baselines</a></li>
  <li><a href="#the-arma-model-family" id="toc-the-arma-model-family" class="nav-link" data-scroll-target="#the-arma-model-family"><span class="header-section-number">10.2.3</span> The ARMA Model Family</a></li>
  <li><a href="#preparing-the-data-for-machine-learning-models" id="toc-preparing-the-data-for-machine-learning-models" class="nav-link" data-scroll-target="#preparing-the-data-for-machine-learning-models"><span class="header-section-number">10.2.4</span> Preparing the Data for Machine Learning Models</a></li>
  <li><a href="#forecasting-using-a-linear-model" id="toc-forecasting-using-a-linear-model" class="nav-link" data-scroll-target="#forecasting-using-a-linear-model"><span class="header-section-number">10.2.5</span> Forecasting Using a Linear Model</a></li>
  <li><a href="#forecasting-using-a-simple-rnn" id="toc-forecasting-using-a-simple-rnn" class="nav-link" data-scroll-target="#forecasting-using-a-simple-rnn"><span class="header-section-number">10.2.6</span> Forecasting Using a Simple RNN</a></li>
  <li><a href="#forecasting-using-a-deep-rnn" id="toc-forecasting-using-a-deep-rnn" class="nav-link" data-scroll-target="#forecasting-using-a-deep-rnn"><span class="header-section-number">10.2.7</span> Forecasting Using a Deep RNN</a></li>
  <li><a href="#forecasting-multivariate-time-series" id="toc-forecasting-multivariate-time-series" class="nav-link" data-scroll-target="#forecasting-multivariate-time-series"><span class="header-section-number">10.2.8</span> Forecasting Multivariate Time Series</a></li>
  <li><a href="#forecasting-several-time-steps-ahead" id="toc-forecasting-several-time-steps-ahead" class="nav-link" data-scroll-target="#forecasting-several-time-steps-ahead"><span class="header-section-number">10.2.9</span> Forecasting Several Time Steps Ahead</a></li>
  <li><a href="#forecasting-using-a-sequence-to-sequence-model" id="toc-forecasting-using-a-sequence-to-sequence-model" class="nav-link" data-scroll-target="#forecasting-using-a-sequence-to-sequence-model"><span class="header-section-number">10.2.10</span> Forecasting Using a Sequence-to-Sequence Model</a></li>
  <li><a href="#deep-rnns-with-layer-norm" id="toc-deep-rnns-with-layer-norm" class="nav-link" data-scroll-target="#deep-rnns-with-layer-norm"><span class="header-section-number">10.2.11</span> Deep RNNs with Layer Norm</a></li>
  <li><a href="#lstms" id="toc-lstms" class="nav-link" data-scroll-target="#lstms"><span class="header-section-number">10.2.12</span> LSTMs</a></li>
  <li><a href="#grus" id="toc-grus" class="nav-link" data-scroll-target="#grus"><span class="header-section-number">10.2.13</span> GRUs</a></li>
  <li><a href="#using-one-dimensional-convolutional-layers-to-process-sequences" id="toc-using-one-dimensional-convolutional-layers-to-process-sequences" class="nav-link" data-scroll-target="#using-one-dimensional-convolutional-layers-to-process-sequences"><span class="header-section-number">10.2.14</span> Using One-Dimensional Convolutional Layers to Process Sequences</a></li>
  </ul></li>
  <li><a href="#natural-language-processing" id="toc-natural-language-processing" class="nav-link" data-scroll-target="#natural-language-processing"><span class="header-section-number">10.3</span> Natural-language processing</a>
  <ul class="collapse">
  <li><a href="#preparing-text-data" id="toc-preparing-text-data" class="nav-link" data-scroll-target="#preparing-text-data"><span class="header-section-number">10.3.1</span> Preparing text data</a></li>
  <li><a href="#two-approaches-for-representing-groups-of-words-sets-and-sequences" id="toc-two-approaches-for-representing-groups-of-words-sets-and-sequences" class="nav-link" data-scroll-target="#two-approaches-for-representing-groups-of-words-sets-and-sequences"><span class="header-section-number">10.3.2</span> Two approaches for representing groups of words: Sets and sequences</a></li>
  <li><a href="#processing-words-as-a-sequence-the-sequence-model-approach" id="toc-processing-words-as-a-sequence-the-sequence-model-approach" class="nav-link" data-scroll-target="#processing-words-as-a-sequence-the-sequence-model-approach"><span class="header-section-number">10.3.3</span> Processing words as a sequence: The sequence model approach</a></li>
  <li><a href="#the-transformer-encoder-optional" id="toc-the-transformer-encoder-optional" class="nav-link" data-scroll-target="#the-transformer-encoder-optional"><span class="header-section-number">10.3.4</span> The Transformer encoder (Optional)</a></li>
  </ul></li>
  <li><a href="#hugging-faces-transformers-library" id="toc-hugging-faces-transformers-library" class="nav-link" data-scroll-target="#hugging-faces-transformers-library"><span class="header-section-number">10.4</span> Hugging Face’s <code>Transformers</code> Library</a>
  <ul class="collapse">
  <li><a href="#deal-with-imdb-optional" id="toc-deal-with-imdb-optional" class="nav-link" data-scroll-target="#deal-with-imdb-optional"><span class="header-section-number">10.4.1</span> Deal with IMDB (Optional)</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">10.5</span> References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sequence Processing with RNNs and Attention - Tensforflow</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>phonchi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 8, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>


<table align="left">
<tbody><tr><td>
<a href="https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/10_Recurrent Neural Networks.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>
</td>
<td>
<a target="_blank" href="https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/10_Recurrent Neural Networks.ipynb"><img src="https://kaggle.com/static/images/open-in-kaggle.svg"></a>
</td>

</tr></tbody></table>
<p><br></p>
<section id="setup" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="setup"><span class="header-section-number">10.1</span> Setup</h2>
<p>First, let’s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:48096,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683431203044,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="8d689d36-b146-48e9-b5b3-384f4b14fb82" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> sys</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="cf">if</span> <span class="st">"google.colab"</span> <span class="kw">in</span> sys.modules <span class="kw">or</span> <span class="st">"kaggle_secrets"</span> <span class="kw">in</span> sys.modules:</span>
<span id="cb1-3"><a href="#cb1-3"></a>    <span class="op">%</span>pip install <span class="op">-</span>q <span class="op">-</span>U transformers</span>
<span id="cb1-4"><a href="#cb1-4"></a>    <span class="op">%</span>pip install <span class="op">-</span>q <span class="op">-</span>U datasets</span>
<span id="cb1-5"><a href="#cb1-5"></a>    <span class="op">%</span>pip install <span class="op">-</span>q <span class="op">-</span>U evaluate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 37.9 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 19.1 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 83.6 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 474.6/474.6 kB 8.9 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 21.6 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.3/134.3 kB 14.9 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 38.2 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 7.9 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 10.2 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 20.0 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 6.7 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 3.4 MB/s eta 0:00:00</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:14128,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683431217169,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Python ≥3.7 is recommended</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="cf">assert</span> sys.version_info <span class="op">&gt;=</span> (<span class="dv">3</span>, <span class="dv">7</span>)</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">import</span> os</span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="im">import</span> shutil</span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="im">import</span> random</span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="im">from</span> time <span class="im">import</span> strftime</span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="im">import</span> string</span>
<span id="cb3-9"><a href="#cb3-9"></a></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="co"># Scikit-Learn ≥1.01 is recommended</span></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="im">from</span> packaging <span class="im">import</span> version</span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="im">import</span> sklearn</span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="cf">assert</span> version.parse(sklearn.__version__) <span class="op">&gt;=</span> version.parse(<span class="st">"1.0.1"</span>)</span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="im">from</span> statsmodels.tsa.arima.model <span class="im">import</span> ARIMA</span>
<span id="cb3-15"><a href="#cb3-15"></a><span class="im">from</span> statsmodels.graphics.tsaplots <span class="im">import</span> plot_acf, plot_pacf</span>
<span id="cb3-16"><a href="#cb3-16"></a></span>
<span id="cb3-17"><a href="#cb3-17"></a><span class="co"># Tensorflow ≥2.8.0 is recommended</span></span>
<span id="cb3-18"><a href="#cb3-18"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb3-19"><a href="#cb3-19"></a><span class="cf">assert</span> version.parse(tf.__version__) <span class="op">&gt;=</span> version.parse(<span class="st">"2.8.0"</span>)</span>
<span id="cb3-20"><a href="#cb3-20"></a></span>
<span id="cb3-21"><a href="#cb3-21"></a><span class="co"># Huggingface transformer</span></span>
<span id="cb3-22"><a href="#cb3-22"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb3-23"><a href="#cb3-23"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, TFAutoModelForSequenceClassification, AutoModelForSequenceClassification</span>
<span id="cb3-24"><a href="#cb3-24"></a><span class="im">from</span> transformers <span class="im">import</span> TrainingArguments, Trainer</span>
<span id="cb3-25"><a href="#cb3-25"></a><span class="im">from</span> transformers <span class="im">import</span> DataCollatorWithPadding</span>
<span id="cb3-26"><a href="#cb3-26"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb3-27"><a href="#cb3-27"></a><span class="im">import</span> evaluate</span>
<span id="cb3-28"><a href="#cb3-28"></a></span>
<span id="cb3-29"><a href="#cb3-29"></a><span class="co"># Common imports</span></span>
<span id="cb3-30"><a href="#cb3-30"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-31"><a href="#cb3-31"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-32"><a href="#cb3-32"></a></span>
<span id="cb3-33"><a href="#cb3-33"></a><span class="co"># To plot pretty figures</span></span>
<span id="cb3-34"><a href="#cb3-34"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb3-35"><a href="#cb3-35"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb3-36"><a href="#cb3-36"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-37"><a href="#cb3-37"></a>mpl.rc(<span class="st">'axes'</span>, labelsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb3-38"><a href="#cb3-38"></a>mpl.rc(<span class="st">'xtick'</span>, labelsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-39"><a href="#cb3-39"></a>mpl.rc(<span class="st">'ytick'</span>, labelsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-40"><a href="#cb3-40"></a></span>
<span id="cb3-41"><a href="#cb3-41"></a><span class="co"># to make this notebook's output stable across runs</span></span>
<span id="cb3-42"><a href="#cb3-42"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb3-43"><a href="#cb3-43"></a>tf.random.set_seed(<span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683431217170,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="cf">if</span> <span class="kw">not</span> tf.config.list_physical_devices(<span class="st">'GPU'</span>):</span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="bu">print</span>(<span class="st">"No GPU was detected. Neural nets can be very slow without a GPU."</span>)</span>
<span id="cb4-3"><a href="#cb4-3"></a>    <span class="cf">if</span> <span class="st">"google.colab"</span> <span class="kw">in</span> sys.modules:</span>
<span id="cb4-4"><a href="#cb4-4"></a>        <span class="bu">print</span>(<span class="st">"Go to Runtime &gt; Change runtime and select a GPU hardware "</span></span>
<span id="cb4-5"><a href="#cb4-5"></a>              <span class="st">"accelerator."</span>)</span>
<span id="cb4-6"><a href="#cb4-6"></a>    <span class="cf">if</span> <span class="st">"kaggle_secrets"</span> <span class="kw">in</span> sys.modules:</span>
<span id="cb4-7"><a href="#cb4-7"></a>        <span class="bu">print</span>(<span class="st">"Go to Settings &gt; Accelerator and select GPU."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="basic-rnns-for-forecasting-times-series" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="basic-rnns-for-forecasting-times-series"><span class="header-section-number">10.2</span> Basic RNNs for forecasting times series</h2>
<section id="get-the-dataset" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="get-the-dataset"><span class="header-section-number">10.2.1</span> Get the Dataset</h3>
<p>Let’s pretend you’ve just been hired as a data scientist by Chicago’s Transit Authority. Your first task is to build a model capable of forecasting the number of passengers that will ride on bus and rail the next day. You have access to daily ridership data since 2001. Let’s walk through together how you would handle this. Let’s download the organized ridership data from the ageron/data project. It originally comes from Chicago’s Transit Authority, and was downloaded from the <a href="https://data.cityofchicago.org/Transportation/CTA-Ridership-Daily-Boarding-Totals/6iiy-9s97">Chicago’s Data Portal</a>:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1450,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683347055774,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="46466ae8-e08d-4d5f-bf00-e26827481b83">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>tf.keras.utils.get_file(</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="st">"ridership.tgz"</span>,</span>
<span id="cb5-3"><a href="#cb5-3"></a>    <span class="st">"https://github.com/ageron/data/raw/main/ridership.tgz"</span>,</span>
<span id="cb5-4"><a href="#cb5-4"></a>    cache_dir<span class="op">=</span><span class="st">"."</span>,</span>
<span id="cb5-5"><a href="#cb5-5"></a>    extract<span class="op">=</span><span class="va">True</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading data from https://github.com/ageron/data/raw/main/ridership.tgz
108512/108512 [==============================] - 0s 0us/step</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>'./datasets/ridership.tgz'</code></pre>
</div>
</div>
<p>We’ll start by loading and cleaning up the data:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>path <span class="op">=</span> Path(<span class="st">"datasets/ridership/CTA_-_Ridership_-_Daily_Boarding_Totals.csv"</span>)</span>
<span id="cb8-2"><a href="#cb8-2"></a>df <span class="op">=</span> pd.read_csv(path, parse_dates<span class="op">=</span>[<span class="st">"service_date"</span>])</span>
<span id="cb8-3"><a href="#cb8-3"></a>df.columns <span class="op">=</span> [<span class="st">"date"</span>, <span class="st">"day_type"</span>, <span class="st">"bus"</span>, <span class="st">"rail"</span>, <span class="st">"total"</span>]  <span class="co"># gives columns with shorter names</span></span>
<span id="cb8-4"><a href="#cb8-4"></a>df <span class="op">=</span> df.sort_values(<span class="st">"date"</span>).set_index(<span class="st">"date"</span>)</span>
<span id="cb8-5"><a href="#cb8-5"></a>df <span class="op">=</span> df.drop(<span class="st">"total"</span>, axis<span class="op">=</span><span class="dv">1</span>)  <span class="co"># no need for total, it's just bus + rail which may be non-informative</span></span>
<span id="cb8-6"><a href="#cb8-6"></a>df <span class="op">=</span> df.drop_duplicates()  <span class="co"># remove duplicated months (2011-10 and 2014-07)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We load the CSV file, set short column names, sort the rows by date, remove the redundant total column, and drop duplicate rows. Now let’s check what the first few rows look like:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:458,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683347132105,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="f736e983-42ea-4d00-9294-b59affbf82c3">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">

  <div id="df-61970bdd-0ce6-4054-b361-251b0a66c3e1">
    <div class="colab-df-container">
      <div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">day_type</th>
<th data-quarto-table-cell-role="th">bus</th>
<th data-quarto-table-cell-role="th">rail</th>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">date</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">2001-01-01</td>
<td>U</td>
<td>297192</td>
<td>126455</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">2001-01-02</td>
<td>W</td>
<td>780827</td>
<td>501952</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2001-01-03</td>
<td>W</td>
<td>824923</td>
<td>536432</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">2001-01-04</td>
<td>W</td>
<td>870021</td>
<td>550011</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2001-01-05</td>
<td>W</td>
<td>890426</td>
<td>557917</td>
</tr>
</tbody>
</table>


</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-61970bdd-0ce6-4054-b361-251b0a66c3e1')" title="Convert this dataframe to an interactive table." style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"></path>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"></path><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"></path>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-61970bdd-0ce6-4054-b361-251b0a66c3e1 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-61970bdd-0ce6-4054-b361-251b0a66c3e1');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<p>On January 1st, 2001, 297,192 people boarded a bus in Chicago, and 126,455 boarded a train. The <code>day_type</code> column contains <strong>W</strong> for Weekdays, <strong>A</strong> for Saturdays, and <strong>U</strong> for Sundays or holidays. Now let’s plot the bus and rail ridership figures over a few months in 2019, to see what it looks like:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:4,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683347428665,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="d94a4032-74b3-42f8-822f-e0d5390286b3">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>df[<span class="st">"2019-03"</span>:<span class="st">"2019-05"</span>].plot(grid<span class="op">=</span><span class="va">True</span>, marker<span class="op">=</span><span class="st">"."</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.5</span>))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="10_Recurrent_Neural_Networks_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This is a time series: data with values at different time steps, usually at regular intervals. More specifically, since there are multiple values per time step, this is called a <strong>multivariate time series</strong>. If we only looked at the bus column, it would be a <strong>univariate time series</strong>, with a single value per time step. Typical tasks are:</p>
<ol type="1">
<li><p>Predicting future values (i.e., forecasting) is the most typical task when dealing with time series, and this is what we will focus on.</p></li>
<li><p>Other tasks include imputation (filling in missing past values), classification, anomaly detection, and more.</p></li>
</ol>
</section>
<section id="computing-some-baselines" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="computing-some-baselines"><span class="header-section-number">10.2.2</span> Computing Some Baselines</h3>
<p>We can see that a similar pattern is clearly repeated every week. This is called a weekly <em>seasonality</em>. In fact, it’s so strong in this case that forecasting tomorrow’s ridership by just copying the values from a week earlier will yield reasonably good results. This is called <strong>naive forecasting</strong>: simply copying a past value to make our forecast. Naive forecasting is often a great baseline, and it can even be tricky to beat in some cases.</p>
<p>To visualize these naive forecasts, let’s overlay the two time series (for bus and rail) as well as the same time series lagged by one week (i.e., shifted toward the right) using dotted lines. We’ll also plot the difference between the two (i.e., the value at time <span class="math inline">\(t\)</span> minus the value at time <span class="math inline">\(t–7\)</span>); this is called <em>differencing</em>:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1194,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683347440314,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="a1d5b121-5c35-459d-d0db-d6a7bf8d205b">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>diff_7 <span class="op">=</span> df[[<span class="st">"bus"</span>, <span class="st">"rail"</span>]].diff(<span class="dv">7</span>)[<span class="st">"2019-03"</span>:<span class="st">"2019-05"</span>]</span>
<span id="cb11-2"><a href="#cb11-2"></a></span>
<span id="cb11-3"><a href="#cb11-3"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, sharex<span class="op">=</span><span class="va">True</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb11-4"><a href="#cb11-4"></a>df.plot(ax<span class="op">=</span>axs[<span class="dv">0</span>], legend<span class="op">=</span><span class="va">False</span>, marker<span class="op">=</span><span class="st">"."</span>)  <span class="co"># original time series</span></span>
<span id="cb11-5"><a href="#cb11-5"></a>df.shift(<span class="dv">7</span>).plot(ax<span class="op">=</span>axs[<span class="dv">0</span>], grid<span class="op">=</span><span class="va">True</span>, legend<span class="op">=</span><span class="va">False</span>, linestyle<span class="op">=</span><span class="st">":"</span>)  <span class="co"># lagged time series</span></span>
<span id="cb11-6"><a href="#cb11-6"></a>diff_7.plot(ax<span class="op">=</span>axs[<span class="dv">1</span>], grid<span class="op">=</span><span class="va">True</span>, marker<span class="op">=</span><span class="st">"."</span>)  <span class="co"># 7-day difference time series</span></span>
<span id="cb11-7"><a href="#cb11-7"></a>axs[<span class="dv">0</span>].set_ylim([<span class="dv">170_000</span>, <span class="dv">900_000</span>])<span class="op">;</span>  <span class="co"># extra code – beautifies the plot</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="10_Recurrent_Neural_Networks_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Notice how closely the lagged time series track the actual time series. <strong>When a time series is correlated with a lagged version of itself, we say that the time series is autocorrelated.</strong> As you can see, most of the differences are fairly small, except at the end of May. Maybe there was a holiday at that time? Let’s check the <code>day_type</code> column:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:568,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683347469424,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="d84753a0-4efc-4ab8-f7bf-35b79b0494e9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="bu">list</span>(df.loc[<span class="st">"2019-05-25"</span>:<span class="st">"2019-05-27"</span>][<span class="st">"day_type"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>['A', 'U', 'U']</code></pre>
</div>
</div>
<p>Indeed, there was a long weekend back then: the Monday was the Memorial Day holiday. We could use this column to improve our forecasts, but for now let’s just measure the mean absolute error over the three-month period we’re arbitrarily focusing on previously — March, April, and May 2019 — to get a rough idea:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:489,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683347524594,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="9273ed1d-18f8-4790-8f59-970ad46deaac">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>diff_7.<span class="bu">abs</span>().mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>bus     43915.608696
rail    42143.271739
dtype: float64</code></pre>
</div>
</div>
<p>Our naive forecasts get an MAE of about 43,916 bus riders, and about 42,143 rail riders. It’s hard to tell at a glance how good or bad this is, so let’s put the forecast errors into perspective by dividing them by the target values:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:447,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683347581392,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="ac5513c8-c6fd-4c63-90aa-86a0a9d18305">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>targets <span class="op">=</span> df[[<span class="st">"bus"</span>, <span class="st">"rail"</span>]][<span class="st">"2019-03"</span>:<span class="st">"2019-05"</span>]</span>
<span id="cb16-2"><a href="#cb16-2"></a>(diff_7 <span class="op">/</span> targets).<span class="bu">abs</span>().mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>bus     0.082938
rail    0.089948
dtype: float64</code></pre>
</div>
</div>
<p>What we just computed is called the <strong>mean absolute percentage error (MAPE)</strong>: it looks like our naive forecasts give us a MAPE of roughly 8.3% for bus and 9.0% for rail. It’s interesting to note that the MAE for the rail forecasts looks slightly better than the MAE for the bus forecasts, while the opposite is true for the MAPE. That’s because the bus ridership is larger than the rail ridership, so naturally the forecast errors are also larger, but when we put the errors into perspective, it turns out that the bus forecasts are actually slightly better than the rail forecasts.</p>
<p>Looking at the time series, there doesn’t appear to be any significant monthly seasonality, but let’s check whether there’s any yearly seasonality. We’ll look at the data from 2001 to 2019. To reduce the risk of data snooping, we’ll ignore more recent data for now. Lets also plot a 12-month rolling average for each series to visualize long-term trends:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2423,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683347921270,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="a4b060b4-fba2-46f0-fa2c-c17317bef80b">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>df_monthly <span class="op">=</span> df.resample(<span class="st">'M'</span>).mean()  <span class="co"># compute the mean for each month</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>rolling_average_12_months <span class="op">=</span> df_monthly[<span class="st">"2001"</span>:<span class="st">"2019"</span>].rolling(window<span class="op">=</span><span class="dv">12</span>).mean()</span>
<span id="cb18-3"><a href="#cb18-3"></a></span>
<span id="cb18-4"><a href="#cb18-4"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb18-5"><a href="#cb18-5"></a>df_monthly[<span class="st">"2001"</span>:<span class="st">"2019"</span>].plot(ax<span class="op">=</span>ax, marker<span class="op">=</span><span class="st">"."</span>)</span>
<span id="cb18-6"><a href="#cb18-6"></a>rolling_average_12_months.plot(ax<span class="op">=</span>ax, grid<span class="op">=</span><span class="va">True</span>, legend<span class="op">=</span><span class="va">False</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.
  df_monthly = df.resample('M').mean()  # compute the mean for each month</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="10_Recurrent_Neural_Networks_files/figure-html/cell-13-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>There’s definitely some yearly seasonality as well, although it is noisier than the weekly seasonality, and more visible for the rail series than the bus series: we see peaks and troughs at roughly the same dates each year. Let’s check what we get if we plot the 12-month difference:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1230,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683347973443,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="278b422e-f887-41f6-bc91-3a004f1af64d">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>df_monthly.diff(<span class="dv">12</span>)[<span class="st">"2001"</span>:<span class="st">"2019"</span>].plot(grid<span class="op">=</span><span class="va">True</span>, marker<span class="op">=</span><span class="st">"."</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span>))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="10_Recurrent_Neural_Networks_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Notice how differencing not only removed the yearly seasonality, but it also removed the long-term trends. For example, the linear downward trend present in the time series from 2016 to 2019 became a roughly constant negative value in the differenced time series. In fact, differencing is a common technique used to remove trend and seasonality from a time series: it’s easier to study a <strong>stationary</strong> time series, meaning one whose statistical properties remain constant over time, without any seasonality or trends. <strong>Once you’re able to make accurate forecasts on the differenced time series, it’s easy to turn them into forecasts for the actual time series by just adding back the past values that were previously subtracted.</strong></p>
<p>You may be thinking that we’re only trying to predict tomorrow’s ridership, so the long-term patterns matter much less than the short-term ones. But still, we may be able to improve performance slightly by taking long-term patterns into account. For example, daily bus ridership dropped by about 2,500 in October 2017, which represents about 570 fewer passengers each week, so if we were at the end of October 2017, <strong>it would make sense to forecast tomorrow’s ridership by copying the value from last week, minus 570.</strong> Accounting for the trend will make your forecasts a bit more accurate on average.</p>
</section>
<section id="the-arma-model-family" class="level3" data-number="10.2.3">
<h3 data-number="10.2.3" class="anchored" data-anchor-id="the-arma-model-family"><span class="header-section-number">10.2.3</span> The ARMA Model Family</h3>
<p>We’ll start with the <em>autoregressive moving average (ARMA) model</em>, developed by Herman Wold in the 1930s: it computes its forecasts using a simple weighted sum of lagged values and corrects these forecasts by adding a moving average. Specifically, the moving average component is computed using a weighted sum of the last few forecast errors. This model assumes that the time series is stationary. If it is not, then differencing may help. Using differencing over a single time step will produce an approximation of the derivative of the time series. If the original time series has a quadratic trend instead of a linear trend, then a single round of differencing will not be enough. In this case, running <code>d</code> consecutive rounds of differencing computes an approximation of the <code>d</code>-th order derivative of the time series, so it will eliminate polynomial trends up to degree <code>d</code>. This hyperparameter <code>d</code> is called the <strong>order of integration</strong>. Differencing is the central contribution of the <em>autoregressive integrated moving average (ARIMA) model</em>, introduced in 1970 by George Box and Gwilym Jenkins in their book: this model runs <code>d</code> rounds of differencing to make the time series more stationary, then it applies a regular ARMA model. When making forecasts, it uses this ARMA model, then it adds back the terms that were subtracted by differencing.</p>
<p>Let’s see how to fit a SARIMA model to the rail time series, and use it to make a forecast for tomorrow’s ridership. We’ll pretend today is the last day of May 2019, and we want to forecast the rail ridership for “tomorrow”, the 1st of June, 2019. For this, we can use the <code>statsmodels</code> library, which contains many different statistical models, including the ARMA model and its variants, implemented by the <a href="https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMA.html"><code>ARIMA</code></a> class:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:454,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683348401646,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="0bff8ab2-4629-4f63-9952-1ad1c48a9222">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>origin, today <span class="op">=</span> <span class="st">"2019-01-01"</span>, <span class="st">"2019-05-31"</span></span>
<span id="cb21-2"><a href="#cb21-2"></a>rail_series <span class="op">=</span> df.loc[origin:today][<span class="st">"rail"</span>].asfreq(<span class="st">"D"</span>) <span class="co"># Convert time series to specified frequency</span></span>
<span id="cb21-3"><a href="#cb21-3"></a>model <span class="op">=</span> ARIMA(rail_series, order<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>), seasonal_order<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">7</span>))</span>
<span id="cb21-4"><a href="#cb21-4"></a>model <span class="op">=</span> model.fit()</span>
<span id="cb21-5"><a href="#cb21-5"></a>y_pred <span class="op">=</span> model.forecast() </span>
<span id="cb21-6"><a href="#cb21-6"></a></span>
<span id="cb21-7"><a href="#cb21-7"></a>y_pred[<span class="dv">0</span>]  <span class="co"># ARIMA forecast </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>427758.62641035335</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683348404182,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="e25a8c8d-5d56-4ce3-ab9e-640b177a7d65">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="bu">print</span>(df[<span class="st">"rail"</span>].loc[<span class="st">"2019-06-01"</span>])  <span class="co"># target value</span></span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="bu">print</span>(df[<span class="st">"rail"</span>].loc[<span class="st">"2019-05-25"</span>])  <span class="co"># naive forecast (value from one week earlier)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>379044
426932</code></pre>
</div>
</div>
<ul>
<li><p>We start by importing the <code>ARIMA</code> class, then we take the rail ridership data from the start of 2019 up to “today”, and we use <code>asfreq("D")</code> to set the time series’frequency to daily: this doesn’t change the data at all in this case, since it’s already daily, but without this the <code>ARIMA</code> class would have to guess the frequency, and it would display a warning.</p></li>
<li><p>We create an <code>ARIMA</code> instance, passing it all the data until “today”, and we set the model hyperparameters: <code>order=(1, 0, 0)</code> and <code>seasonal_order=(0, 1, 1, 7)</code> for the model (See API for more descriptions). Notice that the <code>statsmodels</code> API differs a bit from <code>Scikit-Learn</code>’s API, since we pass the data to the model at construction time, instead of passing it to the <code>fit()</code> method.</p></li>
</ul>
<p>The forecast is 427,759 passengers, when in fact there were 379,044. Yikes, we’re 12.9% off - that’s pretty bad. It’s actually slightly worse than naive forecasting, which forecasts 426,932, off by 12.6%. But perhaps we were just unlucky that day? To check this, we can run the same code in a loop to make forecasts for every day in March, April, and May, and compute the MAE over that period:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:9488,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683348817025,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="12630b1c-8f09-4776-a770-fab371f5bcfd">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>origin, start_date, end_date <span class="op">=</span> <span class="st">"2019-01-01"</span>, <span class="st">"2019-03-01"</span>, <span class="st">"2019-05-31"</span></span>
<span id="cb25-2"><a href="#cb25-2"></a>time_period <span class="op">=</span> pd.date_range(start_date, end_date)</span>
<span id="cb25-3"><a href="#cb25-3"></a>rail_series <span class="op">=</span> df.loc[origin:end_date][<span class="st">"rail"</span>].asfreq(<span class="st">"D"</span>)</span>
<span id="cb25-4"><a href="#cb25-4"></a></span>
<span id="cb25-5"><a href="#cb25-5"></a>y_preds <span class="op">=</span> []</span>
<span id="cb25-6"><a href="#cb25-6"></a><span class="cf">for</span> today <span class="kw">in</span> time_period.shift(<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb25-7"><a href="#cb25-7"></a>    model <span class="op">=</span> ARIMA(rail_series[origin:today],  <span class="co"># train on data from January up to "today", expanding window approach</span></span>
<span id="cb25-8"><a href="#cb25-8"></a>                  order<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb25-9"><a href="#cb25-9"></a>                  seasonal_order<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">7</span>))</span>
<span id="cb25-10"><a href="#cb25-10"></a>    model <span class="op">=</span> model.fit()  <span class="co"># note that we retrain the model every day!</span></span>
<span id="cb25-11"><a href="#cb25-11"></a>    y_pred <span class="op">=</span> model.forecast()[<span class="dv">0</span>]</span>
<span id="cb25-12"><a href="#cb25-12"></a>    y_preds.append(y_pred)</span>
<span id="cb25-13"><a href="#cb25-13"></a></span>
<span id="cb25-14"><a href="#cb25-14"></a>y_preds <span class="op">=</span> pd.Series(y_preds, index<span class="op">=</span>time_period)</span>
<span id="cb25-15"><a href="#cb25-15"></a>mae <span class="op">=</span> (y_preds <span class="op">-</span> rail_series[time_period]).<span class="bu">abs</span>().mean()</span>
<span id="cb25-16"><a href="#cb25-16"></a>mae</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>32040.720089453378</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1179,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683348911180,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="fd435a24-c195-4c8f-f37c-33f98b090a51">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span>))</span>
<span id="cb27-2"><a href="#cb27-2"></a>rail_series.loc[time_period].plot(label<span class="op">=</span><span class="st">"True"</span>, ax<span class="op">=</span>ax, marker<span class="op">=</span><span class="st">"."</span>, grid<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-3"><a href="#cb27-3"></a>ax.plot(y_preds, color<span class="op">=</span><span class="st">"r"</span>, marker<span class="op">=</span><span class="st">"."</span>, label<span class="op">=</span><span class="st">"ARIMA Forecasts"</span>)</span>
<span id="cb27-4"><a href="#cb27-4"></a>plt.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="10_Recurrent_Neural_Networks_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>That’s much better! The MAE is about 32,041, which is significantly lower than the MAE we got with naive forecasting (42,143). So although the model is not perfect, it still beats naive forecasting by a large margin, on average. There are approaches for selecting good hyperparameters, based on analyzing the autocorrelation function (ACF) and partial autocorrelation function (PACF), or minimizing the AIC or BIC metrics to penalize models that use too many parameters and reduce the risk of overfitting the data:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2932,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683348934326,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="46b4dd27-be94-4105-f218-8564f641b458">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>fig, axs <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb28-2"><a href="#cb28-2"></a>plot_acf(df[<span class="st">"2001"</span>:<span class="st">"2019"</span>][<span class="st">"rail"</span>], ax<span class="op">=</span>axs[<span class="dv">0</span>], lags<span class="op">=</span><span class="dv">35</span>)</span>
<span id="cb28-3"><a href="#cb28-3"></a>axs[<span class="dv">0</span>].grid()</span>
<span id="cb28-4"><a href="#cb28-4"></a>plot_pacf(df[<span class="st">"2001"</span>:<span class="st">"2019"</span>][<span class="st">"rail"</span>], ax<span class="op">=</span>axs[<span class="dv">1</span>], lags<span class="op">=</span><span class="dv">35</span>, method<span class="op">=</span><span class="st">"ywm"</span>)</span>
<span id="cb28-5"><a href="#cb28-5"></a>axs[<span class="dv">1</span>].grid()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="10_Recurrent_Neural_Networks_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Refer to <a href="https://www.statsmodels.org/devel/graphics.html#time-series-plots">https://www.statsmodels.org/devel/graphics.html#time-series-plots</a> for more details.</p>
</section>
<section id="preparing-the-data-for-machine-learning-models" class="level3" data-number="10.2.4">
<h3 data-number="10.2.4" class="anchored" data-anchor-id="preparing-the-data-for-machine-learning-models"><span class="header-section-number">10.2.4</span> Preparing the Data for Machine Learning Models</h3>
<p>Now that we have two baselines, naive forecasting and ARIMA, let’s try to use the machine learning models we’ve covered so far to forecast this time series, starting with a basic linear model. Our goal will be to forecast tomorrow’s ridership based on the ridership of the past 8 weeks of data (56 days). The inputs to our model will therefore be sequences (usually a single sequence per day once the model is in production), each containing 56 values from time steps <span class="math inline">\(t–55\)</span> to <span class="math inline">\(t\)</span>. For each input sequence, the model will output a single value: the forecast for time step <span class="math inline">\(t+1\)</span>. We will use every 56-day window from the past as training data, and the target for each window will be the value immediately following it.</p>
<p><code>tf.Keras</code> actually has a nice utility function called <code>tf.keras.utils.timeseries_dataset_from_array()</code> to help us prepare the training set. It takes a time series as input, and it builds a <code>tf.data.Dataset</code> containing all the windows of the desired length, as well as their corresponding targets. Here’s an example that takes a time series containing the numbers 0 to 5 and creates a dataset containing all the windows of length 3, with their corresponding targets, grouped into batches of size 2:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:449,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683349093666,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="c2cd0ede-10e6-4a46-af21-d858d290ce34">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a>my_series <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]</span>
<span id="cb29-2"><a href="#cb29-2"></a>my_dataset <span class="op">=</span> tf.keras.utils.timeseries_dataset_from_array(</span>
<span id="cb29-3"><a href="#cb29-3"></a>    my_series,</span>
<span id="cb29-4"><a href="#cb29-4"></a>    targets<span class="op">=</span>my_series[<span class="dv">3</span>:],  <span class="co"># the targets are 3 steps into the future</span></span>
<span id="cb29-5"><a href="#cb29-5"></a>    sequence_length<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb29-6"><a href="#cb29-6"></a>    batch_size<span class="op">=</span><span class="dv">2</span></span>
<span id="cb29-7"><a href="#cb29-7"></a>)</span>
<span id="cb29-8"><a href="#cb29-8"></a><span class="bu">list</span>(my_dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>[(&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=
  array([[0, 1, 2],
         [1, 2, 3]], dtype=int32)&gt;,
  &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)&gt;),
 (&lt;tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[2, 3, 4]], dtype=int32)&gt;,
  &lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)&gt;)]</code></pre>
</div>
</div>
<p>Each sample in the dataset is a window of length 3, along with its corresponding target (i.e., the value immediately after the window). The windows are <code>[0, 1, 2]</code>, <code>[1, 2, 3]</code>, and <code>[2, 3, 4]</code>, and their respective targets are 3, 4, and 5. Since there are three windows in total, which is not a multiple of the batch size, the last batch only contains one window instead of two.</p>
<p>Another way to get the same result is to use the <code>window()</code> method of <code>tf.data</code>’s <code>Dataset</code> class. It’s more complex, but it gives you full control, which will come in handy later in this chapter, so let’s see how it works. The <code>window()</code> method returns a dataset of window datasets:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:527,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683349195875,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="a5e21bdd-ea9c-43c7-bfa6-4170e9f1e2bf">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a><span class="cf">for</span> window_dataset <span class="kw">in</span> tf.data.Dataset.<span class="bu">range</span>(<span class="dv">6</span>).window(<span class="dv">4</span>, shift<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb31-2"><a href="#cb31-2"></a>    <span class="cf">for</span> element <span class="kw">in</span> window_dataset:</span>
<span id="cb31-3"><a href="#cb31-3"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>element<span class="sc">}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">" "</span>)</span>
<span id="cb31-4"><a href="#cb31-4"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 1 2 3 
1 2 3 4 
2 3 4 5 
3 4 5 
4 5 
5 </code></pre>
</div>
</div>
<p>In this example, the dataset contains six windows, each shifted by one step compared to the previous one, and the last three windows are smaller because they’ve reached the end of the series. In general you’ll want to get rid of these smaller windows by passing <code>drop_remainder=True</code> to the <code>window()</code> method. The <code>window()</code> method returns a nested dataset, analogous to a <em>list of lists</em>. This is useful when you want to transform each window by calling its dataset methods (e.g., to shuffle them or batch them). However, we cannot use a nested dataset directly for training, as our model will expect tensors as input, not datasets. Therefore, we must call the <code>flat_map()</code> method: it converts a nested dataset into a flat dataset (one that contains tensors, not datasets). For example, suppose <code>{1, 2, 3}</code> represents a dataset containing the sequence of tensors 1, 2, and 3. If you flatten the nested dataset <code>{{1, 2}, {3, 4, 5, 6}}</code>, you get back the flat dataset <code>{1, 2, 3, 4, 5, 6}</code>. Moreover, the <code>flat_map()</code> method takes a function as an argument, which allows you to transform each dataset in the nested dataset before flattening. For example, if you pass the function <code>lambda ds: ds.batch(2)</code> to <code>flat_map()</code>, then it will transform the nested dataset <code>{{1, 2}, {3, 4, 5, 6}}</code> into the flat dataset <code>{[1, 2], [3, 4], [5, 6]}</code>: it’s a dataset containing 3 tensors, each of size 2:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1290,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683349276057,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="52e52644-7c0e-478e-db14-dccb57d01aea">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a>dataset <span class="op">=</span> tf.data.Dataset.<span class="bu">range</span>(<span class="dv">6</span>).window(<span class="dv">4</span>, shift<span class="op">=</span><span class="dv">1</span>, drop_remainder<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-2"><a href="#cb33-2"></a>dataset <span class="op">=</span> dataset.flat_map(<span class="kw">lambda</span> window_dataset: window_dataset.batch(<span class="dv">4</span>))</span>
<span id="cb33-3"><a href="#cb33-3"></a><span class="cf">for</span> window_tensor <span class="kw">in</span> dataset:</span>
<span id="cb33-4"><a href="#cb33-4"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>window_tensor<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0 1 2 3]
[1 2 3 4]
[2 3 4 5]</code></pre>
</div>
</div>
<p>Since each window dataset contains exactly four items, calling <code>batch(4)</code> on a window produces a single tensor of size 4. Great! We now have a dataset containing consecutive windows represented as tensors. Let’s create a little helper function to make it easier to extract windows from a dataset:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a><span class="kw">def</span> to_windows(dataset, length):</span>
<span id="cb35-2"><a href="#cb35-2"></a>    dataset <span class="op">=</span> dataset.window(length, shift<span class="op">=</span><span class="dv">1</span>, drop_remainder<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-3"><a href="#cb35-3"></a>    <span class="cf">return</span> dataset.flat_map(<span class="kw">lambda</span> window_ds: window_ds.batch(length))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The last step is to split each window into inputs and targets, using the <code>map()</code> method. We can also group the resulting windows into batches of size 2:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:550,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683349317724,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="c3488652-c01a-4f3f-c176-4e13f9456e7a">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>dataset <span class="op">=</span> to_windows(tf.data.Dataset.<span class="bu">range</span>(<span class="dv">6</span>), <span class="dv">4</span>)</span>
<span id="cb36-2"><a href="#cb36-2"></a>dataset <span class="op">=</span> dataset.<span class="bu">map</span>(<span class="kw">lambda</span> window: (window[:<span class="op">-</span><span class="dv">1</span>], window[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb36-3"><a href="#cb36-3"></a><span class="bu">list</span>(dataset.batch(<span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>[(&lt;tf.Tensor: shape=(2, 3), dtype=int64, numpy=
  array([[0, 1, 2],
         [1, 2, 3]])&gt;,
  &lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([3, 4])&gt;),
 (&lt;tf.Tensor: shape=(1, 3), dtype=int64, numpy=array([[2, 3, 4]])&gt;,
  &lt;tf.Tensor: shape=(1,), dtype=int64, numpy=array([5])&gt;)]</code></pre>
</div>
</div>
<p>As you can see, we now have the same output as we got earlier with the <code>timeseries_dataset_from_array()</code> function!</p>
<p>Before we continue looking at the data, let’s split the time series into three periods, for training, validation and testing. We won’t look at the test data for now:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>rail_train <span class="op">=</span> df[<span class="st">"rail"</span>][<span class="st">"2016-01"</span>:<span class="st">"2018-12"</span>] <span class="op">/</span> <span class="fl">1e6</span> <span class="co"># Normalize to have the unit of million</span></span>
<span id="cb38-2"><a href="#cb38-2"></a>rail_valid <span class="op">=</span> df[<span class="st">"rail"</span>][<span class="st">"2019-01"</span>:<span class="st">"2019-05"</span>] <span class="op">/</span> <span class="fl">1e6</span></span>
<span id="cb38-3"><a href="#cb38-3"></a>rail_test <span class="op">=</span> df[<span class="st">"rail"</span>][<span class="st">"2019-06"</span>:] <span class="op">/</span> <span class="fl">1e6</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, let’s use <code>timeseries_dataset_from_array()</code> to create datasets for training and validation. Since gradient descent expects the instances in the training set to be independent and identically distributed (IID), argument <code>shuffle=True</code> to shuffle the training windows (but not their contents):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a>seq_length <span class="op">=</span> <span class="dv">56</span></span>
<span id="cb39-2"><a href="#cb39-2"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility</span></span>
<span id="cb39-3"><a href="#cb39-3"></a>train_ds <span class="op">=</span> tf.keras.utils.timeseries_dataset_from_array(</span>
<span id="cb39-4"><a href="#cb39-4"></a>    rail_train.to_numpy(),</span>
<span id="cb39-5"><a href="#cb39-5"></a>    targets<span class="op">=</span>rail_train[seq_length:],</span>
<span id="cb39-6"><a href="#cb39-6"></a>    sequence_length<span class="op">=</span>seq_length, <span class="co"># Sliding window approach</span></span>
<span id="cb39-7"><a href="#cb39-7"></a>    batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb39-8"><a href="#cb39-8"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb39-9"><a href="#cb39-9"></a>    seed<span class="op">=</span><span class="dv">42</span></span>
<span id="cb39-10"><a href="#cb39-10"></a>)</span>
<span id="cb39-11"><a href="#cb39-11"></a>valid_ds <span class="op">=</span> tf.keras.utils.timeseries_dataset_from_array(</span>
<span id="cb39-12"><a href="#cb39-12"></a>    rail_valid.to_numpy(),</span>
<span id="cb39-13"><a href="#cb39-13"></a>    targets<span class="op">=</span>rail_valid[seq_length:],</span>
<span id="cb39-14"><a href="#cb39-14"></a>    sequence_length<span class="op">=</span>seq_length,</span>
<span id="cb39-15"><a href="#cb39-15"></a>    batch_size<span class="op">=</span><span class="dv">32</span></span>
<span id="cb39-16"><a href="#cb39-16"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="forecasting-using-a-linear-model" class="level3" data-number="10.2.5">
<h3 data-number="10.2.5" class="anchored" data-anchor-id="forecasting-using-a-linear-model"><span class="header-section-number">10.2.5</span> Forecasting Using a Linear Model</h3>
<p>Let’s try a basic linear model first. We will use the Huber loss, which usually works better than minimizing the MAE directly. We’ll also use early stopping:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:614,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683349454665,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="176d9c4a-7fd2-41dd-bbc5-968fad0d7d51">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb40-2"><a href="#cb40-2"></a>model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb40-3"><a href="#cb40-3"></a>    tf.keras.layers.Dense(<span class="dv">1</span>, input_shape<span class="op">=</span>[seq_length])</span>
<span id="cb40-4"><a href="#cb40-4"></a>])</span>
<span id="cb40-5"><a href="#cb40-5"></a></span>
<span id="cb40-6"><a href="#cb40-6"></a>early_stopping_cb <span class="op">=</span> tf.keras.callbacks.EarlyStopping(</span>
<span id="cb40-7"><a href="#cb40-7"></a>    monitor<span class="op">=</span><span class="st">"val_mae"</span>, patience<span class="op">=</span><span class="dv">50</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb40-8"><a href="#cb40-8"></a>opt <span class="op">=</span> tf.keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">0.02</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb40-9"><a href="#cb40-9"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span>tf.keras.losses.Huber(), optimizer<span class="op">=</span>opt, metrics<span class="op">=</span>[<span class="st">"mae"</span>])</span>
<span id="cb40-10"><a href="#cb40-10"></a></span>
<span id="cb40-11"><a href="#cb40-11"></a>model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 1)                 57        
                                                                 
=================================================================
Total params: 57
Trainable params: 57
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:99426,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683349561408,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="f5a74c86-81ff-4ce7-9630-206b1e61fe66">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a>history <span class="op">=</span> model.fit(train_ds, validation_data<span class="op">=</span>valid_ds, epochs<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb42-2"><a href="#cb42-2"></a>                    callbacks<span class="op">=</span>[early_stopping_cb])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/500
33/33 [==============================] - 5s 12ms/step - loss: 0.1395 - mae: 0.4360 - val_loss: 0.0117 - val_mae: 0.1257
Epoch 2/500
33/33 [==============================] - 0s 9ms/step - loss: 0.0145 - mae: 0.1306 - val_loss: 0.0106 - val_mae: 0.1097
Epoch 3/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0068 - mae: 0.0827 - val_loss: 0.0067 - val_mae: 0.0844
Epoch 4/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0064 - mae: 0.0797 - val_loss: 0.0058 - val_mae: 0.0797
Epoch 5/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0064 - mae: 0.0803 - val_loss: 0.0068 - val_mae: 0.0843
Epoch 6/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0059 - mae: 0.0770 - val_loss: 0.0053 - val_mae: 0.0737
Epoch 7/500
33/33 [==============================] - 1s 13ms/step - loss: 0.0054 - mae: 0.0718 - val_loss: 0.0051 - val_mae: 0.0713
Epoch 8/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0058 - mae: 0.0770 - val_loss: 0.0050 - val_mae: 0.0753
Epoch 9/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0049 - mae: 0.0692 - val_loss: 0.0053 - val_mae: 0.0737
Epoch 10/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0049 - mae: 0.0674 - val_loss: 0.0042 - val_mae: 0.0656
Epoch 11/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0050 - mae: 0.0691 - val_loss: 0.0042 - val_mae: 0.0682
Epoch 12/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0046 - mae: 0.0645 - val_loss: 0.0039 - val_mae: 0.0614
Epoch 13/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0044 - mae: 0.0631 - val_loss: 0.0052 - val_mae: 0.0775
Epoch 14/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0044 - mae: 0.0632 - val_loss: 0.0046 - val_mae: 0.0722
Epoch 15/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0048 - mae: 0.0695 - val_loss: 0.0043 - val_mae: 0.0694
Epoch 16/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0046 - mae: 0.0672 - val_loss: 0.0036 - val_mae: 0.0608
Epoch 17/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0046 - mae: 0.0663 - val_loss: 0.0034 - val_mae: 0.0553
Epoch 18/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0041 - mae: 0.0607 - val_loss: 0.0034 - val_mae: 0.0571
Epoch 19/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0042 - mae: 0.0627 - val_loss: 0.0032 - val_mae: 0.0544
Epoch 20/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0044 - mae: 0.0654 - val_loss: 0.0044 - val_mae: 0.0692
Epoch 21/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0047 - mae: 0.0696 - val_loss: 0.0031 - val_mae: 0.0521
Epoch 22/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0037 - mae: 0.0573 - val_loss: 0.0030 - val_mae: 0.0512
Epoch 23/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0035 - mae: 0.0546 - val_loss: 0.0031 - val_mae: 0.0537
Epoch 24/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0037 - mae: 0.0570 - val_loss: 0.0029 - val_mae: 0.0504
Epoch 25/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0035 - mae: 0.0554 - val_loss: 0.0028 - val_mae: 0.0486
Epoch 26/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0036 - mae: 0.0558 - val_loss: 0.0030 - val_mae: 0.0513
Epoch 27/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0036 - mae: 0.0553 - val_loss: 0.0034 - val_mae: 0.0569
Epoch 28/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0035 - mae: 0.0574 - val_loss: 0.0032 - val_mae: 0.0565
Epoch 29/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0033 - mae: 0.0530 - val_loss: 0.0030 - val_mae: 0.0516
Epoch 30/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0037 - mae: 0.0574 - val_loss: 0.0028 - val_mae: 0.0492
Epoch 31/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0034 - mae: 0.0543 - val_loss: 0.0026 - val_mae: 0.0455
Epoch 32/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0033 - mae: 0.0516 - val_loss: 0.0026 - val_mae: 0.0452
Epoch 33/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0032 - mae: 0.0523 - val_loss: 0.0028 - val_mae: 0.0508
Epoch 34/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0033 - mae: 0.0524 - val_loss: 0.0029 - val_mae: 0.0513
Epoch 35/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0031 - mae: 0.0500 - val_loss: 0.0026 - val_mae: 0.0467
Epoch 36/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0031 - mae: 0.0515 - val_loss: 0.0026 - val_mae: 0.0456
Epoch 37/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0033 - mae: 0.0537 - val_loss: 0.0029 - val_mae: 0.0515
Epoch 38/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0032 - mae: 0.0512 - val_loss: 0.0028 - val_mae: 0.0498
Epoch 39/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0031 - mae: 0.0501 - val_loss: 0.0025 - val_mae: 0.0424
Epoch 40/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0031 - mae: 0.0494 - val_loss: 0.0024 - val_mae: 0.0421
Epoch 41/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0030 - mae: 0.0500 - val_loss: 0.0025 - val_mae: 0.0426
Epoch 42/500
33/33 [==============================] - 0s 10ms/step - loss: 0.0030 - mae: 0.0490 - val_loss: 0.0025 - val_mae: 0.0436
Epoch 43/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0033 - mae: 0.0535 - val_loss: 0.0033 - val_mae: 0.0551
Epoch 44/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0031 - mae: 0.0522 - val_loss: 0.0024 - val_mae: 0.0415
Epoch 45/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0031 - mae: 0.0488 - val_loss: 0.0025 - val_mae: 0.0426
Epoch 46/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0030 - mae: 0.0504 - val_loss: 0.0024 - val_mae: 0.0418
Epoch 47/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0028 - mae: 0.0465 - val_loss: 0.0024 - val_mae: 0.0413
Epoch 48/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0030 - mae: 0.0500 - val_loss: 0.0024 - val_mae: 0.0417
Epoch 49/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0029 - mae: 0.0470 - val_loss: 0.0024 - val_mae: 0.0407
Epoch 50/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0030 - mae: 0.0492 - val_loss: 0.0024 - val_mae: 0.0416
Epoch 51/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0029 - mae: 0.0473 - val_loss: 0.0023 - val_mae: 0.0406
Epoch 52/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0028 - mae: 0.0464 - val_loss: 0.0024 - val_mae: 0.0407
Epoch 53/500
33/33 [==============================] - 0s 10ms/step - loss: 0.0031 - mae: 0.0497 - val_loss: 0.0023 - val_mae: 0.0398
Epoch 54/500
33/33 [==============================] - 1s 12ms/step - loss: 0.0028 - mae: 0.0467 - val_loss: 0.0023 - val_mae: 0.0398
Epoch 55/500
33/33 [==============================] - 0s 10ms/step - loss: 0.0030 - mae: 0.0495 - val_loss: 0.0027 - val_mae: 0.0467
Epoch 56/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0030 - mae: 0.0497 - val_loss: 0.0024 - val_mae: 0.0425
Epoch 57/500
33/33 [==============================] - 1s 13ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0023 - val_mae: 0.0404
Epoch 58/500
33/33 [==============================] - 1s 12ms/step - loss: 0.0028 - mae: 0.0475 - val_loss: 0.0023 - val_mae: 0.0408
Epoch 59/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0028 - mae: 0.0454 - val_loss: 0.0028 - val_mae: 0.0476
Epoch 60/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0029 - mae: 0.0484 - val_loss: 0.0027 - val_mae: 0.0458
Epoch 61/500
33/33 [==============================] - 0s 10ms/step - loss: 0.0029 - mae: 0.0471 - val_loss: 0.0023 - val_mae: 0.0401
Epoch 62/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0023 - val_mae: 0.0395
Epoch 63/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0028 - mae: 0.0465 - val_loss: 0.0023 - val_mae: 0.0397
Epoch 64/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0028 - mae: 0.0465 - val_loss: 0.0023 - val_mae: 0.0398
Epoch 65/500
33/33 [==============================] - 0s 10ms/step - loss: 0.0031 - mae: 0.0536 - val_loss: 0.0023 - val_mae: 0.0409
Epoch 66/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0027 - mae: 0.0443 - val_loss: 0.0023 - val_mae: 0.0389
Epoch 67/500
33/33 [==============================] - 0s 10ms/step - loss: 0.0029 - mae: 0.0476 - val_loss: 0.0023 - val_mae: 0.0395
Epoch 68/500
33/33 [==============================] - 1s 18ms/step - loss: 0.0027 - mae: 0.0448 - val_loss: 0.0024 - val_mae: 0.0403
Epoch 69/500
33/33 [==============================] - 1s 18ms/step - loss: 0.0027 - mae: 0.0459 - val_loss: 0.0032 - val_mae: 0.0534
Epoch 70/500
33/33 [==============================] - 1s 14ms/step - loss: 0.0029 - mae: 0.0501 - val_loss: 0.0023 - val_mae: 0.0399
Epoch 71/500
33/33 [==============================] - 1s 18ms/step - loss: 0.0028 - mae: 0.0467 - val_loss: 0.0028 - val_mae: 0.0479
Epoch 72/500
33/33 [==============================] - 1s 10ms/step - loss: 0.0030 - mae: 0.0499 - val_loss: 0.0023 - val_mae: 0.0387
Epoch 73/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0464 - val_loss: 0.0023 - val_mae: 0.0389
Epoch 74/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0458 - val_loss: 0.0023 - val_mae: 0.0390
Epoch 75/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0460 - val_loss: 0.0023 - val_mae: 0.0394
Epoch 76/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0453 - val_loss: 0.0023 - val_mae: 0.0390
Epoch 77/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0474 - val_loss: 0.0030 - val_mae: 0.0513
Epoch 78/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0029 - mae: 0.0492 - val_loss: 0.0022 - val_mae: 0.0385
Epoch 79/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0468 - val_loss: 0.0023 - val_mae: 0.0396
Epoch 80/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0463 - val_loss: 0.0027 - val_mae: 0.0501
Epoch 81/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0465 - val_loss: 0.0023 - val_mae: 0.0388
Epoch 82/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0455 - val_loss: 0.0024 - val_mae: 0.0434
Epoch 83/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0438 - val_loss: 0.0023 - val_mae: 0.0387
Epoch 84/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0448 - val_loss: 0.0022 - val_mae: 0.0383
Epoch 85/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0431 - val_loss: 0.0023 - val_mae: 0.0390
Epoch 86/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0453 - val_loss: 0.0023 - val_mae: 0.0401
Epoch 87/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0489 - val_loss: 0.0033 - val_mae: 0.0552
Epoch 88/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0033 - mae: 0.0557 - val_loss: 0.0024 - val_mae: 0.0443
Epoch 89/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0440 - val_loss: 0.0023 - val_mae: 0.0393
Epoch 90/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0029 - mae: 0.0489 - val_loss: 0.0031 - val_mae: 0.0522
Epoch 91/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0490 - val_loss: 0.0023 - val_mae: 0.0383
Epoch 92/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0447 - val_loss: 0.0023 - val_mae: 0.0388
Epoch 93/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0454 - val_loss: 0.0026 - val_mae: 0.0431
Epoch 94/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0031 - mae: 0.0515 - val_loss: 0.0029 - val_mae: 0.0486
Epoch 95/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0476 - val_loss: 0.0027 - val_mae: 0.0488
Epoch 96/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0468 - val_loss: 0.0023 - val_mae: 0.0402
Epoch 97/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0447 - val_loss: 0.0024 - val_mae: 0.0399
Epoch 98/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0429 - val_loss: 0.0023 - val_mae: 0.0405
Epoch 99/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0449 - val_loss: 0.0022 - val_mae: 0.0386
Epoch 100/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0435 - val_loss: 0.0022 - val_mae: 0.0379
Epoch 101/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0436 - val_loss: 0.0024 - val_mae: 0.0392
Epoch 102/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0029 - mae: 0.0480 - val_loss: 0.0024 - val_mae: 0.0395
Epoch 103/500
33/33 [==============================] - 0s 9ms/step - loss: 0.0027 - mae: 0.0453 - val_loss: 0.0024 - val_mae: 0.0390
Epoch 104/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0028 - mae: 0.0468 - val_loss: 0.0023 - val_mae: 0.0385
Epoch 105/500
33/33 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0478 - val_loss: 0.0024 - val_mae: 0.0391
Epoch 106/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0026 - mae: 0.0433 - val_loss: 0.0023 - val_mae: 0.0384
Epoch 107/500
33/33 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0462 - val_loss: 0.0024 - val_mae: 0.0403
Epoch 108/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0026 - mae: 0.0435 - val_loss: 0.0023 - val_mae: 0.0405
Epoch 109/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0027 - mae: 0.0449 - val_loss: 0.0023 - val_mae: 0.0383
Epoch 110/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0434 - val_loss: 0.0023 - val_mae: 0.0386
Epoch 111/500
33/33 [==============================] - 1s 20ms/step - loss: 0.0027 - mae: 0.0453 - val_loss: 0.0023 - val_mae: 0.0390
Epoch 112/500
33/33 [==============================] - 1s 14ms/step - loss: 0.0027 - mae: 0.0459 - val_loss: 0.0025 - val_mae: 0.0411
Epoch 113/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0452 - val_loss: 0.0024 - val_mae: 0.0424
Epoch 114/500
33/33 [==============================] - 1s 21ms/step - loss: 0.0028 - mae: 0.0466 - val_loss: 0.0024 - val_mae: 0.0389
Epoch 115/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0031 - mae: 0.0521 - val_loss: 0.0023 - val_mae: 0.0386
Epoch 116/500
33/33 [==============================] - 1s 19ms/step - loss: 0.0026 - mae: 0.0434 - val_loss: 0.0023 - val_mae: 0.0383
Epoch 117/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0466 - val_loss: 0.0025 - val_mae: 0.0466
Epoch 118/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0449 - val_loss: 0.0027 - val_mae: 0.0443
Epoch 119/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0030 - mae: 0.0514 - val_loss: 0.0025 - val_mae: 0.0450
Epoch 120/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0452 - val_loss: 0.0023 - val_mae: 0.0380
Epoch 121/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0440 - val_loss: 0.0025 - val_mae: 0.0464
Epoch 122/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0466 - val_loss: 0.0030 - val_mae: 0.0508
Epoch 123/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0022 - val_mae: 0.0388
Epoch 124/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0432 - val_loss: 0.0029 - val_mae: 0.0480
Epoch 125/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0494 - val_loss: 0.0024 - val_mae: 0.0422
Epoch 126/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0033 - mae: 0.0556 - val_loss: 0.0032 - val_mae: 0.0531
Epoch 127/500
33/33 [==============================] - 0s 9ms/step - loss: 0.0028 - mae: 0.0472 - val_loss: 0.0031 - val_mae: 0.0520
Epoch 128/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0030 - mae: 0.0500 - val_loss: 0.0023 - val_mae: 0.0406
Epoch 129/500
33/33 [==============================] - 1s 15ms/step - loss: 0.0025 - mae: 0.0424 - val_loss: 0.0023 - val_mae: 0.0381
Epoch 130/500
33/33 [==============================] - 1s 15ms/step - loss: 0.0026 - mae: 0.0431 - val_loss: 0.0023 - val_mae: 0.0404
Epoch 131/500
33/33 [==============================] - 1s 19ms/step - loss: 0.0025 - mae: 0.0435 - val_loss: 0.0025 - val_mae: 0.0408
Epoch 132/500
33/33 [==============================] - 1s 27ms/step - loss: 0.0026 - mae: 0.0440 - val_loss: 0.0022 - val_mae: 0.0378
Epoch 133/500
33/33 [==============================] - 1s 19ms/step - loss: 0.0026 - mae: 0.0449 - val_loss: 0.0022 - val_mae: 0.0378
Epoch 134/500
33/33 [==============================] - 1s 18ms/step - loss: 0.0026 - mae: 0.0439 - val_loss: 0.0022 - val_mae: 0.0383
Epoch 135/500
33/33 [==============================] - 1s 24ms/step - loss: 0.0026 - mae: 0.0438 - val_loss: 0.0024 - val_mae: 0.0400
Epoch 136/500
33/33 [==============================] - 1s 15ms/step - loss: 0.0026 - mae: 0.0429 - val_loss: 0.0023 - val_mae: 0.0382
Epoch 137/500
33/33 [==============================] - 1s 13ms/step - loss: 0.0028 - mae: 0.0481 - val_loss: 0.0023 - val_mae: 0.0400
Epoch 138/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0028 - mae: 0.0468 - val_loss: 0.0023 - val_mae: 0.0391
Epoch 139/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0027 - mae: 0.0460 - val_loss: 0.0024 - val_mae: 0.0426
Epoch 140/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0025 - mae: 0.0430 - val_loss: 0.0038 - val_mae: 0.0639
Epoch 141/500
33/33 [==============================] - 1s 16ms/step - loss: 0.0027 - mae: 0.0455 - val_loss: 0.0023 - val_mae: 0.0386
Epoch 142/500
33/33 [==============================] - 1s 16ms/step - loss: 0.0026 - mae: 0.0449 - val_loss: 0.0023 - val_mae: 0.0386
Epoch 143/500
33/33 [==============================] - 1s 11ms/step - loss: 0.0029 - mae: 0.0497 - val_loss: 0.0023 - val_mae: 0.0382
Epoch 144/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0489 - val_loss: 0.0024 - val_mae: 0.0398
Epoch 145/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0486 - val_loss: 0.0024 - val_mae: 0.0430
Epoch 146/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0436 - val_loss: 0.0030 - val_mae: 0.0502
Epoch 147/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0027 - val_mae: 0.0510
Epoch 148/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0439 - val_loss: 0.0025 - val_mae: 0.0407
Epoch 149/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0429 - val_loss: 0.0022 - val_mae: 0.0380
Epoch 150/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0440 - val_loss: 0.0023 - val_mae: 0.0384
Epoch 151/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0435 - val_loss: 0.0023 - val_mae: 0.0389
Epoch 152/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0429 - val_loss: 0.0022 - val_mae: 0.0385
Epoch 153/500
33/33 [==============================] - 0s 9ms/step - loss: 0.0026 - mae: 0.0439 - val_loss: 0.0025 - val_mae: 0.0406
Epoch 154/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0488 - val_loss: 0.0028 - val_mae: 0.0465
Epoch 155/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0449 - val_loss: 0.0024 - val_mae: 0.0389
Epoch 156/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0464 - val_loss: 0.0026 - val_mae: 0.0485
Epoch 157/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0439 - val_loss: 0.0022 - val_mae: 0.0386
Epoch 158/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0464 - val_loss: 0.0024 - val_mae: 0.0403
Epoch 159/500
33/33 [==============================] - 1s 12ms/step - loss: 0.0027 - mae: 0.0469 - val_loss: 0.0023 - val_mae: 0.0394
Epoch 160/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0023 - val_mae: 0.0400
Epoch 161/500
33/33 [==============================] - 1s 13ms/step - loss: 0.0032 - mae: 0.0549 - val_loss: 0.0023 - val_mae: 0.0380
Epoch 162/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0026 - mae: 0.0441 - val_loss: 0.0022 - val_mae: 0.0379
Epoch 163/500
33/33 [==============================] - 1s 12ms/step - loss: 0.0026 - mae: 0.0460 - val_loss: 0.0025 - val_mae: 0.0477
Epoch 164/500
33/33 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0458 - val_loss: 0.0023 - val_mae: 0.0381
Epoch 165/500
33/33 [==============================] - 0s 11ms/step - loss: 0.0026 - mae: 0.0433 - val_loss: 0.0022 - val_mae: 0.0377
Epoch 166/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0488 - val_loss: 0.0026 - val_mae: 0.0418
Epoch 167/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0439 - val_loss: 0.0029 - val_mae: 0.0486
Epoch 168/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0484 - val_loss: 0.0023 - val_mae: 0.0380
Epoch 169/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0443 - val_loss: 0.0022 - val_mae: 0.0379
Epoch 170/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0471 - val_loss: 0.0024 - val_mae: 0.0401
Epoch 171/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0434 - val_loss: 0.0024 - val_mae: 0.0434
Epoch 172/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0454 - val_loss: 0.0033 - val_mae: 0.0560
Epoch 173/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0432 - val_loss: 0.0023 - val_mae: 0.0416
Epoch 174/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0462 - val_loss: 0.0024 - val_mae: 0.0398
Epoch 175/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0462 - val_loss: 0.0024 - val_mae: 0.0393
Epoch 176/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0472 - val_loss: 0.0026 - val_mae: 0.0481
Epoch 177/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0025 - mae: 0.0427 - val_loss: 0.0026 - val_mae: 0.0421
Epoch 178/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0466 - val_loss: 0.0023 - val_mae: 0.0411
Epoch 179/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0431 - val_loss: 0.0023 - val_mae: 0.0378
Epoch 180/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0482 - val_loss: 0.0023 - val_mae: 0.0392
Epoch 181/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0448 - val_loss: 0.0024 - val_mae: 0.0399
Epoch 182/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0423 - val_loss: 0.0022 - val_mae: 0.0384
Epoch 183/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0025 - mae: 0.0423 - val_loss: 0.0022 - val_mae: 0.0378
Epoch 184/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0025 - mae: 0.0427 - val_loss: 0.0029 - val_mae: 0.0496
Epoch 185/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0031 - mae: 0.0527 - val_loss: 0.0028 - val_mae: 0.0464
Epoch 186/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0468 - val_loss: 0.0023 - val_mae: 0.0384
Epoch 187/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0441 - val_loss: 0.0026 - val_mae: 0.0429
Epoch 188/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0028 - mae: 0.0474 - val_loss: 0.0028 - val_mae: 0.0463
Epoch 189/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0029 - mae: 0.0484 - val_loss: 0.0027 - val_mae: 0.0446
Epoch 190/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0451 - val_loss: 0.0028 - val_mae: 0.0465
Epoch 191/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0492 - val_loss: 0.0023 - val_mae: 0.0418
Epoch 192/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0032 - mae: 0.0530 - val_loss: 0.0023 - val_mae: 0.0404
Epoch 193/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0027 - mae: 0.0456 - val_loss: 0.0027 - val_mae: 0.0515
Epoch 194/500
33/33 [==============================] - 0s 13ms/step - loss: 0.0029 - mae: 0.0504 - val_loss: 0.0023 - val_mae: 0.0391
Epoch 195/500
33/33 [==============================] - 1s 12ms/step - loss: 0.0025 - mae: 0.0428 - val_loss: 0.0028 - val_mae: 0.0462
Epoch 196/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0026 - mae: 0.0443 - val_loss: 0.0030 - val_mae: 0.0579
Epoch 197/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0025 - mae: 0.0444 - val_loss: 0.0025 - val_mae: 0.0464
Epoch 198/500
33/33 [==============================] - 0s 12ms/step - loss: 0.0031 - mae: 0.0540 - val_loss: 0.0033 - val_mae: 0.0556
Epoch 199/500
33/33 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0459 - val_loss: 0.0022 - val_mae: 0.0382
Epoch 200/500
33/33 [==============================] - 1s 14ms/step - loss: 0.0027 - mae: 0.0450 - val_loss: 0.0023 - val_mae: 0.0420
Epoch 201/500
33/33 [==============================] - 0s 10ms/step - loss: 0.0026 - mae: 0.0447 - val_loss: 0.0023 - val_mae: 0.0388
Epoch 202/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0475 - val_loss: 0.0023 - val_mae: 0.0387
Epoch 203/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0440 - val_loss: 0.0023 - val_mae: 0.0405
Epoch 204/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0425 - val_loss: 0.0025 - val_mae: 0.0417
Epoch 205/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0027 - mae: 0.0452 - val_loss: 0.0023 - val_mae: 0.0381
Epoch 206/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0425 - val_loss: 0.0030 - val_mae: 0.0497
Epoch 207/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0028 - mae: 0.0476 - val_loss: 0.0024 - val_mae: 0.0425
Epoch 208/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0426 - val_loss: 0.0023 - val_mae: 0.0383
Epoch 209/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0432 - val_loss: 0.0023 - val_mae: 0.0410
Epoch 210/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0025 - mae: 0.0425 - val_loss: 0.0024 - val_mae: 0.0395
Epoch 211/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0430 - val_loss: 0.0024 - val_mae: 0.0426
Epoch 212/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0026 - mae: 0.0432 - val_loss: 0.0025 - val_mae: 0.0403
Epoch 213/500
33/33 [==============================] - 0s 8ms/step - loss: 0.0026 - mae: 0.0449 - val_loss: 0.0023 - val_mae: 0.0387
Epoch 214/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0029 - mae: 0.0502 - val_loss: 0.0033 - val_mae: 0.0558
Epoch 215/500
33/33 [==============================] - 0s 7ms/step - loss: 0.0029 - mae: 0.0500 - val_loss: 0.0022 - val_mae: 0.0381</code></pre>
</div>
</div>
<p>This model reaches a validation MAE of about 38,100 (your mileage may vary). That’s better than naive forecasting, but worse than the ARIMA model. Can we do better with an RNN? Let’s see!</p>
</section>
<section id="forecasting-using-a-simple-rnn" class="level3" data-number="10.2.6">
<h3 data-number="10.2.6" class="anchored" data-anchor-id="forecasting-using-a-simple-rnn"><span class="header-section-number">10.2.6</span> Forecasting Using a Simple RNN</h3>
<p>Let’s try the most basic RNN, containing a single recurrent layer with just one recurrent neuron:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683349703389,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="08214b5c-4e75-4e35-e8f2-8607d25bc73d">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility</span></span>
<span id="cb44-2"><a href="#cb44-2"></a>model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb44-3"><a href="#cb44-3"></a>    tf.keras.layers.SimpleRNN(<span class="dv">1</span>, input_shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">1</span>]) <span class="co"># 1*(1+1) + 1*1</span></span>
<span id="cb44-4"><a href="#cb44-4"></a>])</span>
<span id="cb44-5"><a href="#cb44-5"></a></span>
<span id="cb44-6"><a href="#cb44-6"></a>model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn (SimpleRNN)      (None, 1)                 3         
                                                                 
=================================================================
Total params: 3
Trainable params: 3
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<p>All recurrent layers in Keras expect 3D inputs of shape <code>[batch size, time steps, dimensionality]</code>, where <code>dimensionality</code> is 1 for univariate time series and more for multivariate time series. Recall that the <code>input_shape</code> argument ignores the first dimension (i.e., the batch size), and since recurrent layers can accept input sequences of any length, we can set the second dimension to <code>None</code>, which means “any size”. Lastly, since we’re dealing with a univariate time series, we need the last dimension’s size to be 1. This is why we specified the input shape <code>[None, 1]</code>: it means “univariate sequences of any length”. Note that the datasets actually contain inputs of shape <code>[batch size, timesteps]</code>, so we’re missing the last dimension, of size 1, but <code>Keras</code> is kind enough to add it for us in this case.</p>
<p>So that’s our first recurrent model! It’s a sequence-to-vector model. Since there’s a single output neuron, the output vector has a size of 1. We define another helper function for fit and evaluation:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a><span class="kw">def</span> fit_and_evaluate(model, train_set, valid_set, learning_rate, epochs<span class="op">=</span><span class="dv">500</span>):</span>
<span id="cb46-2"><a href="#cb46-2"></a>    early_stopping_cb <span class="op">=</span> tf.keras.callbacks.EarlyStopping(</span>
<span id="cb46-3"><a href="#cb46-3"></a>        monitor<span class="op">=</span><span class="st">"val_mae"</span>, patience<span class="op">=</span><span class="dv">50</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-4"><a href="#cb46-4"></a>    opt <span class="op">=</span> tf.keras.optimizers.SGD(learning_rate<span class="op">=</span>learning_rate, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb46-5"><a href="#cb46-5"></a>    </span>
<span id="cb46-6"><a href="#cb46-6"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span>tf.keras.losses.Huber(), optimizer<span class="op">=</span>opt, metrics<span class="op">=</span>[<span class="st">"mae"</span>])</span>
<span id="cb46-7"><a href="#cb46-7"></a>    </span>
<span id="cb46-8"><a href="#cb46-8"></a>    history <span class="op">=</span> model.fit(train_set, validation_data<span class="op">=</span>valid_set, epochs<span class="op">=</span>epochs,</span>
<span id="cb46-9"><a href="#cb46-9"></a>                        callbacks<span class="op">=</span>[early_stopping_cb])</span>
<span id="cb46-10"><a href="#cb46-10"></a>    valid_loss, valid_mae <span class="op">=</span> model.evaluate(valid_set)</span>
<span id="cb46-11"><a href="#cb46-11"></a>    <span class="cf">return</span> valid_mae <span class="op">*</span> <span class="fl">1e6</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:656270,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683350420113,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="e5105930-87aa-498e-c7c1-33313e35837c">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a>fit_and_evaluate(model, train_ds, valid_ds, learning_rate<span class="op">=</span><span class="fl">0.02</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/500
33/33 [==============================] - 2s 45ms/step - loss: 0.8407 - mae: 1.3014 - val_loss: 0.0304 - val_mae: 0.1623
Epoch 2/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0467 - mae: 0.2393 - val_loss: 0.0493 - val_mae: 0.2517
Epoch 3/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0317 - mae: 0.1811 - val_loss: 0.0246 - val_mae: 0.1599
Epoch 4/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0239 - mae: 0.1783 - val_loss: 0.0233 - val_mae: 0.1617
Epoch 5/500
33/33 [==============================] - 2s 64ms/step - loss: 0.0232 - mae: 0.1759 - val_loss: 0.0230 - val_mae: 0.1557
Epoch 6/500
33/33 [==============================] - 3s 80ms/step - loss: 0.0225 - mae: 0.1717 - val_loss: 0.0221 - val_mae: 0.1541
Epoch 7/500
33/33 [==============================] - 3s 80ms/step - loss: 0.0218 - mae: 0.1717 - val_loss: 0.0214 - val_mae: 0.1518
Epoch 8/500
33/33 [==============================] - 2s 58ms/step - loss: 0.0211 - mae: 0.1686 - val_loss: 0.0207 - val_mae: 0.1493
Epoch 9/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0205 - mae: 0.1670 - val_loss: 0.0200 - val_mae: 0.1476
Epoch 10/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0199 - mae: 0.1658 - val_loss: 0.0195 - val_mae: 0.1448
Epoch 11/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0193 - mae: 0.1652 - val_loss: 0.0189 - val_mae: 0.1431
Epoch 12/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0188 - mae: 0.1634 - val_loss: 0.0183 - val_mae: 0.1425
Epoch 13/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0182 - mae: 0.1617 - val_loss: 0.0178 - val_mae: 0.1412
Epoch 14/500
33/33 [==============================] - 2s 68ms/step - loss: 0.0177 - mae: 0.1603 - val_loss: 0.0172 - val_mae: 0.1415
Epoch 15/500
33/33 [==============================] - 2s 65ms/step - loss: 0.0173 - mae: 0.1598 - val_loss: 0.0169 - val_mae: 0.1392
Epoch 16/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0168 - mae: 0.1586 - val_loss: 0.0164 - val_mae: 0.1390
Epoch 17/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0164 - mae: 0.1571 - val_loss: 0.0159 - val_mae: 0.1386
Epoch 18/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0160 - mae: 0.1560 - val_loss: 0.0156 - val_mae: 0.1378
Epoch 19/500
33/33 [==============================] - 1s 37ms/step - loss: 0.0157 - mae: 0.1550 - val_loss: 0.0152 - val_mae: 0.1372
Epoch 20/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0153 - mae: 0.1543 - val_loss: 0.0149 - val_mae: 0.1355
Epoch 21/500
33/33 [==============================] - 2s 63ms/step - loss: 0.0150 - mae: 0.1525 - val_loss: 0.0146 - val_mae: 0.1345
Epoch 22/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0147 - mae: 0.1505 - val_loss: 0.0144 - val_mae: 0.1336
Epoch 23/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0145 - mae: 0.1515 - val_loss: 0.0140 - val_mae: 0.1345
Epoch 24/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0142 - mae: 0.1487 - val_loss: 0.0139 - val_mae: 0.1319
Epoch 25/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0139 - mae: 0.1477 - val_loss: 0.0136 - val_mae: 0.1318
Epoch 26/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0137 - mae: 0.1477 - val_loss: 0.0133 - val_mae: 0.1315
Epoch 27/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0135 - mae: 0.1464 - val_loss: 0.0132 - val_mae: 0.1302
Epoch 28/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0133 - mae: 0.1444 - val_loss: 0.0130 - val_mae: 0.1292
Epoch 29/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0131 - mae: 0.1440 - val_loss: 0.0128 - val_mae: 0.1291
Epoch 30/500
33/33 [==============================] - 2s 67ms/step - loss: 0.0130 - mae: 0.1430 - val_loss: 0.0127 - val_mae: 0.1274
Epoch 31/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0128 - mae: 0.1416 - val_loss: 0.0125 - val_mae: 0.1270
Epoch 32/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0127 - mae: 0.1408 - val_loss: 0.0123 - val_mae: 0.1267
Epoch 33/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0125 - mae: 0.1410 - val_loss: 0.0122 - val_mae: 0.1250
Epoch 34/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0124 - mae: 0.1390 - val_loss: 0.0121 - val_mae: 0.1239
Epoch 35/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0123 - mae: 0.1381 - val_loss: 0.0120 - val_mae: 0.1243
Epoch 36/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0121 - mae: 0.1376 - val_loss: 0.0119 - val_mae: 0.1231
Epoch 37/500
33/33 [==============================] - 2s 46ms/step - loss: 0.0120 - mae: 0.1356 - val_loss: 0.0118 - val_mae: 0.1221
Epoch 38/500
33/33 [==============================] - 3s 76ms/step - loss: 0.0119 - mae: 0.1354 - val_loss: 0.0117 - val_mae: 0.1221
Epoch 39/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0119 - mae: 0.1358 - val_loss: 0.0116 - val_mae: 0.1210
Epoch 40/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0117 - mae: 0.1338 - val_loss: 0.0115 - val_mae: 0.1210
Epoch 41/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0116 - mae: 0.1330 - val_loss: 0.0114 - val_mae: 0.1200
Epoch 42/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0116 - mae: 0.1324 - val_loss: 0.0114 - val_mae: 0.1196
Epoch 43/500
33/33 [==============================] - 3s 74ms/step - loss: 0.0115 - mae: 0.1319 - val_loss: 0.0113 - val_mae: 0.1195
Epoch 44/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0114 - mae: 0.1319 - val_loss: 0.0112 - val_mae: 0.1188
Epoch 45/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0114 - mae: 0.1303 - val_loss: 0.0112 - val_mae: 0.1180
Epoch 46/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0113 - mae: 0.1300 - val_loss: 0.0111 - val_mae: 0.1180
Epoch 47/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0112 - mae: 0.1296 - val_loss: 0.0110 - val_mae: 0.1172
Epoch 48/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0112 - mae: 0.1288 - val_loss: 0.0110 - val_mae: 0.1165
Epoch 49/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0111 - mae: 0.1281 - val_loss: 0.0109 - val_mae: 0.1165
Epoch 50/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0111 - mae: 0.1273 - val_loss: 0.0109 - val_mae: 0.1160
Epoch 51/500
33/33 [==============================] - 3s 80ms/step - loss: 0.0110 - mae: 0.1274 - val_loss: 0.0108 - val_mae: 0.1154
Epoch 52/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0110 - mae: 0.1260 - val_loss: 0.0108 - val_mae: 0.1149
Epoch 53/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0109 - mae: 0.1265 - val_loss: 0.0107 - val_mae: 0.1149
Epoch 54/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0109 - mae: 0.1256 - val_loss: 0.0107 - val_mae: 0.1141
Epoch 55/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0108 - mae: 0.1242 - val_loss: 0.0107 - val_mae: 0.1135
Epoch 56/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0108 - mae: 0.1243 - val_loss: 0.0106 - val_mae: 0.1136
Epoch 57/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0108 - mae: 0.1241 - val_loss: 0.0106 - val_mae: 0.1128
Epoch 58/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0107 - mae: 0.1233 - val_loss: 0.0106 - val_mae: 0.1125
Epoch 59/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0107 - mae: 0.1229 - val_loss: 0.0106 - val_mae: 0.1120
Epoch 60/500
33/33 [==============================] - 2s 67ms/step - loss: 0.0107 - mae: 0.1228 - val_loss: 0.0105 - val_mae: 0.1120
Epoch 61/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0107 - mae: 0.1221 - val_loss: 0.0105 - val_mae: 0.1114
Epoch 62/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0106 - mae: 0.1214 - val_loss: 0.0105 - val_mae: 0.1108
Epoch 63/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0106 - mae: 0.1215 - val_loss: 0.0104 - val_mae: 0.1109
Epoch 64/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0106 - mae: 0.1210 - val_loss: 0.0104 - val_mae: 0.1102
Epoch 65/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0106 - mae: 0.1211 - val_loss: 0.0104 - val_mae: 0.1101
Epoch 66/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0106 - mae: 0.1201 - val_loss: 0.0104 - val_mae: 0.1097
Epoch 67/500
33/33 [==============================] - 3s 81ms/step - loss: 0.0105 - mae: 0.1198 - val_loss: 0.0104 - val_mae: 0.1092
Epoch 68/500
33/33 [==============================] - 2s 58ms/step - loss: 0.0105 - mae: 0.1197 - val_loss: 0.0104 - val_mae: 0.1090
Epoch 69/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0105 - mae: 0.1193 - val_loss: 0.0103 - val_mae: 0.1089
Epoch 70/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0105 - mae: 0.1189 - val_loss: 0.0103 - val_mae: 0.1086
Epoch 71/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0105 - mae: 0.1188 - val_loss: 0.0103 - val_mae: 0.1083
Epoch 72/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0105 - mae: 0.1185 - val_loss: 0.0103 - val_mae: 0.1082
Epoch 73/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0105 - mae: 0.1183 - val_loss: 0.0103 - val_mae: 0.1079
Epoch 74/500
33/33 [==============================] - 3s 75ms/step - loss: 0.0104 - mae: 0.1179 - val_loss: 0.0103 - val_mae: 0.1076
Epoch 75/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0104 - mae: 0.1182 - val_loss: 0.0103 - val_mae: 0.1075
Epoch 76/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1176 - val_loss: 0.0103 - val_mae: 0.1071
Epoch 77/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1172 - val_loss: 0.0103 - val_mae: 0.1071
Epoch 78/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1175 - val_loss: 0.0103 - val_mae: 0.1069
Epoch 79/500
33/33 [==============================] - 1s 43ms/step - loss: 0.0104 - mae: 0.1168 - val_loss: 0.0103 - val_mae: 0.1067
Epoch 80/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0104 - mae: 0.1168 - val_loss: 0.0103 - val_mae: 0.1065
Epoch 81/500
33/33 [==============================] - 3s 76ms/step - loss: 0.0104 - mae: 0.1167 - val_loss: 0.0102 - val_mae: 0.1065
Epoch 82/500
33/33 [==============================] - 2s 64ms/step - loss: 0.0104 - mae: 0.1165 - val_loss: 0.0103 - val_mae: 0.1063
Epoch 83/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1164 - val_loss: 0.0102 - val_mae: 0.1062
Epoch 84/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1162 - val_loss: 0.0103 - val_mae: 0.1059
Epoch 85/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1166 - val_loss: 0.0102 - val_mae: 0.1060
Epoch 86/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1160 - val_loss: 0.0103 - val_mae: 0.1056
Epoch 87/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1158 - val_loss: 0.0103 - val_mae: 0.1054
Epoch 88/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1155 - val_loss: 0.0102 - val_mae: 0.1054
Epoch 89/500
33/33 [==============================] - 2s 73ms/step - loss: 0.0104 - mae: 0.1155 - val_loss: 0.0103 - val_mae: 0.1053
Epoch 90/500
33/33 [==============================] - 2s 59ms/step - loss: 0.0104 - mae: 0.1155 - val_loss: 0.0103 - val_mae: 0.1052
Epoch 91/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1155 - val_loss: 0.0102 - val_mae: 0.1051
Epoch 92/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1152 - val_loss: 0.0103 - val_mae: 0.1049
Epoch 93/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1152 - val_loss: 0.0103 - val_mae: 0.1049
Epoch 94/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0104 - mae: 0.1152 - val_loss: 0.0103 - val_mae: 0.1047
Epoch 95/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1148 - val_loss: 0.0103 - val_mae: 0.1047
Epoch 96/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1151 - val_loss: 0.0102 - val_mae: 0.1049
Epoch 97/500
33/33 [==============================] - 2s 44ms/step - loss: 0.0104 - mae: 0.1148 - val_loss: 0.0102 - val_mae: 0.1046
Epoch 98/500
33/33 [==============================] - 3s 76ms/step - loss: 0.0104 - mae: 0.1150 - val_loss: 0.0103 - val_mae: 0.1044
Epoch 99/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1145 - val_loss: 0.0102 - val_mae: 0.1044
Epoch 100/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1144 - val_loss: 0.0103 - val_mae: 0.1043
Epoch 101/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1146 - val_loss: 0.0102 - val_mae: 0.1044
Epoch 102/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0104 - mae: 0.1145 - val_loss: 0.0102 - val_mae: 0.1042
Epoch 103/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0104 - mae: 0.1145 - val_loss: 0.0102 - val_mae: 0.1043
Epoch 104/500
33/33 [==============================] - 3s 100ms/step - loss: 0.0104 - mae: 0.1146 - val_loss: 0.0102 - val_mae: 0.1042
Epoch 105/500
33/33 [==============================] - 3s 96ms/step - loss: 0.0104 - mae: 0.1143 - val_loss: 0.0103 - val_mae: 0.1039
Epoch 106/500
33/33 [==============================] - 3s 73ms/step - loss: 0.0104 - mae: 0.1140 - val_loss: 0.0103 - val_mae: 0.1039
Epoch 107/500
33/33 [==============================] - 2s 66ms/step - loss: 0.0104 - mae: 0.1143 - val_loss: 0.0103 - val_mae: 0.1039
Epoch 108/500
33/33 [==============================] - 2s 64ms/step - loss: 0.0104 - mae: 0.1141 - val_loss: 0.0103 - val_mae: 0.1038
Epoch 109/500
33/33 [==============================] - 3s 78ms/step - loss: 0.0104 - mae: 0.1142 - val_loss: 0.0102 - val_mae: 0.1039
Epoch 110/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1140 - val_loss: 0.0103 - val_mae: 0.1038
Epoch 111/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0104 - mae: 0.1140 - val_loss: 0.0102 - val_mae: 0.1038
Epoch 112/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0104 - mae: 0.1141 - val_loss: 0.0103 - val_mae: 0.1036
Epoch 113/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1142 - val_loss: 0.0102 - val_mae: 0.1040
Epoch 114/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1138 - val_loss: 0.0103 - val_mae: 0.1037
Epoch 115/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1139 - val_loss: 0.0103 - val_mae: 0.1036
Epoch 116/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1137 - val_loss: 0.0103 - val_mae: 0.1036
Epoch 117/500
33/33 [==============================] - 2s 56ms/step - loss: 0.0104 - mae: 0.1139 - val_loss: 0.0103 - val_mae: 0.1035
Epoch 118/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0104 - mae: 0.1139 - val_loss: 0.0102 - val_mae: 0.1036
Epoch 119/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1137 - val_loss: 0.0103 - val_mae: 0.1034
Epoch 120/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1137 - val_loss: 0.0102 - val_mae: 0.1034
Epoch 121/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1138 - val_loss: 0.0103 - val_mae: 0.1034
Epoch 122/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1134 - val_loss: 0.0103 - val_mae: 0.1033
Epoch 123/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1137 - val_loss: 0.0103 - val_mae: 0.1034
Epoch 124/500
33/33 [==============================] - 2s 44ms/step - loss: 0.0104 - mae: 0.1136 - val_loss: 0.0103 - val_mae: 0.1034
Epoch 125/500
33/33 [==============================] - 3s 99ms/step - loss: 0.0104 - mae: 0.1136 - val_loss: 0.0102 - val_mae: 0.1034
Epoch 126/500
33/33 [==============================] - 3s 95ms/step - loss: 0.0104 - mae: 0.1138 - val_loss: 0.0102 - val_mae: 0.1033
Epoch 127/500
33/33 [==============================] - 4s 124ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0103 - val_mae: 0.1033
Epoch 128/500
33/33 [==============================] - 3s 78ms/step - loss: 0.0104 - mae: 0.1138 - val_loss: 0.0102 - val_mae: 0.1033
Epoch 129/500
33/33 [==============================] - 4s 106ms/step - loss: 0.0104 - mae: 0.1136 - val_loss: 0.0102 - val_mae: 0.1034
Epoch 130/500
33/33 [==============================] - 3s 78ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0103 - val_mae: 0.1032
Epoch 131/500
33/33 [==============================] - 3s 78ms/step - loss: 0.0104 - mae: 0.1136 - val_loss: 0.0102 - val_mae: 0.1033
Epoch 132/500
33/33 [==============================] - 3s 85ms/step - loss: 0.0104 - mae: 0.1135 - val_loss: 0.0103 - val_mae: 0.1032
Epoch 133/500
33/33 [==============================] - 3s 85ms/step - loss: 0.0104 - mae: 0.1136 - val_loss: 0.0103 - val_mae: 0.1032
Epoch 134/500
33/33 [==============================] - 4s 114ms/step - loss: 0.0104 - mae: 0.1135 - val_loss: 0.0102 - val_mae: 0.1032
Epoch 135/500
33/33 [==============================] - 3s 77ms/step - loss: 0.0104 - mae: 0.1137 - val_loss: 0.0102 - val_mae: 0.1034
Epoch 136/500
33/33 [==============================] - 3s 78ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1032
Epoch 137/500
33/33 [==============================] - 3s 90ms/step - loss: 0.0103 - mae: 0.1133 - val_loss: 0.0103 - val_mae: 0.1032
Epoch 138/500
33/33 [==============================] - 4s 131ms/step - loss: 0.0103 - mae: 0.1135 - val_loss: 0.0103 - val_mae: 0.1031
Epoch 139/500
33/33 [==============================] - 3s 102ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0102 - val_mae: 0.1033
Epoch 140/500
33/33 [==============================] - 3s 79ms/step - loss: 0.0103 - mae: 0.1134 - val_loss: 0.0103 - val_mae: 0.1031
Epoch 141/500
33/33 [==============================] - 3s 86ms/step - loss: 0.0104 - mae: 0.1136 - val_loss: 0.0103 - val_mae: 0.1032
Epoch 142/500
33/33 [==============================] - 3s 80ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1031
Epoch 143/500
33/33 [==============================] - 2s 68ms/step - loss: 0.0104 - mae: 0.1135 - val_loss: 0.0102 - val_mae: 0.1031
Epoch 144/500
33/33 [==============================] - 2s 59ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1031
Epoch 145/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1133 - val_loss: 0.0103 - val_mae: 0.1031
Epoch 146/500
33/33 [==============================] - 2s 54ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1031
Epoch 147/500
33/33 [==============================] - 2s 65ms/step - loss: 0.0104 - mae: 0.1136 - val_loss: 0.0102 - val_mae: 0.1032
Epoch 148/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1030
Epoch 149/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1031
Epoch 150/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0103 - val_mae: 0.1031
Epoch 151/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1031
Epoch 152/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1031
Epoch 153/500
33/33 [==============================] - 2s 46ms/step - loss: 0.0103 - mae: 0.1133 - val_loss: 0.0103 - val_mae: 0.1030
Epoch 154/500
33/33 [==============================] - 2s 67ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 155/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1030
Epoch 156/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1030
Epoch 157/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0103 - val_mae: 0.1030
Epoch 158/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 159/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1030
Epoch 160/500
33/33 [==============================] - 2s 60ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0102 - val_mae: 0.1031
Epoch 161/500
33/33 [==============================] - 2s 63ms/step - loss: 0.0103 - mae: 0.1133 - val_loss: 0.0103 - val_mae: 0.1030
Epoch 162/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1030
Epoch 163/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 164/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1030
Epoch 165/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 166/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 167/500
33/33 [==============================] - 1s 43ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 168/500
33/33 [==============================] - 3s 98ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 169/500
33/33 [==============================] - 3s 75ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1029
Epoch 170/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 171/500
33/33 [==============================] - 3s 77ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 172/500
33/33 [==============================] - 2s 46ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 173/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 174/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 175/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 176/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 177/500
33/33 [==============================] - 1s 43ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 178/500
33/33 [==============================] - 3s 75ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1029
Epoch 179/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 180/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 181/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0102 - val_mae: 0.1029
Epoch 182/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 183/500
33/33 [==============================] - 2s 53ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 184/500
33/33 [==============================] - 2s 68ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 185/500
33/33 [==============================] - 3s 76ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 186/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 187/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 188/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 189/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1031
Epoch 190/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 191/500
33/33 [==============================] - 2s 65ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 192/500
33/33 [==============================] - 3s 75ms/step - loss: 0.0104 - mae: 0.1135 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 193/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 194/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 195/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 196/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1135 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 197/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 198/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 199/500
33/33 [==============================] - 2s 74ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 200/500
33/33 [==============================] - 2s 67ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 201/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 202/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 203/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 204/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 205/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 206/500
33/33 [==============================] - 2s 73ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1031
Epoch 207/500
33/33 [==============================] - 2s 67ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 208/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 209/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 210/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1029
Epoch 211/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 212/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 213/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 214/500
33/33 [==============================] - 2s 73ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 215/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 216/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 217/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 218/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 219/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 220/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 221/500
33/33 [==============================] - 2s 71ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 222/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 223/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 224/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 225/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0102 - val_mae: 0.1029
Epoch 226/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1031
Epoch 227/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 228/500
33/33 [==============================] - 3s 75ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 229/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 230/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 231/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 232/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 233/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 234/500
33/33 [==============================] - 3s 78ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 235/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1030
Epoch 236/500
33/33 [==============================] - 2s 55ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 237/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 238/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 239/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 240/500
33/33 [==============================] - 2s 68ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1029
Epoch 241/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 242/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 243/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1127 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 244/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 245/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1134 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 246/500
33/33 [==============================] - 2s 68ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 247/500
33/33 [==============================] - 2s 64ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1029
Epoch 248/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 249/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 250/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 251/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0102 - val_mae: 0.1031
Epoch 252/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 253/500
33/33 [==============================] - 3s 94ms/step - loss: 0.0104 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 254/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 255/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 256/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 257/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 258/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 259/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 260/500
33/33 [==============================] - 2s 67ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 261/500
33/33 [==============================] - 3s 74ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 262/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 263/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 264/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 265/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 266/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 267/500
33/33 [==============================] - 3s 77ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 268/500
33/33 [==============================] - 2s 45ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 269/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 270/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 271/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 272/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 273/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 274/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 275/500
33/33 [==============================] - 3s 77ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 276/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 277/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 278/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 279/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 280/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 281/500
33/33 [==============================] - 2s 57ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 282/500
33/33 [==============================] - 4s 105ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 283/500
33/33 [==============================] - 3s 98ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 284/500
33/33 [==============================] - 2s 70ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 285/500
33/33 [==============================] - 3s 74ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 286/500
33/33 [==============================] - 3s 89ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 287/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1132 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 288/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 289/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1132 - val_loss: 0.0102 - val_mae: 0.1029
Epoch 290/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 291/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 292/500
33/33 [==============================] - 2s 60ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 293/500
33/33 [==============================] - 3s 78ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 294/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 295/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 296/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1126 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 297/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0102 - val_mae: 0.1029
Epoch 298/500
33/33 [==============================] - 2s 53ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 299/500
33/33 [==============================] - 3s 76ms/step - loss: 0.0104 - mae: 0.1131 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 300/500
33/33 [==============================] - 2s 45ms/step - loss: 0.0103 - mae: 0.1127 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 301/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1029
Epoch 302/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 303/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0104 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1028
Epoch 304/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0104 - mae: 0.1133 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 305/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 306/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1129 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 307/500
33/33 [==============================] - 3s 79ms/step - loss: 0.0103 - mae: 0.1128 - val_loss: 0.0103 - val_mae: 0.1029
Epoch 308/500
33/33 [==============================] - 1s 44ms/step - loss: 0.0104 - mae: 0.1129 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 309/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0103 - mae: 0.1131 - val_loss: 0.0102 - val_mae: 0.1030
Epoch 310/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0103 - mae: 0.1130 - val_loss: 0.0103 - val_mae: 0.1029
3/3 [==============================] - 0s 12ms/step - loss: 0.0103 - mae: 0.1028</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>102805.45055866241</code></pre>
</div>
</div>
<p>Its validation MAE is greater than 100,000! That was to be expected, for two reasons:</p>
<ol type="1">
<li><p>The model only has a single recurrent neuron, so the only data it can use to make a prediction at each time step is the input value at the current time step and the output value from the previous time step. That’s not much to go on! In other words, the RNN’s memory is extremely limited: it’s just a single number, its previous output. And let’s count how many parameters this model has: since there’s just one recurrent neuron with only two input values, the whole model only has three parameters (two weights plus a bias term). That’s far from enough for this time series. In contrast, our previous model could look at all 56 previous values at once, and it had a total of 57 parameters.</p></li>
<li><p>The time series actually contains values from 0 to about 1.4, but since the default activation function is <code>tanh</code>, the recurrent layer can only output values between –1 and +1. There’s no way it can predict values between 1.0 and 1.4.</p></li>
</ol>
<p>Let’s fix both of these issues: we will create a model with a larger recurrent layer, containing 32 recurrent neurons, and we will add a dense output layer on top of it with a single output neuron and no activation function. The recurrent layer will be able to carry much more information from one time step to the next, and the <code>dense</code> output layer will project the final output from 32 dimensions down to 1, without any value range constraints:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:23,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683350420113,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="4143e08d-a003-4b8c-e440-d52899e39ece">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility</span></span>
<span id="cb50-2"><a href="#cb50-2"></a>univar_model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb50-3"><a href="#cb50-3"></a>    tf.keras.layers.SimpleRNN(<span class="dv">32</span>, input_shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">1</span>]), <span class="co"># 32*(1+1) + 32*32</span></span>
<span id="cb50-4"><a href="#cb50-4"></a>    tf.keras.layers.Dense(<span class="dv">1</span>)  <span class="co"># no activation function by default, 1*(32+1)</span></span>
<span id="cb50-5"><a href="#cb50-5"></a>])</span>
<span id="cb50-6"><a href="#cb50-6"></a></span>
<span id="cb50-7"><a href="#cb50-7"></a>univar_model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn_1 (SimpleRNN)    (None, 32)                1088      
                                                                 
 dense_1 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 1,121
Trainable params: 1,121
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:270481,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683350690589,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="759be025-71ef-477c-8877-adccf7fbfa91">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1"></a>fit_and_evaluate(univar_model, train_ds, valid_ds, learning_rate<span class="op">=</span><span class="fl">0.05</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/500
33/33 [==============================] - 3s 44ms/step - loss: 0.0358 - mae: 0.2027 - val_loss: 0.0115 - val_mae: 0.1353
Epoch 2/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0059 - mae: 0.0787 - val_loss: 0.0050 - val_mae: 0.0766
Epoch 3/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0046 - mae: 0.0667 - val_loss: 0.0026 - val_mae: 0.0507
Epoch 4/500
33/33 [==============================] - 2s 71ms/step - loss: 0.0038 - mae: 0.0592 - val_loss: 0.0040 - val_mae: 0.0698
Epoch 5/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0036 - mae: 0.0572 - val_loss: 0.0020 - val_mae: 0.0393
Epoch 6/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0486 - val_loss: 0.0030 - val_mae: 0.0509
Epoch 7/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0530 - val_loss: 0.0026 - val_mae: 0.0511
Epoch 8/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0031 - mae: 0.0499 - val_loss: 0.0021 - val_mae: 0.0356
Epoch 9/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0028 - mae: 0.0459 - val_loss: 0.0030 - val_mae: 0.0487
Epoch 10/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0032 - mae: 0.0516 - val_loss: 0.0019 - val_mae: 0.0310
Epoch 11/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0028 - mae: 0.0456 - val_loss: 0.0021 - val_mae: 0.0345
Epoch 12/500
33/33 [==============================] - 1s 44ms/step - loss: 0.0030 - mae: 0.0476 - val_loss: 0.0021 - val_mae: 0.0371
Epoch 13/500
33/33 [==============================] - 3s 79ms/step - loss: 0.0030 - mae: 0.0499 - val_loss: 0.0023 - val_mae: 0.0444
Epoch 14/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0029 - mae: 0.0491 - val_loss: 0.0023 - val_mae: 0.0413
Epoch 15/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0028 - mae: 0.0457 - val_loss: 0.0030 - val_mae: 0.0544
Epoch 16/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0029 - mae: 0.0489 - val_loss: 0.0020 - val_mae: 0.0329
Epoch 17/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0027 - mae: 0.0456 - val_loss: 0.0026 - val_mae: 0.0429
Epoch 18/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0027 - val_mae: 0.0530
Epoch 19/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0028 - mae: 0.0465 - val_loss: 0.0021 - val_mae: 0.0398
Epoch 20/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0029 - mae: 0.0488 - val_loss: 0.0021 - val_mae: 0.0384
Epoch 21/500
33/33 [==============================] - 3s 76ms/step - loss: 0.0028 - mae: 0.0470 - val_loss: 0.0023 - val_mae: 0.0385
Epoch 22/500
33/33 [==============================] - 2s 45ms/step - loss: 0.0027 - mae: 0.0441 - val_loss: 0.0023 - val_mae: 0.0383
Epoch 23/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0028 - mae: 0.0464 - val_loss: 0.0023 - val_mae: 0.0419
Epoch 24/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0498 - val_loss: 0.0020 - val_mae: 0.0371
Epoch 25/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0027 - mae: 0.0447 - val_loss: 0.0022 - val_mae: 0.0399
Epoch 26/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0027 - mae: 0.0465 - val_loss: 0.0020 - val_mae: 0.0341
Epoch 27/500
33/33 [==============================] - 2s 44ms/step - loss: 0.0026 - mae: 0.0437 - val_loss: 0.0025 - val_mae: 0.0410
Epoch 28/500
33/33 [==============================] - 3s 77ms/step - loss: 0.0026 - mae: 0.0432 - val_loss: 0.0019 - val_mae: 0.0334
Epoch 29/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0517 - val_loss: 0.0029 - val_mae: 0.0558
Epoch 30/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0512 - val_loss: 0.0024 - val_mae: 0.0408
Epoch 31/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0437 - val_loss: 0.0021 - val_mae: 0.0395
Epoch 32/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0450 - val_loss: 0.0021 - val_mae: 0.0400
Epoch 33/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0028 - mae: 0.0458 - val_loss: 0.0034 - val_mae: 0.0594
Epoch 34/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0515 - val_loss: 0.0022 - val_mae: 0.0405
Epoch 35/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0032 - mae: 0.0543 - val_loss: 0.0039 - val_mae: 0.0655
Epoch 36/500
33/33 [==============================] - 2s 60ms/step - loss: 0.0032 - mae: 0.0529 - val_loss: 0.0026 - val_mae: 0.0519
Epoch 37/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0449 - val_loss: 0.0018 - val_mae: 0.0337
Epoch 38/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0027 - mae: 0.0460 - val_loss: 0.0020 - val_mae: 0.0356
Epoch 39/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0426 - val_loss: 0.0019 - val_mae: 0.0352
Epoch 40/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0029 - mae: 0.0485 - val_loss: 0.0020 - val_mae: 0.0373
Epoch 41/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0441 - val_loss: 0.0021 - val_mae: 0.0413
Epoch 42/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0418 - val_loss: 0.0019 - val_mae: 0.0346
Epoch 43/500
33/33 [==============================] - 2s 70ms/step - loss: 0.0025 - mae: 0.0446 - val_loss: 0.0021 - val_mae: 0.0358
Epoch 44/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0422 - val_loss: 0.0019 - val_mae: 0.0327
Epoch 45/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0431 - val_loss: 0.0022 - val_mae: 0.0415
Epoch 46/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0025 - mae: 0.0435 - val_loss: 0.0021 - val_mae: 0.0422
Epoch 47/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0434 - val_loss: 0.0023 - val_mae: 0.0397
Epoch 48/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0538 - val_loss: 0.0019 - val_mae: 0.0318
Epoch 49/500
33/33 [==============================] - 2s 60ms/step - loss: 0.0025 - mae: 0.0418 - val_loss: 0.0020 - val_mae: 0.0349
Epoch 50/500
33/33 [==============================] - 3s 74ms/step - loss: 0.0026 - mae: 0.0446 - val_loss: 0.0026 - val_mae: 0.0452
Epoch 51/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0025 - mae: 0.0433 - val_loss: 0.0022 - val_mae: 0.0400
Epoch 52/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0024 - mae: 0.0441 - val_loss: 0.0021 - val_mae: 0.0343
Epoch 53/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0025 - mae: 0.0421 - val_loss: 0.0024 - val_mae: 0.0451
Epoch 54/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0024 - mae: 0.0424 - val_loss: 0.0025 - val_mae: 0.0432
Epoch 55/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0027 - mae: 0.0473 - val_loss: 0.0021 - val_mae: 0.0349
Epoch 56/500
33/33 [==============================] - 2s 54ms/step - loss: 0.0027 - mae: 0.0441 - val_loss: 0.0023 - val_mae: 0.0425
Epoch 57/500
33/33 [==============================] - 2s 63ms/step - loss: 0.0025 - mae: 0.0423 - val_loss: 0.0021 - val_mae: 0.0395
Epoch 58/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0026 - mae: 0.0447 - val_loss: 0.0024 - val_mae: 0.0431
Epoch 59/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0029 - mae: 0.0487 - val_loss: 0.0019 - val_mae: 0.0291
Epoch 60/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0026 - mae: 0.0428 - val_loss: 0.0018 - val_mae: 0.0328
Epoch 61/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0025 - mae: 0.0400 - val_loss: 0.0018 - val_mae: 0.0308
Epoch 62/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0027 - mae: 0.0480 - val_loss: 0.0018 - val_mae: 0.0306
Epoch 63/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0026 - mae: 0.0437 - val_loss: 0.0022 - val_mae: 0.0401
Epoch 64/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0445 - val_loss: 0.0020 - val_mae: 0.0358
Epoch 65/500
33/33 [==============================] - 2s 71ms/step - loss: 0.0024 - mae: 0.0435 - val_loss: 0.0026 - val_mae: 0.0452
Epoch 66/500
33/33 [==============================] - 2s 60ms/step - loss: 0.0024 - mae: 0.0423 - val_loss: 0.0028 - val_mae: 0.0504
Epoch 67/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0418 - val_loss: 0.0019 - val_mae: 0.0361
Epoch 68/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0411 - val_loss: 0.0018 - val_mae: 0.0342
Epoch 69/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0426 - val_loss: 0.0021 - val_mae: 0.0355
Epoch 70/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0027 - mae: 0.0470 - val_loss: 0.0025 - val_mae: 0.0461
Epoch 71/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0467 - val_loss: 0.0025 - val_mae: 0.0445
Epoch 72/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0023 - mae: 0.0408 - val_loss: 0.0025 - val_mae: 0.0473
Epoch 73/500
33/33 [==============================] - 2s 58ms/step - loss: 0.0026 - mae: 0.0434 - val_loss: 0.0026 - val_mae: 0.0479
Epoch 74/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0025 - mae: 0.0427 - val_loss: 0.0025 - val_mae: 0.0474
Epoch 75/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0431 - val_loss: 0.0020 - val_mae: 0.0335
Epoch 76/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0027 - mae: 0.0483 - val_loss: 0.0028 - val_mae: 0.0505
Epoch 77/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0420 - val_loss: 0.0032 - val_mae: 0.0541
Epoch 78/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0024 - mae: 0.0407 - val_loss: 0.0023 - val_mae: 0.0395
Epoch 79/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0026 - mae: 0.0430 - val_loss: 0.0017 - val_mae: 0.0292
Epoch 80/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0419 - val_loss: 0.0020 - val_mae: 0.0371
Epoch 81/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0026 - mae: 0.0450 - val_loss: 0.0027 - val_mae: 0.0543
Epoch 82/500
33/33 [==============================] - 2s 64ms/step - loss: 0.0028 - mae: 0.0482 - val_loss: 0.0024 - val_mae: 0.0476
Epoch 83/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0023 - mae: 0.0404 - val_loss: 0.0019 - val_mae: 0.0353
Epoch 84/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0025 - mae: 0.0431 - val_loss: 0.0031 - val_mae: 0.0494
Epoch 85/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0416 - val_loss: 0.0020 - val_mae: 0.0318
Epoch 86/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0025 - mae: 0.0416 - val_loss: 0.0019 - val_mae: 0.0349
Epoch 87/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0412 - val_loss: 0.0018 - val_mae: 0.0294
Epoch 88/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0401 - val_loss: 0.0019 - val_mae: 0.0345
Epoch 89/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0434 - val_loss: 0.0022 - val_mae: 0.0390
Epoch 90/500
33/33 [==============================] - 2s 71ms/step - loss: 0.0024 - mae: 0.0412 - val_loss: 0.0018 - val_mae: 0.0316
Epoch 91/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0024 - mae: 0.0414 - val_loss: 0.0019 - val_mae: 0.0332
Epoch 92/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0430 - val_loss: 0.0022 - val_mae: 0.0425
Epoch 93/500
33/33 [==============================] - 1s 37ms/step - loss: 0.0024 - mae: 0.0426 - val_loss: 0.0018 - val_mae: 0.0297
Epoch 94/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0413 - val_loss: 0.0019 - val_mae: 0.0342
Epoch 95/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0027 - mae: 0.0456 - val_loss: 0.0023 - val_mae: 0.0448
Epoch 96/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0425 - val_loss: 0.0025 - val_mae: 0.0467
Epoch 97/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0023 - mae: 0.0395 - val_loss: 0.0021 - val_mae: 0.0363
Epoch 98/500
33/33 [==============================] - 2s 63ms/step - loss: 0.0024 - mae: 0.0412 - val_loss: 0.0026 - val_mae: 0.0469
Epoch 99/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0024 - mae: 0.0433 - val_loss: 0.0017 - val_mae: 0.0285
Epoch 100/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0421 - val_loss: 0.0027 - val_mae: 0.0508
Epoch 101/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0435 - val_loss: 0.0019 - val_mae: 0.0340
Epoch 102/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0023 - mae: 0.0419 - val_loss: 0.0025 - val_mae: 0.0467
Epoch 103/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0409 - val_loss: 0.0025 - val_mae: 0.0466
Epoch 104/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0024 - mae: 0.0408 - val_loss: 0.0022 - val_mae: 0.0393
Epoch 105/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0023 - mae: 0.0399 - val_loss: 0.0023 - val_mae: 0.0444
Epoch 106/500
33/33 [==============================] - 2s 64ms/step - loss: 0.0025 - mae: 0.0432 - val_loss: 0.0019 - val_mae: 0.0343
Epoch 107/500
33/33 [==============================] - 2s 60ms/step - loss: 0.0024 - mae: 0.0430 - val_loss: 0.0024 - val_mae: 0.0414
Epoch 108/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0451 - val_loss: 0.0024 - val_mae: 0.0402
Epoch 109/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0437 - val_loss: 0.0022 - val_mae: 0.0371
Epoch 110/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0023 - mae: 0.0410 - val_loss: 0.0022 - val_mae: 0.0334
Epoch 111/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0023 - mae: 0.0398 - val_loss: 0.0026 - val_mae: 0.0464
Epoch 112/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0023 - mae: 0.0418 - val_loss: 0.0018 - val_mae: 0.0319
Epoch 113/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0024 - mae: 0.0435 - val_loss: 0.0022 - val_mae: 0.0421
Epoch 114/500
33/33 [==============================] - 3s 73ms/step - loss: 0.0028 - mae: 0.0488 - val_loss: 0.0023 - val_mae: 0.0439
Epoch 115/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0025 - mae: 0.0456 - val_loss: 0.0018 - val_mae: 0.0322
Epoch 116/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0395 - val_loss: 0.0018 - val_mae: 0.0339
Epoch 117/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0426 - val_loss: 0.0024 - val_mae: 0.0438
Epoch 118/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0022 - mae: 0.0400 - val_loss: 0.0021 - val_mae: 0.0351
Epoch 119/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0023 - mae: 0.0400 - val_loss: 0.0025 - val_mae: 0.0457
Epoch 120/500
33/33 [==============================] - 3s 75ms/step - loss: 0.0025 - mae: 0.0413 - val_loss: 0.0018 - val_mae: 0.0301
Epoch 121/500
33/33 [==============================] - 2s 43ms/step - loss: 0.0026 - mae: 0.0451 - val_loss: 0.0020 - val_mae: 0.0361
Epoch 122/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0023 - mae: 0.0401 - val_loss: 0.0019 - val_mae: 0.0353
Epoch 123/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0420 - val_loss: 0.0025 - val_mae: 0.0466
Epoch 124/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0025 - mae: 0.0432 - val_loss: 0.0023 - val_mae: 0.0409
Epoch 125/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0025 - mae: 0.0453 - val_loss: 0.0030 - val_mae: 0.0560
Epoch 126/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0028 - mae: 0.0451 - val_loss: 0.0024 - val_mae: 0.0455
Epoch 127/500
33/33 [==============================] - 3s 76ms/step - loss: 0.0024 - mae: 0.0436 - val_loss: 0.0020 - val_mae: 0.0351
Epoch 128/500
33/33 [==============================] - 2s 52ms/step - loss: 0.0023 - mae: 0.0410 - val_loss: 0.0023 - val_mae: 0.0401
Epoch 129/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0024 - mae: 0.0434 - val_loss: 0.0021 - val_mae: 0.0330
Epoch 130/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0024 - mae: 0.0427 - val_loss: 0.0018 - val_mae: 0.0317
Epoch 131/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0425 - val_loss: 0.0023 - val_mae: 0.0399
Epoch 132/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0023 - mae: 0.0395 - val_loss: 0.0023 - val_mae: 0.0385
Epoch 133/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0023 - mae: 0.0395 - val_loss: 0.0018 - val_mae: 0.0302
Epoch 134/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0023 - mae: 0.0406 - val_loss: 0.0025 - val_mae: 0.0485
Epoch 135/500
33/33 [==============================] - 2s 66ms/step - loss: 0.0025 - mae: 0.0451 - val_loss: 0.0027 - val_mae: 0.0497
Epoch 136/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0022 - mae: 0.0399 - val_loss: 0.0022 - val_mae: 0.0393
Epoch 137/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0024 - mae: 0.0423 - val_loss: 0.0020 - val_mae: 0.0364
Epoch 138/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0022 - mae: 0.0404 - val_loss: 0.0019 - val_mae: 0.0313
Epoch 139/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0434 - val_loss: 0.0020 - val_mae: 0.0347
Epoch 140/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0027 - mae: 0.0472 - val_loss: 0.0032 - val_mae: 0.0534
Epoch 141/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0024 - mae: 0.0435 - val_loss: 0.0028 - val_mae: 0.0496
Epoch 142/500
33/33 [==============================] - 2s 70ms/step - loss: 0.0026 - mae: 0.0452 - val_loss: 0.0042 - val_mae: 0.0742
Epoch 143/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0024 - mae: 0.0431 - val_loss: 0.0027 - val_mae: 0.0442
Epoch 144/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0023 - mae: 0.0420 - val_loss: 0.0026 - val_mae: 0.0498
Epoch 145/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0443 - val_loss: 0.0027 - val_mae: 0.0476
Epoch 146/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0027 - mae: 0.0456 - val_loss: 0.0032 - val_mae: 0.0578
Epoch 147/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0022 - mae: 0.0404 - val_loss: 0.0021 - val_mae: 0.0331
Epoch 148/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0022 - mae: 0.0393 - val_loss: 0.0024 - val_mae: 0.0390
Epoch 149/500
33/33 [==============================] - 2s 68ms/step - loss: 0.0022 - mae: 0.0400 - val_loss: 0.0020 - val_mae: 0.0360
3/3 [==============================] - 0s 17ms/step - loss: 0.0017 - mae: 0.0285</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>28464.557603001595</code></pre>
</div>
</div>
<p>Its validation MAE reaches 28,464. That’s the best model we’ve trained so far, and it even beats the ARIMA model: we’re doing pretty well! We’ve only normalized the time series, without removing trend and seasonality, and yet the model still performs well. However, to get the best performance, you may want to try making the time series more stationary; for example, using differencing.</p>
</section>
<section id="forecasting-using-a-deep-rnn" class="level3" data-number="10.2.7">
<h3 data-number="10.2.7" class="anchored" data-anchor-id="forecasting-using-a-deep-rnn"><span class="header-section-number">10.2.7</span> Forecasting Using a Deep RNN</h3>
<p>Implementing a deep RNN with <code>tf.keras</code> is quite simple: just stack recurrent layers. In this example, we use three <code>SimpleRNN</code> layers. Make sure to set <code>return_sequences=True</code> for all recurrent layers (except the last one, if you only care about the last output). <strong>If you don’t, they will output a 2D array (containing only the output of the last time step)</strong> instead of a 3D array (containing outputs for all time steps), and the next recurrent layer will complain that you are not feeding it sequences in the expected 3D format.</p>
<p>The first two are sequence-to-sequence layers, and the last one is a sequence-to-vector layer. Finally, the <code>Dense</code> layer produces the model’s forecast (you can think of it as a vector-to-vector layer).</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:781,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683350691361,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="8fb92118-262e-40e5-de09-c3b2f0957a65">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb55-2"><a href="#cb55-2"></a></span>
<span id="cb55-3"><a href="#cb55-3"></a><span class="co"># By default, recurrent layers in Keras only return the final output. </span></span>
<span id="cb55-4"><a href="#cb55-4"></a><span class="co"># To make them return one output per time step, you must set return_sequences=True</span></span>
<span id="cb55-5"><a href="#cb55-5"></a><span class="co"># number of parameters https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#recurrent-neural-networks-with-hidden-states</span></span>
<span id="cb55-6"><a href="#cb55-6"></a></span>
<span id="cb55-7"><a href="#cb55-7"></a>deep_model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb55-8"><a href="#cb55-8"></a>    tf.keras.layers.SimpleRNN(<span class="dv">32</span>, return_sequences<span class="op">=</span><span class="va">True</span>, input_shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">1</span>]), <span class="co">#1*32+32+32*32</span></span>
<span id="cb55-9"><a href="#cb55-9"></a>    tf.keras.layers.SimpleRNN(<span class="dv">32</span>, return_sequences<span class="op">=</span><span class="va">True</span>), <span class="co">#32*32+32+32*32</span></span>
<span id="cb55-10"><a href="#cb55-10"></a>    tf.keras.layers.SimpleRNN(<span class="dv">32</span>), <span class="co">#32*1+1</span></span>
<span id="cb55-11"><a href="#cb55-11"></a>    tf.keras.layers.Dense(<span class="dv">1</span>)</span>
<span id="cb55-12"><a href="#cb55-12"></a>])</span>
<span id="cb55-13"><a href="#cb55-13"></a></span>
<span id="cb55-14"><a href="#cb55-14"></a>deep_model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn_2 (SimpleRNN)    (None, None, 32)          1088      
                                                                 
 simple_rnn_3 (SimpleRNN)    (None, None, 32)          2080      
                                                                 
 simple_rnn_4 (SimpleRNN)    (None, 32)                2080      
                                                                 
 dense_2 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 5,281
Trainable params: 5,281
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:650876,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683351342229,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="bbec95aa-74ad-4aed-8300-aed30c7698f1">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1"></a>fit_and_evaluate(deep_model, train_ds, valid_ds, learning_rate<span class="op">=</span><span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/500
33/33 [==============================] - 7s 146ms/step - loss: 0.0300 - mae: 0.1895 - val_loss: 0.0143 - val_mae: 0.1314
Epoch 2/500
33/33 [==============================] - 6s 170ms/step - loss: 0.0144 - mae: 0.1474 - val_loss: 0.0178 - val_mae: 0.1244
Epoch 3/500
33/33 [==============================] - 5s 139ms/step - loss: 0.0102 - mae: 0.1181 - val_loss: 0.0065 - val_mae: 0.0875
Epoch 4/500
33/33 [==============================] - 7s 203ms/step - loss: 0.0066 - mae: 0.0839 - val_loss: 0.0029 - val_mae: 0.0553
Epoch 5/500
33/33 [==============================] - 5s 138ms/step - loss: 0.0057 - mae: 0.0747 - val_loss: 0.0025 - val_mae: 0.0507
Epoch 6/500
33/33 [==============================] - 7s 202ms/step - loss: 0.0050 - mae: 0.0672 - val_loss: 0.0022 - val_mae: 0.0442
Epoch 7/500
33/33 [==============================] - 5s 141ms/step - loss: 0.0044 - mae: 0.0620 - val_loss: 0.0020 - val_mae: 0.0402
Epoch 8/500
33/33 [==============================] - 7s 204ms/step - loss: 0.0041 - mae: 0.0587 - val_loss: 0.0020 - val_mae: 0.0409
Epoch 9/500
33/33 [==============================] - 5s 138ms/step - loss: 0.0040 - mae: 0.0572 - val_loss: 0.0025 - val_mae: 0.0491
Epoch 10/500
33/33 [==============================] - 5s 141ms/step - loss: 0.0042 - mae: 0.0609 - val_loss: 0.0020 - val_mae: 0.0427
Epoch 11/500
33/33 [==============================] - 7s 194ms/step - loss: 0.0039 - mae: 0.0570 - val_loss: 0.0020 - val_mae: 0.0381
Epoch 12/500
33/33 [==============================] - 5s 135ms/step - loss: 0.0042 - mae: 0.0613 - val_loss: 0.0031 - val_mae: 0.0602
Epoch 13/500
33/33 [==============================] - 7s 203ms/step - loss: 0.0037 - mae: 0.0566 - val_loss: 0.0018 - val_mae: 0.0368
Epoch 14/500
33/33 [==============================] - 4s 134ms/step - loss: 0.0035 - mae: 0.0520 - val_loss: 0.0022 - val_mae: 0.0444
Epoch 15/500
33/33 [==============================] - 5s 147ms/step - loss: 0.0035 - mae: 0.0521 - val_loss: 0.0020 - val_mae: 0.0397
Epoch 16/500
33/33 [==============================] - 6s 180ms/step - loss: 0.0034 - mae: 0.0511 - val_loss: 0.0018 - val_mae: 0.0315
Epoch 17/500
33/33 [==============================] - 5s 136ms/step - loss: 0.0035 - mae: 0.0529 - val_loss: 0.0035 - val_mae: 0.0649
Epoch 18/500
33/33 [==============================] - 7s 201ms/step - loss: 0.0041 - mae: 0.0619 - val_loss: 0.0017 - val_mae: 0.0318
Epoch 19/500
33/33 [==============================] - 5s 138ms/step - loss: 0.0034 - mae: 0.0506 - val_loss: 0.0017 - val_mae: 0.0303
Epoch 20/500
33/33 [==============================] - 5s 147ms/step - loss: 0.0031 - mae: 0.0476 - val_loss: 0.0017 - val_mae: 0.0301
Epoch 21/500
33/33 [==============================] - 6s 177ms/step - loss: 0.0030 - mae: 0.0466 - val_loss: 0.0031 - val_mae: 0.0597
Epoch 22/500
33/33 [==============================] - 5s 139ms/step - loss: 0.0032 - mae: 0.0506 - val_loss: 0.0018 - val_mae: 0.0301
Epoch 23/500
33/33 [==============================] - 7s 206ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0017 - val_mae: 0.0307
Epoch 24/500
33/33 [==============================] - 5s 139ms/step - loss: 0.0030 - mae: 0.0471 - val_loss: 0.0019 - val_mae: 0.0347
Epoch 25/500
33/33 [==============================] - 5s 159ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0018 - val_mae: 0.0311
Epoch 26/500
33/33 [==============================] - 5s 144ms/step - loss: 0.0030 - mae: 0.0472 - val_loss: 0.0023 - val_mae: 0.0427
Epoch 27/500
33/33 [==============================] - 7s 209ms/step - loss: 0.0031 - mae: 0.0496 - val_loss: 0.0017 - val_mae: 0.0330
Epoch 28/500
33/33 [==============================] - 5s 136ms/step - loss: 0.0031 - mae: 0.0500 - val_loss: 0.0018 - val_mae: 0.0321
Epoch 29/500
33/33 [==============================] - 7s 204ms/step - loss: 0.0037 - mae: 0.0573 - val_loss: 0.0020 - val_mae: 0.0351
Epoch 30/500
33/33 [==============================] - 5s 136ms/step - loss: 0.0031 - mae: 0.0504 - val_loss: 0.0018 - val_mae: 0.0348
Epoch 31/500
33/33 [==============================] - 7s 201ms/step - loss: 0.0032 - mae: 0.0524 - val_loss: 0.0018 - val_mae: 0.0346
Epoch 32/500
33/33 [==============================] - 5s 138ms/step - loss: 0.0030 - mae: 0.0475 - val_loss: 0.0027 - val_mae: 0.0504
Epoch 33/500
33/33 [==============================] - 7s 203ms/step - loss: 0.0030 - mae: 0.0465 - val_loss: 0.0017 - val_mae: 0.0287
Epoch 34/500
33/33 [==============================] - 5s 140ms/step - loss: 0.0029 - mae: 0.0479 - val_loss: 0.0020 - val_mae: 0.0361
Epoch 35/500
33/33 [==============================] - 5s 136ms/step - loss: 0.0028 - mae: 0.0444 - val_loss: 0.0018 - val_mae: 0.0307
Epoch 36/500
33/33 [==============================] - 7s 194ms/step - loss: 0.0038 - mae: 0.0613 - val_loss: 0.0030 - val_mae: 0.0608
Epoch 37/500
33/33 [==============================] - 5s 139ms/step - loss: 0.0038 - mae: 0.0607 - val_loss: 0.0018 - val_mae: 0.0345
Epoch 38/500
33/33 [==============================] - 7s 206ms/step - loss: 0.0029 - mae: 0.0478 - val_loss: 0.0020 - val_mae: 0.0387
Epoch 39/500
33/33 [==============================] - 5s 142ms/step - loss: 0.0028 - mae: 0.0461 - val_loss: 0.0017 - val_mae: 0.0330
Epoch 40/500
33/33 [==============================] - 13s 411ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0020 - val_mae: 0.0357
Epoch 41/500
33/33 [==============================] - 9s 255ms/step - loss: 0.0029 - mae: 0.0468 - val_loss: 0.0017 - val_mae: 0.0313
Epoch 42/500
33/33 [==============================] - 7s 201ms/step - loss: 0.0028 - mae: 0.0463 - val_loss: 0.0020 - val_mae: 0.0358
Epoch 43/500
33/33 [==============================] - 5s 139ms/step - loss: 0.0027 - mae: 0.0425 - val_loss: 0.0018 - val_mae: 0.0336
Epoch 44/500
33/33 [==============================] - 5s 160ms/step - loss: 0.0029 - mae: 0.0478 - val_loss: 0.0017 - val_mae: 0.0321
Epoch 45/500
33/33 [==============================] - 6s 179ms/step - loss: 0.0030 - mae: 0.0484 - val_loss: 0.0019 - val_mae: 0.0352
Epoch 46/500
33/33 [==============================] - 14s 433ms/step - loss: 0.0030 - mae: 0.0489 - val_loss: 0.0022 - val_mae: 0.0399
Epoch 47/500
33/33 [==============================] - 12s 374ms/step - loss: 0.0029 - mae: 0.0475 - val_loss: 0.0017 - val_mae: 0.0310
Epoch 48/500
33/33 [==============================] - 12s 357ms/step - loss: 0.0027 - mae: 0.0438 - val_loss: 0.0018 - val_mae: 0.0305
Epoch 49/500
33/33 [==============================] - 5s 140ms/step - loss: 0.0028 - mae: 0.0454 - val_loss: 0.0018 - val_mae: 0.0317
Epoch 50/500
33/33 [==============================] - 8s 241ms/step - loss: 0.0026 - mae: 0.0430 - val_loss: 0.0018 - val_mae: 0.0334
Epoch 51/500
33/33 [==============================] - 11s 342ms/step - loss: 0.0027 - mae: 0.0459 - val_loss: 0.0017 - val_mae: 0.0312
Epoch 52/500
33/33 [==============================] - 8s 253ms/step - loss: 0.0027 - mae: 0.0444 - val_loss: 0.0017 - val_mae: 0.0316
Epoch 53/500
33/33 [==============================] - 7s 204ms/step - loss: 0.0029 - mae: 0.0469 - val_loss: 0.0022 - val_mae: 0.0394
Epoch 54/500
33/33 [==============================] - 5s 144ms/step - loss: 0.0027 - mae: 0.0434 - val_loss: 0.0019 - val_mae: 0.0343
Epoch 55/500
33/33 [==============================] - 6s 180ms/step - loss: 0.0027 - mae: 0.0452 - val_loss: 0.0017 - val_mae: 0.0301
Epoch 56/500
33/33 [==============================] - 7s 205ms/step - loss: 0.0026 - mae: 0.0438 - val_loss: 0.0024 - val_mae: 0.0450
Epoch 57/500
33/33 [==============================] - 5s 137ms/step - loss: 0.0027 - mae: 0.0441 - val_loss: 0.0017 - val_mae: 0.0311
Epoch 58/500
33/33 [==============================] - 5s 136ms/step - loss: 0.0027 - mae: 0.0447 - val_loss: 0.0020 - val_mae: 0.0375
Epoch 59/500
33/33 [==============================] - 8s 231ms/step - loss: 0.0025 - mae: 0.0420 - val_loss: 0.0023 - val_mae: 0.0450
Epoch 60/500
33/33 [==============================] - 13s 395ms/step - loss: 0.0026 - mae: 0.0433 - val_loss: 0.0019 - val_mae: 0.0349
Epoch 61/500
33/33 [==============================] - 10s 295ms/step - loss: 0.0026 - mae: 0.0430 - val_loss: 0.0017 - val_mae: 0.0301
Epoch 62/500
33/33 [==============================] - 5s 148ms/step - loss: 0.0027 - mae: 0.0444 - val_loss: 0.0019 - val_mae: 0.0363
Epoch 63/500
33/33 [==============================] - 5s 163ms/step - loss: 0.0026 - mae: 0.0426 - val_loss: 0.0017 - val_mae: 0.0316
Epoch 64/500
33/33 [==============================] - 5s 140ms/step - loss: 0.0025 - mae: 0.0417 - val_loss: 0.0018 - val_mae: 0.0324
Epoch 65/500
33/33 [==============================] - 7s 203ms/step - loss: 0.0025 - mae: 0.0420 - val_loss: 0.0022 - val_mae: 0.0422
Epoch 66/500
33/33 [==============================] - 5s 140ms/step - loss: 0.0027 - mae: 0.0452 - val_loss: 0.0032 - val_mae: 0.0607
Epoch 67/500
33/33 [==============================] - 12s 362ms/step - loss: 0.0030 - mae: 0.0509 - val_loss: 0.0024 - val_mae: 0.0451
Epoch 68/500
33/33 [==============================] - 12s 342ms/step - loss: 0.0029 - mae: 0.0476 - val_loss: 0.0028 - val_mae: 0.0538
Epoch 69/500
33/33 [==============================] - 8s 235ms/step - loss: 0.0031 - mae: 0.0514 - val_loss: 0.0024 - val_mae: 0.0441
Epoch 70/500
33/33 [==============================] - 5s 142ms/step - loss: 0.0030 - mae: 0.0497 - val_loss: 0.0019 - val_mae: 0.0379
Epoch 71/500
33/33 [==============================] - 7s 204ms/step - loss: 0.0033 - mae: 0.0571 - val_loss: 0.0023 - val_mae: 0.0436
Epoch 72/500
33/33 [==============================] - 5s 164ms/step - loss: 0.0029 - mae: 0.0480 - val_loss: 0.0028 - val_mae: 0.0523
Epoch 73/500
33/33 [==============================] - 6s 172ms/step - loss: 0.0027 - mae: 0.0447 - val_loss: 0.0023 - val_mae: 0.0430
Epoch 74/500
33/33 [==============================] - 7s 200ms/step - loss: 0.0028 - mae: 0.0458 - val_loss: 0.0019 - val_mae: 0.0343
Epoch 75/500
33/33 [==============================] - 5s 138ms/step - loss: 0.0026 - mae: 0.0433 - val_loss: 0.0017 - val_mae: 0.0324
Epoch 76/500
33/33 [==============================] - 12s 360ms/step - loss: 0.0026 - mae: 0.0422 - val_loss: 0.0018 - val_mae: 0.0329
Epoch 77/500
33/33 [==============================] - 12s 357ms/step - loss: 0.0028 - mae: 0.0468 - val_loss: 0.0019 - val_mae: 0.0344
Epoch 78/500
33/33 [==============================] - 5s 140ms/step - loss: 0.0029 - mae: 0.0497 - val_loss: 0.0018 - val_mae: 0.0344
Epoch 79/500
33/33 [==============================] - 5s 159ms/step - loss: 0.0027 - mae: 0.0445 - val_loss: 0.0026 - val_mae: 0.0510
Epoch 80/500
33/33 [==============================] - 8s 230ms/step - loss: 0.0031 - mae: 0.0531 - val_loss: 0.0017 - val_mae: 0.0317
Epoch 81/500
33/33 [==============================] - 5s 143ms/step - loss: 0.0025 - mae: 0.0429 - val_loss: 0.0017 - val_mae: 0.0315
Epoch 82/500
33/33 [==============================] - 7s 191ms/step - loss: 0.0024 - mae: 0.0409 - val_loss: 0.0017 - val_mae: 0.0313
Epoch 83/500
33/33 [==============================] - 6s 190ms/step - loss: 0.0025 - mae: 0.0424 - val_loss: 0.0026 - val_mae: 0.0502
3/3 [==============================] - 0s 37ms/step - loss: 0.0017 - mae: 0.0287</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>28707.98110961914</code></pre>
</div>
</div>
<p>You will find that it reaches an MAE of about 28,707. That’s better than baseline, but it doesn’t better than our “shallower” RNN. It looks like this RNN is a bit too large for our task.</p>
</section>
<section id="forecasting-multivariate-time-series" class="level3" data-number="10.2.8">
<h3 data-number="10.2.8" class="anchored" data-anchor-id="forecasting-multivariate-time-series"><span class="header-section-number">10.2.8</span> Forecasting Multivariate Time Series</h3>
<p>A great quality of neural networks is their flexibility: in particular, they can deal with multivariate time series with almost no change to their architecture. For example, let’s try to forecast the rail time series using both the bus and rail data as input. In fact, let’s also throw in the day type! Since we can always know in advance whether tomorrow is going to be a weekday, a weekend, or a holiday, we can shift the day type series one day into the future, so that the model is given tomorrow’s day type as input. For simplicity, we’ll do this processing using <code>Pandas</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1"></a>df_mulvar <span class="op">=</span> df[[<span class="st">"bus"</span>, <span class="st">"rail"</span>]] <span class="op">/</span> <span class="fl">1e6</span>  <span class="co"># use both bus &amp; rail series as input</span></span>
<span id="cb60-2"><a href="#cb60-2"></a>df_mulvar[<span class="st">"next_day_type"</span>] <span class="op">=</span> df[<span class="st">"day_type"</span>].shift(<span class="op">-</span><span class="dv">1</span>)  <span class="co"># we know tomorrow's type use it as another feature!</span></span>
<span id="cb60-3"><a href="#cb60-3"></a>df_mulvar <span class="op">=</span> pd.get_dummies(df_mulvar)  <span class="co"># one-hot encode the day type</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683351576958,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="aa97a553-a1cd-4404-d809-08a64c1c961a">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1"></a>df_mulvar.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">

  <div id="df-9dbd4511-a329-456c-9603-c3ff14ec0b8a">
    <div class="colab-df-container">
      <div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">bus</th>
<th data-quarto-table-cell-role="th">rail</th>
<th data-quarto-table-cell-role="th">next_day_type_A</th>
<th data-quarto-table-cell-role="th">next_day_type_U</th>
<th data-quarto-table-cell-role="th">next_day_type_W</th>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">date</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">2001-01-01</td>
<td>0.297192</td>
<td>0.126455</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">2001-01-02</td>
<td>0.780827</td>
<td>0.501952</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2001-01-03</td>
<td>0.824923</td>
<td>0.536432</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">2001-01-04</td>
<td>0.870021</td>
<td>0.550011</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2001-01-05</td>
<td>0.890426</td>
<td>0.557917</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>


</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-9dbd4511-a329-456c-9603-c3ff14ec0b8a')" title="Convert this dataframe to an interactive table." style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"></path>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"></path><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"></path>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-9dbd4511-a329-456c-9603-c3ff14ec0b8a button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-9dbd4511-a329-456c-9603-c3ff14ec0b8a');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<p>Now <code>df_mulvar</code> is a DataFrame with five columns: <strong>the bus and rail data, plus three columns containing the one-hot encoding of the next day’s type (recall that there are three possible day types, <code>W</code>, <code>A</code>, and <code>U</code>)</strong>. Next we can proceed much like we did earlier. First we split the data into three periods, for training, validation, and testing:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1"></a>mulvar_train <span class="op">=</span> df_mulvar[<span class="st">"2016-01"</span>:<span class="st">"2018-12"</span>]</span>
<span id="cb62-2"><a href="#cb62-2"></a>mulvar_valid <span class="op">=</span> df_mulvar[<span class="st">"2019-01"</span>:<span class="st">"2019-05"</span>]</span>
<span id="cb62-3"><a href="#cb62-3"></a>mulvar_test <span class="op">=</span> df_mulvar[<span class="st">"2019-06"</span>:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility</span></span>
<span id="cb63-2"><a href="#cb63-2"></a></span>
<span id="cb63-3"><a href="#cb63-3"></a>train_mulvar_ds <span class="op">=</span> tf.keras.utils.timeseries_dataset_from_array(</span>
<span id="cb63-4"><a href="#cb63-4"></a>    mulvar_train.to_numpy(),  <span class="co"># use all 5 columns as input</span></span>
<span id="cb63-5"><a href="#cb63-5"></a>    targets<span class="op">=</span>mulvar_train[<span class="st">"rail"</span>][seq_length:],  <span class="co"># forecast only the rail series!</span></span>
<span id="cb63-6"><a href="#cb63-6"></a>    sequence_length<span class="op">=</span>seq_length,</span>
<span id="cb63-7"><a href="#cb63-7"></a>    batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb63-8"><a href="#cb63-8"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb63-9"><a href="#cb63-9"></a>    seed<span class="op">=</span><span class="dv">42</span></span>
<span id="cb63-10"><a href="#cb63-10"></a>)</span>
<span id="cb63-11"><a href="#cb63-11"></a>valid_mulvar_ds <span class="op">=</span> tf.keras.utils.timeseries_dataset_from_array(</span>
<span id="cb63-12"><a href="#cb63-12"></a>    mulvar_valid.to_numpy(),</span>
<span id="cb63-13"><a href="#cb63-13"></a>    targets<span class="op">=</span>mulvar_valid[<span class="st">"rail"</span>][seq_length:],</span>
<span id="cb63-14"><a href="#cb63-14"></a>    sequence_length<span class="op">=</span>seq_length,</span>
<span id="cb63-15"><a href="#cb63-15"></a>    batch_size<span class="op">=</span><span class="dv">32</span></span>
<span id="cb63-16"><a href="#cb63-16"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility</span></span>
<span id="cb64-2"><a href="#cb64-2"></a>mulvar_model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb64-3"><a href="#cb64-3"></a>    tf.keras.layers.SimpleRNN(<span class="dv">32</span>, input_shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">5</span>]), <span class="co"># Now input has five dimension</span></span>
<span id="cb64-4"><a href="#cb64-4"></a>    tf.keras.layers.Dense(<span class="dv">1</span>)</span>
<span id="cb64-5"><a href="#cb64-5"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:244099,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683351842338,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="2daad5a0-40f7-4b68-94d9-5934ab7dc46d">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1"></a>fit_and_evaluate(mulvar_model, train_mulvar_ds, valid_mulvar_ds, learning_rate<span class="op">=</span><span class="fl">0.05</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/500
33/33 [==============================] - 3s 46ms/step - loss: 0.0528 - mae: 0.2433 - val_loss: 0.0025 - val_mae: 0.0499
Epoch 2/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0027 - mae: 0.0535 - val_loss: 0.0011 - val_mae: 0.0369
Epoch 3/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0019 - mae: 0.0423 - val_loss: 7.1735e-04 - val_mae: 0.0276
Epoch 4/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0018 - mae: 0.0431 - val_loss: 0.0016 - val_mae: 0.0471
Epoch 5/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0015 - mae: 0.0382 - val_loss: 7.3583e-04 - val_mae: 0.0283
Epoch 6/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0014 - mae: 0.0368 - val_loss: 0.0012 - val_mae: 0.0387
Epoch 7/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0014 - mae: 0.0377 - val_loss: 9.7083e-04 - val_mae: 0.0341
Epoch 8/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0353 - val_loss: 6.9914e-04 - val_mae: 0.0273
Epoch 9/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0013 - mae: 0.0349 - val_loss: 6.9447e-04 - val_mae: 0.0260
Epoch 10/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0013 - mae: 0.0365 - val_loss: 7.4059e-04 - val_mae: 0.0284
Epoch 11/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0013 - mae: 0.0352 - val_loss: 0.0018 - val_mae: 0.0513
Epoch 12/500
33/33 [==============================] - 2s 71ms/step - loss: 0.0014 - mae: 0.0379 - val_loss: 6.4515e-04 - val_mae: 0.0248
Epoch 13/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0014 - mae: 0.0385 - val_loss: 0.0014 - val_mae: 0.0436
Epoch 14/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0339 - val_loss: 6.5858e-04 - val_mae: 0.0254
Epoch 15/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0345 - val_loss: 6.5689e-04 - val_mae: 0.0251
Epoch 16/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0335 - val_loss: 0.0013 - val_mae: 0.0412
Epoch 17/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0013 - mae: 0.0354 - val_loss: 8.6519e-04 - val_mae: 0.0328
Epoch 18/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0013 - mae: 0.0362 - val_loss: 7.7581e-04 - val_mae: 0.0279
Epoch 19/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0356 - val_loss: 7.0924e-04 - val_mae: 0.0262
Epoch 20/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0013 - mae: 0.0365 - val_loss: 6.2600e-04 - val_mae: 0.0244
Epoch 21/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 5.9935e-04 - val_mae: 0.0239
Epoch 22/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0326 - val_loss: 8.8976e-04 - val_mae: 0.0331
Epoch 23/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0330 - val_loss: 6.7831e-04 - val_mae: 0.0256
Epoch 24/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0357 - val_loss: 8.2207e-04 - val_mae: 0.0294
Epoch 25/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0350 - val_loss: 6.1505e-04 - val_mae: 0.0241
Epoch 26/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0322 - val_loss: 6.4027e-04 - val_mae: 0.0254
Epoch 27/500
33/33 [==============================] - 3s 100ms/step - loss: 0.0012 - mae: 0.0337 - val_loss: 0.0016 - val_mae: 0.0484
Epoch 28/500
33/33 [==============================] - 2s 68ms/step - loss: 0.0011 - mae: 0.0331 - val_loss: 6.6656e-04 - val_mae: 0.0261
Epoch 29/500
33/33 [==============================] - 3s 79ms/step - loss: 0.0012 - mae: 0.0342 - val_loss: 7.0325e-04 - val_mae: 0.0273
Epoch 30/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0011 - mae: 0.0318 - val_loss: 7.6200e-04 - val_mae: 0.0280
Epoch 31/500
33/33 [==============================] - 4s 102ms/step - loss: 0.0014 - mae: 0.0387 - val_loss: 6.5211e-04 - val_mae: 0.0257
Epoch 32/500
33/33 [==============================] - 3s 76ms/step - loss: 0.0011 - mae: 0.0320 - val_loss: 6.1206e-04 - val_mae: 0.0243
Epoch 33/500
33/33 [==============================] - 2s 64ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 0.0011 - val_mae: 0.0370
Epoch 34/500
33/33 [==============================] - 2s 60ms/step - loss: 0.0012 - mae: 0.0345 - val_loss: 8.6551e-04 - val_mae: 0.0315
Epoch 35/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0349 - val_loss: 8.7583e-04 - val_mae: 0.0327
Epoch 36/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0011 - mae: 0.0330 - val_loss: 6.2368e-04 - val_mae: 0.0247
Epoch 37/500
33/33 [==============================] - 2s 70ms/step - loss: 0.0014 - mae: 0.0385 - val_loss: 6.2868e-04 - val_mae: 0.0252
Epoch 38/500
33/33 [==============================] - 2s 58ms/step - loss: 0.0012 - mae: 0.0342 - val_loss: 7.7667e-04 - val_mae: 0.0288
Epoch 39/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0317 - val_loss: 0.0011 - val_mae: 0.0382
Epoch 40/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0342 - val_loss: 7.2795e-04 - val_mae: 0.0266
Epoch 41/500
33/33 [==============================] - 1s 37ms/step - loss: 0.0013 - mae: 0.0364 - val_loss: 6.5642e-04 - val_mae: 0.0255
Epoch 42/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0011 - mae: 0.0319 - val_loss: 6.0994e-04 - val_mae: 0.0240
Epoch 43/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0360 - val_loss: 8.2312e-04 - val_mae: 0.0301
Epoch 44/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0013 - mae: 0.0375 - val_loss: 7.6323e-04 - val_mae: 0.0273
Epoch 45/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0013 - mae: 0.0371 - val_loss: 7.8322e-04 - val_mae: 0.0292
Epoch 46/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 9.6853e-04 - val_mae: 0.0356
Epoch 47/500
33/33 [==============================] - 1s 37ms/step - loss: 0.0011 - mae: 0.0323 - val_loss: 6.6022e-04 - val_mae: 0.0259
Epoch 48/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 8.5946e-04 - val_mae: 0.0316
Epoch 49/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0336 - val_loss: 0.0012 - val_mae: 0.0402
Epoch 50/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 9.5002e-04 - val_mae: 0.0336
Epoch 51/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 8.2247e-04 - val_mae: 0.0305
Epoch 52/500
33/33 [==============================] - 3s 76ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 8.8756e-04 - val_mae: 0.0322
Epoch 53/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0010 - mae: 0.0318 - val_loss: 0.0013 - val_mae: 0.0429
Epoch 54/500
33/33 [==============================] - 3s 87ms/step - loss: 0.0013 - mae: 0.0367 - val_loss: 7.0816e-04 - val_mae: 0.0267
Epoch 55/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0315 - val_loss: 7.1429e-04 - val_mae: 0.0270
Epoch 56/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0333 - val_loss: 7.4288e-04 - val_mae: 0.0279
Epoch 57/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0012 - mae: 0.0347 - val_loss: 6.5882e-04 - val_mae: 0.0265
Epoch 58/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0013 - mae: 0.0376 - val_loss: 5.6581e-04 - val_mae: 0.0232
Epoch 59/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0319 - val_loss: 9.7140e-04 - val_mae: 0.0356
Epoch 60/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0010 - mae: 0.0306 - val_loss: 6.6180e-04 - val_mae: 0.0267
Epoch 61/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0315 - val_loss: 9.5210e-04 - val_mae: 0.0339
Epoch 62/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0013 - mae: 0.0373 - val_loss: 7.9024e-04 - val_mae: 0.0285
Epoch 63/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0335 - val_loss: 6.9436e-04 - val_mae: 0.0266
Epoch 64/500
33/33 [==============================] - 2s 70ms/step - loss: 0.0010 - mae: 0.0310 - val_loss: 0.0011 - val_mae: 0.0387
Epoch 65/500
33/33 [==============================] - 2s 66ms/step - loss: 0.0011 - mae: 0.0329 - val_loss: 5.7811e-04 - val_mae: 0.0235
Epoch 66/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0010 - mae: 0.0302 - val_loss: 7.4761e-04 - val_mae: 0.0293
Epoch 67/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0014 - mae: 0.0376 - val_loss: 0.0019 - val_mae: 0.0549
Epoch 68/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0342 - val_loss: 5.8371e-04 - val_mae: 0.0238
Epoch 69/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0011 - mae: 0.0316 - val_loss: 0.0011 - val_mae: 0.0377
Epoch 70/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0321 - val_loss: 5.9109e-04 - val_mae: 0.0244
Epoch 71/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0344 - val_loss: 5.6596e-04 - val_mae: 0.0227
Epoch 72/500
33/33 [==============================] - 2s 65ms/step - loss: 0.0010 - mae: 0.0306 - val_loss: 8.0942e-04 - val_mae: 0.0285
Epoch 73/500
33/33 [==============================] - 2s 70ms/step - loss: 0.0010 - mae: 0.0311 - val_loss: 0.0015 - val_mae: 0.0466
Epoch 74/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0375 - val_loss: 7.6460e-04 - val_mae: 0.0291
Epoch 75/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0342 - val_loss: 0.0010 - val_mae: 0.0358
Epoch 76/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 0.0010 - val_mae: 0.0354
Epoch 77/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0339 - val_loss: 6.6938e-04 - val_mae: 0.0266
Epoch 78/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0323 - val_loss: 0.0015 - val_mae: 0.0457
Epoch 79/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0340 - val_loss: 0.0013 - val_mae: 0.0415
Epoch 80/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0350 - val_loss: 9.1183e-04 - val_mae: 0.0330
Epoch 81/500
33/33 [==============================] - 3s 76ms/step - loss: 0.0012 - mae: 0.0356 - val_loss: 9.6658e-04 - val_mae: 0.0344
Epoch 82/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0011 - mae: 0.0313 - val_loss: 8.6747e-04 - val_mae: 0.0328
Epoch 83/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0349 - val_loss: 7.4910e-04 - val_mae: 0.0275
Epoch 84/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 8.7549e-04 - val_mae: 0.0326
Epoch 85/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0010 - mae: 0.0303 - val_loss: 6.5017e-04 - val_mae: 0.0257
Epoch 86/500
33/33 [==============================] - 1s 40ms/step - loss: 9.9429e-04 - mae: 0.0295 - val_loss: 7.1371e-04 - val_mae: 0.0264
Epoch 87/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0010 - mae: 0.0314 - val_loss: 0.0012 - val_mae: 0.0398
Epoch 88/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0318 - val_loss: 6.6436e-04 - val_mae: 0.0250
Epoch 89/500
33/33 [==============================] - 2s 44ms/step - loss: 0.0011 - mae: 0.0335 - val_loss: 5.7223e-04 - val_mae: 0.0234
Epoch 90/500
33/33 [==============================] - 3s 74ms/step - loss: 0.0012 - mae: 0.0346 - val_loss: 7.2035e-04 - val_mae: 0.0285
Epoch 91/500
33/33 [==============================] - 2s 53ms/step - loss: 0.0010 - mae: 0.0304 - val_loss: 6.9319e-04 - val_mae: 0.0261
Epoch 92/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0010 - mae: 0.0301 - val_loss: 6.7648e-04 - val_mae: 0.0249
Epoch 93/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0315 - val_loss: 7.1064e-04 - val_mae: 0.0269
Epoch 94/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0010 - mae: 0.0314 - val_loss: 7.3646e-04 - val_mae: 0.0283
Epoch 95/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0010 - mae: 0.0314 - val_loss: 6.3475e-04 - val_mae: 0.0250
Epoch 96/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0013 - mae: 0.0372 - val_loss: 6.2060e-04 - val_mae: 0.0239
Epoch 97/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0010 - mae: 0.0313 - val_loss: 9.6446e-04 - val_mae: 0.0351
Epoch 98/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0011 - mae: 0.0320 - val_loss: 7.0585e-04 - val_mae: 0.0261
Epoch 99/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0321 - val_loss: 7.6712e-04 - val_mae: 0.0299
Epoch 100/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0364 - val_loss: 0.0025 - val_mae: 0.0637
Epoch 101/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0330 - val_loss: 6.2223e-04 - val_mae: 0.0241
Epoch 102/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0011 - mae: 0.0309 - val_loss: 6.4828e-04 - val_mae: 0.0254
Epoch 103/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0010 - mae: 0.0302 - val_loss: 0.0012 - val_mae: 0.0399
Epoch 104/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0335 - val_loss: 7.6277e-04 - val_mae: 0.0284
Epoch 105/500
33/33 [==============================] - 1s 40ms/step - loss: 9.6551e-04 - mae: 0.0298 - val_loss: 7.8750e-04 - val_mae: 0.0306
Epoch 106/500
33/33 [==============================] - 3s 81ms/step - loss: 9.3845e-04 - mae: 0.0287 - val_loss: 6.4166e-04 - val_mae: 0.0244
Epoch 107/500
33/33 [==============================] - 2s 70ms/step - loss: 9.5507e-04 - mae: 0.0292 - val_loss: 7.0416e-04 - val_mae: 0.0281
Epoch 108/500
33/33 [==============================] - 2s 72ms/step - loss: 9.6012e-04 - mae: 0.0294 - val_loss: 6.3996e-04 - val_mae: 0.0261
Epoch 109/500
33/33 [==============================] - 3s 78ms/step - loss: 0.0010 - mae: 0.0306 - val_loss: 6.6623e-04 - val_mae: 0.0251
Epoch 110/500
33/33 [==============================] - 3s 85ms/step - loss: 0.0010 - mae: 0.0310 - val_loss: 0.0012 - val_mae: 0.0414
Epoch 111/500
33/33 [==============================] - 3s 95ms/step - loss: 0.0011 - mae: 0.0337 - val_loss: 6.7468e-04 - val_mae: 0.0258
Epoch 112/500
33/33 [==============================] - 2s 68ms/step - loss: 0.0012 - mae: 0.0349 - val_loss: 5.8859e-04 - val_mae: 0.0232
Epoch 113/500
33/33 [==============================] - 3s 77ms/step - loss: 0.0011 - mae: 0.0324 - val_loss: 7.2965e-04 - val_mae: 0.0260
Epoch 114/500
33/33 [==============================] - 3s 83ms/step - loss: 0.0010 - mae: 0.0313 - val_loss: 6.9316e-04 - val_mae: 0.0273
Epoch 115/500
33/33 [==============================] - 4s 107ms/step - loss: 9.9251e-04 - mae: 0.0301 - val_loss: 6.8965e-04 - val_mae: 0.0250
Epoch 116/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 6.6329e-04 - val_mae: 0.0253
Epoch 117/500
33/33 [==============================] - 1s 38ms/step - loss: 9.9937e-04 - mae: 0.0310 - val_loss: 7.2512e-04 - val_mae: 0.0284
Epoch 118/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0014 - mae: 0.0393 - val_loss: 7.6992e-04 - val_mae: 0.0300
Epoch 119/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0014 - mae: 0.0397 - val_loss: 6.2056e-04 - val_mae: 0.0235
Epoch 120/500
33/33 [==============================] - 1s 41ms/step - loss: 9.5764e-04 - mae: 0.0299 - val_loss: 0.0014 - val_mae: 0.0463
Epoch 121/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0349 - val_loss: 7.6205e-04 - val_mae: 0.0291
3/3 [==============================] - 0s 13ms/step - loss: 5.6596e-04 - mae: 0.0227</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>22680.68492412567</code></pre>
</div>
</div>
<p>Notice that the only difference from the <code>univar_model</code> RNN we built earlier is the input shape: at each time step, the model now receives five inputs instead of one. This model actually reaches a validation MAE of 22,680. Now we’re making big progress!</p>
<p>In fact, it’s not too hard to make the RNN forecast both the bus and rail ridership. We can add an extra neuron in the output <code>Dense</code> layer, since it must now make two forecasts: one for tomorrow’s bus ridership, and the other for rail:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:276426,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683352118757,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="c79e78b1-a129-4c36-de5d-c9ed95e23c4e">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb68-2"><a href="#cb68-2"></a></span>
<span id="cb68-3"><a href="#cb68-3"></a>seq_length <span class="op">=</span> <span class="dv">56</span></span>
<span id="cb68-4"><a href="#cb68-4"></a>train_multask_ds <span class="op">=</span> tf.keras.utils.timeseries_dataset_from_array(</span>
<span id="cb68-5"><a href="#cb68-5"></a>    mulvar_train.to_numpy(),</span>
<span id="cb68-6"><a href="#cb68-6"></a>    targets<span class="op">=</span>mulvar_train[[<span class="st">"bus"</span>, <span class="st">"rail"</span>]][seq_length:],  <span class="co"># 2 targets per day</span></span>
<span id="cb68-7"><a href="#cb68-7"></a>    sequence_length<span class="op">=</span>seq_length,</span>
<span id="cb68-8"><a href="#cb68-8"></a>    batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb68-9"><a href="#cb68-9"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb68-10"><a href="#cb68-10"></a>    seed<span class="op">=</span><span class="dv">42</span></span>
<span id="cb68-11"><a href="#cb68-11"></a>)</span>
<span id="cb68-12"><a href="#cb68-12"></a>valid_multask_ds <span class="op">=</span> tf.keras.utils.timeseries_dataset_from_array(</span>
<span id="cb68-13"><a href="#cb68-13"></a>    mulvar_valid.to_numpy(),</span>
<span id="cb68-14"><a href="#cb68-14"></a>    targets<span class="op">=</span>mulvar_valid[[<span class="st">"bus"</span>, <span class="st">"rail"</span>]][seq_length:],</span>
<span id="cb68-15"><a href="#cb68-15"></a>    sequence_length<span class="op">=</span>seq_length,</span>
<span id="cb68-16"><a href="#cb68-16"></a>    batch_size<span class="op">=</span><span class="dv">32</span></span>
<span id="cb68-17"><a href="#cb68-17"></a>)</span>
<span id="cb68-18"><a href="#cb68-18"></a></span>
<span id="cb68-19"><a href="#cb68-19"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb68-20"><a href="#cb68-20"></a>multask_model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb68-21"><a href="#cb68-21"></a>    tf.keras.layers.SimpleRNN(<span class="dv">32</span>, input_shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">5</span>]),</span>
<span id="cb68-22"><a href="#cb68-22"></a>    tf.keras.layers.Dense(<span class="dv">2</span>)</span>
<span id="cb68-23"><a href="#cb68-23"></a>])</span>
<span id="cb68-24"><a href="#cb68-24"></a></span>
<span id="cb68-25"><a href="#cb68-25"></a>fit_and_evaluate(multask_model, train_multask_ds, valid_multask_ds,</span>
<span id="cb68-26"><a href="#cb68-26"></a>                 learning_rate<span class="op">=</span><span class="fl">0.02</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/500
33/33 [==============================] - 4s 74ms/step - loss: 0.0255 - mae: 0.1584 - val_loss: 0.0031 - val_mae: 0.0633
Epoch 2/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0036 - mae: 0.0635 - val_loss: 0.0021 - val_mae: 0.0503
Epoch 3/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0024 - mae: 0.0485 - val_loss: 0.0011 - val_mae: 0.0357
Epoch 4/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0022 - mae: 0.0465 - val_loss: 0.0021 - val_mae: 0.0530
Epoch 5/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0021 - mae: 0.0459 - val_loss: 0.0010 - val_mae: 0.0335
Epoch 6/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0020 - mae: 0.0444 - val_loss: 0.0011 - val_mae: 0.0363
Epoch 7/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0019 - mae: 0.0434 - val_loss: 9.0769e-04 - val_mae: 0.0318
Epoch 8/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0018 - mae: 0.0425 - val_loss: 0.0013 - val_mae: 0.0404
Epoch 9/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0018 - mae: 0.0419 - val_loss: 0.0011 - val_mae: 0.0368
Epoch 10/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0019 - mae: 0.0432 - val_loss: 0.0011 - val_mae: 0.0353
Epoch 11/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0018 - mae: 0.0421 - val_loss: 0.0013 - val_mae: 0.0413
Epoch 12/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0017 - mae: 0.0409 - val_loss: 0.0011 - val_mae: 0.0354
Epoch 13/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0016 - mae: 0.0399 - val_loss: 8.3978e-04 - val_mae: 0.0307
Epoch 14/500
33/33 [==============================] - 2s 71ms/step - loss: 0.0015 - mae: 0.0387 - val_loss: 0.0012 - val_mae: 0.0382
Epoch 15/500
33/33 [==============================] - 2s 59ms/step - loss: 0.0015 - mae: 0.0385 - val_loss: 0.0010 - val_mae: 0.0347
Epoch 16/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0015 - mae: 0.0376 - val_loss: 7.6741e-04 - val_mae: 0.0298
Epoch 17/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0015 - mae: 0.0386 - val_loss: 0.0015 - val_mae: 0.0447
Epoch 18/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0016 - mae: 0.0400 - val_loss: 0.0012 - val_mae: 0.0398
Epoch 19/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0014 - mae: 0.0377 - val_loss: 7.9699e-04 - val_mae: 0.0300
Epoch 20/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0014 - mae: 0.0374 - val_loss: 7.4050e-04 - val_mae: 0.0289
Epoch 21/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0014 - mae: 0.0370 - val_loss: 9.6869e-04 - val_mae: 0.0342
Epoch 22/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0014 - mae: 0.0366 - val_loss: 0.0012 - val_mae: 0.0389
Epoch 23/500
33/33 [==============================] - 2s 60ms/step - loss: 0.0014 - mae: 0.0370 - val_loss: 9.7866e-04 - val_mae: 0.0343
Epoch 24/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0014 - mae: 0.0372 - val_loss: 8.6441e-04 - val_mae: 0.0316
Epoch 25/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0013 - mae: 0.0363 - val_loss: 0.0010 - val_mae: 0.0359
Epoch 26/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0014 - mae: 0.0366 - val_loss: 9.1490e-04 - val_mae: 0.0329
Epoch 27/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0013 - mae: 0.0359 - val_loss: 7.5073e-04 - val_mae: 0.0299
Epoch 28/500
33/33 [==============================] - 1s 37ms/step - loss: 0.0013 - mae: 0.0363 - val_loss: 7.3197e-04 - val_mae: 0.0294
Epoch 29/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0013 - mae: 0.0364 - val_loss: 8.3700e-04 - val_mae: 0.0310
Epoch 30/500
33/33 [==============================] - 2s 71ms/step - loss: 0.0013 - mae: 0.0356 - val_loss: 8.2648e-04 - val_mae: 0.0308
Epoch 31/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0013 - mae: 0.0362 - val_loss: 9.7559e-04 - val_mae: 0.0346
Epoch 32/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0362 - val_loss: 6.9691e-04 - val_mae: 0.0279
Epoch 33/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0355 - val_loss: 8.4579e-04 - val_mae: 0.0317
Epoch 34/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0013 - mae: 0.0353 - val_loss: 7.9581e-04 - val_mae: 0.0306
Epoch 35/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0349 - val_loss: 7.7935e-04 - val_mae: 0.0301
Epoch 36/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0359 - val_loss: 7.4623e-04 - val_mae: 0.0291
Epoch 37/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0013 - mae: 0.0357 - val_loss: 7.1986e-04 - val_mae: 0.0287
Epoch 38/500
33/33 [==============================] - 2s 67ms/step - loss: 0.0013 - mae: 0.0353 - val_loss: 6.8000e-04 - val_mae: 0.0277
Epoch 39/500
33/33 [==============================] - 2s 68ms/step - loss: 0.0012 - mae: 0.0346 - val_loss: 8.2842e-04 - val_mae: 0.0314
Epoch 40/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0347 - val_loss: 8.6199e-04 - val_mae: 0.0321
Epoch 41/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0013 - mae: 0.0357 - val_loss: 8.5757e-04 - val_mae: 0.0320
Epoch 42/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0013 - mae: 0.0353 - val_loss: 8.3496e-04 - val_mae: 0.0314
Epoch 43/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0349 - val_loss: 0.0011 - val_mae: 0.0370
Epoch 44/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0349 - val_loss: 6.8912e-04 - val_mae: 0.0280
Epoch 45/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0346 - val_loss: 7.4413e-04 - val_mae: 0.0292
Epoch 46/500
33/33 [==============================] - 2s 68ms/step - loss: 0.0012 - mae: 0.0344 - val_loss: 7.0734e-04 - val_mae: 0.0289
Epoch 47/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0012 - mae: 0.0350 - val_loss: 7.3556e-04 - val_mae: 0.0298
Epoch 48/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 7.5389e-04 - val_mae: 0.0299
Epoch 49/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 7.5621e-04 - val_mae: 0.0295
Epoch 50/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0344 - val_loss: 0.0010 - val_mae: 0.0359
Epoch 51/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0344 - val_loss: 7.1420e-04 - val_mae: 0.0293
Epoch 52/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0340 - val_loss: 8.2671e-04 - val_mae: 0.0312
Epoch 53/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0338 - val_loss: 7.3749e-04 - val_mae: 0.0292
Epoch 54/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0345 - val_loss: 7.0496e-04 - val_mae: 0.0283
Epoch 55/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 8.3124e-04 - val_mae: 0.0316
Epoch 56/500
33/33 [==============================] - 2s 57ms/step - loss: 0.0012 - mae: 0.0346 - val_loss: 6.7089e-04 - val_mae: 0.0273
Epoch 57/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0349 - val_loss: 8.3932e-04 - val_mae: 0.0326
Epoch 58/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 6.8043e-04 - val_mae: 0.0280
Epoch 59/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0337 - val_loss: 7.0953e-04 - val_mae: 0.0284
Epoch 60/500
33/33 [==============================] - 1s 37ms/step - loss: 0.0012 - mae: 0.0336 - val_loss: 7.2480e-04 - val_mae: 0.0289
Epoch 61/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0335 - val_loss: 7.3572e-04 - val_mae: 0.0289
Epoch 62/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0334 - val_loss: 8.1480e-04 - val_mae: 0.0312
Epoch 63/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0012 - mae: 0.0333 - val_loss: 7.8571e-04 - val_mae: 0.0301
Epoch 64/500
33/33 [==============================] - 2s 66ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 9.3828e-04 - val_mae: 0.0338
Epoch 65/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0013 - mae: 0.0352 - val_loss: 7.4398e-04 - val_mae: 0.0294
Epoch 66/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0335 - val_loss: 7.4975e-04 - val_mae: 0.0293
Epoch 67/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0332 - val_loss: 6.8271e-04 - val_mae: 0.0279
Epoch 68/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0331 - val_loss: 6.9375e-04 - val_mae: 0.0281
Epoch 69/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0012 - mae: 0.0333 - val_loss: 7.2032e-04 - val_mae: 0.0289
Epoch 70/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0350 - val_loss: 8.6118e-04 - val_mae: 0.0318
Epoch 71/500
33/33 [==============================] - 2s 67ms/step - loss: 0.0012 - mae: 0.0340 - val_loss: 7.9333e-04 - val_mae: 0.0302
Epoch 72/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0012 - mae: 0.0344 - val_loss: 0.0010 - val_mae: 0.0353
Epoch 73/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0344 - val_loss: 6.6572e-04 - val_mae: 0.0278
Epoch 74/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0329 - val_loss: 6.5416e-04 - val_mae: 0.0270
Epoch 75/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0011 - mae: 0.0326 - val_loss: 6.5704e-04 - val_mae: 0.0275
Epoch 76/500
33/33 [==============================] - 2s 63ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 6.6209e-04 - val_mae: 0.0273
Epoch 77/500
33/33 [==============================] - 2s 60ms/step - loss: 0.0011 - mae: 0.0332 - val_loss: 7.0564e-04 - val_mae: 0.0286
Epoch 78/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0331 - val_loss: 7.5438e-04 - val_mae: 0.0296
Epoch 79/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0334 - val_loss: 7.2475e-04 - val_mae: 0.0295
Epoch 80/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 6.8965e-04 - val_mae: 0.0282
Epoch 81/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0331 - val_loss: 7.2314e-04 - val_mae: 0.0293
Epoch 82/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0330 - val_loss: 6.4668e-04 - val_mae: 0.0271
Epoch 83/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 7.4061e-04 - val_mae: 0.0291
Epoch 84/500
33/33 [==============================] - 2s 45ms/step - loss: 0.0011 - mae: 0.0330 - val_loss: 6.9921e-04 - val_mae: 0.0278
Epoch 85/500
33/33 [==============================] - 3s 79ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 7.5894e-04 - val_mae: 0.0296
Epoch 86/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0011 - mae: 0.0329 - val_loss: 6.4594e-04 - val_mae: 0.0266
Epoch 87/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0328 - val_loss: 7.8189e-04 - val_mae: 0.0300
Epoch 88/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0336 - val_loss: 6.3389e-04 - val_mae: 0.0262
Epoch 89/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0012 - mae: 0.0337 - val_loss: 7.5339e-04 - val_mae: 0.0302
Epoch 90/500
33/33 [==============================] - 1s 43ms/step - loss: 0.0011 - mae: 0.0330 - val_loss: 8.4770e-04 - val_mae: 0.0316
Epoch 91/500
33/33 [==============================] - 4s 125ms/step - loss: 0.0012 - mae: 0.0340 - val_loss: 0.0011 - val_mae: 0.0359
Epoch 92/500
33/33 [==============================] - 3s 96ms/step - loss: 0.0012 - mae: 0.0344 - val_loss: 6.9146e-04 - val_mae: 0.0278
Epoch 93/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0326 - val_loss: 6.4651e-04 - val_mae: 0.0270
Epoch 94/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0328 - val_loss: 8.8887e-04 - val_mae: 0.0329
Epoch 95/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0333 - val_loss: 7.8628e-04 - val_mae: 0.0306
Epoch 96/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0012 - mae: 0.0340 - val_loss: 6.9680e-04 - val_mae: 0.0285
Epoch 97/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0011 - mae: 0.0319 - val_loss: 6.9095e-04 - val_mae: 0.0282
Epoch 98/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0331 - val_loss: 0.0010 - val_mae: 0.0359
Epoch 99/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0329 - val_loss: 6.2891e-04 - val_mae: 0.0259
Epoch 100/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0326 - val_loss: 7.0776e-04 - val_mae: 0.0286
Epoch 101/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0324 - val_loss: 0.0010 - val_mae: 0.0358
Epoch 102/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0012 - mae: 0.0336 - val_loss: 6.2758e-04 - val_mae: 0.0260
Epoch 103/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0011 - mae: 0.0321 - val_loss: 6.9090e-04 - val_mae: 0.0277
Epoch 104/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0012 - mae: 0.0340 - val_loss: 6.9598e-04 - val_mae: 0.0281
Epoch 105/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0334 - val_loss: 6.9317e-04 - val_mae: 0.0282
Epoch 106/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0012 - mae: 0.0334 - val_loss: 7.6347e-04 - val_mae: 0.0290
Epoch 107/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 7.3428e-04 - val_mae: 0.0292
Epoch 108/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0011 - mae: 0.0321 - val_loss: 6.4750e-04 - val_mae: 0.0268
Epoch 109/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0318 - val_loss: 7.3960e-04 - val_mae: 0.0294
Epoch 110/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0324 - val_loss: 6.7338e-04 - val_mae: 0.0275
Epoch 111/500
33/33 [==============================] - 2s 58ms/step - loss: 0.0011 - mae: 0.0326 - val_loss: 7.0008e-04 - val_mae: 0.0284
Epoch 112/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0011 - mae: 0.0332 - val_loss: 9.0026e-04 - val_mae: 0.0326
Epoch 113/500
33/33 [==============================] - 1s 43ms/step - loss: 0.0011 - mae: 0.0331 - val_loss: 6.4597e-04 - val_mae: 0.0266
Epoch 114/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0319 - val_loss: 9.2826e-04 - val_mae: 0.0337
Epoch 115/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 6.2865e-04 - val_mae: 0.0260
Epoch 116/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 6.3459e-04 - val_mae: 0.0265
Epoch 117/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0320 - val_loss: 7.9237e-04 - val_mae: 0.0300
Epoch 118/500
33/33 [==============================] - 2s 73ms/step - loss: 0.0011 - mae: 0.0336 - val_loss: 7.2099e-04 - val_mae: 0.0289
Epoch 119/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 6.5630e-04 - val_mae: 0.0270
Epoch 120/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 0.0010 - val_mae: 0.0363
Epoch 121/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0324 - val_loss: 6.3759e-04 - val_mae: 0.0265
Epoch 122/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0313 - val_loss: 8.1906e-04 - val_mae: 0.0310
Epoch 123/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 6.9466e-04 - val_mae: 0.0276
Epoch 124/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0322 - val_loss: 6.7003e-04 - val_mae: 0.0275
Epoch 125/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0011 - mae: 0.0323 - val_loss: 6.9022e-04 - val_mae: 0.0279
Epoch 126/500
33/33 [==============================] - 2s 75ms/step - loss: 0.0011 - mae: 0.0317 - val_loss: 6.8580e-04 - val_mae: 0.0278
Epoch 127/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0011 - mae: 0.0322 - val_loss: 0.0010 - val_mae: 0.0351
Epoch 128/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0331 - val_loss: 7.0300e-04 - val_mae: 0.0286
Epoch 129/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0327 - val_loss: 8.3916e-04 - val_mae: 0.0317
Epoch 130/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0011 - mae: 0.0316 - val_loss: 6.9474e-04 - val_mae: 0.0285
Epoch 131/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0324 - val_loss: 6.6539e-04 - val_mae: 0.0274
Epoch 132/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0322 - val_loss: 7.1757e-04 - val_mae: 0.0284
Epoch 133/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0319 - val_loss: 9.3395e-04 - val_mae: 0.0341
Epoch 134/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0011 - mae: 0.0324 - val_loss: 6.8902e-04 - val_mae: 0.0282
Epoch 135/500
33/33 [==============================] - 2s 70ms/step - loss: 0.0011 - mae: 0.0316 - val_loss: 6.3758e-04 - val_mae: 0.0265
Epoch 136/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0318 - val_loss: 7.0915e-04 - val_mae: 0.0280
Epoch 137/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0011 - mae: 0.0322 - val_loss: 6.8697e-04 - val_mae: 0.0281
Epoch 138/500
33/33 [==============================] - 1s 43ms/step - loss: 0.0010 - mae: 0.0313 - val_loss: 6.8903e-04 - val_mae: 0.0280
Epoch 139/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0011 - mae: 0.0318 - val_loss: 6.4388e-04 - val_mae: 0.0268
Epoch 140/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0012 - mae: 0.0341 - val_loss: 0.0011 - val_mae: 0.0379
Epoch 141/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0011 - mae: 0.0326 - val_loss: 7.9957e-04 - val_mae: 0.0310
Epoch 142/500
33/33 [==============================] - 3s 76ms/step - loss: 0.0011 - mae: 0.0325 - val_loss: 6.6206e-04 - val_mae: 0.0274
Epoch 143/500
33/33 [==============================] - 2s 57ms/step - loss: 0.0011 - mae: 0.0315 - val_loss: 6.7114e-04 - val_mae: 0.0278
Epoch 144/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0010 - mae: 0.0315 - val_loss: 6.3836e-04 - val_mae: 0.0262
Epoch 145/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0010 - mae: 0.0312 - val_loss: 6.7101e-04 - val_mae: 0.0277
Epoch 146/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0326 - val_loss: 8.0096e-04 - val_mae: 0.0308
Epoch 147/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0011 - mae: 0.0332 - val_loss: 6.3757e-04 - val_mae: 0.0270
Epoch 148/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0011 - mae: 0.0318 - val_loss: 6.3463e-04 - val_mae: 0.0263
Epoch 149/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0010 - mae: 0.0312 - val_loss: 9.4175e-04 - val_mae: 0.0339
3/3 [==============================] - 0s 16ms/step - loss: 6.2891e-04 - mae: 0.0259</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="54">
<pre><code>25919.86581683159</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:8,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683352118757,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="95ccb889-153c-4525-ff96-e3e63f5cb78f">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1"></a>bus_naive <span class="op">=</span> mulvar_valid[<span class="st">"bus"</span>].shift(<span class="dv">7</span>)[seq_length:]</span>
<span id="cb71-2"><a href="#cb71-2"></a>bus_target <span class="op">=</span> mulvar_valid[<span class="st">"bus"</span>][seq_length:]</span>
<span id="cb71-3"><a href="#cb71-3"></a>(bus_target <span class="op">-</span> bus_naive).<span class="bu">abs</span>().mean() <span class="op">*</span> <span class="fl">1e6</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>43441.63157894738</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1291,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683352120047,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="e4d352df-2c15-46b8-a36e-61ed65aa368a">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1"></a>Y_preds_valid <span class="op">=</span> multask_model.predict(valid_multask_ds)</span>
<span id="cb73-2"><a href="#cb73-2"></a><span class="cf">for</span> idx, name <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="st">"bus"</span>, <span class="st">"rail"</span>]):</span>
<span id="cb73-3"><a href="#cb73-3"></a>    mae <span class="op">=</span> <span class="fl">1e6</span> <span class="op">*</span> tf.keras.metrics.mean_absolute_error(</span>
<span id="cb73-4"><a href="#cb73-4"></a>        mulvar_valid[name][seq_length:], Y_preds_valid[:, idx])</span>
<span id="cb73-5"><a href="#cb73-5"></a>    <span class="bu">print</span>(name, <span class="bu">int</span>(mae))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3/3 [==============================] - 1s 14ms/step
bus 27433
rail 24406</code></pre>
</div>
</div>
<p>Using a single model for multiple related tasks often results in better performance than using a separate model for each task, since features learned for one task may be useful for the other tasks, and also because having to perform well across multiple tasks prevents the model from overfitting (it’s a form of regularization). However, it depends on the task, and in this particular case the multitask RNN that forecasts both the bus and the rail ridership doesn’t perform quite as well as dedicated models that forecast one or the other (using all five columns as input). Still, it reaches a validation MAE of 27,433 for bus and 24,406 for rail, which is pretty good.</p>
<p>You might find <a href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/timeseries_dataset_from_array">https://www.tensorflow.org/api_docs/python/tf/keras/utils/timeseries_dataset_from_array</a> or <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/TimeseriesGenerator">https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/TimeseriesGenerator</a> useful</p>
</section>
<section id="forecasting-several-time-steps-ahead" class="level3" data-number="10.2.9">
<h3 data-number="10.2.9" class="anchored" data-anchor-id="forecasting-several-time-steps-ahead"><span class="header-section-number">10.2.9</span> Forecasting Several Time Steps Ahead</h3>
<p>So far we have only predicted the value at the next time step, but we could just as easily have predicted the value several steps ahead by changing the targets appropriately (e.g., to predict the ridership 2 weeks from now, we could just change the targets to be the value 14 days ahead instead of 1 day ahead). But what if we want to predict the next 14 values?</p>
<p>The first option is to take the <code>univar_model</code> RNN we trained earlier for the rail time series, make it predict the next value, and add that value to the inputs, acting as if the predicted value had actually occurred; we would then use the model again to predict the following value, and so on, as in the following code:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2056,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683352122097,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="4ddf7063-8447-487d-88ab-e0ae0fc8452c">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1"></a>X <span class="op">=</span> rail_valid.to_numpy()[np.newaxis, :seq_length, np.newaxis]</span>
<span id="cb75-2"><a href="#cb75-2"></a><span class="cf">for</span> step_ahead <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">14</span>):</span>
<span id="cb75-3"><a href="#cb75-3"></a>    y_pred_one <span class="op">=</span> univar_model.predict(X)</span>
<span id="cb75-4"><a href="#cb75-4"></a>    X <span class="op">=</span> np.concatenate([X, y_pred_one.reshape(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)], axis<span class="op">=</span><span class="dv">1</span>) <span class="co"># Concatenate it as input (expanding window)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 216ms/step
1/1 [==============================] - 0s 205ms/step
1/1 [==============================] - 0s 51ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 41ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 41ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 36ms/step
1/1 [==============================] - 0s 29ms/step</code></pre>
</div>
</div>
<p>In this code, we take the rail ridership of the first 56 days of the validation period, and we convert the data to a <code>NumPy</code> array of shape <code>[1, 56, 1]</code> (recall that recurrent layers expect 3D inputs). Then we repeatedly use the model to forecast the next value, and we append each forecast to the input series, along the time axis <code>(axis=1)</code>.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1631,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683352123726,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="2a73a523-9333-45e3-b83e-42434fde6afe">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1"></a><span class="co"># The forecasts start on 2019-02-26, as it is the 57th day of 2019, and they end</span></span>
<span id="cb77-2"><a href="#cb77-2"></a><span class="co"># on 2019-03-11. That's 14 days in total.</span></span>
<span id="cb77-3"><a href="#cb77-3"></a>Y_pred <span class="op">=</span> pd.Series(X[<span class="dv">0</span>, <span class="op">-</span><span class="dv">14</span>:, <span class="dv">0</span>], index<span class="op">=</span>pd.date_range(<span class="st">"2019-02-26"</span>, <span class="st">"2019-03-11"</span>))</span>
<span id="cb77-4"><a href="#cb77-4"></a></span>
<span id="cb77-5"><a href="#cb77-5"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.5</span>))</span>
<span id="cb77-6"><a href="#cb77-6"></a></span>
<span id="cb77-7"><a href="#cb77-7"></a>(rail_valid <span class="op">*</span> <span class="fl">1e6</span>)[<span class="st">"2019-02-01"</span>:<span class="st">"2019-03-11"</span>].plot(</span>
<span id="cb77-8"><a href="#cb77-8"></a>    label<span class="op">=</span><span class="st">"True"</span>, marker<span class="op">=</span><span class="st">"."</span>, ax<span class="op">=</span>ax)</span>
<span id="cb77-9"><a href="#cb77-9"></a>(Y_pred <span class="op">*</span> <span class="fl">1e6</span>).plot(</span>
<span id="cb77-10"><a href="#cb77-10"></a>    label<span class="op">=</span><span class="st">"Predictions"</span>, grid<span class="op">=</span><span class="va">True</span>, marker<span class="op">=</span><span class="st">"x"</span>, color<span class="op">=</span><span class="st">"r"</span>, ax<span class="op">=</span>ax)</span>
<span id="cb77-11"><a href="#cb77-11"></a></span>
<span id="cb77-12"><a href="#cb77-12"></a>ax.vlines(<span class="st">"2019-02-25"</span>, <span class="dv">0</span>, <span class="fl">1e6</span>, color<span class="op">=</span><span class="st">"k"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, label<span class="op">=</span><span class="st">"Today"</span>)</span>
<span id="cb77-13"><a href="#cb77-13"></a>ax.set_ylim([<span class="dv">200_000</span>, <span class="dv">800_000</span>])</span>
<span id="cb77-14"><a href="#cb77-14"></a>plt.legend(loc<span class="op">=</span><span class="st">"center left"</span>)</span>
<span id="cb77-15"><a href="#cb77-15"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="10_Recurrent_Neural_Networks_files/figure-html/cell-46-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The second option is to train an RNN <strong>to predict the next 14 values in one shot.</strong> We can still use a sequence-to-vector model, but it will output 14 values instead of 1. However, we first need to change the targets to be vectors containing the next 14 values. To do this, we can use <code>timeseries_dataset_from_array()</code> again, but this time asking it to create datasets without targets (<code>targets=None</code>) and with longer sequences, of length <code>seq_length + 14</code>. Then we can use the datasets’ <code>map()</code> method to apply a custom function to each batch of sequences, splitting them into inputs and targets. In this example, we use the multivariate time series as input (using all five columns), and we forecast the rail ridership for the next 14 days:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility</span></span>
<span id="cb78-2"><a href="#cb78-2"></a></span>
<span id="cb78-3"><a href="#cb78-3"></a><span class="kw">def</span> split_inputs_and_targets(mulvar_series, ahead<span class="op">=</span><span class="dv">14</span>, target_col<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb78-4"><a href="#cb78-4"></a>    <span class="cf">return</span> mulvar_series[:, :<span class="op">-</span>ahead], mulvar_series[:, <span class="op">-</span>ahead:, target_col]</span>
<span id="cb78-5"><a href="#cb78-5"></a></span>
<span id="cb78-6"><a href="#cb78-6"></a>ahead_train_ds <span class="op">=</span> tf.keras.utils.timeseries_dataset_from_array(</span>
<span id="cb78-7"><a href="#cb78-7"></a>    mulvar_train.to_numpy(),</span>
<span id="cb78-8"><a href="#cb78-8"></a>    targets<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb78-9"><a href="#cb78-9"></a>    sequence_length<span class="op">=</span>seq_length <span class="op">+</span> <span class="dv">14</span>,</span>
<span id="cb78-10"><a href="#cb78-10"></a>    batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb78-11"><a href="#cb78-11"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb78-12"><a href="#cb78-12"></a>    seed<span class="op">=</span><span class="dv">42</span></span>
<span id="cb78-13"><a href="#cb78-13"></a>).<span class="bu">map</span>(split_inputs_and_targets)</span>
<span id="cb78-14"><a href="#cb78-14"></a></span>
<span id="cb78-15"><a href="#cb78-15"></a>ahead_valid_ds <span class="op">=</span> tf.keras.utils.timeseries_dataset_from_array(</span>
<span id="cb78-16"><a href="#cb78-16"></a>    mulvar_valid.to_numpy(),</span>
<span id="cb78-17"><a href="#cb78-17"></a>    targets<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb78-18"><a href="#cb78-18"></a>    sequence_length<span class="op">=</span>seq_length <span class="op">+</span> <span class="dv">14</span>,</span>
<span id="cb78-19"><a href="#cb78-19"></a>    batch_size<span class="op">=</span><span class="dv">32</span></span>
<span id="cb78-20"><a href="#cb78-20"></a>).<span class="bu">map</span>(split_inputs_and_targets)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we just need the output layer to have 14 units instead of 1:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb79-2"><a href="#cb79-2"></a></span>
<span id="cb79-3"><a href="#cb79-3"></a>ahead_model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb79-4"><a href="#cb79-4"></a>    tf.keras.layers.SimpleRNN(<span class="dv">32</span>, input_shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">5</span>]),</span>
<span id="cb79-5"><a href="#cb79-5"></a>    tf.keras.layers.Dense(<span class="dv">14</span>) <span class="co"># Output is 14 dimension!</span></span>
<span id="cb79-6"><a href="#cb79-6"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:506509,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683352630228,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="7069a72b-a8cd-4c97-a9e8-1d4645fa67bc">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1"></a>fit_and_evaluate(ahead_model, ahead_train_ds, ahead_valid_ds, learning_rate<span class="op">=</span><span class="fl">0.02</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/500
33/33 [==============================] - 5s 69ms/step - loss: 0.1590 - mae: 0.4254 - val_loss: 0.0288 - val_mae: 0.1948
Epoch 2/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0202 - mae: 0.1616 - val_loss: 0.0138 - val_mae: 0.1287
Epoch 3/500
33/33 [==============================] - 4s 107ms/step - loss: 0.0133 - mae: 0.1292 - val_loss: 0.0106 - val_mae: 0.1128
Epoch 4/500
33/33 [==============================] - 3s 78ms/step - loss: 0.0111 - mae: 0.1170 - val_loss: 0.0088 - val_mae: 0.1036
Epoch 5/500
33/33 [==============================] - 2s 70ms/step - loss: 0.0099 - mae: 0.1096 - val_loss: 0.0079 - val_mae: 0.0965
Epoch 6/500
33/33 [==============================] - 2s 70ms/step - loss: 0.0090 - mae: 0.1033 - val_loss: 0.0070 - val_mae: 0.0915
Epoch 7/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0084 - mae: 0.0986 - val_loss: 0.0063 - val_mae: 0.0864
Epoch 8/500
33/33 [==============================] - 2s 71ms/step - loss: 0.0080 - mae: 0.0951 - val_loss: 0.0060 - val_mae: 0.0832
Epoch 9/500
33/33 [==============================] - 2s 57ms/step - loss: 0.0076 - mae: 0.0921 - val_loss: 0.0057 - val_mae: 0.0802
Epoch 10/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0072 - mae: 0.0889 - val_loss: 0.0052 - val_mae: 0.0774
Epoch 11/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0069 - mae: 0.0864 - val_loss: 0.0049 - val_mae: 0.0746
Epoch 12/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0067 - mae: 0.0845 - val_loss: 0.0046 - val_mae: 0.0724
Epoch 13/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0066 - mae: 0.0832 - val_loss: 0.0047 - val_mae: 0.0726
Epoch 14/500
33/33 [==============================] - 2s 68ms/step - loss: 0.0064 - mae: 0.0810 - val_loss: 0.0043 - val_mae: 0.0698
Epoch 15/500
33/33 [==============================] - 4s 122ms/step - loss: 0.0062 - mae: 0.0797 - val_loss: 0.0040 - val_mae: 0.0677
Epoch 16/500
33/33 [==============================] - 3s 74ms/step - loss: 0.0060 - mae: 0.0783 - val_loss: 0.0040 - val_mae: 0.0679
Epoch 17/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0059 - mae: 0.0771 - val_loss: 0.0040 - val_mae: 0.0674
Epoch 18/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0058 - mae: 0.0761 - val_loss: 0.0037 - val_mae: 0.0649
Epoch 19/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0057 - mae: 0.0750 - val_loss: 0.0035 - val_mae: 0.0634
Epoch 20/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0055 - mae: 0.0735 - val_loss: 0.0035 - val_mae: 0.0635
Epoch 21/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0054 - mae: 0.0727 - val_loss: 0.0034 - val_mae: 0.0617
Epoch 22/500
33/33 [==============================] - 2s 67ms/step - loss: 0.0053 - mae: 0.0718 - val_loss: 0.0032 - val_mae: 0.0607
Epoch 23/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0052 - mae: 0.0710 - val_loss: 0.0031 - val_mae: 0.0597
Epoch 24/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0051 - mae: 0.0701 - val_loss: 0.0033 - val_mae: 0.0612
Epoch 25/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0051 - mae: 0.0696 - val_loss: 0.0031 - val_mae: 0.0592
Epoch 26/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0050 - mae: 0.0684 - val_loss: 0.0029 - val_mae: 0.0575
Epoch 27/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0049 - mae: 0.0676 - val_loss: 0.0029 - val_mae: 0.0573
Epoch 28/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0048 - mae: 0.0670 - val_loss: 0.0027 - val_mae: 0.0552
Epoch 29/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0047 - mae: 0.0659 - val_loss: 0.0026 - val_mae: 0.0535
Epoch 30/500
33/33 [==============================] - 2s 59ms/step - loss: 0.0046 - mae: 0.0652 - val_loss: 0.0025 - val_mae: 0.0527
Epoch 31/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0046 - mae: 0.0652 - val_loss: 0.0025 - val_mae: 0.0525
Epoch 32/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0045 - mae: 0.0637 - val_loss: 0.0025 - val_mae: 0.0523
Epoch 33/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0045 - mae: 0.0631 - val_loss: 0.0024 - val_mae: 0.0511
Epoch 34/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0044 - mae: 0.0626 - val_loss: 0.0023 - val_mae: 0.0501
Epoch 35/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0043 - mae: 0.0618 - val_loss: 0.0024 - val_mae: 0.0513
Epoch 36/500
33/33 [==============================] - 2s 64ms/step - loss: 0.0043 - mae: 0.0614 - val_loss: 0.0023 - val_mae: 0.0498
Epoch 37/500
33/33 [==============================] - 2s 58ms/step - loss: 0.0043 - mae: 0.0610 - val_loss: 0.0021 - val_mae: 0.0479
Epoch 38/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0042 - mae: 0.0605 - val_loss: 0.0021 - val_mae: 0.0474
Epoch 39/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0041 - mae: 0.0597 - val_loss: 0.0021 - val_mae: 0.0475
Epoch 40/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0041 - mae: 0.0592 - val_loss: 0.0021 - val_mae: 0.0479
Epoch 41/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0041 - mae: 0.0585 - val_loss: 0.0022 - val_mae: 0.0483
Epoch 42/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0040 - mae: 0.0582 - val_loss: 0.0020 - val_mae: 0.0467
Epoch 43/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0040 - mae: 0.0576 - val_loss: 0.0019 - val_mae: 0.0445
Epoch 44/500
33/33 [==============================] - 2s 53ms/step - loss: 0.0040 - mae: 0.0577 - val_loss: 0.0019 - val_mae: 0.0446
Epoch 45/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0040 - mae: 0.0577 - val_loss: 0.0018 - val_mae: 0.0434
Epoch 46/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0039 - mae: 0.0568 - val_loss: 0.0019 - val_mae: 0.0446
Epoch 47/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0039 - mae: 0.0561 - val_loss: 0.0018 - val_mae: 0.0434
Epoch 48/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0039 - mae: 0.0559 - val_loss: 0.0018 - val_mae: 0.0424
Epoch 49/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0038 - mae: 0.0555 - val_loss: 0.0018 - val_mae: 0.0435
Epoch 50/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0038 - mae: 0.0550 - val_loss: 0.0017 - val_mae: 0.0416
Epoch 51/500
33/33 [==============================] - 2s 65ms/step - loss: 0.0038 - mae: 0.0551 - val_loss: 0.0017 - val_mae: 0.0406
Epoch 52/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0037 - mae: 0.0544 - val_loss: 0.0017 - val_mae: 0.0411
Epoch 53/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0037 - mae: 0.0540 - val_loss: 0.0016 - val_mae: 0.0402
Epoch 54/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0038 - mae: 0.0558 - val_loss: 0.0017 - val_mae: 0.0409
Epoch 55/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0037 - mae: 0.0536 - val_loss: 0.0017 - val_mae: 0.0416
Epoch 56/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0037 - mae: 0.0533 - val_loss: 0.0016 - val_mae: 0.0397
Epoch 57/500
33/33 [==============================] - 2s 57ms/step - loss: 0.0037 - mae: 0.0537 - val_loss: 0.0017 - val_mae: 0.0410
Epoch 58/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0036 - mae: 0.0530 - val_loss: 0.0016 - val_mae: 0.0400
Epoch 59/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0036 - mae: 0.0528 - val_loss: 0.0016 - val_mae: 0.0395
Epoch 60/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0036 - mae: 0.0528 - val_loss: 0.0017 - val_mae: 0.0408
Epoch 61/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0036 - mae: 0.0527 - val_loss: 0.0016 - val_mae: 0.0388
Epoch 62/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0036 - mae: 0.0527 - val_loss: 0.0017 - val_mae: 0.0412
Epoch 63/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0036 - mae: 0.0520 - val_loss: 0.0015 - val_mae: 0.0382
Epoch 64/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0035 - mae: 0.0519 - val_loss: 0.0015 - val_mae: 0.0381
Epoch 65/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0035 - mae: 0.0514 - val_loss: 0.0016 - val_mae: 0.0387
Epoch 66/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0035 - mae: 0.0515 - val_loss: 0.0016 - val_mae: 0.0387
Epoch 67/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0035 - mae: 0.0526 - val_loss: 0.0016 - val_mae: 0.0404
Epoch 68/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0035 - mae: 0.0514 - val_loss: 0.0016 - val_mae: 0.0399
Epoch 69/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0035 - mae: 0.0520 - val_loss: 0.0018 - val_mae: 0.0431
Epoch 70/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0035 - mae: 0.0509 - val_loss: 0.0015 - val_mae: 0.0367
Epoch 71/500
33/33 [==============================] - 2s 54ms/step - loss: 0.0034 - mae: 0.0508 - val_loss: 0.0016 - val_mae: 0.0386
Epoch 72/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0035 - mae: 0.0505 - val_loss: 0.0015 - val_mae: 0.0370
Epoch 73/500
33/33 [==============================] - 1s 43ms/step - loss: 0.0034 - mae: 0.0506 - val_loss: 0.0015 - val_mae: 0.0370
Epoch 74/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0034 - mae: 0.0503 - val_loss: 0.0015 - val_mae: 0.0383
Epoch 75/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0034 - mae: 0.0503 - val_loss: 0.0015 - val_mae: 0.0380
Epoch 76/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0034 - mae: 0.0498 - val_loss: 0.0016 - val_mae: 0.0389
Epoch 77/500
33/33 [==============================] - 1s 37ms/step - loss: 0.0034 - mae: 0.0499 - val_loss: 0.0015 - val_mae: 0.0368
Epoch 78/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0034 - mae: 0.0497 - val_loss: 0.0014 - val_mae: 0.0359
Epoch 79/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0034 - mae: 0.0497 - val_loss: 0.0015 - val_mae: 0.0371
Epoch 80/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0034 - mae: 0.0496 - val_loss: 0.0016 - val_mae: 0.0405
Epoch 81/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0034 - mae: 0.0499 - val_loss: 0.0014 - val_mae: 0.0353
Epoch 82/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0034 - mae: 0.0499 - val_loss: 0.0014 - val_mae: 0.0366
Epoch 83/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0033 - mae: 0.0492 - val_loss: 0.0014 - val_mae: 0.0359
Epoch 84/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0495 - val_loss: 0.0014 - val_mae: 0.0354
Epoch 85/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0491 - val_loss: 0.0016 - val_mae: 0.0395
Epoch 86/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0034 - mae: 0.0502 - val_loss: 0.0015 - val_mae: 0.0370
Epoch 87/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0492 - val_loss: 0.0014 - val_mae: 0.0359
Epoch 88/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0033 - mae: 0.0487 - val_loss: 0.0014 - val_mae: 0.0358
Epoch 89/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0033 - mae: 0.0488 - val_loss: 0.0014 - val_mae: 0.0355
Epoch 90/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0487 - val_loss: 0.0014 - val_mae: 0.0359
Epoch 91/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0486 - val_loss: 0.0015 - val_mae: 0.0372
Epoch 92/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0033 - mae: 0.0493 - val_loss: 0.0014 - val_mae: 0.0356
Epoch 93/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0033 - mae: 0.0483 - val_loss: 0.0014 - val_mae: 0.0347
Epoch 94/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0033 - mae: 0.0486 - val_loss: 0.0014 - val_mae: 0.0350
Epoch 95/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0033 - mae: 0.0488 - val_loss: 0.0014 - val_mae: 0.0354
Epoch 96/500
33/33 [==============================] - 2s 73ms/step - loss: 0.0033 - mae: 0.0486 - val_loss: 0.0014 - val_mae: 0.0346
Epoch 97/500
33/33 [==============================] - 2s 58ms/step - loss: 0.0032 - mae: 0.0480 - val_loss: 0.0014 - val_mae: 0.0349
Epoch 98/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0014 - val_mae: 0.0345
Epoch 99/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0032 - mae: 0.0485 - val_loss: 0.0014 - val_mae: 0.0356
Epoch 100/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0014 - val_mae: 0.0348
Epoch 101/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0014 - val_mae: 0.0363
Epoch 102/500
33/33 [==============================] - 2s 64ms/step - loss: 0.0032 - mae: 0.0480 - val_loss: 0.0015 - val_mae: 0.0375
Epoch 103/500
33/33 [==============================] - 2s 73ms/step - loss: 0.0032 - mae: 0.0486 - val_loss: 0.0014 - val_mae: 0.0355
Epoch 104/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0033 - mae: 0.0490 - val_loss: 0.0013 - val_mae: 0.0344
Epoch 105/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0014 - val_mae: 0.0344
Epoch 106/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0014 - val_mae: 0.0351
Epoch 107/500
33/33 [==============================] - 3s 104ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0014 - val_mae: 0.0361
Epoch 108/500
33/33 [==============================] - 4s 117ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0014 - val_mae: 0.0348
Epoch 109/500
33/33 [==============================] - 3s 98ms/step - loss: 0.0032 - mae: 0.0474 - val_loss: 0.0013 - val_mae: 0.0344
Epoch 110/500
33/33 [==============================] - 2s 59ms/step - loss: 0.0032 - mae: 0.0475 - val_loss: 0.0014 - val_mae: 0.0343
Epoch 111/500
33/33 [==============================] - 3s 101ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0014 - val_mae: 0.0347
Epoch 112/500
33/33 [==============================] - 4s 103ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0014 - val_mae: 0.0348
Epoch 113/500
33/33 [==============================] - 2s 73ms/step - loss: 0.0032 - mae: 0.0477 - val_loss: 0.0014 - val_mae: 0.0345
Epoch 114/500
33/33 [==============================] - 2s 73ms/step - loss: 0.0032 - mae: 0.0475 - val_loss: 0.0015 - val_mae: 0.0369
Epoch 115/500
33/33 [==============================] - 2s 71ms/step - loss: 0.0032 - mae: 0.0487 - val_loss: 0.0014 - val_mae: 0.0361
Epoch 116/500
33/33 [==============================] - 2s 64ms/step - loss: 0.0032 - mae: 0.0472 - val_loss: 0.0014 - val_mae: 0.0354
Epoch 117/500
33/33 [==============================] - 2s 73ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0014 - val_mae: 0.0344
Epoch 118/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0032 - mae: 0.0480 - val_loss: 0.0014 - val_mae: 0.0347
Epoch 119/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0032 - mae: 0.0474 - val_loss: 0.0013 - val_mae: 0.0340
Epoch 120/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0014 - val_mae: 0.0344
Epoch 121/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0031 - mae: 0.0475 - val_loss: 0.0014 - val_mae: 0.0356
Epoch 122/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0015 - val_mae: 0.0374
Epoch 123/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0014 - val_mae: 0.0346
Epoch 124/500
33/33 [==============================] - 4s 103ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0014 - val_mae: 0.0347
Epoch 125/500
33/33 [==============================] - 3s 84ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0014 - val_mae: 0.0352
Epoch 126/500
33/33 [==============================] - 3s 83ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0014 - val_mae: 0.0346
Epoch 127/500
33/33 [==============================] - 3s 86ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0014 - val_mae: 0.0345
Epoch 128/500
33/33 [==============================] - 4s 103ms/step - loss: 0.0031 - mae: 0.0475 - val_loss: 0.0015 - val_mae: 0.0367
Epoch 129/500
33/33 [==============================] - 2s 56ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0014 - val_mae: 0.0348
Epoch 130/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0474 - val_loss: 0.0014 - val_mae: 0.0343
Epoch 131/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0014 - val_mae: 0.0350
Epoch 132/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0014 - val_mae: 0.0348
Epoch 133/500
33/33 [==============================] - 2s 53ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0013 - val_mae: 0.0338
Epoch 134/500
33/33 [==============================] - 2s 74ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0014 - val_mae: 0.0350
Epoch 135/500
33/33 [==============================] - 2s 43ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0015 - val_mae: 0.0372
Epoch 136/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0014 - val_mae: 0.0353
Epoch 137/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0464 - val_loss: 0.0014 - val_mae: 0.0349
Epoch 138/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0478 - val_loss: 0.0015 - val_mae: 0.0366
Epoch 139/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0013 - val_mae: 0.0337
Epoch 140/500
33/33 [==============================] - 1s 44ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0014 - val_mae: 0.0351
Epoch 141/500
33/33 [==============================] - 2s 65ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0014 - val_mae: 0.0356
Epoch 142/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0475 - val_loss: 0.0014 - val_mae: 0.0342
Epoch 143/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0013 - val_mae: 0.0342
Epoch 144/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0463 - val_loss: 0.0014 - val_mae: 0.0345
Epoch 145/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0032 - mae: 0.0485 - val_loss: 0.0013 - val_mae: 0.0341
Epoch 146/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0014 - val_mae: 0.0352
Epoch 147/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0013 - val_mae: 0.0339
Epoch 148/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0015 - val_mae: 0.0365
Epoch 149/500
33/33 [==============================] - 3s 75ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0014 - val_mae: 0.0348
Epoch 150/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0477 - val_loss: 0.0014 - val_mae: 0.0349
Epoch 151/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0014 - val_mae: 0.0345
Epoch 152/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0014 - val_mae: 0.0349
Epoch 153/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0464 - val_loss: 0.0014 - val_mae: 0.0348
Epoch 154/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0014 - val_mae: 0.0353
Epoch 155/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0013 - val_mae: 0.0336
Epoch 156/500
33/33 [==============================] - 3s 76ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0014 - val_mae: 0.0344
Epoch 157/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0031 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0345
Epoch 158/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0014 - val_mae: 0.0347
Epoch 159/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0015 - val_mae: 0.0363
Epoch 160/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0014 - val_mae: 0.0350
Epoch 161/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0013 - val_mae: 0.0339
Epoch 162/500
33/33 [==============================] - 2s 66ms/step - loss: 0.0030 - mae: 0.0466 - val_loss: 0.0014 - val_mae: 0.0346
Epoch 163/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0014 - val_mae: 0.0347
Epoch 164/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0014 - val_mae: 0.0341
Epoch 165/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0347
Epoch 166/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0014 - val_mae: 0.0352
Epoch 167/500
33/33 [==============================] - 2s 74ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0014 - val_mae: 0.0357
Epoch 168/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0013 - val_mae: 0.0338
Epoch 169/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0344
Epoch 170/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0014 - val_mae: 0.0344
Epoch 171/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0354
Epoch 172/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0013 - val_mae: 0.0340
Epoch 173/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0347
Epoch 174/500
33/33 [==============================] - 2s 46ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0342
Epoch 175/500
33/33 [==============================] - 2s 68ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0344
Epoch 176/500
33/33 [==============================] - 2s 65ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0014 - val_mae: 0.0354
Epoch 177/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0014 - val_mae: 0.0341
Epoch 178/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0013 - val_mae: 0.0338
Epoch 179/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0475 - val_loss: 0.0014 - val_mae: 0.0343
Epoch 180/500
33/33 [==============================] - 2s 60ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0014 - val_mae: 0.0344
Epoch 181/500
33/33 [==============================] - 2s 65ms/step - loss: 0.0030 - mae: 0.0457 - val_loss: 0.0014 - val_mae: 0.0356
Epoch 182/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0014 - val_mae: 0.0354
Epoch 183/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0015 - val_mae: 0.0377
Epoch 184/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0465 - val_loss: 0.0014 - val_mae: 0.0351
Epoch 185/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0341
Epoch 186/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0013 - val_mae: 0.0338
Epoch 187/500
33/33 [==============================] - 1s 43ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0014 - val_mae: 0.0344
Epoch 188/500
33/33 [==============================] - 3s 93ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0014 - val_mae: 0.0348
Epoch 189/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0348
Epoch 190/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0014 - val_mae: 0.0345
Epoch 191/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0014 - val_mae: 0.0348
Epoch 192/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0014 - val_mae: 0.0349
Epoch 193/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0014 - val_mae: 0.0355
Epoch 194/500
33/33 [==============================] - 2s 70ms/step - loss: 0.0030 - mae: 0.0455 - val_loss: 0.0015 - val_mae: 0.0367
Epoch 195/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0031 - mae: 0.0474 - val_loss: 0.0013 - val_mae: 0.0336
Epoch 196/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0014 - val_mae: 0.0346
Epoch 197/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0014 - val_mae: 0.0341
Epoch 198/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0348
Epoch 199/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0013 - val_mae: 0.0340
Epoch 200/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0454 - val_loss: 0.0014 - val_mae: 0.0352
Epoch 201/500
33/33 [==============================] - 2s 71ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0356
Epoch 202/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0013 - val_mae: 0.0339
Epoch 203/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0013 - val_mae: 0.0339
Epoch 204/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0454 - val_loss: 0.0015 - val_mae: 0.0364
Epoch 205/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0469 - val_loss: 0.0014 - val_mae: 0.0344
Epoch 206/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0341
Epoch 207/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0453 - val_loss: 0.0014 - val_mae: 0.0360
Epoch 208/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0453 - val_loss: 0.0014 - val_mae: 0.0350
Epoch 209/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0015 - val_mae: 0.0371
Epoch 210/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0020 - val_mae: 0.0458
Epoch 211/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0032 - mae: 0.0489 - val_loss: 0.0013 - val_mae: 0.0338
Epoch 212/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0341
Epoch 213/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0455 - val_loss: 0.0014 - val_mae: 0.0346
Epoch 214/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0457 - val_loss: 0.0014 - val_mae: 0.0356
Epoch 215/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0030 - mae: 0.0465 - val_loss: 0.0014 - val_mae: 0.0350
Epoch 216/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0030 - mae: 0.0457 - val_loss: 0.0014 - val_mae: 0.0346
Epoch 217/500
33/33 [==============================] - 2s 73ms/step - loss: 0.0029 - mae: 0.0451 - val_loss: 0.0014 - val_mae: 0.0349
Epoch 218/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0458 - val_loss: 0.0014 - val_mae: 0.0344
Epoch 219/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0348
Epoch 220/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0014 - val_mae: 0.0360
Epoch 221/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0455 - val_loss: 0.0014 - val_mae: 0.0350
Epoch 222/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0029 - mae: 0.0453 - val_loss: 0.0014 - val_mae: 0.0355
Epoch 223/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0029 - mae: 0.0452 - val_loss: 0.0014 - val_mae: 0.0341
Epoch 224/500
33/33 [==============================] - 2s 54ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0351
Epoch 225/500
33/33 [==============================] - 2s 63ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0014 - val_mae: 0.0348
Epoch 226/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0015 - val_mae: 0.0362
Epoch 227/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0361
Epoch 228/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0030 - mae: 0.0455 - val_loss: 0.0014 - val_mae: 0.0353
Epoch 229/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0029 - mae: 0.0459 - val_loss: 0.0014 - val_mae: 0.0351
Epoch 230/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0029 - mae: 0.0457 - val_loss: 0.0014 - val_mae: 0.0340
Epoch 231/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0030 - mae: 0.0466 - val_loss: 0.0014 - val_mae: 0.0346
Epoch 232/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0343
Epoch 233/500
33/33 [==============================] - 2s 67ms/step - loss: 0.0029 - mae: 0.0454 - val_loss: 0.0015 - val_mae: 0.0364
Epoch 234/500
33/33 [==============================] - 1s 39ms/step - loss: 0.0029 - mae: 0.0455 - val_loss: 0.0014 - val_mae: 0.0350
Epoch 235/500
33/33 [==============================] - 1s 41ms/step - loss: 0.0029 - mae: 0.0451 - val_loss: 0.0014 - val_mae: 0.0360
Epoch 236/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0014 - val_mae: 0.0343
Epoch 237/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0014 - val_mae: 0.0345
Epoch 238/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0029 - mae: 0.0450 - val_loss: 0.0019 - val_mae: 0.0438
Epoch 239/500
33/33 [==============================] - 2s 68ms/step - loss: 0.0031 - mae: 0.0490 - val_loss: 0.0014 - val_mae: 0.0344
Epoch 240/500
33/33 [==============================] - 2s 61ms/step - loss: 0.0029 - mae: 0.0456 - val_loss: 0.0013 - val_mae: 0.0338
Epoch 241/500
33/33 [==============================] - 1s 38ms/step - loss: 0.0030 - mae: 0.0456 - val_loss: 0.0014 - val_mae: 0.0353
Epoch 242/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0029 - mae: 0.0453 - val_loss: 0.0013 - val_mae: 0.0337
Epoch 243/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0029 - mae: 0.0451 - val_loss: 0.0014 - val_mae: 0.0347
Epoch 244/500
33/33 [==============================] - 1s 42ms/step - loss: 0.0029 - mae: 0.0450 - val_loss: 0.0013 - val_mae: 0.0344
Epoch 245/500
33/33 [==============================] - 1s 40ms/step - loss: 0.0029 - mae: 0.0452 - val_loss: 0.0015 - val_mae: 0.0376
3/3 [==============================] - 0s 12ms/step - loss: 0.0013 - mae: 0.0336</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="61">
<pre><code>33568.80694627762</code></pre>
</div>
</div>
<p>After training this model, you can predict the next 14 values at once like this:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:912,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683352631130,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="ef613f80-c064-41a2-8552-5d60ca7439bd">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1"></a>X <span class="op">=</span> mulvar_valid.to_numpy()[np.newaxis, :seq_length]  <span class="co"># shape [1, 56, 5]</span></span>
<span id="cb83-2"><a href="#cb83-2"></a>Y_pred <span class="op">=</span> ahead_model.predict(X)  <span class="co"># shape [1, 14]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 197ms/step</code></pre>
</div>
</div>
<p>This approach works quite well. Its forecasts for the next day are obviously better than its forecasts for 14 days into the future, but it doesn’t accumulate errors like the previous approach did. However, we can still do better, using a sequence-to-sequence (or seq2seq) model.</p>
</section>
<section id="forecasting-using-a-sequence-to-sequence-model" class="level3" data-number="10.2.10">
<h3 data-number="10.2.10" class="anchored" data-anchor-id="forecasting-using-a-sequence-to-sequence-model"><span class="header-section-number">10.2.10</span> Forecasting Using a Sequence-to-Sequence Model</h3>
<p>Instead of training the model to forecast the next 14 values only at the very last time step, we can train it to forecast the next 14 values at each and every time step. <strong>In other words, we can turn this sequence-to-vector RNN into a sequence-to-sequence RNN.</strong> The advantage of this technique is that the loss will contain a term for the output of the RNN at each and every time step, not just for the output at the last time step.</p>
<p>This means there will be many more error gradients flowing through the model, and they won’t have to flow through time as much since they will come from the output of each time step, not just the last one. This will both stabilize and speed up training. <strong>To be clear, at time step 0 the model will output a vector containing the forecasts for time steps 1 to 14, then at time step 1 the model will forecast time steps 2 to 15, and so on.</strong> In other words, the targets are sequences of consecutive windows, shifted by one time step at each time step. The target is not a vector anymore, but a sequence of the same length as the inputs, containing a 14-dimensional vector at each step.</p>
<p>Preparing the datasets is not trivial, since each instance has a window as input and a sequence of windows as output. One way to do this is to use the <code>to_windows()</code> utility function we created earlier, twice in a row, to get windows of consecutive windows. For example, let’s turn the series of numbers 0 to 6 into a dataset containing sequences of 4 consecutive windows, each of length 3:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:11,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683352633019,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="2850969e-4abe-4a85-e40e-e3fa8021b153">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1"></a>my_series <span class="op">=</span> tf.data.Dataset.<span class="bu">range</span>(<span class="dv">7</span>)</span>
<span id="cb85-2"><a href="#cb85-2"></a>dataset <span class="op">=</span> to_windows(to_windows(my_series, <span class="dv">3</span>), <span class="dv">4</span>)</span>
<span id="cb85-3"><a href="#cb85-3"></a><span class="bu">list</span>(dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="63">
<pre><code>[&lt;tf.Tensor: shape=(4, 3), dtype=int64, numpy=
 array([[0, 1, 2],
        [1, 2, 3],
        [2, 3, 4],
        [3, 4, 5]])&gt;,
 &lt;tf.Tensor: shape=(4, 3), dtype=int64, numpy=
 array([[1, 2, 3],
        [2, 3, 4],
        [3, 4, 5],
        [4, 5, 6]])&gt;]</code></pre>
</div>
</div>
<p>Now we can use the <code>map()</code> method to split these windows of windows into inputs and targets:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:10,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683352633020,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="a2fce76a-b7a6-46b9-a017-c5c3e666996d">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1"></a>dataset <span class="op">=</span> dataset.<span class="bu">map</span>(<span class="kw">lambda</span> S: (S[:, <span class="dv">0</span>], S[:, <span class="dv">1</span>:]))</span>
<span id="cb87-2"><a href="#cb87-2"></a><span class="bu">list</span>(dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>[(&lt;tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 1, 2, 3])&gt;,
  &lt;tf.Tensor: shape=(4, 2), dtype=int64, numpy=
  array([[1, 2],
         [2, 3],
         [3, 4],
         [4, 5]])&gt;),
 (&lt;tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 2, 3, 4])&gt;,
  &lt;tf.Tensor: shape=(4, 2), dtype=int64, numpy=
  array([[2, 3],
         [3, 4],
         [4, 5],
         [5, 6]])&gt;)]</code></pre>
</div>
</div>
<p>Now the dataset contains sequences of length 4 as inputs, and the targets are sequences containing the next two steps, for each time step. For example, the first input sequence is <code>[0, 1, 2, 3]</code>, and its corresponding targets are <code>[[1, 2], [2, 3], [3, 4], [4, 5]]</code>, which are the next two values for each time step.</p>
<p>Let’s create another little utility function to prepare the datasets for our sequence-to-sequence model. It will also take care of shuffling (optional) and batching:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1"></a><span class="kw">def</span> to_seq2seq_dataset(series, seq_length<span class="op">=</span><span class="dv">56</span>, ahead<span class="op">=</span><span class="dv">14</span>, target_col<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb89-2"><a href="#cb89-2"></a>                       batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>, seed<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb89-3"><a href="#cb89-3"></a>    ds <span class="op">=</span> to_windows(tf.data.Dataset.from_tensor_slices(series), ahead <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb89-4"><a href="#cb89-4"></a>    ds <span class="op">=</span> to_windows(ds, seq_length).<span class="bu">map</span>(<span class="kw">lambda</span> S: (S[:, <span class="dv">0</span>], S[:, <span class="dv">1</span>:, <span class="dv">1</span>]))</span>
<span id="cb89-5"><a href="#cb89-5"></a>    <span class="cf">if</span> shuffle:</span>
<span id="cb89-6"><a href="#cb89-6"></a>        ds <span class="op">=</span> ds.shuffle(<span class="dv">8</span> <span class="op">*</span> batch_size, seed<span class="op">=</span>seed)</span>
<span id="cb89-7"><a href="#cb89-7"></a>    <span class="cf">return</span> ds.batch(batch_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can use this function to create the datasets:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1"></a>seq2seq_train <span class="op">=</span> to_seq2seq_dataset(mulvar_train, shuffle<span class="op">=</span><span class="va">True</span>, seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb90-2"><a href="#cb90-2"></a>seq2seq_valid <span class="op">=</span> to_seq2seq_dataset(mulvar_valid)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And lastly, we can build the sequence-to-sequence model:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility</span></span>
<span id="cb91-2"><a href="#cb91-2"></a>seq2seq_model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb91-3"><a href="#cb91-3"></a>    tf.keras.layers.SimpleRNN(<span class="dv">32</span>, return_sequences<span class="op">=</span><span class="va">True</span>, input_shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">5</span>]),</span>
<span id="cb91-4"><a href="#cb91-4"></a>    tf.keras.layers.Dense(<span class="dv">14</span>) <span class="co"># Output is 14 dimension</span></span>
<span id="cb91-5"><a href="#cb91-5"></a>    <span class="co"># equivalent: tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(14))</span></span>
<span id="cb91-6"><a href="#cb91-6"></a>    <span class="co"># also equivalent: tf.keras.layers.Conv1D(14, kernel_size=1)</span></span>
<span id="cb91-7"><a href="#cb91-7"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:11,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683353211969,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="5497a1e9-da57-44c1-84e3-4d7182ebd3f7">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1"></a>seq2seq_model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn_8 (SimpleRNN)    (None, None, 32)          1216      
                                                                 
 dense_6 (Dense)             (None, None, 14)          462       
                                                                 
=================================================================
Total params: 1,678
Trainable params: 1,678
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<p>It is almost identical to our previous model: the only difference is that we set <code>return_sequences=True</code> in the <code>SimpleRNN</code> layer. This way, it will output a sequence of vectors (each of size 32), instead of outputting a single vector at the last time step. The <code>Dense</code> layer is smart enough to handle sequences as input: it will be applied at each time step, taking a 32-dimensional vector as input and outputting a 14-dimensional vector. In fact, another way to get the exact same result is to use a <code>Conv1D</code> layer with a kernel size of 1: <code>Conv1D(14, kernel_size=1)</code>.</p>
<blockquote class="blockquote">
<p><code>tf.Keras</code> offers a <code>TimeDistributed</code> layer that lets you apply any vector-to-vector layer to every vector in the input sequences, at every time step. It does this efficiently, by reshaping the inputs so that each time step is treated as a separate instance, then it reshapes the layer’s outputs to recover the time dimension. <strong>In our case, we don’t need it since the <code>Dense</code> layer already supports sequences as inputs.</strong></p>
</blockquote>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:577276,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683353210290,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="8a21a726-2c90-40e2-fd38-ac817410c9d7">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1"></a>fit_and_evaluate(seq2seq_model, seq2seq_train, seq2seq_valid, learning_rate<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/500
33/33 [==============================] - 3s 68ms/step - loss: 0.0542 - mae: 0.2405 - val_loss: 0.0141 - val_mae: 0.1308
Epoch 2/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0093 - mae: 0.1001 - val_loss: 0.0067 - val_mae: 0.0815
Epoch 3/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0065 - mae: 0.0788 - val_loss: 0.0057 - val_mae: 0.0723
Epoch 4/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0056 - mae: 0.0709 - val_loss: 0.0051 - val_mae: 0.0672
Epoch 5/500
33/33 [==============================] - 2s 70ms/step - loss: 0.0051 - mae: 0.0665 - val_loss: 0.0046 - val_mae: 0.0621
Epoch 6/500
33/33 [==============================] - 3s 80ms/step - loss: 0.0048 - mae: 0.0645 - val_loss: 0.0047 - val_mae: 0.0637
Epoch 7/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0046 - mae: 0.0625 - val_loss: 0.0045 - val_mae: 0.0604
Epoch 8/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0044 - mae: 0.0609 - val_loss: 0.0043 - val_mae: 0.0582
Epoch 9/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0043 - mae: 0.0602 - val_loss: 0.0041 - val_mae: 0.0564
Epoch 10/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0042 - mae: 0.0593 - val_loss: 0.0043 - val_mae: 0.0584
Epoch 11/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0041 - mae: 0.0585 - val_loss: 0.0042 - val_mae: 0.0574
Epoch 12/500
33/33 [==============================] - 2s 65ms/step - loss: 0.0041 - mae: 0.0580 - val_loss: 0.0041 - val_mae: 0.0556
Epoch 13/500
33/33 [==============================] - 3s 89ms/step - loss: 0.0040 - mae: 0.0568 - val_loss: 0.0042 - val_mae: 0.0574
Epoch 14/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0039 - mae: 0.0560 - val_loss: 0.0042 - val_mae: 0.0567
Epoch 15/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0039 - mae: 0.0557 - val_loss: 0.0040 - val_mae: 0.0544
Epoch 16/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0039 - mae: 0.0555 - val_loss: 0.0040 - val_mae: 0.0544
Epoch 17/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0038 - mae: 0.0550 - val_loss: 0.0040 - val_mae: 0.0539
Epoch 18/500
33/33 [==============================] - 3s 100ms/step - loss: 0.0038 - mae: 0.0545 - val_loss: 0.0041 - val_mae: 0.0553
Epoch 19/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0037 - mae: 0.0540 - val_loss: 0.0040 - val_mae: 0.0539
Epoch 20/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0037 - mae: 0.0537 - val_loss: 0.0039 - val_mae: 0.0533
Epoch 21/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0037 - mae: 0.0537 - val_loss: 0.0040 - val_mae: 0.0535
Epoch 22/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0037 - mae: 0.0535 - val_loss: 0.0040 - val_mae: 0.0533
Epoch 23/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0037 - mae: 0.0536 - val_loss: 0.0039 - val_mae: 0.0527
Epoch 24/500
33/33 [==============================] - 3s 98ms/step - loss: 0.0036 - mae: 0.0526 - val_loss: 0.0039 - val_mae: 0.0529
Epoch 25/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0036 - mae: 0.0527 - val_loss: 0.0039 - val_mae: 0.0522
Epoch 26/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0036 - mae: 0.0523 - val_loss: 0.0039 - val_mae: 0.0518
Epoch 27/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0036 - mae: 0.0524 - val_loss: 0.0039 - val_mae: 0.0526
Epoch 28/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0035 - mae: 0.0520 - val_loss: 0.0038 - val_mae: 0.0513
Epoch 29/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0035 - mae: 0.0521 - val_loss: 0.0039 - val_mae: 0.0526
Epoch 30/500
33/33 [==============================] - 3s 83ms/step - loss: 0.0036 - mae: 0.0524 - val_loss: 0.0040 - val_mae: 0.0535
Epoch 31/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0035 - mae: 0.0518 - val_loss: 0.0038 - val_mae: 0.0515
Epoch 32/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0035 - mae: 0.0509 - val_loss: 0.0038 - val_mae: 0.0513
Epoch 33/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0035 - mae: 0.0514 - val_loss: 0.0038 - val_mae: 0.0508
Epoch 34/500
33/33 [==============================] - 3s 100ms/step - loss: 0.0035 - mae: 0.0511 - val_loss: 0.0039 - val_mae: 0.0517
Epoch 35/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0034 - mae: 0.0506 - val_loss: 0.0038 - val_mae: 0.0510
Epoch 36/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0034 - mae: 0.0507 - val_loss: 0.0038 - val_mae: 0.0507
Epoch 37/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0034 - mae: 0.0507 - val_loss: 0.0038 - val_mae: 0.0509
Epoch 38/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0034 - mae: 0.0503 - val_loss: 0.0039 - val_mae: 0.0519
Epoch 39/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0034 - mae: 0.0506 - val_loss: 0.0038 - val_mae: 0.0510
Epoch 40/500
33/33 [==============================] - 3s 104ms/step - loss: 0.0034 - mae: 0.0501 - val_loss: 0.0038 - val_mae: 0.0507
Epoch 41/500
33/33 [==============================] - 2s 53ms/step - loss: 0.0034 - mae: 0.0500 - val_loss: 0.0039 - val_mae: 0.0515
Epoch 42/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0034 - mae: 0.0499 - val_loss: 0.0038 - val_mae: 0.0506
Epoch 43/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0034 - mae: 0.0497 - val_loss: 0.0038 - val_mae: 0.0509
Epoch 44/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0034 - mae: 0.0496 - val_loss: 0.0038 - val_mae: 0.0508
Epoch 45/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0034 - mae: 0.0501 - val_loss: 0.0038 - val_mae: 0.0506
Epoch 46/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0034 - mae: 0.0499 - val_loss: 0.0039 - val_mae: 0.0519
Epoch 47/500
33/33 [==============================] - 3s 77ms/step - loss: 0.0033 - mae: 0.0494 - val_loss: 0.0038 - val_mae: 0.0509
Epoch 48/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0033 - mae: 0.0495 - val_loss: 0.0039 - val_mae: 0.0534
Epoch 49/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0033 - mae: 0.0498 - val_loss: 0.0038 - val_mae: 0.0518
Epoch 50/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0034 - mae: 0.0501 - val_loss: 0.0038 - val_mae: 0.0504
Epoch 51/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0033 - mae: 0.0491 - val_loss: 0.0038 - val_mae: 0.0504
Epoch 52/500
33/33 [==============================] - 3s 77ms/step - loss: 0.0033 - mae: 0.0489 - val_loss: 0.0038 - val_mae: 0.0504
Epoch 53/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0033 - mae: 0.0493 - val_loss: 0.0038 - val_mae: 0.0504
Epoch 54/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0033 - mae: 0.0490 - val_loss: 0.0040 - val_mae: 0.0534
Epoch 55/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0033 - mae: 0.0492 - val_loss: 0.0038 - val_mae: 0.0505
Epoch 56/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0033 - mae: 0.0493 - val_loss: 0.0038 - val_mae: 0.0512
Epoch 57/500
33/33 [==============================] - 3s 96ms/step - loss: 0.0033 - mae: 0.0489 - val_loss: 0.0039 - val_mae: 0.0530
Epoch 58/500
33/33 [==============================] - 2s 60ms/step - loss: 0.0033 - mae: 0.0490 - val_loss: 0.0038 - val_mae: 0.0507
Epoch 59/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0033 - mae: 0.0493 - val_loss: 0.0037 - val_mae: 0.0498
Epoch 60/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0033 - mae: 0.0488 - val_loss: 0.0041 - val_mae: 0.0567
Epoch 61/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0033 - mae: 0.0496 - val_loss: 0.0038 - val_mae: 0.0521
Epoch 62/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0033 - mae: 0.0491 - val_loss: 0.0039 - val_mae: 0.0523
Epoch 63/500
33/33 [==============================] - 2s 68ms/step - loss: 0.0033 - mae: 0.0494 - val_loss: 0.0039 - val_mae: 0.0526
Epoch 64/500
33/33 [==============================] - 3s 90ms/step - loss: 0.0033 - mae: 0.0494 - val_loss: 0.0037 - val_mae: 0.0497
Epoch 65/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0486 - val_loss: 0.0037 - val_mae: 0.0497
Epoch 66/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0483 - val_loss: 0.0037 - val_mae: 0.0496
Epoch 67/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0486 - val_loss: 0.0038 - val_mae: 0.0502
Epoch 68/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0032 - mae: 0.0483 - val_loss: 0.0038 - val_mae: 0.0502
Epoch 69/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0032 - mae: 0.0485 - val_loss: 0.0037 - val_mae: 0.0497
Epoch 70/500
33/33 [==============================] - 3s 86ms/step - loss: 0.0032 - mae: 0.0484 - val_loss: 0.0037 - val_mae: 0.0498
Epoch 71/500
33/33 [==============================] - 2s 66ms/step - loss: 0.0032 - mae: 0.0485 - val_loss: 0.0037 - val_mae: 0.0499
Epoch 72/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0032 - mae: 0.0486 - val_loss: 0.0037 - val_mae: 0.0498
Epoch 73/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0485 - val_loss: 0.0039 - val_mae: 0.0538
Epoch 74/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0033 - mae: 0.0500 - val_loss: 0.0037 - val_mae: 0.0494
Epoch 75/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0037 - val_mae: 0.0500
Epoch 76/500
33/33 [==============================] - 2s 52ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0037 - val_mae: 0.0496
Epoch 77/500
33/33 [==============================] - 3s 85ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0037 - val_mae: 0.0494
Epoch 78/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0037 - val_mae: 0.0495
Epoch 79/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0484 - val_loss: 0.0039 - val_mae: 0.0529
Epoch 80/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0488 - val_loss: 0.0037 - val_mae: 0.0498
Epoch 81/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0032 - mae: 0.0484 - val_loss: 0.0041 - val_mae: 0.0555
Epoch 82/500
33/33 [==============================] - 3s 75ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0037 - val_mae: 0.0492
Epoch 83/500
33/33 [==============================] - 3s 83ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0038 - val_mae: 0.0502
Epoch 84/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0483 - val_loss: 0.0037 - val_mae: 0.0496
Epoch 85/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0484 - val_loss: 0.0037 - val_mae: 0.0494
Epoch 86/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0037 - val_mae: 0.0493
Epoch 87/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0037 - val_mae: 0.0494
Epoch 88/500
33/33 [==============================] - 3s 94ms/step - loss: 0.0032 - mae: 0.0485 - val_loss: 0.0037 - val_mae: 0.0493
Epoch 89/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0037 - val_mae: 0.0493
Epoch 90/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0475 - val_loss: 0.0038 - val_mae: 0.0506
Epoch 91/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0037 - val_mae: 0.0495
Epoch 92/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0479 - val_loss: 0.0038 - val_mae: 0.0510
Epoch 93/500
33/33 [==============================] - 2s 70ms/step - loss: 0.0032 - mae: 0.0475 - val_loss: 0.0038 - val_mae: 0.0503
Epoch 94/500
33/33 [==============================] - 3s 82ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0037 - val_mae: 0.0492
Epoch 95/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0032 - mae: 0.0474 - val_loss: 0.0037 - val_mae: 0.0494
Epoch 96/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0475 - val_loss: 0.0038 - val_mae: 0.0524
Epoch 97/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0484 - val_loss: 0.0037 - val_mae: 0.0493
Epoch 98/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0037 - val_mae: 0.0499
Epoch 99/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0032 - mae: 0.0480 - val_loss: 0.0037 - val_mae: 0.0495
Epoch 100/500
33/33 [==============================] - 4s 103ms/step - loss: 0.0032 - mae: 0.0484 - val_loss: 0.0037 - val_mae: 0.0494
Epoch 101/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0038 - val_mae: 0.0527
Epoch 102/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0037 - val_mae: 0.0492
Epoch 103/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0032 - mae: 0.0477 - val_loss: 0.0037 - val_mae: 0.0496
Epoch 104/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0037 - val_mae: 0.0492
Epoch 105/500
33/33 [==============================] - 3s 94ms/step - loss: 0.0032 - mae: 0.0486 - val_loss: 0.0037 - val_mae: 0.0490
Epoch 106/500
33/33 [==============================] - 2s 59ms/step - loss: 0.0032 - mae: 0.0475 - val_loss: 0.0037 - val_mae: 0.0491
Epoch 107/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0037 - val_mae: 0.0494
Epoch 108/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0039 - val_mae: 0.0527
Epoch 109/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0483 - val_loss: 0.0037 - val_mae: 0.0491
Epoch 110/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0038 - val_mae: 0.0501
Epoch 111/500
33/33 [==============================] - 3s 100ms/step - loss: 0.0032 - mae: 0.0485 - val_loss: 0.0039 - val_mae: 0.0526
Epoch 112/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0032 - mae: 0.0488 - val_loss: 0.0038 - val_mae: 0.0512
Epoch 113/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 114/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0038 - val_mae: 0.0504
Epoch 115/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0037 - val_mae: 0.0494
Epoch 116/500
33/33 [==============================] - 3s 95ms/step - loss: 0.0031 - mae: 0.0476 - val_loss: 0.0037 - val_mae: 0.0501
Epoch 117/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0476 - val_loss: 0.0037 - val_mae: 0.0493
Epoch 118/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0037 - val_mae: 0.0490
Epoch 119/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0032 - mae: 0.0480 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 120/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0037 - val_mae: 0.0498
Epoch 121/500
33/33 [==============================] - 2s 58ms/step - loss: 0.0031 - mae: 0.0474 - val_loss: 0.0037 - val_mae: 0.0496
Epoch 122/500
33/33 [==============================] - 3s 81ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0491
Epoch 123/500
33/33 [==============================] - 2s 52ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0038 - val_mae: 0.0517
Epoch 124/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0493
Epoch 125/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0477 - val_loss: 0.0037 - val_mae: 0.0490
Epoch 126/500
33/33 [==============================] - 2s 46ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0504
Epoch 127/500
33/33 [==============================] - 2s 67ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0490
Epoch 128/500
33/33 [==============================] - 3s 81ms/step - loss: 0.0032 - mae: 0.0481 - val_loss: 0.0039 - val_mae: 0.0525
Epoch 129/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0039 - val_mae: 0.0536
Epoch 130/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0032 - mae: 0.0478 - val_loss: 0.0038 - val_mae: 0.0517
Epoch 131/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0032 - mae: 0.0482 - val_loss: 0.0037 - val_mae: 0.0498
Epoch 132/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0475 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 133/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 134/500
33/33 [==============================] - 4s 103ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0492
Epoch 135/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0493
Epoch 136/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0490
Epoch 137/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0494
Epoch 138/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0038 - val_mae: 0.0518
Epoch 139/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0037 - val_mae: 0.0491
Epoch 140/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0490
Epoch 141/500
33/33 [==============================] - 3s 85ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0037 - val_mae: 0.0492
Epoch 142/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0037 - val_mae: 0.0494
Epoch 143/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0497
Epoch 144/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0038 - val_mae: 0.0507
Epoch 145/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0491
Epoch 146/500
33/33 [==============================] - 3s 91ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0039 - val_mae: 0.0533
Epoch 147/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0480 - val_loss: 0.0037 - val_mae: 0.0496
Epoch 148/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0491
Epoch 149/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0476 - val_loss: 0.0037 - val_mae: 0.0500
Epoch 150/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0037 - val_mae: 0.0491
Epoch 151/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 152/500
33/33 [==============================] - 3s 97ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0490
Epoch 153/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0037 - val_mae: 0.0488
Epoch 154/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0493
Epoch 155/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0495
Epoch 156/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0037 - val_mae: 0.0497
Epoch 157/500
33/33 [==============================] - 2s 67ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0491
Epoch 158/500
33/33 [==============================] - 3s 86ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 159/500
33/33 [==============================] - 2s 53ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0037 - val_mae: 0.0487
Epoch 160/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 161/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 162/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0488
Epoch 163/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0038 - val_mae: 0.0503
Epoch 164/500
33/33 [==============================] - 3s 90ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0488
Epoch 165/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0485
Epoch 166/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0038 - val_mae: 0.0510
Epoch 167/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0037 - val_mae: 0.0497
Epoch 168/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0038 - val_mae: 0.0509
Epoch 169/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0037 - val_mae: 0.0496
Epoch 170/500
33/33 [==============================] - 3s 84ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0492
Epoch 171/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0473 - val_loss: 0.0037 - val_mae: 0.0496
Epoch 172/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0496
Epoch 173/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0036 - val_mae: 0.0486
Epoch 174/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0491
Epoch 175/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 176/500
33/33 [==============================] - 2s 72ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0490
Epoch 177/500
33/33 [==============================] - 3s 84ms/step - loss: 0.0031 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0488
Epoch 178/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0037 - val_mae: 0.0493
Epoch 179/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0486
Epoch 180/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0037 - val_mae: 0.0495
Epoch 181/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0031 - mae: 0.0471 - val_loss: 0.0037 - val_mae: 0.0487
Epoch 182/500
33/33 [==============================] - 3s 88ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0490
Epoch 183/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0488
Epoch 184/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0490
Epoch 185/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0508
Epoch 186/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0469 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 187/500
33/33 [==============================] - 2s 64ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 188/500
33/33 [==============================] - 3s 91ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0037 - val_mae: 0.0493
Epoch 189/500
33/33 [==============================] - 2s 48ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0037 - val_mae: 0.0488
Epoch 190/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 191/500
33/33 [==============================] - 2s 47ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0492
Epoch 192/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0488
Epoch 193/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0031 - mae: 0.0476 - val_loss: 0.0036 - val_mae: 0.0486
Epoch 194/500
33/33 [==============================] - 3s 97ms/step - loss: 0.0030 - mae: 0.0465 - val_loss: 0.0038 - val_mae: 0.0509
Epoch 195/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0036 - val_mae: 0.0485
Epoch 196/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0493
Epoch 197/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0037 - val_mae: 0.0488
Epoch 198/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0486
Epoch 199/500
33/33 [==============================] - 3s 94ms/step - loss: 0.0031 - mae: 0.0467 - val_loss: 0.0037 - val_mae: 0.0485
Epoch 200/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0501
Epoch 201/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0488
Epoch 202/500
33/33 [==============================] - 2s 55ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0037 - val_mae: 0.0488
Epoch 203/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 204/500
33/33 [==============================] - 3s 95ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0490
Epoch 205/500
33/33 [==============================] - 2s 62ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0488
Epoch 206/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0037 - val_mae: 0.0486
Epoch 207/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0037 - val_mae: 0.0502
Epoch 208/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0487
Epoch 209/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0498
Epoch 210/500
33/33 [==============================] - 2s 69ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0492
Epoch 211/500
33/33 [==============================] - 3s 92ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0487
Epoch 212/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0038 - val_mae: 0.0513
Epoch 213/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0030 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0485
Epoch 214/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0037 - val_mae: 0.0487
Epoch 215/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0488
Epoch 216/500
33/33 [==============================] - 2s 53ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0486
Epoch 217/500
33/33 [==============================] - 3s 89ms/step - loss: 0.0030 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0497
Epoch 218/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0031 - mae: 0.0466 - val_loss: 0.0037 - val_mae: 0.0486
Epoch 219/500
33/33 [==============================] - 2s 52ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0037 - val_mae: 0.0486
Epoch 220/500
33/33 [==============================] - 2s 53ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 221/500
33/33 [==============================] - 2s 58ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0037 - val_mae: 0.0496
Epoch 222/500
33/33 [==============================] - 3s 99ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0036 - val_mae: 0.0485
Epoch 223/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0037 - val_mae: 0.0485
Epoch 224/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0037 - val_mae: 0.0504
Epoch 225/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0463 - val_loss: 0.0037 - val_mae: 0.0497
Epoch 226/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0037 - val_mae: 0.0496
Epoch 227/500
33/33 [==============================] - 4s 109ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0491
Epoch 228/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0037 - val_mae: 0.0496
Epoch 229/500
33/33 [==============================] - 2s 53ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0039 - val_mae: 0.0545
Epoch 230/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0031 - mae: 0.0472 - val_loss: 0.0037 - val_mae: 0.0486
Epoch 231/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0037 - val_mae: 0.0491
Epoch 232/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0487
Epoch 233/500
33/33 [==============================] - 3s 96ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0488
Epoch 234/500
33/33 [==============================] - 2s 66ms/step - loss: 0.0030 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0487
Epoch 235/500
33/33 [==============================] - 2s 52ms/step - loss: 0.0030 - mae: 0.0461 - val_loss: 0.0036 - val_mae: 0.0485
Epoch 236/500
33/33 [==============================] - 2s 52ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0037 - val_mae: 0.0487
Epoch 237/500
33/33 [==============================] - 2s 50ms/step - loss: 0.0030 - mae: 0.0460 - val_loss: 0.0037 - val_mae: 0.0490
Epoch 238/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0030 - mae: 0.0462 - val_loss: 0.0037 - val_mae: 0.0489
Epoch 239/500
33/33 [==============================] - 3s 101ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0500
Epoch 240/500
33/33 [==============================] - 2s 49ms/step - loss: 0.0030 - mae: 0.0465 - val_loss: 0.0037 - val_mae: 0.0498
Epoch 241/500
33/33 [==============================] - 2s 52ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0037 - val_mae: 0.0491
Epoch 242/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0030 - mae: 0.0459 - val_loss: 0.0038 - val_mae: 0.0516
Epoch 243/500
33/33 [==============================] - 2s 51ms/step - loss: 0.0030 - mae: 0.0464 - val_loss: 0.0037 - val_mae: 0.0490
Epoch 244/500
33/33 [==============================] - 2s 53ms/step - loss: 0.0031 - mae: 0.0468 - val_loss: 0.0037 - val_mae: 0.0497
Epoch 245/500
33/33 [==============================] - 4s 104ms/step - loss: 0.0030 - mae: 0.0467 - val_loss: 0.0037 - val_mae: 0.0497
3/3 [==============================] - 0s 52ms/step - loss: 0.0036 - mae: 0.0485</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="68">
<pre><code>48454.705625772476</code></pre>
</div>
</div>
<p>The training code is the same as usual. During training, all the model’s outputs are used, but after training only the output of the very last time step matters, and the rest can be ignored. For example, we can forecast the rail ridership for the next 14 days like this:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1690,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683353211969,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="d3c1377f-983d-4a85-fcd5-c60cd8b1ebd7">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1"></a>X <span class="op">=</span> mulvar_valid.to_numpy()[np.newaxis, :seq_length]</span>
<span id="cb97-2"><a href="#cb97-2"></a>y_pred_14 <span class="op">=</span> seq2seq_model.predict(X)[<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]  <span class="co"># only the last time step's output</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 146ms/step</code></pre>
</div>
</div>
<p>If you evaluate this model’s forecasts for <span class="math inline">\(t+1\)</span>, you will find a validation MAE of 24,655. For <span class="math inline">\(t+2\)</span> it’s 29,310, and the performance continues to drop gradually as the model tries to forecast further into the future. At <span class="math inline">\(t+14\)</span>, the MAE is 34,311.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:7,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683353211970,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="0ac58bf9-dd97-46ae-879d-a14c95493714">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1"></a>Y_pred_valid <span class="op">=</span> seq2seq_model.predict(seq2seq_valid)</span>
<span id="cb99-2"><a href="#cb99-2"></a><span class="cf">for</span> ahead <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">14</span>):</span>
<span id="cb99-3"><a href="#cb99-3"></a>    preds <span class="op">=</span> pd.Series(Y_pred_valid[:<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, ahead],</span>
<span id="cb99-4"><a href="#cb99-4"></a>                      index<span class="op">=</span>mulvar_valid.index[<span class="dv">56</span> <span class="op">+</span> ahead : <span class="op">-</span><span class="dv">14</span> <span class="op">+</span> ahead])</span>
<span id="cb99-5"><a href="#cb99-5"></a>    mae <span class="op">=</span> (preds <span class="op">-</span> mulvar_valid[<span class="st">"rail"</span>]).<span class="bu">abs</span>().mean() <span class="op">*</span> <span class="fl">1e6</span></span>
<span id="cb99-6"><a href="#cb99-6"></a>    <span class="bu">print</span>(<span class="ss">f"MAE for +</span><span class="sc">{</span>ahead <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>mae<span class="sc">:,</span><span class="fl">.0</span><span class="er">f</span><span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3/3 [==============================] - 0s 23ms/step
MAE for +1: 24,655
MAE for +2: 29,310
MAE for +3: 32,148
MAE for +4: 34,271
MAE for +5: 34,646
MAE for +6: 34,537
MAE for +7: 36,120
MAE for +8: 38,538
MAE for +9: 34,308
MAE for +10: 31,896
MAE for +11: 37,567
MAE for +12: 36,741
MAE for +13: 36,003
MAE for +14: 34,311</code></pre>
</div>
</div>
<p>Simple RNNs can be quite good at forecasting time series or handling other kinds of sequences, but they do not perform as well on long time series or sequences.</p>
</section>
<section id="deep-rnns-with-layer-norm" class="level3" data-number="10.2.11">
<h3 data-number="10.2.11" class="anchored" data-anchor-id="deep-rnns-with-layer-norm"><span class="header-section-number">10.2.11</span> Deep RNNs with Layer Norm</h3>
<p>Let’s use <code>tf.keras</code> to implement Layer Normalization within a simple memory cell. We need to define a custom memory cell. It is just like a regular layer, except its <code>call()</code> method takes two arguments: the inputs at the current time step and the hidden states from the previous time step. Note that the <code>states</code> argument is a list containing one or more tensors. In the case of a simple RNN cell it contains a single tensor equal to the outputs of the previous time step, but other cells may have multiple state tensors (e.g., an <code>LSTMCell</code> has a long-term state and a short-term state). A cell must also have a <code>state_size</code> attribute and an <code>output_size</code> attribute. In a simple RNN, both are simply equal to the number of units. The following code implements a custom memory cell which will behave like a <code>SimpleRNNCell</code>, except it will also apply Layer Normalization at each time step:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1"></a><span class="kw">class</span> LNSimpleRNNCell(tf.keras.layers.Layer):</span>
<span id="cb101-2"><a href="#cb101-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, units, activation<span class="op">=</span><span class="st">"tanh"</span>, <span class="op">**</span>kwargs):</span>
<span id="cb101-3"><a href="#cb101-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb101-4"><a href="#cb101-4"></a>        <span class="va">self</span>.state_size <span class="op">=</span> units</span>
<span id="cb101-5"><a href="#cb101-5"></a>        <span class="va">self</span>.output_size <span class="op">=</span> units</span>
<span id="cb101-6"><a href="#cb101-6"></a>        <span class="va">self</span>.simple_rnn_cell <span class="op">=</span> tf.keras.layers.SimpleRNNCell(units,</span>
<span id="cb101-7"><a href="#cb101-7"></a>                                                             activation<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb101-8"><a href="#cb101-8"></a>        <span class="va">self</span>.layer_norm <span class="op">=</span> tf.keras.layers.LayerNormalization()</span>
<span id="cb101-9"><a href="#cb101-9"></a>        <span class="va">self</span>.activation <span class="op">=</span> tf.keras.activations.get(activation)</span>
<span id="cb101-10"><a href="#cb101-10"></a></span>
<span id="cb101-11"><a href="#cb101-11"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, states):</span>
<span id="cb101-12"><a href="#cb101-12"></a>        outputs, new_states <span class="op">=</span> <span class="va">self</span>.simple_rnn_cell(inputs, states)</span>
<span id="cb101-13"><a href="#cb101-13"></a>        norm_outputs <span class="op">=</span> <span class="va">self</span>.activation(<span class="va">self</span>.layer_norm(outputs))</span>
<span id="cb101-14"><a href="#cb101-14"></a>        <span class="cf">return</span> norm_outputs, [norm_outputs]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility</span></span>
<span id="cb102-2"><a href="#cb102-2"></a>custom_ln_model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb102-3"><a href="#cb102-3"></a>    tf.keras.layers.RNN(LNSimpleRNNCell(<span class="dv">32</span>), return_sequences<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb102-4"><a href="#cb102-4"></a>                        input_shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">5</span>]),</span>
<span id="cb102-5"><a href="#cb102-5"></a>    tf.keras.layers.Dense(<span class="dv">14</span>)</span>
<span id="cb102-6"><a href="#cb102-6"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:41367,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683353929892,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="bee58c16-e972-4f9f-cf1f-5f61ce186bd1">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1"></a>fit_and_evaluate(custom_ln_model, seq2seq_train, seq2seq_valid, learning_rate<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/5
33/33 [==============================] - 6s 108ms/step - loss: 0.0660 - mae: 0.2576 - val_loss: 0.0178 - val_mae: 0.1455
Epoch 2/5
33/33 [==============================] - 5s 165ms/step - loss: 0.0150 - mae: 0.1458 - val_loss: 0.0169 - val_mae: 0.1272
Epoch 3/5
33/33 [==============================] - 4s 119ms/step - loss: 0.0130 - mae: 0.1351 - val_loss: 0.0147 - val_mae: 0.1236
Epoch 4/5
33/33 [==============================] - 9s 265ms/step - loss: 0.0121 - mae: 0.1291 - val_loss: 0.0137 - val_mae: 0.1190
Epoch 5/5
33/33 [==============================] - 9s 259ms/step - loss: 0.0115 - mae: 0.1232 - val_loss: 0.0134 - val_mae: 0.1156
3/3 [==============================] - 0s 33ms/step - loss: 0.0134 - mae: 0.1156</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="74">
<pre><code>115600.42947530746</code></pre>
</div>
</div>
<p>Similarly, you could create a custom cell to apply dropout between each time step. But there’s a simpler way: all recurrent layers and all cells provided by <code>tf.Keras</code> have a <code>dropout</code> hyperparameter and a <a href="https://www.tensorflow.org/guide/keras/rnn"><code>recurrent_dropout</code></a> hyperparameter: the <strong>former defines the dropout rate to apply to the inputs</strong> (at each time step), and the latter defines the <strong>dropout rate for the hidden states</strong> (also at each time step).</p>
<p>With these techniques, you can alleviate the unstable gradients problem and train an RNN much more efficiently. Now let’s look at how to deal with the short-term memory problem.</p>
</section>
<section id="lstms" class="level3" data-number="10.2.12">
<h3 data-number="10.2.12" class="anchored" data-anchor-id="lstms"><span class="header-section-number">10.2.12</span> LSTMs</h3>
<p>In <code>tf.Keras</code>, you can simply use the <code>LSTM</code> layer instead of the <code>SimpleRNN</code> layer:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1446,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354094231,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="99ab2743-6044-42ee-aaef-3a6632fec53d" data-scrolled="true">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility</span></span>
<span id="cb106-2"><a href="#cb106-2"></a>lstm_model <span class="op">=</span> tf.keras.models.Sequential([</span>
<span id="cb106-3"><a href="#cb106-3"></a>    tf.keras.layers.LSTM(<span class="dv">32</span>, return_sequences<span class="op">=</span><span class="va">True</span>, input_shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">5</span>]),</span>
<span id="cb106-4"><a href="#cb106-4"></a>    tf.keras.layers.Dense(<span class="dv">14</span>)</span>
<span id="cb106-5"><a href="#cb106-5"></a>])</span>
<span id="cb106-6"><a href="#cb106-6"></a></span>
<span id="cb106-7"><a href="#cb106-7"></a>lstm_model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, None, 32)          4864      
                                                                 
 dense_8 (Dense)             (None, None, 14)          462       
                                                                 
=================================================================
Total params: 5,326
Trainable params: 5,326
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>Alternatively, you could use the general-purpose <code>tf.keras.layers.RNN</code> layer, giving it an <code>LSTMCell</code> as an argument. However, the <code>LSTM</code> layer uses an optimized implementation when running on a GPU.</p>
</blockquote>
<div class="cell">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1"></a><span class="co"># keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True, input_shape=[None, 1]) also works</span></span>
<span id="cb108-2"><a href="#cb108-2"></a><span class="co"># However, the LSTM layer uses an optimized implementation when running on a GPU</span></span>
<span id="cb108-3"><a href="#cb108-3"></a><span class="co"># RNN layer is mostly useful when you define custom cells, as we did earl</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:9160,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354257929,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="571041d7-5f89-4af6-f6c0-2f13ecfd4f6b">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1"></a>fit_and_evaluate(lstm_model, seq2seq_train, seq2seq_valid,</span>
<span id="cb109-2"><a href="#cb109-2"></a>                 learning_rate<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/5
33/33 [==============================] - 5s 47ms/step - loss: 0.0665 - mae: 0.2765 - val_loss: 0.0186 - val_mae: 0.1574
Epoch 2/5
33/33 [==============================] - 0s 13ms/step - loss: 0.0166 - mae: 0.1573 - val_loss: 0.0175 - val_mae: 0.1448
Epoch 3/5
33/33 [==============================] - 1s 14ms/step - loss: 0.0155 - mae: 0.1510 - val_loss: 0.0166 - val_mae: 0.1425
Epoch 4/5
33/33 [==============================] - 0s 13ms/step - loss: 0.0149 - mae: 0.1473 - val_loss: 0.0160 - val_mae: 0.1396
Epoch 5/5
33/33 [==============================] - 1s 13ms/step - loss: 0.0143 - mae: 0.1444 - val_loss: 0.0155 - val_mae: 0.1362
3/3 [==============================] - 0s 27ms/step - loss: 0.0155 - mae: 0.1362</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="76">
<pre><code>136158.73456001282</code></pre>
</div>
</div>
</section>
<section id="grus" class="level3" data-number="10.2.13">
<h3 data-number="10.2.13" class="anchored" data-anchor-id="grus"><span class="header-section-number">10.2.13</span> GRUs</h3>
<p><code>tf.Keras</code> provides a <code>tf.keras.layers.GRU</code> layer: using it is just a matter of replacing <code>SimpleRNN</code> or <code>LSTM</code> with <code>GRU</code>. It also provides a <code>tf.keras.layers.GRUCell</code>, in case you want to create a custom cell based on a <code>GRU</code> cell.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility</span></span>
<span id="cb112-2"><a href="#cb112-2"></a>gru_model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb112-3"><a href="#cb112-3"></a>    tf.keras.layers.GRU(<span class="dv">32</span>, return_sequences<span class="op">=</span><span class="va">True</span>, input_shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">5</span>]),</span>
<span id="cb112-4"><a href="#cb112-4"></a>    tf.keras.layers.Dense(<span class="dv">14</span>)</span>
<span id="cb112-5"><a href="#cb112-5"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:6574,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354273305,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="58b8918f-b430-488d-d353-b26fa07a1857">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1"></a>fit_and_evaluate(gru_model, seq2seq_train, seq2seq_valid, learning_rate<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/5
33/33 [==============================] - 4s 28ms/step - loss: 0.0568 - mae: 0.2545 - val_loss: 0.0187 - val_mae: 0.1694
Epoch 2/5
33/33 [==============================] - 0s 13ms/step - loss: 0.0162 - mae: 0.1500 - val_loss: 0.0156 - val_mae: 0.1373
Epoch 3/5
33/33 [==============================] - 0s 12ms/step - loss: 0.0134 - mae: 0.1369 - val_loss: 0.0140 - val_mae: 0.1283
Epoch 4/5
33/33 [==============================] - 0s 12ms/step - loss: 0.0121 - mae: 0.1286 - val_loss: 0.0128 - val_mae: 0.1229
Epoch 5/5
33/33 [==============================] - 1s 13ms/step - loss: 0.0112 - mae: 0.1225 - val_loss: 0.0121 - val_mae: 0.1179
3/3 [==============================] - 0s 27ms/step - loss: 0.0121 - mae: 0.1179</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="78">
<pre><code>117899.73080158234</code></pre>
</div>
</div>
</section>
<section id="using-one-dimensional-convolutional-layers-to-process-sequences" class="level3" data-number="10.2.14">
<h3 data-number="10.2.14" class="anchored" data-anchor-id="using-one-dimensional-convolutional-layers-to-process-sequences"><span class="header-section-number">10.2.14</span> Using One-Dimensional Convolutional Layers to Process Sequences</h3>
<p>The following model is the same as earlier, except it starts with a 1D convolutional layer that downsamples the input sequence by a factor of 2, using a stride of 2. The kernel size is larger than the stride, so all inputs will be used to compute the layer’s output, and therefore the model can learn to preserve the useful information, dropping only the unimportant details. By shortening the sequences the convolutional layer may help the GRU layers detect longer patterns, so we can afford to double the input sequence length to 112 days. Note that we must also crop off the first three time steps in the targets: indeed, the kernel’s size is 4, so the first output of the convolutional layer will be based on the input time steps 0 to 3, and the first forecasts will be for time steps 4 to 17 (instead of time steps 1 to 14). Moreover, we must downsample the targets by a factor of 2, because of the stride:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:965,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354319715,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="eb1b9370-6ac6-46a0-cb33-76dd0b3c0260">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1"></a>tf.random.set_seed(<span class="dv">42</span>)  <span class="co"># extra code – ensures reproducibility</span></span>
<span id="cb116-2"><a href="#cb116-2"></a>conv_rnn_model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb116-3"><a href="#cb116-3"></a>    tf.keras.layers.Conv1D(filters<span class="op">=</span><span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb116-4"><a href="#cb116-4"></a>                           activation<span class="op">=</span><span class="st">"relu"</span>, input_shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">5</span>]),</span>
<span id="cb116-5"><a href="#cb116-5"></a>    tf.keras.layers.GRU(<span class="dv">32</span>, return_sequences<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb116-6"><a href="#cb116-6"></a>    tf.keras.layers.Dense(<span class="dv">14</span>)</span>
<span id="cb116-7"><a href="#cb116-7"></a>])</span>
<span id="cb116-8"><a href="#cb116-8"></a></span>
<span id="cb116-9"><a href="#cb116-9"></a>conv_rnn_model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d (Conv1D)             (None, None, 32)          672       
                                                                 
 gru_1 (GRU)                 (None, None, 32)          6336      
                                                                 
 dense_10 (Dense)            (None, None, 14)          462       
                                                                 
=================================================================
Total params: 7,470
Trainable params: 7,470
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1"></a>longer_train <span class="op">=</span> to_seq2seq_dataset(mulvar_train, seq_length<span class="op">=</span><span class="dv">112</span>, shuffle<span class="op">=</span><span class="va">True</span>, seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb118-2"><a href="#cb118-2"></a>longer_valid <span class="op">=</span> to_seq2seq_dataset(mulvar_valid, seq_length<span class="op">=</span><span class="dv">112</span>)</span>
<span id="cb118-3"><a href="#cb118-3"></a>downsampled_train <span class="op">=</span> longer_train.<span class="bu">map</span>(<span class="kw">lambda</span> X, Y: (X, Y[:, <span class="dv">3</span>::<span class="dv">2</span>]))</span>
<span id="cb118-4"><a href="#cb118-4"></a>downsampled_valid <span class="op">=</span> longer_valid.<span class="bu">map</span>(<span class="kw">lambda</span> X, Y: (X, Y[:, <span class="dv">3</span>::<span class="dv">2</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:17348,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354350339,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="46ecb9ee-f312-4a8c-e596-8e6bbcc8b6ab">
<div class="sourceCode cell-code" id="cb119"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1"></a>fit_and_evaluate(conv_rnn_model, downsampled_train, downsampled_valid, learning_rate<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/5
31/31 [==============================] - 7s 33ms/step - loss: 0.0555 - mae: 0.2556 - val_loss: 0.0209 - val_mae: 0.1595
Epoch 2/5
31/31 [==============================] - 1s 21ms/step - loss: 0.0160 - mae: 0.1503 - val_loss: 0.0166 - val_mae: 0.1425
Epoch 3/5
31/31 [==============================] - 1s 36ms/step - loss: 0.0140 - mae: 0.1417 - val_loss: 0.0155 - val_mae: 0.1335
Epoch 4/5
31/31 [==============================] - 1s 28ms/step - loss: 0.0128 - mae: 0.1338 - val_loss: 0.0143 - val_mae: 0.1274
Epoch 5/5
31/31 [==============================] - 1s 21ms/step - loss: 0.0117 - mae: 0.1260 - val_loss: 0.0130 - val_mae: 0.1225
1/1 [==============================] - 0s 154ms/step - loss: 0.0130 - mae: 0.1225</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="81">
<pre><code>122495.1446056366</code></pre>
</div>
</div>
</section>
</section>
<section id="natural-language-processing" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="natural-language-processing"><span class="header-section-number">10.3</span> Natural-language processing</h2>
<section id="preparing-text-data" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="preparing-text-data"><span class="header-section-number">10.3.1</span> Preparing text data</h3>
<p>Vectorizing process using <code>Python</code> may be done as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1"></a><span class="kw">class</span> Vectorizer:</span>
<span id="cb122-2"><a href="#cb122-2"></a>    <span class="kw">def</span> standardize(<span class="va">self</span>, text):</span>
<span id="cb122-3"><a href="#cb122-3"></a>        text <span class="op">=</span> text.lower()</span>
<span id="cb122-4"><a href="#cb122-4"></a>        <span class="cf">return</span> <span class="st">""</span>.join(char <span class="cf">for</span> char <span class="kw">in</span> text <span class="cf">if</span> char <span class="kw">not</span> <span class="kw">in</span> string.punctuation)</span>
<span id="cb122-5"><a href="#cb122-5"></a></span>
<span id="cb122-6"><a href="#cb122-6"></a>    <span class="kw">def</span> tokenize(<span class="va">self</span>, text):</span>
<span id="cb122-7"><a href="#cb122-7"></a>        text <span class="op">=</span> <span class="va">self</span>.standardize(text)</span>
<span id="cb122-8"><a href="#cb122-8"></a>        <span class="cf">return</span> text.split()</span>
<span id="cb122-9"><a href="#cb122-9"></a></span>
<span id="cb122-10"><a href="#cb122-10"></a>    <span class="kw">def</span> make_vocabulary(<span class="va">self</span>, dataset):</span>
<span id="cb122-11"><a href="#cb122-11"></a>        <span class="va">self</span>.vocabulary <span class="op">=</span> {<span class="st">""</span>: <span class="dv">0</span>, <span class="st">"[UNK]"</span>: <span class="dv">1</span>}</span>
<span id="cb122-12"><a href="#cb122-12"></a>        <span class="cf">for</span> text <span class="kw">in</span> dataset:</span>
<span id="cb122-13"><a href="#cb122-13"></a>            text <span class="op">=</span> <span class="va">self</span>.standardize(text)</span>
<span id="cb122-14"><a href="#cb122-14"></a>            tokens <span class="op">=</span> <span class="va">self</span>.tokenize(text)</span>
<span id="cb122-15"><a href="#cb122-15"></a>            <span class="cf">for</span> token <span class="kw">in</span> tokens:</span>
<span id="cb122-16"><a href="#cb122-16"></a>                <span class="cf">if</span> token <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.vocabulary:</span>
<span id="cb122-17"><a href="#cb122-17"></a>                    <span class="va">self</span>.vocabulary[token] <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.vocabulary)</span>
<span id="cb122-18"><a href="#cb122-18"></a>        <span class="va">self</span>.inverse_vocabulary <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb122-19"><a href="#cb122-19"></a>            (v, k) <span class="cf">for</span> k, v <span class="kw">in</span> <span class="va">self</span>.vocabulary.items())</span>
<span id="cb122-20"><a href="#cb122-20"></a></span>
<span id="cb122-21"><a href="#cb122-21"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, text):</span>
<span id="cb122-22"><a href="#cb122-22"></a>        text <span class="op">=</span> <span class="va">self</span>.standardize(text)</span>
<span id="cb122-23"><a href="#cb122-23"></a>        tokens <span class="op">=</span> <span class="va">self</span>.tokenize(text)</span>
<span id="cb122-24"><a href="#cb122-24"></a>        <span class="cf">return</span> [<span class="va">self</span>.vocabulary.get(token, <span class="dv">1</span>) <span class="cf">for</span> token <span class="kw">in</span> tokens]</span>
<span id="cb122-25"><a href="#cb122-25"></a></span>
<span id="cb122-26"><a href="#cb122-26"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, int_sequence):</span>
<span id="cb122-27"><a href="#cb122-27"></a>        <span class="cf">return</span> <span class="st">" "</span>.join(</span>
<span id="cb122-28"><a href="#cb122-28"></a>            <span class="va">self</span>.inverse_vocabulary.get(i, <span class="st">"[UNK]"</span>) <span class="cf">for</span> i <span class="kw">in</span> int_sequence)</span>
<span id="cb122-29"><a href="#cb122-29"></a></span>
<span id="cb122-30"><a href="#cb122-30"></a>vectorizer <span class="op">=</span> Vectorizer()</span>
<span id="cb122-31"><a href="#cb122-31"></a>dataset <span class="op">=</span> [</span>
<span id="cb122-32"><a href="#cb122-32"></a>    <span class="st">"I write, erase, rewrite"</span>,</span>
<span id="cb122-33"><a href="#cb122-33"></a>    <span class="st">"Erase again, and then"</span>,</span>
<span id="cb122-34"><a href="#cb122-34"></a>    <span class="st">"A poppy blooms."</span>,</span>
<span id="cb122-35"><a href="#cb122-35"></a>]</span>
<span id="cb122-36"><a href="#cb122-36"></a>vectorizer.make_vocabulary(dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1052,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354378767,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="b55520a8-36b0-48e9-f31b-de68baeffa59">
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1"></a>test_sentence <span class="op">=</span> <span class="st">"I write, rewrite, and still rewrite again"</span></span>
<span id="cb123-2"><a href="#cb123-2"></a>encoded_sentence <span class="op">=</span> vectorizer.encode(test_sentence)</span>
<span id="cb123-3"><a href="#cb123-3"></a><span class="bu">print</span>(encoded_sentence)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[2, 3, 5, 7, 1, 5, 6]</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:9,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354382721,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="acdac628-4f34-48b6-9b25-96e4e17cf17e">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1"></a>decoded_sentence <span class="op">=</span> vectorizer.decode(encoded_sentence)</span>
<span id="cb125-2"><a href="#cb125-2"></a><span class="bu">print</span>(decoded_sentence)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>i write rewrite and [UNK] rewrite again</code></pre>
</div>
</div>
<p>However, using something like this wouldn’t be very performant. In practice, you’ll work with the <code>tf.Keras</code> <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization"><code>TextVectorization</code></a> layer, which is fast and efficient and can be dropped directly into a <code>tf.data</code> pipeline or a <code>tf.Keras</code> model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb127"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1"></a><span class="co"># Configures the layer to return sequences of words encoded</span></span>
<span id="cb127-2"><a href="#cb127-2"></a><span class="co"># as integer indices.</span></span>
<span id="cb127-3"><a href="#cb127-3"></a>text_vectorization <span class="op">=</span> tf.keras.layers.TextVectorization(</span>
<span id="cb127-4"><a href="#cb127-4"></a>    output_mode<span class="op">=</span><span class="st">"int"</span>,</span>
<span id="cb127-5"><a href="#cb127-5"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>By default, the <code>TextVectorization</code> layer will use the setting “convert to lowercase and remove punctuation” for text standardization, and “split on whitespace” for tokenization.</p>
<p>But importantly, you can provide custom functions for standardization and tokenization, which means the layer is flexible enough to handle any use case. To index the vocabulary of a text corpus, just call the <code>adapt()</code> method of the layer with a <code>Dataset</code> object that yields strings, or just with a list of <code>Python</code> strings:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1"></a>dataset <span class="op">=</span> [</span>
<span id="cb128-2"><a href="#cb128-2"></a>    <span class="st">"I write, erase, rewrite"</span>,</span>
<span id="cb128-3"><a href="#cb128-3"></a>    <span class="st">"Erase again, and then"</span>,</span>
<span id="cb128-4"><a href="#cb128-4"></a>    <span class="st">"A poppy blooms."</span>,</span>
<span id="cb128-5"><a href="#cb128-5"></a>]</span>
<span id="cb128-6"><a href="#cb128-6"></a>text_vectorization.adapt(dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that you can retrieve the computed vocabulary via <code>get_vocabulary()</code>—this can be useful if you need to convert text encoded as integer sequences back into words. The first two entries in the vocabulary are the mask token (index 0) and the OOV token (index 1). Entries in the vocabulary list are sorted by frequency, so with a realworld dataset, very common words like “the” or “a” would come first.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:478,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354486414,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="1ac55855-af39-4c89-a7b3-e36ac43198d5">
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1"></a>text_vectorization.get_vocabulary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="88">
<pre><code>['',
 '[UNK]',
 'erase',
 'write',
 'then',
 'rewrite',
 'poppy',
 'i',
 'blooms',
 'and',
 'again',
 'a']</code></pre>
</div>
</div>
<p>For a demonstration, let’s try to encode and then decode an example sentence:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1046,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354536268,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="404be976-200a-4888-8ed9-87e39eefdf48">
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1"></a>vocabulary <span class="op">=</span> text_vectorization.get_vocabulary()</span>
<span id="cb131-2"><a href="#cb131-2"></a>test_sentence <span class="op">=</span> <span class="st">"I write, rewrite, and still rewrite again"</span></span>
<span id="cb131-3"><a href="#cb131-3"></a>encoded_sentence <span class="op">=</span> text_vectorization(test_sentence)</span>
<span id="cb131-4"><a href="#cb131-4"></a><span class="bu">print</span>(encoded_sentence)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:761,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354543306,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="d2c03317-9679-4727-96a6-aaf13ae6182c">
<div class="sourceCode cell-code" id="cb133"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1"></a>inverse_vocab <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">enumerate</span>(vocabulary))</span>
<span id="cb133-2"><a href="#cb133-2"></a>decoded_sentence <span class="op">=</span> <span class="st">" "</span>.join(inverse_vocab[<span class="bu">int</span>(i)] <span class="cf">for</span> i <span class="kw">in</span> encoded_sentence)</span>
<span id="cb133-3"><a href="#cb133-3"></a><span class="bu">print</span>(decoded_sentence)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>i write rewrite and [UNK] rewrite again</code></pre>
</div>
</div>
</section>
<section id="two-approaches-for-representing-groups-of-words-sets-and-sequences" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="two-approaches-for-representing-groups-of-words-sets-and-sequences"><span class="header-section-number">10.3.2</span> Two approaches for representing groups of words: Sets and sequences</h3>
<p>We’ll demonstrate bag-of-words and sequence approach on a well-known text classification benchmark: the IMDB movie review sentiment-classification dataset. Let’s process the raw IMDB text data, just like you would do when approaching a new text-classification problem in the real world. You can check out the data <a href="https://ai.stanford.edu/~amaas/data/sentiment/">here</a>.</p>
<p>Let’s start by downloading the dataset from the Stanford page of Andrew Maas and uncompressing it</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:16343,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354579955,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="972a0008-7580-4244-9ec7-d153f1a96805">
<div class="sourceCode cell-code" id="cb135"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1"></a><span class="op">!</span>curl <span class="op">-</span>O https:<span class="op">//</span>ai.stanford.edu<span class="op">/~</span>amaas<span class="op">/</span>data<span class="op">/</span>sentiment<span class="op">/</span>aclImdb_v1.tar.gz</span>
<span id="cb135-2"><a href="#cb135-2"></a><span class="op">!</span>tar <span class="op">-</span>xf aclImdb_v1.tar.gz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 80.2M  100 80.2M    0     0  8848k      0  0:00:09  0:00:09 --:--:-- 16.4M</code></pre>
</div>
</div>
<p>The <code>train/pos/</code> directory contains a set of 12,500 text files, each of which contains the text body of a positive-sentiment movie review to be used as training data. The negative-sentiment reviews live in the <code>train/neg</code> directories. In total, there are 25,000 text files for training and another 25,000 for testing. There’s also a <code>train/unsup</code> subdirectory in there, which we don’t need. Let’s delete it:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb137"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1"></a><span class="op">!</span>rm <span class="op">-</span>r aclImdb<span class="op">/</span>train<span class="op">/</span>unsup</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Take a look at the content of a few of these text files. Whether you’re working with text data or image data, remember to always inspect what your data looks like before you dive into modeling it:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1128,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354613987,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="06819cb5-bb1d-4574-8690-dd95f7378ec1">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1"></a><span class="op">!</span>cat aclImdb<span class="op">/</span>train<span class="op">/</span>pos<span class="op">/</span><span class="fl">4077_10.</span><span class="er">txt</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.&lt;br /&gt;&lt;br /&gt;Enjoy</code></pre>
</div>
</div>
<p>Next, let’s prepare a validation set by setting apart 20% of the training text files in a new directory, <code>aclImdb/val</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb140"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1"></a>base_dir <span class="op">=</span> Path(<span class="st">"aclImdb"</span>)</span>
<span id="cb140-2"><a href="#cb140-2"></a>val_dir <span class="op">=</span> base_dir <span class="op">/</span> <span class="st">"val"</span></span>
<span id="cb140-3"><a href="#cb140-3"></a>train_dir <span class="op">=</span> base_dir <span class="op">/</span> <span class="st">"train"</span></span>
<span id="cb140-4"><a href="#cb140-4"></a></span>
<span id="cb140-5"><a href="#cb140-5"></a><span class="cf">for</span> category <span class="kw">in</span> (<span class="st">"neg"</span>, <span class="st">"pos"</span>):</span>
<span id="cb140-6"><a href="#cb140-6"></a>    os.makedirs(val_dir <span class="op">/</span> category)</span>
<span id="cb140-7"><a href="#cb140-7"></a>    files <span class="op">=</span> os.listdir(train_dir <span class="op">/</span> category)</span>
<span id="cb140-8"><a href="#cb140-8"></a>    random.Random(<span class="dv">1337</span>).shuffle(files)</span>
<span id="cb140-9"><a href="#cb140-9"></a>    num_val_samples <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.2</span> <span class="op">*</span> <span class="bu">len</span>(files))</span>
<span id="cb140-10"><a href="#cb140-10"></a>    val_files <span class="op">=</span> files[<span class="op">-</span>num_val_samples:]</span>
<span id="cb140-11"><a href="#cb140-11"></a>    <span class="cf">for</span> fname <span class="kw">in</span> val_files:</span>
<span id="cb140-12"><a href="#cb140-12"></a>        shutil.move(train_dir <span class="op">/</span> category <span class="op">/</span> fname,</span>
<span id="cb140-13"><a href="#cb140-13"></a>                    val_dir <span class="op">/</span> category <span class="op">/</span> fname)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s create three <code>Dataset</code> objects for training, validation, and testing:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2927,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354640295,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="26a38a2c-64d5-4f68-e78f-a2e3fe1e382f">
<div class="sourceCode cell-code" id="cb141"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb141-2"><a href="#cb141-2"></a></span>
<span id="cb141-3"><a href="#cb141-3"></a>train_ds <span class="op">=</span> tf.keras.utils.text_dataset_from_directory(</span>
<span id="cb141-4"><a href="#cb141-4"></a>    <span class="st">"aclImdb/train"</span>, batch_size<span class="op">=</span>batch_size</span>
<span id="cb141-5"><a href="#cb141-5"></a>)</span>
<span id="cb141-6"><a href="#cb141-6"></a>val_ds <span class="op">=</span> tf.keras.utils.text_dataset_from_directory(</span>
<span id="cb141-7"><a href="#cb141-7"></a>    <span class="st">"aclImdb/val"</span>, batch_size<span class="op">=</span>batch_size</span>
<span id="cb141-8"><a href="#cb141-8"></a>)</span>
<span id="cb141-9"><a href="#cb141-9"></a>test_ds <span class="op">=</span> tf.keras.utils.text_dataset_from_directory(</span>
<span id="cb141-10"><a href="#cb141-10"></a>    <span class="st">"aclImdb/test"</span>, batch_size<span class="op">=</span>batch_size</span>
<span id="cb141-11"><a href="#cb141-11"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 20000 files belonging to 2 classes.
Found 5000 files belonging to 2 classes.
Found 25000 files belonging to 2 classes.</code></pre>
</div>
</div>
<p>These datasets yield inputs that are TensorFlow <code>tf.string</code> tensors and targets that are <code>int32</code> tensors encoding the value “0” or “1.”</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1090,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354654311,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="96d3a003-df91-43b0-bcb7-13de85317788">
<div class="sourceCode cell-code" id="cb143"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1"></a><span class="cf">for</span> inputs, targets <span class="kw">in</span> train_ds:</span>
<span id="cb143-2"><a href="#cb143-2"></a>    <span class="bu">print</span>(<span class="st">"inputs.shape:"</span>, inputs.shape)</span>
<span id="cb143-3"><a href="#cb143-3"></a>    <span class="bu">print</span>(<span class="st">"inputs.dtype:"</span>, inputs.dtype)</span>
<span id="cb143-4"><a href="#cb143-4"></a>    <span class="bu">print</span>(<span class="st">"targets.shape:"</span>, targets.shape)</span>
<span id="cb143-5"><a href="#cb143-5"></a>    <span class="bu">print</span>(<span class="st">"targets.dtype:"</span>, targets.dtype)</span>
<span id="cb143-6"><a href="#cb143-6"></a>    <span class="bu">print</span>(<span class="st">"inputs[0]:"</span>, inputs[<span class="dv">0</span>])</span>
<span id="cb143-7"><a href="#cb143-7"></a>    <span class="bu">print</span>(<span class="st">"targets[0]:"</span>, targets[<span class="dv">0</span>])</span>
<span id="cb143-8"><a href="#cb143-8"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>inputs.shape: (32,)
inputs.dtype: &lt;dtype: 'string'&gt;
targets.shape: (32,)
targets.dtype: &lt;dtype: 'int32'&gt;
inputs[0]: tf.Tensor(b'I think that this is possibly the funniest movie I have ever seen. Robert Harling\'s script is near perfect, just check out the "quotes" section; on second thought, just rent the DVD, since it\'s the delivery that really makes the lines sing.&lt;br /&gt;&lt;br /&gt;Sally Field gives a comic, over-the-top performance like you\'ve never seen from her anywhere else, and Kevin Kline is effortlessly hilarious. Robert Downey, Jr. is typically brilliant, and in a very small role, Kathy Najimy is a riot as the beleaguered costumer. I was never much of a fan of Elisabeth Shue, but she\'s great here as the one *real* person surrounded by a bevy of cartoon characters on the set of "The Sun Also Sets" -- that rumbling you feel beneath you is Hemingway rolling over in his grave. Either that, or he\'s laughing really hard.&lt;br /&gt;&lt;br /&gt;Five stars. Funny, funny, funny.', shape=(), dtype=string)
targets[0]: tf.Tensor(1, shape=(), dtype=int32)</code></pre>
</div>
</div>
<section id="single-words-unigrams-with-binary-encoding" class="level4" data-number="10.3.2.1">
<h4 data-number="10.3.2.1" class="anchored" data-anchor-id="single-words-unigrams-with-binary-encoding"><span class="header-section-number">10.3.2.1</span> Single words (unigrams) with binary encoding</h4>
<p>First, let’s process our raw text datasets with a <code>TextVectorization</code> layer so that they yield multi-hot encoded binary word vectors. Our layer will only look at single words (that is to say, unigrams). We will limit the vocabulary to the 20,000 most frequent words. Otherwise we’d be indexing every word in the training data— potentially tens of thousands of terms that only occur once or twice and thus aren’t informative. In general, 20,000 is the right vocabulary size for text classification.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb145"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1"></a><span class="co"># Encode the output tokens as multi-hot binary vectors.</span></span>
<span id="cb145-2"><a href="#cb145-2"></a>text_vectorization <span class="op">=</span> tf.keras.layers.TextVectorization(</span>
<span id="cb145-3"><a href="#cb145-3"></a>    max_tokens<span class="op">=</span><span class="dv">20000</span>,</span>
<span id="cb145-4"><a href="#cb145-4"></a>    output_mode<span class="op">=</span><span class="st">"multi_hot"</span>,</span>
<span id="cb145-5"><a href="#cb145-5"></a>)</span>
<span id="cb145-6"><a href="#cb145-6"></a><span class="co"># Prepare a dataset that only yields raw text inputs (no labels).</span></span>
<span id="cb145-7"><a href="#cb145-7"></a>text_only_train_ds <span class="op">=</span> train_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x)</span>
<span id="cb145-8"><a href="#cb145-8"></a>text_vectorization.adapt(text_only_train_ds)</span>
<span id="cb145-9"><a href="#cb145-9"></a></span>
<span id="cb145-10"><a href="#cb145-10"></a>binary_1gram_train_ds <span class="op">=</span> train_ds.<span class="bu">map</span>(</span>
<span id="cb145-11"><a href="#cb145-11"></a>    <span class="kw">lambda</span> x, y: (text_vectorization(x), y),</span>
<span id="cb145-12"><a href="#cb145-12"></a>    num_parallel_calls<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb145-13"><a href="#cb145-13"></a>binary_1gram_val_ds <span class="op">=</span> val_ds.<span class="bu">map</span>(</span>
<span id="cb145-14"><a href="#cb145-14"></a>    <span class="kw">lambda</span> x, y: (text_vectorization(x), y),</span>
<span id="cb145-15"><a href="#cb145-15"></a>    num_parallel_calls<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb145-16"><a href="#cb145-16"></a>binary_1gram_test_ds <span class="op">=</span> test_ds.<span class="bu">map</span>(</span>
<span id="cb145-17"><a href="#cb145-17"></a>    <span class="kw">lambda</span> x, y: (text_vectorization(x), y),</span>
<span id="cb145-18"><a href="#cb145-18"></a>    num_parallel_calls<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can try to inspect the output of one of these datasets:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:468,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354739417,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="edcb3658-d104-4ee4-b642-1b8cb0a539d5">
<div class="sourceCode cell-code" id="cb146"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1"></a><span class="cf">for</span> inputs, targets <span class="kw">in</span> binary_1gram_train_ds:</span>
<span id="cb146-2"><a href="#cb146-2"></a>    <span class="bu">print</span>(<span class="st">"inputs.shape:"</span>, inputs.shape)</span>
<span id="cb146-3"><a href="#cb146-3"></a>    <span class="bu">print</span>(<span class="st">"inputs.dtype:"</span>, inputs.dtype)</span>
<span id="cb146-4"><a href="#cb146-4"></a>    <span class="bu">print</span>(<span class="st">"targets.shape:"</span>, targets.shape)</span>
<span id="cb146-5"><a href="#cb146-5"></a>    <span class="bu">print</span>(<span class="st">"targets.dtype:"</span>, targets.dtype)</span>
<span id="cb146-6"><a href="#cb146-6"></a>    <span class="bu">print</span>(<span class="st">"inputs[0]:"</span>, inputs[<span class="dv">0</span>])</span>
<span id="cb146-7"><a href="#cb146-7"></a>    <span class="bu">print</span>(<span class="st">"targets[0]:"</span>, targets[<span class="dv">0</span>])</span>
<span id="cb146-8"><a href="#cb146-8"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>inputs.shape: (32, 20000)
inputs.dtype: &lt;dtype: 'float32'&gt;
targets.shape: (32,)
targets.dtype: &lt;dtype: 'int32'&gt;
inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)
targets[0]: tf.Tensor(0, shape=(), dtype=int32)</code></pre>
</div>
</div>
<p>Next, let’s write a reusable model-building function that we’ll use in all of our experiments in this section.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb148"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1"></a><span class="co"># A densely connected NN</span></span>
<span id="cb148-2"><a href="#cb148-2"></a><span class="kw">def</span> get_model(max_tokens<span class="op">=</span><span class="dv">20000</span>, hidden_dim<span class="op">=</span><span class="dv">16</span>):</span>
<span id="cb148-3"><a href="#cb148-3"></a>    inputs <span class="op">=</span> tf.keras.Input(shape<span class="op">=</span>(max_tokens,))</span>
<span id="cb148-4"><a href="#cb148-4"></a>    x <span class="op">=</span> tf.keras.layers.Dense(hidden_dim, activation<span class="op">=</span><span class="st">"relu"</span>)(inputs)</span>
<span id="cb148-5"><a href="#cb148-5"></a>    x <span class="op">=</span> tf.keras.layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb148-6"><a href="#cb148-6"></a>    outputs <span class="op">=</span> tf.keras.layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>)(x)</span>
<span id="cb148-7"><a href="#cb148-7"></a>    model <span class="op">=</span> tf.keras.Model(inputs, outputs)</span>
<span id="cb148-8"><a href="#cb148-8"></a>    </span>
<span id="cb148-9"><a href="#cb148-9"></a>    model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"nadam"</span>,</span>
<span id="cb148-10"><a href="#cb148-10"></a>                  loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>,</span>
<span id="cb148-11"><a href="#cb148-11"></a>                  metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb148-12"><a href="#cb148-12"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, let’s train and test our model.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:48783,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354817133,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="09f11ba9-a068-4f70-9d3b-40fe3adba821">
<div class="sourceCode cell-code" id="cb149"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1"></a>model <span class="op">=</span> get_model()</span>
<span id="cb149-2"><a href="#cb149-2"></a>model.summary()</span>
<span id="cb149-3"><a href="#cb149-3"></a></span>
<span id="cb149-4"><a href="#cb149-4"></a>callbacks <span class="op">=</span> [</span>
<span id="cb149-5"><a href="#cb149-5"></a>    tf.keras.callbacks.ModelCheckpoint(<span class="st">"binary_1gram.keras"</span>, save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb149-6"><a href="#cb149-6"></a>]</span>
<span id="cb149-7"><a href="#cb149-7"></a>model.fit(binary_1gram_train_ds.cache(),</span>
<span id="cb149-8"><a href="#cb149-8"></a>          validation_data<span class="op">=</span>binary_1gram_val_ds.cache(),</span>
<span id="cb149-9"><a href="#cb149-9"></a>          epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb149-10"><a href="#cb149-10"></a>          callbacks<span class="op">=</span>callbacks)</span>
<span id="cb149-11"><a href="#cb149-11"></a></span>
<span id="cb149-12"><a href="#cb149-12"></a>model <span class="op">=</span> tf.keras.models.load_model(<span class="st">"binary_1gram.keras"</span>)</span>
<span id="cb149-13"><a href="#cb149-13"></a><span class="bu">print</span>(<span class="ss">f"Test acc: </span><span class="sc">{</span>model<span class="sc">.</span>evaluate(binary_1gram_test_ds)[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 20000)]           0         
                                                                 
 dense_11 (Dense)            (None, 16)                320016    
                                                                 
 dropout (Dropout)           (None, 16)                0         
                                                                 
 dense_12 (Dense)            (None, 1)                 17        
                                                                 
=================================================================
Total params: 320,033
Trainable params: 320,033
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
625/625 [==============================] - 9s 12ms/step - loss: 0.3861 - accuracy: 0.8356 - val_loss: 0.2701 - val_accuracy: 0.8942
Epoch 2/10
625/625 [==============================] - 3s 4ms/step - loss: 0.2161 - accuracy: 0.9189 - val_loss: 0.2624 - val_accuracy: 0.8912
Epoch 3/10
625/625 [==============================] - 3s 4ms/step - loss: 0.1517 - accuracy: 0.9446 - val_loss: 0.2799 - val_accuracy: 0.8886
Epoch 4/10
625/625 [==============================] - 3s 4ms/step - loss: 0.1134 - accuracy: 0.9606 - val_loss: 0.3063 - val_accuracy: 0.8870
Epoch 5/10
625/625 [==============================] - 3s 5ms/step - loss: 0.0913 - accuracy: 0.9689 - val_loss: 0.3290 - val_accuracy: 0.8878
Epoch 6/10
625/625 [==============================] - 3s 5ms/step - loss: 0.0752 - accuracy: 0.9729 - val_loss: 0.3540 - val_accuracy: 0.8848
Epoch 7/10
625/625 [==============================] - 3s 4ms/step - loss: 0.0626 - accuracy: 0.9774 - val_loss: 0.4038 - val_accuracy: 0.8842
Epoch 8/10
625/625 [==============================] - 3s 5ms/step - loss: 0.0517 - accuracy: 0.9818 - val_loss: 0.4530 - val_accuracy: 0.8866
Epoch 9/10
625/625 [==============================] - 3s 5ms/step - loss: 0.0478 - accuracy: 0.9819 - val_loss: 0.4665 - val_accuracy: 0.8848
Epoch 10/10
625/625 [==============================] - 3s 4ms/step - loss: 0.0420 - accuracy: 0.9842 - val_loss: 0.5062 - val_accuracy: 0.8844
782/782 [==============================] - 5s 6ms/step - loss: 0.2860 - accuracy: 0.8842
Test acc: 0.884</code></pre>
</div>
</div>
<p>This gets us to a test accuracy of 88.4%: not bad!</p>
</section>
<section id="bigrams-with-binary-encoding" class="level4" data-number="10.3.2.2">
<h4 data-number="10.3.2.2" class="anchored" data-anchor-id="bigrams-with-binary-encoding"><span class="header-section-number">10.3.2.2</span> Bigrams with binary encoding</h4>
<p>Of course, discarding word order is very reductive, because even atomic concepts can be expressed via multiple words. The <code>TextVectorization</code> layer can be configured to return arbitrary N-grams: bigrams, trigrams, etc. Just pass an <code>ngrams=N</code> argument as in the following listing.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb151"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1"></a>text_vectorization <span class="op">=</span> tf.keras.layers.TextVectorization(</span>
<span id="cb151-2"><a href="#cb151-2"></a>    ngrams<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb151-3"><a href="#cb151-3"></a>    max_tokens<span class="op">=</span><span class="dv">20000</span>,</span>
<span id="cb151-4"><a href="#cb151-4"></a>    output_mode<span class="op">=</span><span class="st">"multi_hot"</span>,</span>
<span id="cb151-5"><a href="#cb151-5"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s test how our model performs when trained on such binary-encoded bags of bigrams.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:73800,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354909189,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="ec9d1176-2452-463e-d2fd-ce778048e546">
<div class="sourceCode cell-code" id="cb152"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1"></a>text_vectorization.adapt(text_only_train_ds)</span>
<span id="cb152-2"><a href="#cb152-2"></a>binary_2gram_train_ds <span class="op">=</span> train_ds.<span class="bu">map</span>(</span>
<span id="cb152-3"><a href="#cb152-3"></a>    <span class="kw">lambda</span> x, y: (text_vectorization(x), y),</span>
<span id="cb152-4"><a href="#cb152-4"></a>    num_parallel_calls<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb152-5"><a href="#cb152-5"></a>binary_2gram_val_ds <span class="op">=</span> val_ds.<span class="bu">map</span>(</span>
<span id="cb152-6"><a href="#cb152-6"></a>    <span class="kw">lambda</span> x, y: (text_vectorization(x), y),</span>
<span id="cb152-7"><a href="#cb152-7"></a>    num_parallel_calls<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb152-8"><a href="#cb152-8"></a>binary_2gram_test_ds <span class="op">=</span> test_ds.<span class="bu">map</span>(</span>
<span id="cb152-9"><a href="#cb152-9"></a>    <span class="kw">lambda</span> x, y: (text_vectorization(x), y),</span>
<span id="cb152-10"><a href="#cb152-10"></a>    num_parallel_calls<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb152-11"><a href="#cb152-11"></a></span>
<span id="cb152-12"><a href="#cb152-12"></a>model <span class="op">=</span> get_model()</span>
<span id="cb152-13"><a href="#cb152-13"></a>model.summary()</span>
<span id="cb152-14"><a href="#cb152-14"></a></span>
<span id="cb152-15"><a href="#cb152-15"></a>callbacks <span class="op">=</span> [</span>
<span id="cb152-16"><a href="#cb152-16"></a>    tf.keras.callbacks.ModelCheckpoint(<span class="st">"binary_2gram.keras"</span>,</span>
<span id="cb152-17"><a href="#cb152-17"></a>                                    save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb152-18"><a href="#cb152-18"></a>]</span>
<span id="cb152-19"><a href="#cb152-19"></a>model.fit(binary_2gram_train_ds.cache(),</span>
<span id="cb152-20"><a href="#cb152-20"></a>          validation_data<span class="op">=</span>binary_2gram_val_ds.cache(),</span>
<span id="cb152-21"><a href="#cb152-21"></a>          epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb152-22"><a href="#cb152-22"></a>          callbacks<span class="op">=</span>callbacks)</span>
<span id="cb152-23"><a href="#cb152-23"></a></span>
<span id="cb152-24"><a href="#cb152-24"></a>model <span class="op">=</span> tf.keras.models.load_model(<span class="st">"binary_2gram.keras"</span>)</span>
<span id="cb152-25"><a href="#cb152-25"></a><span class="bu">print</span>(<span class="ss">f"Test acc: </span><span class="sc">{</span>model<span class="sc">.</span>evaluate(binary_2gram_test_ds)[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 20000)]           0         
                                                                 
 dense_13 (Dense)            (None, 16)                320016    
                                                                 
 dropout_1 (Dropout)         (None, 16)                0         
                                                                 
 dense_14 (Dense)            (None, 1)                 17        
                                                                 
=================================================================
Total params: 320,033
Trainable params: 320,033
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
625/625 [==============================] - 8s 9ms/step - loss: 0.3697 - accuracy: 0.8462 - val_loss: 0.2565 - val_accuracy: 0.8986
Epoch 2/10
625/625 [==============================] - 3s 5ms/step - loss: 0.1926 - accuracy: 0.9287 - val_loss: 0.2587 - val_accuracy: 0.8912
Epoch 3/10
625/625 [==============================] - 4s 6ms/step - loss: 0.1319 - accuracy: 0.9527 - val_loss: 0.2706 - val_accuracy: 0.8930
Epoch 4/10
625/625 [==============================] - 3s 5ms/step - loss: 0.0986 - accuracy: 0.9661 - val_loss: 0.3235 - val_accuracy: 0.8914
Epoch 5/10
625/625 [==============================] - 3s 5ms/step - loss: 0.0806 - accuracy: 0.9699 - val_loss: 0.3211 - val_accuracy: 0.8926
Epoch 6/10
625/625 [==============================] - 3s 5ms/step - loss: 0.0633 - accuracy: 0.9775 - val_loss: 0.3796 - val_accuracy: 0.8886
Epoch 7/10
625/625 [==============================] - 3s 5ms/step - loss: 0.0573 - accuracy: 0.9780 - val_loss: 0.4114 - val_accuracy: 0.8874
Epoch 8/10
625/625 [==============================] - 5s 8ms/step - loss: 0.0490 - accuracy: 0.9809 - val_loss: 0.4756 - val_accuracy: 0.8894
Epoch 9/10
625/625 [==============================] - 5s 8ms/step - loss: 0.0473 - accuracy: 0.9814 - val_loss: 0.4639 - val_accuracy: 0.8888
Epoch 10/10
625/625 [==============================] - 4s 6ms/step - loss: 0.0439 - accuracy: 0.9826 - val_loss: 0.4937 - val_accuracy: 0.8876
782/782 [==============================] - 8s 10ms/step - loss: 0.2622 - accuracy: 0.8970
Test acc: 0.897</code></pre>
</div>
</div>
<p>We’re now getting 89.7% test accuracy, a marked improvement! Turns out local order is pretty important.</p>
</section>
<section id="bigrams-with-tf-idf-encoding" class="level4" data-number="10.3.2.3">
<h4 data-number="10.3.2.3" class="anchored" data-anchor-id="bigrams-with-tf-idf-encoding"><span class="header-section-number">10.3.2.3</span> Bigrams with TF-IDF encoding</h4>
<p>You can also add a bit more information to this representation by counting how many times each word or N-gram occurs. TF-IDF is so common that it’s built into the <code>TextVectorization</code> layer. All you need to do to start using it is to switch the <code>output_mode</code> argument to <code>tf_idf</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb154"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb154-1"><a href="#cb154-1"></a>text_vectorization <span class="op">=</span> tf.keras.layers.TextVectorization(</span>
<span id="cb154-2"><a href="#cb154-2"></a>    ngrams<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb154-3"><a href="#cb154-3"></a>    max_tokens<span class="op">=</span><span class="dv">20000</span>,</span>
<span id="cb154-4"><a href="#cb154-4"></a>    output_mode<span class="op">=</span><span class="st">"tf_idf"</span>,</span>
<span id="cb154-5"><a href="#cb154-5"></a>)</span>
<span id="cb154-6"><a href="#cb154-6"></a></span>
<span id="cb154-7"><a href="#cb154-7"></a>text_vectorization.adapt(text_only_train_ds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s train a new model with this scheme.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:53224,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683354985491,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="615becb4-4c52-4b3a-cbea-f5ee114c5f43">
<div class="sourceCode cell-code" id="cb155"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1"></a>tfidf_2gram_train_ds <span class="op">=</span> train_ds.<span class="bu">map</span>(</span>
<span id="cb155-2"><a href="#cb155-2"></a>    <span class="kw">lambda</span> x, y: (text_vectorization(x), y),</span>
<span id="cb155-3"><a href="#cb155-3"></a>    num_parallel_calls<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb155-4"><a href="#cb155-4"></a>tfidf_2gram_val_ds <span class="op">=</span> val_ds.<span class="bu">map</span>(</span>
<span id="cb155-5"><a href="#cb155-5"></a>    <span class="kw">lambda</span> x, y: (text_vectorization(x), y),</span>
<span id="cb155-6"><a href="#cb155-6"></a>    num_parallel_calls<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb155-7"><a href="#cb155-7"></a>tfidf_2gram_test_ds <span class="op">=</span> test_ds.<span class="bu">map</span>(</span>
<span id="cb155-8"><a href="#cb155-8"></a>    <span class="kw">lambda</span> x, y: (text_vectorization(x), y),</span>
<span id="cb155-9"><a href="#cb155-9"></a>    num_parallel_calls<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb155-10"><a href="#cb155-10"></a></span>
<span id="cb155-11"><a href="#cb155-11"></a>model <span class="op">=</span> get_model()</span>
<span id="cb155-12"><a href="#cb155-12"></a>model.summary()</span>
<span id="cb155-13"><a href="#cb155-13"></a></span>
<span id="cb155-14"><a href="#cb155-14"></a>callbacks <span class="op">=</span> [</span>
<span id="cb155-15"><a href="#cb155-15"></a>    tf.keras.callbacks.ModelCheckpoint(<span class="st">"tfidf_2gram.keras"</span>, save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb155-16"><a href="#cb155-16"></a>]</span>
<span id="cb155-17"><a href="#cb155-17"></a>model.fit(tfidf_2gram_train_ds.cache(),</span>
<span id="cb155-18"><a href="#cb155-18"></a>          validation_data<span class="op">=</span>tfidf_2gram_val_ds.cache(),</span>
<span id="cb155-19"><a href="#cb155-19"></a>          epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb155-20"><a href="#cb155-20"></a>          callbacks<span class="op">=</span>callbacks)</span>
<span id="cb155-21"><a href="#cb155-21"></a></span>
<span id="cb155-22"><a href="#cb155-22"></a>model <span class="op">=</span> tf.keras.models.load_model(<span class="st">"tfidf_2gram.keras"</span>)</span>
<span id="cb155-23"><a href="#cb155-23"></a><span class="bu">print</span>(<span class="ss">f"Test acc: </span><span class="sc">{</span>model<span class="sc">.</span>evaluate(tfidf_2gram_test_ds)[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 20000)]           0         
                                                                 
 dense_15 (Dense)            (None, 16)                320016    
                                                                 
 dropout_2 (Dropout)         (None, 16)                0         
                                                                 
 dense_16 (Dense)            (None, 1)                 17        
                                                                 
=================================================================
Total params: 320,033
Trainable params: 320,033
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
625/625 [==============================] - 8s 10ms/step - loss: 0.5177 - accuracy: 0.7586 - val_loss: 0.3129 - val_accuracy: 0.8848
Epoch 2/10
625/625 [==============================] - 3s 5ms/step - loss: 0.3008 - accuracy: 0.8597 - val_loss: 0.2661 - val_accuracy: 0.8934
Epoch 3/10
625/625 [==============================] - 3s 5ms/step - loss: 0.2498 - accuracy: 0.8859 - val_loss: 0.2716 - val_accuracy: 0.8946
Epoch 4/10
625/625 [==============================] - 4s 6ms/step - loss: 0.2016 - accuracy: 0.9042 - val_loss: 0.2982 - val_accuracy: 0.8912
Epoch 5/10
625/625 [==============================] - 3s 5ms/step - loss: 0.1795 - accuracy: 0.9121 - val_loss: 0.3066 - val_accuracy: 0.8898
Epoch 6/10
625/625 [==============================] - 3s 5ms/step - loss: 0.1584 - accuracy: 0.9194 - val_loss: 0.3194 - val_accuracy: 0.8934
Epoch 7/10
625/625 [==============================] - 3s 4ms/step - loss: 0.1473 - accuracy: 0.9231 - val_loss: 0.3821 - val_accuracy: 0.8812
Epoch 8/10
625/625 [==============================] - 3s 6ms/step - loss: 0.1372 - accuracy: 0.9280 - val_loss: 0.4076 - val_accuracy: 0.8850
Epoch 9/10
625/625 [==============================] - 3s 5ms/step - loss: 0.1239 - accuracy: 0.9352 - val_loss: 0.3869 - val_accuracy: 0.8926
Epoch 10/10
625/625 [==============================] - 3s 5ms/step - loss: 0.1174 - accuracy: 0.9379 - val_loss: 0.4329 - val_accuracy: 0.8912
782/782 [==============================] - 6s 8ms/step - loss: 0.2749 - accuracy: 0.8910
Test acc: 0.891</code></pre>
</div>
</div>
<p>This gets us an 89.1% test accuracy on the IMDB classification task: it doesn’t seem to be particularly helpful in this case. However, for many text-classification datasets, it would be typical to see a one-percentage-point increase when using TF-IDF compared to plain binary encoding.</p>
</section>
</section>
<section id="processing-words-as-a-sequence-the-sequence-model-approach" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="processing-words-as-a-sequence-the-sequence-model-approach"><span class="header-section-number">10.3.3</span> Processing words as a sequence: The sequence model approach</h3>
<p>What if, instead of manually crafting order-based features, we exposed the model to raw word sequences and let it figure out such features on its own? This is what sequence models are about. Let’s try out a sequence model in practice. First, let’s prepare datasets that return integer sequences. In order to keep a manageable input size, <strong>we’ll truncate the inputs after the first 600 words.</strong></p>
<blockquote class="blockquote">
<p>This is a reasonable choice, since the average review length is 233 words, and only 5% of reviews are longer than 600 words.</p>
</blockquote>
<div class="cell">
<div class="sourceCode cell-code" id="cb157"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1"></a>max_length <span class="op">=</span> <span class="dv">600</span></span>
<span id="cb157-2"><a href="#cb157-2"></a>max_tokens <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb157-3"><a href="#cb157-3"></a>text_vectorization <span class="op">=</span> tf.keras.layers.TextVectorization(</span>
<span id="cb157-4"><a href="#cb157-4"></a>    max_tokens<span class="op">=</span>max_tokens,</span>
<span id="cb157-5"><a href="#cb157-5"></a>    output_mode<span class="op">=</span><span class="st">"int"</span>,</span>
<span id="cb157-6"><a href="#cb157-6"></a>    output_sequence_length<span class="op">=</span>max_length,</span>
<span id="cb157-7"><a href="#cb157-7"></a>)</span>
<span id="cb157-8"><a href="#cb157-8"></a>text_vectorization.adapt(text_only_train_ds)</span>
<span id="cb157-9"><a href="#cb157-9"></a></span>
<span id="cb157-10"><a href="#cb157-10"></a>int_train_ds <span class="op">=</span> train_ds.<span class="bu">map</span>(</span>
<span id="cb157-11"><a href="#cb157-11"></a>    <span class="kw">lambda</span> x, y: (text_vectorization(x), y),</span>
<span id="cb157-12"><a href="#cb157-12"></a>    num_parallel_calls<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb157-13"><a href="#cb157-13"></a>int_val_ds <span class="op">=</span> val_ds.<span class="bu">map</span>(</span>
<span id="cb157-14"><a href="#cb157-14"></a>    <span class="kw">lambda</span> x, y: (text_vectorization(x), y),</span>
<span id="cb157-15"><a href="#cb157-15"></a>    num_parallel_calls<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb157-16"><a href="#cb157-16"></a>int_test_ds <span class="op">=</span> test_ds.<span class="bu">map</span>(</span>
<span id="cb157-17"><a href="#cb157-17"></a>    <span class="kw">lambda</span> x, y: (text_vectorization(x), y),</span>
<span id="cb157-18"><a href="#cb157-18"></a>    num_parallel_calls<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, let’s make a model. The simplest way to convert our integer sequences to vector sequences is to one-hot encode the integers (each dimension would represent one possible term in the vocabulary). On top of these one-hot vectors, we’ll add a simple bidirectional LSTM.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:644,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683355014677,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="947d5fe0-8341-48ae-991f-0dcf7dd1d72e">
<div class="sourceCode cell-code" id="cb158"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb158-1"><a href="#cb158-1"></a>inputs <span class="op">=</span> tf.keras.Input(shape<span class="op">=</span>(<span class="va">None</span>,), dtype<span class="op">=</span><span class="st">"int64"</span>) <span class="co"># One input is a sequence of integers</span></span>
<span id="cb158-2"><a href="#cb158-2"></a>embedded <span class="op">=</span> tf.one_hot(inputs, depth<span class="op">=</span>max_tokens) <span class="co"># A 3D tensor of shape [batch size, time steps, embedding size]</span></span>
<span id="cb158-3"><a href="#cb158-3"></a>x <span class="op">=</span> tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(<span class="dv">32</span>))(embedded)</span>
<span id="cb158-4"><a href="#cb158-4"></a>x <span class="op">=</span> tf.keras.layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb158-5"><a href="#cb158-5"></a>outputs <span class="op">=</span> tf.keras.layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>)(x) <span class="co"># Classification layer</span></span>
<span id="cb158-6"><a href="#cb158-6"></a>model <span class="op">=</span> tf.keras.Model(inputs, outputs)</span>
<span id="cb158-7"><a href="#cb158-7"></a></span>
<span id="cb158-8"><a href="#cb158-8"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"nadam"</span>,</span>
<span id="cb158-9"><a href="#cb158-9"></a>              loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>,</span>
<span id="cb158-10"><a href="#cb158-10"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb158-11"><a href="#cb158-11"></a>model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, None)]            0         
                                                                 
 tf.one_hot (TFOpLambda)     (None, None, 20000)       0         
                                                                 
 bidirectional (Bidirectiona  (None, 64)               5128448   
 l)                                                              
                                                                 
 dropout_3 (Dropout)         (None, 64)                0         
                                                                 
 dense_17 (Dense)            (None, 1)                 65        
                                                                 
=================================================================
Total params: 5,128,513
Trainable params: 5,128,513
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<p>Now, let’s train our model:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:353444,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683357104907,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="59562fa6-e583-4754-9023-25feea5d4f27">
<div class="sourceCode cell-code" id="cb160"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb160-1"><a href="#cb160-1"></a>callbacks <span class="op">=</span> [</span>
<span id="cb160-2"><a href="#cb160-2"></a>    tf.keras.callbacks.ModelCheckpoint(<span class="st">"one_hot_bidir_lstm.keras"</span>, save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb160-3"><a href="#cb160-3"></a>]</span>
<span id="cb160-4"><a href="#cb160-4"></a>model.fit(int_train_ds, validation_data<span class="op">=</span>int_val_ds, epochs<span class="op">=</span><span class="dv">10</span>, callbacks<span class="op">=</span>callbacks)</span>
<span id="cb160-5"><a href="#cb160-5"></a>model <span class="op">=</span> tf.keras.models.load_model(<span class="st">"one_hot_bidir_lstm.keras"</span>)</span>
<span id="cb160-6"><a href="#cb160-6"></a><span class="bu">print</span>(<span class="ss">f"Test acc: </span><span class="sc">{</span>model<span class="sc">.</span>evaluate(int_test_ds)[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
625/625 [==============================] - 169s 259ms/step - loss: 0.6148 - accuracy: 0.6773 - val_loss: 0.6323 - val_accuracy: 0.6720
Epoch 2/10
625/625 [==============================] - 163s 261ms/step - loss: 0.3993 - accuracy: 0.8426 - val_loss: 0.3126 - val_accuracy: 0.8762
Epoch 3/10
625/625 [==============================] - 162s 260ms/step - loss: 0.2752 - accuracy: 0.8935 - val_loss: 0.6013 - val_accuracy: 0.7044
Epoch 4/10
625/625 [==============================] - 163s 261ms/step - loss: 0.7385 - accuracy: 0.6064 - val_loss: 0.8323 - val_accuracy: 0.5076
Epoch 5/10
625/625 [==============================] - 164s 262ms/step - loss: 0.3485 - accuracy: 0.8608 - val_loss: 0.3655 - val_accuracy: 0.8506
Epoch 6/10
625/625 [==============================] - 163s 261ms/step - loss: 0.4231 - accuracy: 0.8137 - val_loss: 0.3651 - val_accuracy: 0.8582
Epoch 7/10
625/625 [==============================] - 163s 261ms/step - loss: 0.2117 - accuracy: 0.9301 - val_loss: 0.3304 - val_accuracy: 0.8662
Epoch 8/10
625/625 [==============================] - 163s 261ms/step - loss: 0.1301 - accuracy: 0.9624 - val_loss: 0.3815 - val_accuracy: 0.8686
Epoch 9/10
625/625 [==============================] - 162s 258ms/step - loss: 0.1255 - accuracy: 0.9597 - val_loss: 0.3904 - val_accuracy: 0.8774
Epoch 10/10
625/625 [==============================] - 163s 261ms/step - loss: 0.3040 - accuracy: 0.8884 - val_loss: 0.3462 - val_accuracy: 0.8748
782/782 [==============================] - 97s 123ms/step - loss: 0.3429 - accuracy: 0.8618
Test acc: 0.862</code></pre>
</div>
</div>
<p>A first observation: this model will train very slowly, especially compared to the lightweight model of the previous section. This is because our inputs are quite large: each input sample is encoded as a matrix of size <code>(600, 20000)</code> (600 words per sample, 20,000 possible words). That’s 12,000,000 floats for a single movie review. Our bidirectional LSTM has a lot of work to do.</p>
<section id="understanding-word-embeddings" class="level4" data-number="10.3.3.1">
<h4 data-number="10.3.3.1" class="anchored" data-anchor-id="understanding-word-embeddings"><span class="header-section-number">10.3.3.1</span> Understanding word embeddings</h4>
<p>Let’s try word embedding. What makes a good word-embedding space depends heavily on your task: the perfect word-embedding space for an English-language movie-review sentiment-analysis model may look different from the perfect embedding space for an English-language legal-document classification model, because the importance of certain semantic relationships varies from task to task. It’s thus reasonable to learn a new embedding space with every new task. Fortunately, backpropagation makes this easy, and <code>tf.Keras</code> makes it even easier. It’s about learning the weights of a layer: the <code>Embedding</code> layer.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb162"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1"></a><span class="co"># The Embedding layer takes at least two arguments: the number of</span></span>
<span id="cb162-2"><a href="#cb162-2"></a><span class="co"># possible tokens and the dimensionality of the embeddings (here, 256).</span></span>
<span id="cb162-3"><a href="#cb162-3"></a>embedding_layer <span class="op">=</span> tf.keras.layers.Embedding(input_dim<span class="op">=</span>max_tokens, output_dim<span class="op">=</span><span class="dv">256</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>The <code>Embedding</code> layer is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors.</strong> The Embedding layer takes as input a rank-2 tensor of integers, of shape <code>(batch_size, sequence_length)</code>, where each entry is a sequence of integers. The layer then returns a 3D floating-point tensor of shape<code>(batch_size, sequence_length, embedding_dimensionality)</code>.</p>
<p>When you instantiate an <code>Embedding</code> layer, its weights (its internal dictionary of token vectors) are initially random, just as with any other layer. During training, these word vectors are gradually adjusted via backpropagation, structuring the space into something the downstream model can exploit. Once fully trained, the embedding space will show a lot of structure—a kind of structure specialized for the specific problem for which you’re training your model.</p>
<p>Let’s build a model that includes an <code>Embedding</code> layer and benchmark it on our task:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:438049,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1682332071979,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="2c21a584-1794-469f-e00c-8aac8f65fb81">
<div class="sourceCode cell-code" id="cb163"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb163-1"><a href="#cb163-1"></a>inputs <span class="op">=</span> tf.keras.Input(shape<span class="op">=</span>(<span class="va">None</span>,), dtype<span class="op">=</span><span class="st">"int64"</span>)</span>
<span id="cb163-2"><a href="#cb163-2"></a>embedded <span class="op">=</span> tf.keras.layers.Embedding(input_dim<span class="op">=</span>max_tokens, output_dim<span class="op">=</span><span class="dv">256</span>)(inputs)</span>
<span id="cb163-3"><a href="#cb163-3"></a>x <span class="op">=</span> tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(<span class="dv">32</span>))(embedded)</span>
<span id="cb163-4"><a href="#cb163-4"></a>x <span class="op">=</span> tf.keras.layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb163-5"><a href="#cb163-5"></a>outputs <span class="op">=</span> tf.keras.layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>)(x)</span>
<span id="cb163-6"><a href="#cb163-6"></a>model <span class="op">=</span> tf.keras.Model(inputs, outputs)</span>
<span id="cb163-7"><a href="#cb163-7"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"rmsprop"</span>,</span>
<span id="cb163-8"><a href="#cb163-8"></a>              loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>,</span>
<span id="cb163-9"><a href="#cb163-9"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb163-10"><a href="#cb163-10"></a>model.summary()</span>
<span id="cb163-11"><a href="#cb163-11"></a></span>
<span id="cb163-12"><a href="#cb163-12"></a>callbacks <span class="op">=</span> [</span>
<span id="cb163-13"><a href="#cb163-13"></a>    tf.keras.callbacks.ModelCheckpoint(<span class="st">"embeddings_bidir_gru.keras"</span>,</span>
<span id="cb163-14"><a href="#cb163-14"></a>                                    save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb163-15"><a href="#cb163-15"></a>]</span>
<span id="cb163-16"><a href="#cb163-16"></a>model.fit(int_train_ds, validation_data<span class="op">=</span>int_val_ds, epochs<span class="op">=</span><span class="dv">10</span>, callbacks<span class="op">=</span>callbacks)</span>
<span id="cb163-17"><a href="#cb163-17"></a>model <span class="op">=</span> tf.keras.models.load_model(<span class="st">"embeddings_bidir_gru.keras"</span>)</span>
<span id="cb163-18"><a href="#cb163-18"></a><span class="bu">print</span>(<span class="ss">f"Test acc: </span><span class="sc">{</span>model<span class="sc">.</span>evaluate(int_test_ds)[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, None)]            0         
                                                                 
 embedding_1 (Embedding)     (None, None, 256)         5120000   
                                                                 
 bidirectional_1 (Bidirectio  (None, 64)               73984     
 nal)                                                            
                                                                 
 dropout_5 (Dropout)         (None, 64)                0         
                                                                 
 dense_18 (Dense)            (None, 1)                 65        
                                                                 
=================================================================
Total params: 5,194,049
Trainable params: 5,194,049
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
625/625 [==============================] - 106s 163ms/step - loss: 0.5462 - accuracy: 0.7190 - val_loss: 0.4001 - val_accuracy: 0.8340
Epoch 2/10
625/625 [==============================] - 50s 80ms/step - loss: 0.3678 - accuracy: 0.8590 - val_loss: 0.4064 - val_accuracy: 0.8292
Epoch 3/10
625/625 [==============================] - 37s 60ms/step - loss: 0.2948 - accuracy: 0.8942 - val_loss: 0.3913 - val_accuracy: 0.8474
Epoch 4/10
625/625 [==============================] - 34s 55ms/step - loss: 0.2403 - accuracy: 0.9163 - val_loss: 0.3350 - val_accuracy: 0.8678
Epoch 5/10
625/625 [==============================] - 29s 47ms/step - loss: 0.2076 - accuracy: 0.9298 - val_loss: 0.4022 - val_accuracy: 0.8720
Epoch 6/10
625/625 [==============================] - 34s 54ms/step - loss: 0.1757 - accuracy: 0.9408 - val_loss: 0.3736 - val_accuracy: 0.8590
Epoch 7/10
625/625 [==============================] - 29s 46ms/step - loss: 0.1630 - accuracy: 0.9442 - val_loss: 0.4029 - val_accuracy: 0.8652
Epoch 8/10
625/625 [==============================] - 32s 52ms/step - loss: 0.1361 - accuracy: 0.9583 - val_loss: 0.4101 - val_accuracy: 0.8772
Epoch 9/10
625/625 [==============================] - 29s 46ms/step - loss: 0.1196 - accuracy: 0.9638 - val_loss: 0.5701 - val_accuracy: 0.8320
Epoch 10/10
625/625 [==============================] - 28s 44ms/step - loss: 0.1012 - accuracy: 0.9686 - val_loss: 0.4684 - val_accuracy: 0.8718
782/782 [==============================] - 15s 18ms/step - loss: 0.3471 - accuracy: 0.8624
Test acc: 0.862</code></pre>
</div>
</div>
<p>It trains much faster than the one-hot model (since the LSTM only has to process 256-dimensional vectors instead of 20,000-dimensional), and its test accuracy is comparable (86%). We’re still some way off from the results of our basic bigram model. Part of the reason why is simply that the model is looking at slightly less data: the bigram model processed full reviews, while our sequence model truncates sequences after 600 words.</p>
</section>
<section id="understanding-padding-and-masking" class="level4" data-number="10.3.3.2">
<h4 data-number="10.3.3.2" class="anchored" data-anchor-id="understanding-padding-and-masking"><span class="header-section-number">10.3.3.2</span> Understanding padding and masking</h4>
<p>One thing that’s slightly hurting model performance here is that our input sequences are full of zeros. This comes from our use of the <code>output_sequence_length=max_length</code> option in <code>TextVectorization</code> (with <code>max_length</code> equal to 600): <strong>sentences longer than 600 tokens are truncated to a length of 600 tokens, and sentences shorter than 600 tokens are padded with zeros</strong> at the end so that they can be concatenated together with other sequences to form contiguous batches.</p>
<p>We’re using a bidirectional RNN: two RNN layers running in parallel, with one processing the tokens in their natural order, and the other processing the same tokens in reverse. <strong>The RNN that looks at the tokens in their natural order will spend its last iterations seeing only vectors that encode padding—possibly for several hundreds of iterations if the original sentence was short</strong>. The information stored in the internal state of the RNN will gradually fade out as it gets exposed to these meaningless inputs.</p>
<p>We need some way to tell the RNN that it should skip these iterations. There’s an API for that: <em>masking</em>. The <code>Embedding</code> layer is capable of generating a “mask” that corresponds to its input data. This mask is a tensor of ones and zeros (or <code>True/False</code> booleans), of shape <code>(batch_size, sequence_length)</code>, where the entry <code>mask[i, t]</code> indicates where timestep <code>t</code> of sample <code>i</code> should be skipped or not (the timestep will be skipped if <code>mask[i, t]</code> is 0 or <code>False</code>, and processed otherwise).</p>
<p>By default, this option isn’t active — you can turn it on by passing <code>mask_zero=True</code> to your <code>Embedding</code> layer. You can retrieve the mask with the <code>compute_mask()</code> method:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb165"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb165-1"><a href="#cb165-1"></a>embedding_layer <span class="op">=</span> tf.keras.layers.Embedding(input_dim<span class="op">=</span><span class="dv">10</span>, output_dim<span class="op">=</span><span class="dv">256</span>, mask_zero<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:16,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1682332071980,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="8f5803e1-383d-45e9-cef6-7951ae6e1ac2">
<div class="sourceCode cell-code" id="cb166"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb166-1"><a href="#cb166-1"></a>some_input <span class="op">=</span> [</span>
<span id="cb166-2"><a href="#cb166-2"></a>    [<span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb166-3"><a href="#cb166-3"></a>    [<span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb166-4"><a href="#cb166-4"></a>    [<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb166-5"><a href="#cb166-5"></a>]</span>
<span id="cb166-6"><a href="#cb166-6"></a></span>
<span id="cb166-7"><a href="#cb166-7"></a>mask <span class="op">=</span> embedding_layer.compute_mask(some_input)</span>
<span id="cb166-8"><a href="#cb166-8"></a>mask</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="89">
<pre><code>&lt;tf.Tensor: shape=(3, 7), dtype=bool, numpy=
array([[ True,  True,  True,  True, False, False, False],
       [ True,  True,  True,  True,  True, False, False],
       [ True,  True, False, False, False, False, False]])&gt;</code></pre>
</div>
</div>
<p>In practice, you will almost never have to manage masks by hand. Instead, <code>tf.Keras</code> will automatically pass on the mask to every layer that is able to process it. This mask will be used by RNN layers to skip masked steps. If your model returns an entire sequence, the mask will also be used by the loss function to skip masked steps in the output sequence. Let’s try retraining our model with masking enabled:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:535870,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1682332607839,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="125f287e-c5dc-46a5-eaea-3e8fdfc5d458">
<div class="sourceCode cell-code" id="cb168"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1"></a>inputs <span class="op">=</span> tf.keras.Input(shape<span class="op">=</span>(<span class="va">None</span>,), dtype<span class="op">=</span><span class="st">"int64"</span>)</span>
<span id="cb168-2"><a href="#cb168-2"></a>embedded <span class="op">=</span> tf.keras.layers.Embedding(</span>
<span id="cb168-3"><a href="#cb168-3"></a>    input_dim<span class="op">=</span>max_tokens, output_dim<span class="op">=</span><span class="dv">256</span>, mask_zero<span class="op">=</span><span class="va">True</span>)(inputs) <span class="co"># You can turn mask on by passing mask_zero=True</span></span>
<span id="cb168-4"><a href="#cb168-4"></a>x <span class="op">=</span> tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(<span class="dv">32</span>))(embedded)</span>
<span id="cb168-5"><a href="#cb168-5"></a>x <span class="op">=</span> tf.keras.layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb168-6"><a href="#cb168-6"></a>outputs <span class="op">=</span> tf.keras.layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>)(x)</span>
<span id="cb168-7"><a href="#cb168-7"></a>model <span class="op">=</span> tf.keras.Model(inputs, outputs)</span>
<span id="cb168-8"><a href="#cb168-8"></a></span>
<span id="cb168-9"><a href="#cb168-9"></a></span>
<span id="cb168-10"><a href="#cb168-10"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"rmsprop"</span>,</span>
<span id="cb168-11"><a href="#cb168-11"></a>              loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>,</span>
<span id="cb168-12"><a href="#cb168-12"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb168-13"><a href="#cb168-13"></a>model.summary()</span>
<span id="cb168-14"><a href="#cb168-14"></a></span>
<span id="cb168-15"><a href="#cb168-15"></a>callbacks <span class="op">=</span> [</span>
<span id="cb168-16"><a href="#cb168-16"></a>    tf.keras.callbacks.ModelCheckpoint(<span class="st">"embeddings_bidir_gru_with_masking.keras"</span>, save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb168-17"><a href="#cb168-17"></a>]</span>
<span id="cb168-18"><a href="#cb168-18"></a>model.fit(int_train_ds, validation_data<span class="op">=</span>int_val_ds, epochs<span class="op">=</span><span class="dv">10</span>, callbacks<span class="op">=</span>callbacks)</span>
<span id="cb168-19"><a href="#cb168-19"></a>model <span class="op">=</span> tf.keras.models.load_model(<span class="st">"embeddings_bidir_gru_with_masking.keras"</span>)</span>
<span id="cb168-20"><a href="#cb168-20"></a><span class="bu">print</span>(<span class="ss">f"Test acc: </span><span class="sc">{</span>model<span class="sc">.</span>evaluate(int_test_ds)[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, None)]            0         
                                                                 
 embedding_3 (Embedding)     (None, None, 256)         5120000   
                                                                 
 bidirectional_2 (Bidirectio  (None, 64)               73984     
 nal)                                                            
                                                                 
 dropout_6 (Dropout)         (None, 64)                0         
                                                                 
 dense_19 (Dense)            (None, 1)                 65        
                                                                 
=================================================================
Total params: 5,194,049
Trainable params: 5,194,049
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
625/625 [==============================] - 95s 133ms/step - loss: 0.4575 - accuracy: 0.7808 - val_loss: 0.3454 - val_accuracy: 0.8558
Epoch 2/10
625/625 [==============================] - 49s 78ms/step - loss: 0.2750 - accuracy: 0.8925 - val_loss: 0.3157 - val_accuracy: 0.8844
Epoch 3/10
625/625 [==============================] - 40s 65ms/step - loss: 0.2060 - accuracy: 0.9245 - val_loss: 0.2950 - val_accuracy: 0.8808
Epoch 4/10
625/625 [==============================] - 36s 58ms/step - loss: 0.1623 - accuracy: 0.9421 - val_loss: 0.3303 - val_accuracy: 0.8762
Epoch 5/10
625/625 [==============================] - 34s 54ms/step - loss: 0.1211 - accuracy: 0.9577 - val_loss: 0.3438 - val_accuracy: 0.8780
Epoch 6/10
625/625 [==============================] - 35s 56ms/step - loss: 0.0948 - accuracy: 0.9672 - val_loss: 0.4910 - val_accuracy: 0.8684
Epoch 7/10
625/625 [==============================] - 33s 52ms/step - loss: 0.0770 - accuracy: 0.9733 - val_loss: 0.4370 - val_accuracy: 0.8718
Epoch 8/10
625/625 [==============================] - 35s 57ms/step - loss: 0.0569 - accuracy: 0.9807 - val_loss: 0.4648 - val_accuracy: 0.8610
Epoch 9/10
625/625 [==============================] - 35s 56ms/step - loss: 0.0464 - accuracy: 0.9838 - val_loss: 0.6365 - val_accuracy: 0.8464
Epoch 10/10
625/625 [==============================] - 34s 55ms/step - loss: 0.0354 - accuracy: 0.9880 - val_loss: 0.5367 - val_accuracy: 0.8640
782/782 [==============================] - 20s 20ms/step - loss: 0.3014 - accuracy: 0.8774
Test acc: 0.877</code></pre>
</div>
</div>
<p>This time we get to 88% test accuracy — a small but noticeable improvement.</p>
</section>
<section id="using-pretrained-word-embeddings" class="level4" data-number="10.3.3.3">
<h4 data-number="10.3.3.3" class="anchored" data-anchor-id="using-pretrained-word-embeddings"><span class="header-section-number">10.3.3.3</span> Using pretrained word embeddings</h4>
<p>Sometimes you have so little training data available that you can’t use your data alone to learn an appropriate task - specific embedding of your vocabulary. In such cases, instead of learning word embeddings jointly with the problem you want to solve, you can load embedding vectors from a precomputed embedding space that you know is highly structured and exhibits useful properties. There are various precomputed databases of word embeddings that you can download and use in a <code>tf.Keras</code> <code>Embedding</code> layer. <code>Word2vec</code> is one of them. Another popular one is called <code>GloVe</code>, which was developed by Stanford researchers in 2014.</p>
<p>First, let’s download the GloVe word embeddings precomputed on the 2014 English Wikipedia dataset. It’s an 822 MB zip file containing 100-dimensional embedding vectors for 400,000 words (or non-word tokens).</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:184513,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1682332792350,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="7a72bce6-02c8-453b-9df3-b06b62093519">
<div class="sourceCode cell-code" id="cb170"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb170-1"><a href="#cb170-1"></a><span class="op">!</span>wget http:<span class="op">//</span>nlp.stanford.edu<span class="op">/</span>data<span class="op">/</span>glove<span class="fl">.6</span><span class="er">B</span>.<span class="bu">zip</span></span>
<span id="cb170-2"><a href="#cb170-2"></a><span class="op">!</span>unzip <span class="op">-</span>q glove<span class="fl">.6</span><span class="er">B</span>.<span class="bu">zip</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>--2023-04-24 10:36:45--  http://nlp.stanford.edu/data/glove.6B.zip
Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140
Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://nlp.stanford.edu/data/glove.6B.zip [following]
--2023-04-24 10:36:45--  https://nlp.stanford.edu/data/glove.6B.zip
Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]
--2023-04-24 10:36:46--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip
Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22
Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 862182613 (822M) [application/zip]
Saving to: ‘glove.6B.zip’

glove.6B.zip        100%[===================&gt;] 822.24M  5.24MB/s    in 2m 39s  

2023-04-24 10:39:26 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]
</code></pre>
</div>
</div>
<p>Let’s parse the unzipped file (a <code>.txt</code> file) to build an index that maps words (as strings) to their vector representation.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:6822,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1682332799163,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="8161a7ab-e4aa-486c-e36c-7fc990dbea26">
<div class="sourceCode cell-code" id="cb172"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb172-1"><a href="#cb172-1"></a>path_to_glove_file <span class="op">=</span> <span class="st">"glove.6B.100d.txt"</span></span>
<span id="cb172-2"><a href="#cb172-2"></a></span>
<span id="cb172-3"><a href="#cb172-3"></a>embeddings_index <span class="op">=</span> {}</span>
<span id="cb172-4"><a href="#cb172-4"></a><span class="cf">with</span> <span class="bu">open</span>(path_to_glove_file) <span class="im">as</span> f:</span>
<span id="cb172-5"><a href="#cb172-5"></a>    <span class="cf">for</span> line <span class="kw">in</span> f:</span>
<span id="cb172-6"><a href="#cb172-6"></a>        word, coefs <span class="op">=</span> line.split(maxsplit<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb172-7"><a href="#cb172-7"></a>        coefs <span class="op">=</span> np.fromstring(coefs, <span class="st">"f"</span>, sep<span class="op">=</span><span class="st">" "</span>)</span>
<span id="cb172-8"><a href="#cb172-8"></a>        embeddings_index[word] <span class="op">=</span> coefs</span>
<span id="cb172-9"><a href="#cb172-9"></a></span>
<span id="cb172-10"><a href="#cb172-10"></a><span class="bu">print</span>(<span class="ss">f"Found </span><span class="sc">{</span><span class="bu">len</span>(embeddings_index)<span class="sc">}</span><span class="ss"> word vectors."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 400000 word vectors.</code></pre>
</div>
</div>
<p>Next, let’s build an embedding matrix that you can load into an <code>Embedding</code> layer. It must be a matrix of shape <code>(max_words, embedding_dim)</code>, where each entry <code>i</code> contains the <code>embedding_dim</code> - dimensional vector for the word of index <code>i</code> in the reference word index (built during tokenization).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb174"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb174-1"><a href="#cb174-1"></a>embedding_dim <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb174-2"><a href="#cb174-2"></a></span>
<span id="cb174-3"><a href="#cb174-3"></a>vocabulary <span class="op">=</span> text_vectorization.get_vocabulary()</span>
<span id="cb174-4"><a href="#cb174-4"></a>word_index <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(vocabulary, <span class="bu">range</span>(<span class="bu">len</span>(vocabulary))))</span>
<span id="cb174-5"><a href="#cb174-5"></a></span>
<span id="cb174-6"><a href="#cb174-6"></a>embedding_matrix <span class="op">=</span> np.zeros((max_tokens, embedding_dim))</span>
<span id="cb174-7"><a href="#cb174-7"></a><span class="cf">for</span> word, i <span class="kw">in</span> word_index.items():</span>
<span id="cb174-8"><a href="#cb174-8"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> max_tokens:</span>
<span id="cb174-9"><a href="#cb174-9"></a>        embedding_vector <span class="op">=</span> embeddings_index.get(word)</span>
<span id="cb174-10"><a href="#cb174-10"></a>    <span class="cf">if</span> embedding_vector <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb174-11"><a href="#cb174-11"></a>        embedding_matrix[i] <span class="op">=</span> embedding_vector</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, we use a <code>Constant</code> initializer to load the pretrained embeddings in an <code>Embedding</code> layer. So as not to disrupt the pretrained representations during training, we freeze the layer via <code>trainable=False</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb175"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb175-1"><a href="#cb175-1"></a>embedding_layer <span class="op">=</span> tf.keras.layers.Embedding(</span>
<span id="cb175-2"><a href="#cb175-2"></a>    max_tokens,</span>
<span id="cb175-3"><a href="#cb175-3"></a>    embedding_dim,</span>
<span id="cb175-4"><a href="#cb175-4"></a>    embeddings_initializer<span class="op">=</span>tf.keras.initializers.Constant(embedding_matrix),</span>
<span id="cb175-5"><a href="#cb175-5"></a>    trainable<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb175-6"><a href="#cb175-6"></a>    mask_zero<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb175-7"><a href="#cb175-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’re now ready to train a new model—identical to our previous model, but leveraging the 100-dimensional pretrained GloVe embeddings instead of 128-dimensional learned embeddings.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:551531,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1682333350686,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="7bfc9739-c51a-40cf-8bdf-3829550d4b20">
<div class="sourceCode cell-code" id="cb176"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb176-1"><a href="#cb176-1"></a>inputs <span class="op">=</span> tf.keras.Input(shape<span class="op">=</span>(<span class="va">None</span>,), dtype<span class="op">=</span><span class="st">"int64"</span>)</span>
<span id="cb176-2"><a href="#cb176-2"></a>embedded <span class="op">=</span> embedding_layer(inputs)</span>
<span id="cb176-3"><a href="#cb176-3"></a>x <span class="op">=</span> tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(<span class="dv">32</span>))(embedded)</span>
<span id="cb176-4"><a href="#cb176-4"></a>x <span class="op">=</span> tf.keras.layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb176-5"><a href="#cb176-5"></a>outputs <span class="op">=</span> tf.keras.layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>)(x)</span>
<span id="cb176-6"><a href="#cb176-6"></a>model <span class="op">=</span> tf.keras.Model(inputs, outputs)</span>
<span id="cb176-7"><a href="#cb176-7"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"rmsprop"</span>,</span>
<span id="cb176-8"><a href="#cb176-8"></a>              loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>,</span>
<span id="cb176-9"><a href="#cb176-9"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb176-10"><a href="#cb176-10"></a>model.summary()</span>
<span id="cb176-11"><a href="#cb176-11"></a></span>
<span id="cb176-12"><a href="#cb176-12"></a>callbacks <span class="op">=</span> [</span>
<span id="cb176-13"><a href="#cb176-13"></a>    tf.keras.callbacks.ModelCheckpoint(<span class="st">"glove_embeddings_sequence_model.keras"</span>, save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb176-14"><a href="#cb176-14"></a>]</span>
<span id="cb176-15"><a href="#cb176-15"></a>model.fit(int_train_ds, validation_data<span class="op">=</span>int_val_ds, epochs<span class="op">=</span><span class="dv">10</span>, callbacks<span class="op">=</span>callbacks)</span>
<span id="cb176-16"><a href="#cb176-16"></a></span>
<span id="cb176-17"><a href="#cb176-17"></a>model <span class="op">=</span> tf.keras.models.load_model(<span class="st">"glove_embeddings_sequence_model.keras"</span>)</span>
<span id="cb176-18"><a href="#cb176-18"></a><span class="bu">print</span>(<span class="ss">f"Test acc: </span><span class="sc">{</span>model<span class="sc">.</span>evaluate(int_test_ds)[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, None)]            0         
                                                                 
 embedding_4 (Embedding)     (None, None, 100)         2000000   
                                                                 
 bidirectional_3 (Bidirectio  (None, 64)               34048     
 nal)                                                            
                                                                 
 dropout_7 (Dropout)         (None, 64)                0         
                                                                 
 dense_20 (Dense)            (None, 1)                 65        
                                                                 
=================================================================
Total params: 2,034,113
Trainable params: 34,113
Non-trainable params: 2,000,000
_________________________________________________________________
Epoch 1/10
625/625 [==============================] - 49s 65ms/step - loss: 0.5726 - accuracy: 0.6988 - val_loss: 0.5082 - val_accuracy: 0.7418
Epoch 2/10
625/625 [==============================] - 33s 53ms/step - loss: 0.4545 - accuracy: 0.7942 - val_loss: 0.4163 - val_accuracy: 0.8126
Epoch 3/10
625/625 [==============================] - 37s 60ms/step - loss: 0.4056 - accuracy: 0.8196 - val_loss: 0.4130 - val_accuracy: 0.8088
Epoch 4/10
625/625 [==============================] - 40s 63ms/step - loss: 0.3720 - accuracy: 0.8360 - val_loss: 0.3538 - val_accuracy: 0.8462
Epoch 5/10
625/625 [==============================] - 42s 67ms/step - loss: 0.3479 - accuracy: 0.8535 - val_loss: 0.3453 - val_accuracy: 0.8530
Epoch 6/10
625/625 [==============================] - 46s 74ms/step - loss: 0.3288 - accuracy: 0.8623 - val_loss: 0.3401 - val_accuracy: 0.8544
Epoch 7/10
625/625 [==============================] - 38s 61ms/step - loss: 0.3116 - accuracy: 0.8705 - val_loss: 0.3243 - val_accuracy: 0.8576
Epoch 8/10
625/625 [==============================] - 51s 81ms/step - loss: 0.2929 - accuracy: 0.8773 - val_loss: 0.3139 - val_accuracy: 0.8626
Epoch 9/10
625/625 [==============================] - 34s 54ms/step - loss: 0.2817 - accuracy: 0.8844 - val_loss: 0.3128 - val_accuracy: 0.8676
Epoch 10/10
625/625 [==============================] - 34s 55ms/step - loss: 0.2654 - accuracy: 0.8918 - val_loss: 0.3159 - val_accuracy: 0.8628
782/782 [==============================] - 22s 26ms/step - loss: 0.3020 - accuracy: 0.8690
Test acc: 0.869</code></pre>
</div>
</div>
<p>Leveraging pretrained embeddings can be very helpful when you’re working with a smaller dataset.</p>
</section>
</section>
<section id="the-transformer-encoder-optional" class="level3" data-number="10.3.4">
<h3 data-number="10.3.4" class="anchored" data-anchor-id="the-transformer-encoder-optional"><span class="header-section-number">10.3.4</span> The Transformer encoder (Optional)</h3>
<p>The encoder part of transformer can be used for text classification—it’s a very generic module that ingests a sequence and learns to turn it into a more useful representation. Let’s implement a Transformer encoder using <code>tf.Keras</code> subclassing API.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb178"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb178-1"><a href="#cb178-1"></a><span class="kw">class</span> TransformerEncoder(tf.keras.layers.Layer):</span>
<span id="cb178-2"><a href="#cb178-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim, dense_dim, num_heads, <span class="op">**</span>kwargs):</span>
<span id="cb178-3"><a href="#cb178-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb178-4"><a href="#cb178-4"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim  <span class="co"># Size of the input token vectors</span></span>
<span id="cb178-5"><a href="#cb178-5"></a>        <span class="va">self</span>.dense_dim <span class="op">=</span> dense_dim  <span class="co"># Size of the inner dense layer</span></span>
<span id="cb178-6"><a href="#cb178-6"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads  <span class="co"># Number of attention heads</span></span>
<span id="cb178-7"><a href="#cb178-7"></a>        <span class="va">self</span>.attention <span class="op">=</span> tf.keras.layers.MultiHeadAttention(</span>
<span id="cb178-8"><a href="#cb178-8"></a>            num_heads<span class="op">=</span>num_heads, key_dim<span class="op">=</span>embed_dim)</span>
<span id="cb178-9"><a href="#cb178-9"></a>        <span class="va">self</span>.dense_proj <span class="op">=</span> tf.keras.Sequential(</span>
<span id="cb178-10"><a href="#cb178-10"></a>            [tf.keras.layers.Dense(dense_dim, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb178-11"><a href="#cb178-11"></a>             tf.keras.layers.Dense(embed_dim),]</span>
<span id="cb178-12"><a href="#cb178-12"></a>        )</span>
<span id="cb178-13"><a href="#cb178-13"></a>        <span class="va">self</span>.layernorm_1 <span class="op">=</span> tf.keras.layers.LayerNormalization()</span>
<span id="cb178-14"><a href="#cb178-14"></a>        <span class="va">self</span>.layernorm_2 <span class="op">=</span> tf.keras.layers.LayerNormalization()</span>
<span id="cb178-15"><a href="#cb178-15"></a>    <span class="co"># Computation goes in call().</span></span>
<span id="cb178-16"><a href="#cb178-16"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, mask<span class="op">=</span><span class="va">None</span>): </span>
<span id="cb178-17"><a href="#cb178-17"></a>        <span class="co"># The mask that will be generated by the Embedding layer will be 2D, but</span></span>
<span id="cb178-18"><a href="#cb178-18"></a>        <span class="co"># the attention layer expects to be 3D or 4D, so we expand its rank. </span></span>
<span id="cb178-19"><a href="#cb178-19"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb178-20"><a href="#cb178-20"></a>            mask <span class="op">=</span> mask[:, tf.newaxis, :]</span>
<span id="cb178-21"><a href="#cb178-21"></a>        attention_output <span class="op">=</span> <span class="va">self</span>.attention(</span>
<span id="cb178-22"><a href="#cb178-22"></a>            inputs, inputs, attention_mask<span class="op">=</span>mask)</span>
<span id="cb178-23"><a href="#cb178-23"></a>        proj_input <span class="op">=</span> <span class="va">self</span>.layernorm_1(inputs <span class="op">+</span> attention_output)</span>
<span id="cb178-24"><a href="#cb178-24"></a>        proj_output <span class="op">=</span> <span class="va">self</span>.dense_proj(proj_input)</span>
<span id="cb178-25"><a href="#cb178-25"></a>        <span class="cf">return</span> <span class="va">self</span>.layernorm_2(proj_input <span class="op">+</span> proj_output)</span>
<span id="cb178-26"><a href="#cb178-26"></a>    <span class="co"># Implement serialization so we can save the model.</span></span>
<span id="cb178-27"><a href="#cb178-27"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb178-28"><a href="#cb178-28"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb178-29"><a href="#cb178-29"></a>        config.update({</span>
<span id="cb178-30"><a href="#cb178-30"></a>            <span class="st">"embed_dim"</span>: <span class="va">self</span>.embed_dim,</span>
<span id="cb178-31"><a href="#cb178-31"></a>            <span class="st">"num_heads"</span>: <span class="va">self</span>.num_heads,</span>
<span id="cb178-32"><a href="#cb178-32"></a>            <span class="st">"dense_dim"</span>: <span class="va">self</span>.dense_dim,</span>
<span id="cb178-33"><a href="#cb178-33"></a>        })</span>
<span id="cb178-34"><a href="#cb178-34"></a>        <span class="cf">return</span> config</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When you write custom layers, make sure to implement the <code>get_config()</code> method: this enables the layer to be reinstantiated from its <code>config</code> dict, which is useful during model saving and loading.</p>
<p>To add positional encoding, we’ll do something simpler and more effective: we’ll learn position embedding vectors the same way we learn to embed word indices. We’ll then proceed to add our position embeddings to the corresponding word embeddings, to obtain a position-aware word embedding. <strong>It is noted that neural networks don’t like very large input values, or discrete input distributions</strong> therefore simply adding a position information as interger is not a good idea.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb179"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb179-1"><a href="#cb179-1"></a><span class="kw">class</span> PositionalEmbedding(tf.keras.layers.Layer):</span>
<span id="cb179-2"><a href="#cb179-2"></a>    <span class="co"># A downside of position embeddings is that the sequence length needs to be known in advance.</span></span>
<span id="cb179-3"><a href="#cb179-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, sequence_length, input_dim, output_dim, <span class="op">**</span>kwargs):</span>
<span id="cb179-4"><a href="#cb179-4"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb179-5"><a href="#cb179-5"></a>        <span class="co"># Prepare an Embedding layer for the token indices.</span></span>
<span id="cb179-6"><a href="#cb179-6"></a>        <span class="va">self</span>.token_embeddings <span class="op">=</span> tf.keras.layers.Embedding(</span>
<span id="cb179-7"><a href="#cb179-7"></a>            input_dim<span class="op">=</span>input_dim, output_dim<span class="op">=</span>output_dim)</span>
<span id="cb179-8"><a href="#cb179-8"></a>        <span class="co"># And another one for the token positions</span></span>
<span id="cb179-9"><a href="#cb179-9"></a>        <span class="va">self</span>.position_embeddings <span class="op">=</span> tf.keras.layers.Embedding(</span>
<span id="cb179-10"><a href="#cb179-10"></a>            input_dim<span class="op">=</span>sequence_length, output_dim<span class="op">=</span>output_dim)</span>
<span id="cb179-11"><a href="#cb179-11"></a>        <span class="va">self</span>.sequence_length <span class="op">=</span> sequence_length</span>
<span id="cb179-12"><a href="#cb179-12"></a>        <span class="va">self</span>.input_dim <span class="op">=</span> input_dim</span>
<span id="cb179-13"><a href="#cb179-13"></a>        <span class="va">self</span>.output_dim <span class="op">=</span> output_dim</span>
<span id="cb179-14"><a href="#cb179-14"></a></span>
<span id="cb179-15"><a href="#cb179-15"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb179-16"><a href="#cb179-16"></a>        length <span class="op">=</span> tf.shape(inputs)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb179-17"><a href="#cb179-17"></a>        positions <span class="op">=</span> tf.<span class="bu">range</span>(start<span class="op">=</span><span class="dv">0</span>, limit<span class="op">=</span>length, delta<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb179-18"><a href="#cb179-18"></a>        embedded_tokens <span class="op">=</span> <span class="va">self</span>.token_embeddings(inputs)</span>
<span id="cb179-19"><a href="#cb179-19"></a>        embedded_positions <span class="op">=</span> <span class="va">self</span>.position_embeddings(positions)</span>
<span id="cb179-20"><a href="#cb179-20"></a>        <span class="co"># Add both embedding vectors together</span></span>
<span id="cb179-21"><a href="#cb179-21"></a>        <span class="cf">return</span> embedded_tokens <span class="op">+</span> embedded_positions</span>
<span id="cb179-22"><a href="#cb179-22"></a>    </span>
<span id="cb179-23"><a href="#cb179-23"></a>    <span class="co"># Like the Embedding layer, this layer should be able to generate a</span></span>
<span id="cb179-24"><a href="#cb179-24"></a>    <span class="co"># mask so we can ignore padding 0s in the inputs. The compute_mask</span></span>
<span id="cb179-25"><a href="#cb179-25"></a>    <span class="co"># method will called automatically by the framework, and the</span></span>
<span id="cb179-26"><a href="#cb179-26"></a>    <span class="co"># mask will get propagated to the next layer.</span></span>
<span id="cb179-27"><a href="#cb179-27"></a>    <span class="kw">def</span> compute_mask(<span class="va">self</span>, inputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb179-28"><a href="#cb179-28"></a>        <span class="cf">return</span> tf.math.not_equal(inputs, <span class="dv">0</span>)</span>
<span id="cb179-29"><a href="#cb179-29"></a></span>
<span id="cb179-30"><a href="#cb179-30"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb179-31"><a href="#cb179-31"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb179-32"><a href="#cb179-32"></a>        config.update({</span>
<span id="cb179-33"><a href="#cb179-33"></a>            <span class="st">"output_dim"</span>: <span class="va">self</span>.output_dim,</span>
<span id="cb179-34"><a href="#cb179-34"></a>            <span class="st">"sequence_length"</span>: <span class="va">self</span>.sequence_length,</span>
<span id="cb179-35"><a href="#cb179-35"></a>            <span class="st">"input_dim"</span>: <span class="va">self</span>.input_dim,</span>
<span id="cb179-36"><a href="#cb179-36"></a>        })</span>
<span id="cb179-37"><a href="#cb179-37"></a>        <span class="cf">return</span> config</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>All you have to do to start taking word order into account is swap the old <code>Embedding</code> layer with our position-aware version.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1507934,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1682334858618,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="937b2421-4744-4cfd-a3e9-e6e073dfb834">
<div class="sourceCode cell-code" id="cb180"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb180-1"><a href="#cb180-1"></a>vocab_size <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb180-2"><a href="#cb180-2"></a>sequence_length <span class="op">=</span> <span class="dv">600</span></span>
<span id="cb180-3"><a href="#cb180-3"></a>embed_dim <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb180-4"><a href="#cb180-4"></a>num_heads <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb180-5"><a href="#cb180-5"></a>dense_dim <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb180-6"><a href="#cb180-6"></a></span>
<span id="cb180-7"><a href="#cb180-7"></a>inputs <span class="op">=</span> tf.keras.Input(shape<span class="op">=</span>(<span class="va">None</span>,), dtype<span class="op">=</span><span class="st">"int64"</span>)</span>
<span id="cb180-8"><a href="#cb180-8"></a>x <span class="op">=</span> PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)</span>
<span id="cb180-9"><a href="#cb180-9"></a>x <span class="op">=</span> TransformerEncoder(embed_dim, dense_dim, num_heads)(x)</span>
<span id="cb180-10"><a href="#cb180-10"></a>x <span class="op">=</span> tf.keras.layers.GlobalMaxPooling1D()(x)</span>
<span id="cb180-11"><a href="#cb180-11"></a>x <span class="op">=</span> tf.keras.layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb180-12"><a href="#cb180-12"></a>outputs <span class="op">=</span> tf.keras.layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>)(x)</span>
<span id="cb180-13"><a href="#cb180-13"></a>model <span class="op">=</span> tf.keras.Model(inputs, outputs)</span>
<span id="cb180-14"><a href="#cb180-14"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"rmsprop"</span>,</span>
<span id="cb180-15"><a href="#cb180-15"></a>              loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>,</span>
<span id="cb180-16"><a href="#cb180-16"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb180-17"><a href="#cb180-17"></a>model.summary()</span>
<span id="cb180-18"><a href="#cb180-18"></a></span>
<span id="cb180-19"><a href="#cb180-19"></a>callbacks <span class="op">=</span> [</span>
<span id="cb180-20"><a href="#cb180-20"></a>    tf.keras.callbacks.ModelCheckpoint(<span class="st">"full_transformer_encoder.keras"</span>, save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb180-21"><a href="#cb180-21"></a>]</span>
<span id="cb180-22"><a href="#cb180-22"></a>model.fit(int_train_ds, validation_data<span class="op">=</span>int_val_ds, epochs<span class="op">=</span><span class="dv">20</span>, callbacks<span class="op">=</span>callbacks)</span>
<span id="cb180-23"><a href="#cb180-23"></a>model <span class="op">=</span> tf.keras.models.load_model(</span>
<span id="cb180-24"><a href="#cb180-24"></a>    <span class="st">"full_transformer_encoder.keras"</span>,</span>
<span id="cb180-25"><a href="#cb180-25"></a>    custom_objects<span class="op">=</span>{<span class="st">"TransformerEncoder"</span>: TransformerEncoder,</span>
<span id="cb180-26"><a href="#cb180-26"></a>                    <span class="st">"PositionalEmbedding"</span>: PositionalEmbedding})</span>
<span id="cb180-27"><a href="#cb180-27"></a><span class="bu">print</span>(<span class="ss">f"Test acc: </span><span class="sc">{</span>model<span class="sc">.</span>evaluate(int_test_ds)[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, None)]            0         
                                                                 
 positional_embedding (Posit  (None, None, 256)        5273600   
 ionalEmbedding)                                                 
                                                                 
 transformer_encoder (Transf  (None, None, 256)        543776    
 ormerEncoder)                                                   
                                                                 
 global_max_pooling1d (Globa  (None, 256)              0         
 lMaxPooling1D)                                                  
                                                                 
 dropout_8 (Dropout)         (None, 256)               0         
                                                                 
 dense_23 (Dense)            (None, 1)                 257       
                                                                 
=================================================================
Total params: 5,817,633
Trainable params: 5,817,633
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
625/625 [==============================] - 96s 148ms/step - loss: 0.5369 - accuracy: 0.7337 - val_loss: 0.3355 - val_accuracy: 0.8620
Epoch 2/20
625/625 [==============================] - 64s 102ms/step - loss: 0.3016 - accuracy: 0.8734 - val_loss: 0.3112 - val_accuracy: 0.8690
Epoch 3/20
625/625 [==============================] - 65s 104ms/step - loss: 0.2341 - accuracy: 0.9053 - val_loss: 0.3086 - val_accuracy: 0.8766
Epoch 4/20
625/625 [==============================] - 56s 89ms/step - loss: 0.1916 - accuracy: 0.9258 - val_loss: 0.2963 - val_accuracy: 0.8832
Epoch 5/20
625/625 [==============================] - 55s 88ms/step - loss: 0.1659 - accuracy: 0.9363 - val_loss: 0.3897 - val_accuracy: 0.8822
Epoch 6/20
625/625 [==============================] - 49s 78ms/step - loss: 0.1396 - accuracy: 0.9473 - val_loss: 0.3166 - val_accuracy: 0.8756
Epoch 7/20
625/625 [==============================] - 47s 75ms/step - loss: 0.1139 - accuracy: 0.9569 - val_loss: 0.3463 - val_accuracy: 0.8760
Epoch 8/20
625/625 [==============================] - 47s 75ms/step - loss: 0.0933 - accuracy: 0.9669 - val_loss: 0.5166 - val_accuracy: 0.8336
Epoch 9/20
625/625 [==============================] - 47s 75ms/step - loss: 0.0742 - accuracy: 0.9740 - val_loss: 0.5596 - val_accuracy: 0.8640
Epoch 10/20
625/625 [==============================] - 47s 75ms/step - loss: 0.0546 - accuracy: 0.9808 - val_loss: 0.5631 - val_accuracy: 0.8704
Epoch 11/20
625/625 [==============================] - 46s 73ms/step - loss: 0.0435 - accuracy: 0.9845 - val_loss: 0.6425 - val_accuracy: 0.8680
Epoch 12/20
625/625 [==============================] - 46s 74ms/step - loss: 0.0348 - accuracy: 0.9887 - val_loss: 0.7147 - val_accuracy: 0.8726
Epoch 13/20
625/625 [==============================] - 45s 72ms/step - loss: 0.0272 - accuracy: 0.9907 - val_loss: 0.7546 - val_accuracy: 0.8640
Epoch 14/20
625/625 [==============================] - 46s 74ms/step - loss: 0.0214 - accuracy: 0.9930 - val_loss: 1.0937 - val_accuracy: 0.8680
Epoch 15/20
625/625 [==============================] - 46s 73ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 1.3181 - val_accuracy: 0.8612
Epoch 16/20
625/625 [==============================] - 46s 74ms/step - loss: 0.0172 - accuracy: 0.9950 - val_loss: 1.1156 - val_accuracy: 0.8640
Epoch 17/20
625/625 [==============================] - 46s 73ms/step - loss: 0.0141 - accuracy: 0.9955 - val_loss: 1.0343 - val_accuracy: 0.8670
Epoch 18/20
625/625 [==============================] - 46s 73ms/step - loss: 0.0150 - accuracy: 0.9959 - val_loss: 1.2374 - val_accuracy: 0.8644
Epoch 19/20
625/625 [==============================] - 46s 73ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 1.3844 - val_accuracy: 0.8640
Epoch 20/20
625/625 [==============================] - 45s 72ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 1.3549 - val_accuracy: 0.8634
782/782 [==============================] - 22s 28ms/step - loss: 0.2870 - accuracy: 0.8830
Test acc: 0.883</code></pre>
</div>
</div>
<p>We get to 88.3% test accuracy, a solid improvement that clearly demonstrates the value of word order information for text classification. This is our best sequence model so far!</p>
</section>
</section>
<section id="hugging-faces-transformers-library" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="hugging-faces-transformers-library"><span class="header-section-number">10.4</span> Hugging Face’s <code>Transformers</code> Library</h2>
<p>It’s impossible to talk about transformers today without mentioning Hugging Face, an AI company that has built a whole ecosystem of easy-to-use open source tools for NLP, vision, and beyond. The central component of their ecosystem is the <code>Transformers</code> library, which allows you to easily download a pretrained model, including its corresponding tokenizer, and then fine-tune it on your own dataset, if needed. Plus, the library supports <code>TensorFlow</code>, <code>PyTorch</code>, and <code>JAX</code> (with the high-level <code>Flax</code> library).</p>
<p>The simplest way to use the <code>Transformers</code> library is to use the transformers. <code>pipeline()</code> function: you just specify which task you want, such as sentiment analysis, and it downloads a default pretrained model, ready to be used:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2787,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683357814014,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="bec9afae-64bb-4af1-8db2-06a7ee6df08b">
<div class="sourceCode cell-code" id="cb182"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb182-1"><a href="#cb182-1"></a>classifier <span class="op">=</span> pipeline(<span class="st">"sentiment-analysis"</span>)  <span class="co"># many other tasks are available</span></span>
<span id="cb182-2"><a href="#cb182-2"></a>result <span class="op">=</span> classifier(<span class="st">"The actors were very convincing."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.</code></pre>
</div>
</div>
<p>The result is a <code>Python</code> list containing one dictionary per input text:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683357814014,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="03212479-3f1b-403e-ec77-44f787c9f0c8">
<div class="sourceCode cell-code" id="cb184"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb184-1"><a href="#cb184-1"></a>result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="127">
<pre><code>[{'label': 'POSITIVE', 'score': 0.9998071789741516}]</code></pre>
</div>
</div>
<p>In this example, the model correctly found that the sentence is positive, with around 99.98% confidence. Of course, you can also pass a batch of sentences to the model (Models can be very biased. For example, it may like or dislike some countries depending on the data it was trained on, and how it is used, so use it with care):</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:2,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683357814730,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="a9178b1d-66ef-4f48-effd-5b76b027425e">
<div class="sourceCode cell-code" id="cb186"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb186-1"><a href="#cb186-1"></a>classifier([<span class="st">"I am from India."</span>, <span class="st">"I am from Iraq."</span>]) <span class="co"># Note that is has bias</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="128">
<pre><code>[{'label': 'POSITIVE', 'score': 0.9896161556243896},
 {'label': 'NEGATIVE', 'score': 0.9811071157455444}]</code></pre>
</div>
</div>
<p>The <code>pipeline()</code> function uses the default model for the given task. For example, for text classification tasks such as sentiment analysis, it defaults to <code>distilbert-base-uncased-finetuned-sst-2-english</code> — a DistilBERT model with an uncased tokenizer, trained on English Wikipedia and a corpus of English books, and fine-tuned on the Stanford Sentiment Treebank v2 (SST 2) task.</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:454,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683358025320,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="33d5efc6-3d4a-4d2d-9260-5a284ac9d714">
<div class="sourceCode cell-code" id="cb188"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb188-1"><a href="#cb188-1"></a>classifier.model, classifier.tokenizer </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="136">
<pre><code>(DistilBertForSequenceClassification(
   (distilbert): DistilBertModel(
     (embeddings): Embeddings(
       (word_embeddings): Embedding(30522, 768, padding_idx=0)
       (position_embeddings): Embedding(512, 768)
       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
       (dropout): Dropout(p=0.1, inplace=False)
     )
     (transformer): Transformer(
       (layer): ModuleList(
         (0-5): 6 x TransformerBlock(
           (attention): MultiHeadSelfAttention(
             (dropout): Dropout(p=0.1, inplace=False)
             (q_lin): Linear(in_features=768, out_features=768, bias=True)
             (k_lin): Linear(in_features=768, out_features=768, bias=True)
             (v_lin): Linear(in_features=768, out_features=768, bias=True)
             (out_lin): Linear(in_features=768, out_features=768, bias=True)
           )
           (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
           (ffn): FFN(
             (dropout): Dropout(p=0.1, inplace=False)
             (lin1): Linear(in_features=768, out_features=3072, bias=True)
             (lin2): Linear(in_features=3072, out_features=768, bias=True)
             (activation): GELUActivation()
           )
           (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
         )
       )
     )
   )
   (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
   (classifier): Linear(in_features=768, out_features=2, bias=True)
   (dropout): Dropout(p=0.2, inplace=False)
 ),
 DistilBertTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True))</code></pre>
</div>
</div>
<p>It’s also possible to manually specify a different model. For example, you could use a DistilBERT model fine-tuned on the Multi-Genre Natural Language Inference (MultiNLI) task, <strong>which classifies two sentences into three classes: contradiction, neutral, or entailment</strong>. Here is how:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:22522,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683357446771,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="71d920e7-1241-49b5-8dfd-21c80f51737d">
<div class="sourceCode cell-code" id="cb190"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb190-1"><a href="#cb190-1"></a>model_name <span class="op">=</span> <span class="st">"huggingface/distilbert-base-uncased-finetuned-mnli"</span></span>
<span id="cb190-2"><a href="#cb190-2"></a>classifier_mnli <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>, model<span class="op">=</span>model_name)</span>
<span id="cb190-3"><a href="#cb190-3"></a>classifier_mnli(<span class="st">"She loves me. [SEP] She loves me not."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"1c79b4edb99e4e64b65d990428e754fd","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"58b3664860c94eaab7ee093374b20232","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"1529c407fa2c432d82917f38f6908a47","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"73a5386819cb482e946210db4ed95811","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"11fec4a8159640688a30127e9005739a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2ef48bd5710e4da2be9898280ba62710","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="116">
<pre><code>[{'label': 'contradiction', 'score': 0.9790191650390625}]</code></pre>
</div>
</div>
<p>The <code>pipeline()</code> API is very simple and convenient, but sometimes you will need more control. For such cases, the <code>Transformers</code> library provides many classes, including all sorts of tokenizers, models, configurations, callbacks, and much more. For example, let’s load the same DistilBERT model, along with its corresponding tokenizer, using the <code>TFAutoModelForSequenceClassification</code> and <code>AutoTokenizer</code> classes:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:20708,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683357507020,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="82915aa2-a4c9-482d-af09-a228200384b5">
<div class="sourceCode cell-code" id="cb192"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb192-1"><a href="#cb192-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb192-2"><a href="#cb192-2"></a>model <span class="op">=</span> TFAutoModelForSequenceClassification.from_pretrained(model_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"fbcfe0dcb1e248dc93d23926d1e1421d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Some layers from the model checkpoint at huggingface/distilbert-base-uncased-finetuned-mnli were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']
- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at huggingface/distilbert-base-uncased-finetuned-mnli and are newly initialized: ['dropout_23']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
</div>
<p>Next, let’s tokenize a couple of pairs of sentences. In this code, we activate padding and specify that we want <code>TensorFlow</code> tensors instead of <code>Python</code> lists:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb194"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb194-1"><a href="#cb194-1"></a>token_ids <span class="op">=</span> tokenizer([<span class="st">"I like soccer. [SEP] We all love soccer!"</span>,</span>
<span id="cb194-2"><a href="#cb194-2"></a>                       <span class="st">"Joe lived for a very long time. [SEP] Joe is old."</span>],</span>
<span id="cb194-3"><a href="#cb194-3"></a>                      padding<span class="op">=</span><span class="va">True</span>, return_tensors<span class="op">=</span><span class="st">"tf"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<p>Instead of passing <code>"Sentence 1 [SEP] Sentence 2"</code> to the tokenizer, you can equivalently pass it a tuple: <code>("Sentence 1", "Sentence 2")</code>.</p>
</blockquote>
<p>The output is a dictionary-like instance of the <code>BatchEncoding</code> class, which contains the sequences of token IDs, as well as a mask containing 0s for the padding tokens:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:520,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683357516759,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="f0496c7c-9261-4344-a199-04b1677f7768">
<div class="sourceCode cell-code" id="cb195"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb195-1"><a href="#cb195-1"></a>token_ids</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="119">
<pre><code>{'input_ids': &lt;tf.Tensor: shape=(2, 15), dtype=int32, numpy=
array([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,
         102,    0,    0,    0],
       [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,
        2003, 2214, 1012,  102]], dtype=int32)&gt;, 'attention_mask': &lt;tf.Tensor: shape=(2, 15), dtype=int32, numpy=
array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)&gt;}</code></pre>
</div>
</div>
<p>If you set <code>return_token_type_ids=True</code> when calling the tokenizer, you will also get an extra tensor that indicates which sentence each token belongs to. This is needed by some models, but not DistilBERT. Next, we can directly pass this <code>BatchEncoding</code> object to the model; it returns a <code>TFSequenceClassifierOutput</code> object containing its predicted class logits:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:453,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683357569838,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="e60ff19c-caa4-4137-c3c2-7770a409d420">
<div class="sourceCode cell-code" id="cb197"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb197-1"><a href="#cb197-1"></a>outputs <span class="op">=</span> model(token_ids)</span>
<span id="cb197-2"><a href="#cb197-2"></a>outputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="120">
<pre><code>TFSequenceClassifierOutput(loss=None, logits=&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[-2.1123812 ,  1.178679  ,  1.4101001 ],
       [-0.01478288,  1.0962466 , -0.9919953 ]], dtype=float32)&gt;, hidden_states=None, attentions=None)</code></pre>
</div>
</div>
<p>Lastly, we can apply the softmax activation function to convert these logits to class probabilities, and use the <code>argmax()</code> function to predict the class with the highest probability for each input sentence pair:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:783,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683357578227,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="ecef2620-fc9f-4aa1-99bf-65c14cf9f8b5">
<div class="sourceCode cell-code" id="cb199"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb199-1"><a href="#cb199-1"></a>Y_probas <span class="op">=</span> tf.keras.activations.softmax(outputs.logits)</span>
<span id="cb199-2"><a href="#cb199-2"></a>Y_probas</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="121">
<pre><code>&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[0.01619703, 0.43523598, 0.54856706],
       [0.22655995, 0.6881722 , 0.08526783]], dtype=float32)&gt;</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:597,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683357584364,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="f522ecff-346a-4238-87a8-95d9d9b39d51">
<div class="sourceCode cell-code" id="cb201"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb201-1"><a href="#cb201-1"></a>Y_pred <span class="op">=</span> tf.argmax(Y_probas, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb201-2"><a href="#cb201-2"></a>Y_pred  <span class="co"># 0 = contradiction, 1 = entailment, 2 = neutral</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="122">
<pre><code>&lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 1])&gt;</code></pre>
</div>
</div>
<p>In this example, the model correctly classifies the first sentence pair as neutral (the fact that I like soccer does not imply that everyone else does) and the second pair as an entailment (Joe must indeed be quite old).</p>
<p>If you wish to fine-tune this model on your own dataset, you can train the model as usual with <code>tf.Keras</code> since it’s just a regular <code>tf.Keras</code> model with a few extra methods. However, because the model outputs logits instead of probabilities, you must use the <code>tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</code> loss instead of the usual <code>sparse_categorical_crossentropy</code> loss. Moreover, the model does not support <code>BatchEncoding</code> inputs during training, so you must use its data attribute to get a regular dictionary instead:</p>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:62388,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683357728697,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="a33c71af-888f-488c-8086-1cf3ff7cf28e">
<div class="sourceCode cell-code" id="cb203"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb203-1"><a href="#cb203-1"></a>sentences <span class="op">=</span> [(<span class="st">"Sky is blue"</span>, <span class="st">"Sky is red"</span>), (<span class="st">"I love her"</span>, <span class="st">"She loves me"</span>)]</span>
<span id="cb203-2"><a href="#cb203-2"></a>X_train <span class="op">=</span> tokenizer(sentences, padding<span class="op">=</span><span class="va">True</span>, return_tensors<span class="op">=</span><span class="st">"tf"</span>).data</span>
<span id="cb203-3"><a href="#cb203-3"></a>y_train <span class="op">=</span> tf.constant([<span class="dv">0</span>, <span class="dv">2</span>])  <span class="co"># contradiction, neutral</span></span>
<span id="cb203-4"><a href="#cb203-4"></a>loss <span class="op">=</span> tf.keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb203-5"><a href="#cb203-5"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span>loss, optimizer<span class="op">=</span><span class="st">"nadam"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb203-6"><a href="#cb203-6"></a>history <span class="op">=</span> model.fit(X_train, y_train, epochs<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/2
1/1 [==============================] - 47s 47s/step - loss: 0.6666 - accuracy: 0.5000
Epoch 2/2
1/1 [==============================] - 0s 71ms/step - loss: 0.3430 - accuracy: 1.0000</code></pre>
</div>
</div>
<p>Hugging Face has also built a <code>Datasets</code> library that you can use to easily download a standard dataset (such as IMDb) or a custom one, and use it to fine-tune your model. It’s similar to <code>TensorFlow</code> <code>Datasets</code>, but it also provides tools to perform common preprocessing tasks on the fly, such as masking. The list of datasets is available at <a href="https://huggingface.co/datasets">https://huggingface.co/datasets</a>.</p>
<p>This should get you started with Hugging Face’s ecosystem. To learn more, you can head over to <a href="https://huggingface.co/docs">https://huggingface.co/docs</a> for the documentation, which includes many tutorial notebooks, videos, the full API, and more.</p>
<section id="deal-with-imdb-optional" class="level3" data-number="10.4.1">
<h3 data-number="10.4.1" class="anchored" data-anchor-id="deal-with-imdb-optional"><span class="header-section-number">10.4.1</span> Deal with IMDB (Optional)</h3>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:3459,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683359643123,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="06614f30-9b9c-46ec-f295-aba2ed79f178">
<div class="sourceCode cell-code" id="cb205"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb205-1"><a href="#cb205-1"></a>imdb <span class="op">=</span> load_dataset(<span class="st">"imdb"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:datasets.builder:Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"dfbc2523a1434c3f8ea0d7f7e58f2e09","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:9,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683359643123,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="0e505c65-2547-4309-d17a-ca803443d23f">
<div class="sourceCode cell-code" id="cb207"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb207-1"><a href="#cb207-1"></a>imdb[<span class="st">'train'</span>][<span class="dv">100</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>{'text': "Terrible movie. Nuff Said.&lt;br /&gt;&lt;br /&gt;These Lines are Just Filler. The movie was bad. Why I have to expand on that I don't know. This is already a waste of my time. I just wanted to warn others. Avoid this movie. The acting sucks and the writing is just moronic. Bad in every way. The only nice thing about the movie are Deniz Akkaya's breasts. Even that was ruined though by a terrible and unneeded rape scene. The movie is a poorly contrived and totally unbelievable piece of garbage.&lt;br /&gt;&lt;br /&gt;OK now I am just going to rag on IMDb for this stupid rule of 10 lines of text minimum. First I waste my time watching this offal. Then feeling compelled to warn others I create an account with IMDb only to discover that I have to write a friggen essay on the film just to express how bad I think it is. Totally unnecessary.",
 'label': 0}</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:653,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683359643773,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="933e2a29-d086-4982-f803-4fe6ba17a21b">
<div class="sourceCode cell-code" id="cb209"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb209-1"><a href="#cb209-1"></a>id2label <span class="op">=</span> {<span class="dv">0</span>: <span class="st">"NEGATIVE"</span>, <span class="dv">1</span>: <span class="st">"POSITIVE"</span>}</span>
<span id="cb209-2"><a href="#cb209-2"></a>label2id <span class="op">=</span> {<span class="st">"NEGATIVE"</span>: <span class="dv">0</span>, <span class="st">"POSITIVE"</span>: <span class="dv">1</span>}</span>
<span id="cb209-3"><a href="#cb209-3"></a></span>
<span id="cb209-4"><a href="#cb209-4"></a><span class="co"># If we use AutoModelForSequenceClassification, it will use pytorch model</span></span>
<span id="cb209-5"><a href="#cb209-5"></a>model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(</span>
<span id="cb209-6"><a href="#cb209-6"></a>    <span class="st">"distilbert-base-uncased"</span>, num_labels<span class="op">=</span><span class="dv">2</span>, id2label<span class="op">=</span>id2label, label2id<span class="op">=</span>label2id</span>
<span id="cb209-7"><a href="#cb209-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:33589,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683359677355,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="1c6b9f7e-5949-4d86-ce61-92065e5ecd16">
<div class="sourceCode cell-code" id="cb211"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb211-1"><a href="#cb211-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"distilbert-base-uncased"</span>)</span>
<span id="cb211-2"><a href="#cb211-2"></a></span>
<span id="cb211-3"><a href="#cb211-3"></a><span class="kw">def</span> preprocess_function(examples):</span>
<span id="cb211-4"><a href="#cb211-4"></a>    <span class="cf">return</span> tokenizer(examples[<span class="st">"text"</span>], truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb211-5"><a href="#cb211-5"></a> </span>
<span id="cb211-6"><a href="#cb211-6"></a>tokenized_imdb <span class="op">=</span> imdb.<span class="bu">map</span>(preprocess_function, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb211-7"><a href="#cb211-7"></a>data_collator <span class="op">=</span> DataCollatorWithPadding(tokenizer<span class="op">=</span>tokenizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-7a7a5d00cfff76c8.arrow</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"23394b7587564f4691c0a43a698ddb75","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-435fdd3faf14640e.arrow</code></pre>
</div>
</div>
<div class="cell" data-executioninfo="{&quot;elapsed&quot;:1585723,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1683361263069,&quot;user&quot;:{&quot;displayName&quot;:&quot;phonchi chung&quot;,&quot;userId&quot;:&quot;13517391734500420886&quot;},&quot;user_tz&quot;:-480}" data-outputid="ba45d1a8-979a-4faf-d63e-1c20d82688e4">
<div class="sourceCode cell-code" id="cb214"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb214-1"><a href="#cb214-1"></a>accuracy <span class="op">=</span> evaluate.load(<span class="st">"accuracy"</span>)</span>
<span id="cb214-2"><a href="#cb214-2"></a><span class="kw">def</span> compute_metrics(eval_pred):</span>
<span id="cb214-3"><a href="#cb214-3"></a>    predictions, labels <span class="op">=</span> eval_pred</span>
<span id="cb214-4"><a href="#cb214-4"></a>    predictions <span class="op">=</span> np.argmax(predictions, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb214-5"><a href="#cb214-5"></a>    <span class="cf">return</span> accuracy.compute(predictions<span class="op">=</span>predictions, references<span class="op">=</span>labels)</span>
<span id="cb214-6"><a href="#cb214-6"></a></span>
<span id="cb214-7"><a href="#cb214-7"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb214-8"><a href="#cb214-8"></a>    output_dir<span class="op">=</span><span class="st">"my_model"</span>,</span>
<span id="cb214-9"><a href="#cb214-9"></a>    learning_rate<span class="op">=</span><span class="fl">2e-5</span>,</span>
<span id="cb214-10"><a href="#cb214-10"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb214-11"><a href="#cb214-11"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb214-12"><a href="#cb214-12"></a>    num_train_epochs<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb214-13"><a href="#cb214-13"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb214-14"><a href="#cb214-14"></a>    evaluation_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb214-15"><a href="#cb214-15"></a>    save_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb214-16"><a href="#cb214-16"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb214-17"><a href="#cb214-17"></a>    push_to_hub<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb214-18"><a href="#cb214-18"></a>)</span>
<span id="cb214-19"><a href="#cb214-19"></a></span>
<span id="cb214-20"><a href="#cb214-20"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb214-21"><a href="#cb214-21"></a>    model<span class="op">=</span>model,</span>
<span id="cb214-22"><a href="#cb214-22"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb214-23"><a href="#cb214-23"></a>    train_dataset<span class="op">=</span>tokenized_imdb[<span class="st">"train"</span>],</span>
<span id="cb214-24"><a href="#cb214-24"></a>    eval_dataset<span class="op">=</span>tokenized_imdb[<span class="st">"test"</span>],</span>
<span id="cb214-25"><a href="#cb214-25"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb214-26"><a href="#cb214-26"></a>    data_collator<span class="op">=</span>data_collator,</span>
<span id="cb214-27"><a href="#cb214-27"></a>    compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb214-28"><a href="#cb214-28"></a>)</span>
<span id="cb214-29"><a href="#cb214-29"></a></span>
<span id="cb214-30"><a href="#cb214-30"></a>trainer.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"de41cf40f86845fe98d7ff8f50c182c7","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.</code></pre>
</div>
<div class="cell-output cell-output-display">

    <div>
      
      <progress value="1563" max="1563" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [1563/1563 26:16, Epoch 1/1]
    </div>
    
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Epoch</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
<th data-quarto-table-cell-role="th">Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.227100</td>
<td>0.193795</td>
<td>0.926880</td>
</tr>
</tbody>
</table>
<p>
</p></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>TrainOutput(global_step=1563, training_loss=0.2581237346334329, metrics={'train_runtime': 1578.5525, 'train_samples_per_second': 15.837, 'train_steps_per_second': 0.99, 'total_flos': 3281068438885632.0, 'train_loss': 0.2581237346334329, 'epoch': 1.0})</code></pre>
</div>
</div>
</section>
</section>
<section id="references" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="references"><span class="header-section-number">10.5</span> References</h2>
<ol type="1">
<li><p><a href="https://github.com/ageron/handson-ml3/">https://github.com/ageron/handson-ml3/</a></p></li>
<li><p><a href="https://github.com/fchollet/deep-learning-with-python-notebooks">https://github.com/fchollet/deep-learning-with-python-notebooks</a></p></li>
<li><p><a href="https://androidkt.com/save-and-load-fine-tuned-huggingface-transformers-model-from-local-disk/">https://androidkt.com/save-and-load-fine-tuned-huggingface-transformers-model-from-local-disk/</a></p></li>
<li><p><a href="https://github.com/nlp-with-transformers/notebooks/tree/main">https://github.com/nlp-with-transformers/notebooks/tree/main</a></p></li>
</ol>


</section>

</main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"008742b886154e849df548ce6dc47f5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0238ef14f5b3407c90cb32382a826a66":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02ab8134acb14cbcb9e23940b46050eb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"066b8fd515d64976a22effb992492302":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d21410e1a08449218bcd5039aaf0af7f","max":4203,"min":0,"orientation":"horizontal","style":"IPY_MODEL_28700895bda6476ab23f4527cedde21d","value":4203}},"0712c748276b4193a31cd0166deb9e2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95275854d9f943e0babe64bc25d66f99","placeholder":"​","style":"IPY_MODEL_bd72d7160dc64c178daea3b1d137c23d","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"1047661706094b1a819945b26f1f960e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11fec4a8159640688a30127e9005739a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42fc6bba258f402295db3124d44d6a7b","IPY_MODEL_6c470bbaf1a34f7eb0f4b49124b0a784","IPY_MODEL_d2e853e3b19145ccba361cde91f431d9"],"layout":"IPY_MODEL_7d03d16522f24ff5bb49beb0641e60f1"}},"131821eb645844e0a3c21b5d546e55ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1529c407fa2c432d82917f38f6908a47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8b398e83f1c64f308788562b4964534f","IPY_MODEL_bf53ce277d7b44d0aae817f1a42c292c","IPY_MODEL_5cf72e97ddf84adc8144279efa83b27a"],"layout":"IPY_MODEL_1baf8925e6a74cc2b4e0ad61de3fd4e5"}},"16466334edeb4f26a6aebd8074910681":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1886263934364ad8aeda59e9d4630973":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"198625f8aff540d0bbb22697511cdbf8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_008742b886154e849df548ce6dc47f5e","max":729,"min":0,"orientation":"horizontal","style":"IPY_MODEL_30fc2f4563be40e5a71026733f02641a","value":729}},"1b3bedbc85b647829a27c476d371430b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1baf8925e6a74cc2b4e0ad61de3fd4e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c79b4edb99e4e64b65d990428e754fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83a2fd741a234fd3a893e23202a408e2","IPY_MODEL_198625f8aff540d0bbb22697511cdbf8","IPY_MODEL_a5d339bcbb3d4b04830927f5aafb375b"],"layout":"IPY_MODEL_0238ef14f5b3407c90cb32382a826a66"}},"1cb66467b65741c7b88480fd0be62a95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa586556ab6a4b08a6c41ae8cf9616c9","placeholder":"​","style":"IPY_MODEL_8da7e79209b5435da137feae1cc84950","value":"100%"}},"1f5c941f279d4a25adf36a27f52436fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1faadcf97fc04265a2d08aefefccee1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2283d1231edc4bbe8c14602890827907":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_89606157cb2d4260b3f9adadd866a19e","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f6e0d39ef34944c29bfef15996346f04","value":3}},"23394b7587564f4691c0a43a698ddb75":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ce2ff581ff2047998e406d3dd5fbedf3","IPY_MODEL_3f7f5ba48ed64d9b812ba18ac80b99c5","IPY_MODEL_c7a4da19522e49b79a30a11956958ddb"],"layout":"IPY_MODEL_b33cfe48ffb549ec85237ae2e31fff7e"}},"28700895bda6476ab23f4527cedde21d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2cf3259140a54bd3861cef88f2230c68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ef48bd5710e4da2be9898280ba62710":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3f278be8f36d44c8999a09b605cc075f","IPY_MODEL_e75dcc2f4d1f42bcb93860cd6f2d93e8","IPY_MODEL_548a866ae5c34b1c8d062e5d5bc28897"],"layout":"IPY_MODEL_a1e6428c81a74a57bf352a143afdbacf"}},"30fc2f4563be40e5a71026733f02641a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"392e93762dcf44029fd5600735e08bb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa1610cccb314f73950235fc98c0dd9b","placeholder":"​","style":"IPY_MODEL_e65e895ed36d40338790bdbe8de064ed","value":" 3/3 [00:00&lt;00:00, 51.65it/s]"}},"3f278be8f36d44c8999a09b605cc075f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_899c0aae6ac94141aaed22b4ea8de111","placeholder":"​","style":"IPY_MODEL_546911b010784adeacf017a1d93256fd","value":"Downloading (…)cial_tokens_map.json: 100%"}},"3f7f5ba48ed64d9b812ba18ac80b99c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1d6904d72c5465b85a30265b0320d23","max":25000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ea2caf36fa6e411197095127655c010b","value":25000}},"3fc629dcb925482ebdd36aa55337f64e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40d6eeff20b44f7982e1dd2d374aa717":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"413c029d92c8452aa9dfcc542fd248e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42fc6bba258f402295db3124d44d6a7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e41d4b1eee5d4055b6bed71e6904b6ac","placeholder":"​","style":"IPY_MODEL_bf6790f2aae240cc9f5aec9477ee0a41","value":"Downloading (…)in/added_tokens.json: 100%"}},"459b826bd8924b5dbb9776c520b1d29f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4711645f769a4d9680b4ba3969d59989":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"471fd303c6914b90b32585297479729d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c2c67b303a44d17a4caeccfce4c34a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95fefd6f961f4f579b360253424f402b","placeholder":"​","style":"IPY_MODEL_2cf3259140a54bd3861cef88f2230c68","value":" 232k/232k [00:00&lt;00:00, 9.29MB/s]"}},"539692eed4f4467ca9db1ad67518d960":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffd231acab2345878b9de764f9b042b5","placeholder":"​","style":"IPY_MODEL_931a55c125f341fdb96d93d204d66df0","value":"Downloading builder script: 100%"}},"546911b010784adeacf017a1d93256fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"548a866ae5c34b1c8d062e5d5bc28897":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7857887f20c4fae9c9ababeaf6f2c74","placeholder":"​","style":"IPY_MODEL_1047661706094b1a819945b26f1f960e","value":" 112/112 [00:00&lt;00:00, 8.11kB/s]"}},"585dd71a703a4d46b01658e502e2456e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58b3664860c94eaab7ee093374b20232":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_59b35061c6a24ec98722ed2b2db9dba0","IPY_MODEL_aefb429406b44889a1a39f5de835d60c","IPY_MODEL_a6c7241bdbc648da86fa275447fc0637"],"layout":"IPY_MODEL_fb8bc3bafdd14732b4c992ab6069f641"}},"59b35061c6a24ec98722ed2b2db9dba0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aeb4714adc384fea8873feb425e99219","placeholder":"​","style":"IPY_MODEL_a7ac8fbfce2a421db860c7a82ec9d5ef","value":"Downloading pytorch_model.bin: 100%"}},"5adace3aa44b4f2f9d5fce5b217c8ec4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_471fd303c6914b90b32585297479729d","placeholder":"​","style":"IPY_MODEL_3fc629dcb925482ebdd36aa55337f64e","value":" 268M/268M [00:16&lt;00:00, 14.5MB/s]"}},"5cf72e97ddf84adc8144279efa83b27a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e459f1ac20654caa892e035ba7b6e19d","placeholder":"​","style":"IPY_MODEL_c2a577b4dd0e4efb8f10c337f9ee1268","value":" 58.0/58.0 [00:00&lt;00:00, 2.35kB/s]"}},"623aa72c4eba4f83955bd4c4ab06056f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65a8093cdae74338b0648e563f681b8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6655d9353ff84128914ddd80d10eca9a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b71fbe7c0bf4afead06b664a52e8398":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c470bbaf1a34f7eb0f4b49124b0a784":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f5c941f279d4a25adf36a27f52436fb","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_585dd71a703a4d46b01658e502e2456e","value":2}},"6cdfa87e511747c7996d9d4d8c6f5704":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7097d70d0ada48f799e3a4ba185719dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c162e735342c4031b81b45525635e2ed","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c013a4e90bd54044b2b78186780ffa13","value":231508}},"70ac10e209724ad98275aceba1cf3c33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7241fe51bbe1453a8a23706614384669":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73a5386819cb482e946210db4ed95811":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0712c748276b4193a31cd0166deb9e2c","IPY_MODEL_7097d70d0ada48f799e3a4ba185719dc","IPY_MODEL_4c2c67b303a44d17a4caeccfce4c34a6"],"layout":"IPY_MODEL_f1272e517971446d84808737d351caae"}},"7d03d16522f24ff5bb49beb0641e60f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83a2fd741a234fd3a893e23202a408e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16466334edeb4f26a6aebd8074910681","placeholder":"​","style":"IPY_MODEL_1faadcf97fc04265a2d08aefefccee1e","value":"Downloading (…)lve/main/config.json: 100%"}},"859bec0303ba4a06a039ca9b6d9e3d0d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_413c029d92c8452aa9dfcc542fd248e9","placeholder":"​","style":"IPY_MODEL_8e623731ba294a11a5c622b6a81d425d","value":" 4.20k/4.20k [00:00&lt;00:00, 84.2kB/s]"}},"89606157cb2d4260b3f9adadd866a19e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"899c0aae6ac94141aaed22b4ea8de111":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b398e83f1c64f308788562b4964534f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40d6eeff20b44f7982e1dd2d374aa717","placeholder":"​","style":"IPY_MODEL_d0c60485592b4e40a1af609deea8329e","value":"Downloading (…)okenizer_config.json: 100%"}},"8da7e79209b5435da137feae1cc84950":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e623731ba294a11a5c622b6a81d425d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91b082b39c6a4dbdbaab496da53bdb41":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"931a55c125f341fdb96d93d204d66df0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95275854d9f943e0babe64bc25d66f99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95fefd6f961f4f579b360253424f402b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1e6428c81a74a57bf352a143afdbacf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5d339bcbb3d4b04830927f5aafb375b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4711645f769a4d9680b4ba3969d59989","placeholder":"​","style":"IPY_MODEL_91b082b39c6a4dbdbaab496da53bdb41","value":" 729/729 [00:00&lt;00:00, 42.8kB/s]"}},"a6c7241bdbc648da86fa275447fc0637":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7db97f5357948f8b7ccdc1ee9669767","placeholder":"​","style":"IPY_MODEL_70ac10e209724ad98275aceba1cf3c33","value":" 268M/268M [00:15&lt;00:00, 19.1MB/s]"}},"a7ac8fbfce2a421db860c7a82ec9d5ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8d3c68f6bff43679d9eb92dda1abbf3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aeb4714adc384fea8873feb425e99219":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aefb429406b44889a1a39f5de835d60c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cdfa87e511747c7996d9d4d8c6f5704","max":267847358,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e922693bd44b47369008311f5f617c6e","value":267847358}},"b33cfe48ffb549ec85237ae2e31fff7e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"b748672e6b2f4b91a00e6c8500a399aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba9a181874fb4bf6861efac4f790d420":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd72d7160dc64c178daea3b1d137c23d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf53ce277d7b44d0aae817f1a42c292c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_623aa72c4eba4f83955bd4c4ab06056f","max":58,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65a8093cdae74338b0648e563f681b8c","value":58}},"bf6790f2aae240cc9f5aec9477ee0a41":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c013a4e90bd54044b2b78186780ffa13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c162e735342c4031b81b45525635e2ed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2a577b4dd0e4efb8f10c337f9ee1268":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7a4da19522e49b79a30a11956958ddb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba9a181874fb4bf6861efac4f790d420","placeholder":"​","style":"IPY_MODEL_1b3bedbc85b647829a27c476d371430b","value":" 25000/25000 [00:33&lt;00:00, 732.81 examples/s]"}},"ce2ff581ff2047998e406d3dd5fbedf3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_459b826bd8924b5dbb9776c520b1d29f","placeholder":"​","style":"IPY_MODEL_7241fe51bbe1453a8a23706614384669","value":"Map: 100%"}},"d0c60485592b4e40a1af609deea8329e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d21410e1a08449218bcd5039aaf0af7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2e853e3b19145ccba361cde91f431d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b71fbe7c0bf4afead06b664a52e8398","placeholder":"​","style":"IPY_MODEL_1886263934364ad8aeda59e9d4630973","value":" 2.00/2.00 [00:00&lt;00:00, 79.6B/s]"}},"d7857887f20c4fae9c9ababeaf6f2c74":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7db97f5357948f8b7ccdc1ee9669767":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de41cf40f86845fe98d7ff8f50c182c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_539692eed4f4467ca9db1ad67518d960","IPY_MODEL_066b8fd515d64976a22effb992492302","IPY_MODEL_859bec0303ba4a06a039ca9b6d9e3d0d"],"layout":"IPY_MODEL_6655d9353ff84128914ddd80d10eca9a"}},"dfbc2523a1434c3f8ea0d7f7e58f2e09":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1cb66467b65741c7b88480fd0be62a95","IPY_MODEL_2283d1231edc4bbe8c14602890827907","IPY_MODEL_392e93762dcf44029fd5600735e08bb9"],"layout":"IPY_MODEL_f646257837b74ea3be5491b72fba965a"}},"e053ddc848064b8d8752b74b836fdd5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e1d6904d72c5465b85a30265b0320d23":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e41d4b1eee5d4055b6bed71e6904b6ac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e459f1ac20654caa892e035ba7b6e19d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e65e895ed36d40338790bdbe8de064ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e75dcc2f4d1f42bcb93860cd6f2d93e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_02ab8134acb14cbcb9e23940b46050eb","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_131821eb645844e0a3c21b5d546e55ce","value":112}},"e8abdc4762b24fc5adcd5e64cd0b6eb4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e922693bd44b47369008311f5f617c6e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea2caf36fa6e411197095127655c010b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f1272e517971446d84808737d351caae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f45bec828ffc40a08376e65690cfc13d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8abdc4762b24fc5adcd5e64cd0b6eb4","max":267952916,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e053ddc848064b8d8752b74b836fdd5b","value":267952916}},"f646257837b74ea3be5491b72fba965a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6e0d39ef34944c29bfef15996346f04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f983d3c6252c435d86ed8283451936ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b748672e6b2f4b91a00e6c8500a399aa","placeholder":"​","style":"IPY_MODEL_a8d3c68f6bff43679d9eb92dda1abbf3","value":"Downloading tf_model.h5: 100%"}},"fa1610cccb314f73950235fc98c0dd9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa586556ab6a4b08a6c41ae8cf9616c9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb8bc3bafdd14732b4c992ab6069f641":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbcfe0dcb1e248dc93d23926d1e1421d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f983d3c6252c435d86ed8283451936ea","IPY_MODEL_f45bec828ffc40a08376e65690cfc13d","IPY_MODEL_5adace3aa44b4f2f9d5fce5b217c8ec4"],"layout":"IPY_MODEL_fe2fdf16dac7437ca92b1b1d6b8b4c52"}},"fe2fdf16dac7437ca92b1b1d6b8b4c52":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffd231acab2345878b9de764f9b042b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./09_Convolutional_NeuralNetworks_tensorflow.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Image processing with Convolutional Neural Networks - Tensorflow</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./11_Transfer_learning.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Transfer learning and self-supervised learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>