{"title":"Feature selection and extraction","markdown":{"yaml":{"title":"Feature selection and extraction","author":"phonchi","date":"03/20/2023","format":{"html":{"toc":true,"code-line-numbers":true,"code-fold":true,"code-tools":true}}},"headingText":"Setup","containsRefs":false,"markdown":"\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/05_Feature_selection_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/05_Feature_selection_extraction.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>\n<br/>\n\n\n## Feature selection\n\nThe classes in the `sklearn.feature_selection` module can be used for feature selection/extraction methods on datasets, either to improve estimators’ accuracy scores or to boost their performance on very high-dimensional datasets.\n\n### Removing low variance features\n\nSuppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is `0.8 * (1 - 0.8)`\n\nAs expected, `VarianceThreshold()` has removed the first column, which has a probability of containing a zero.\n\n### Univariate feature selection\n\n`Scikit-learn` exposes feature selection routines as objects that implement the `transform()` method. For instance, we can perform a $\\chi^2$ test to the samples to retrieve only the two best features as follows:\n\nThese objects take as input a scoring function that returns univariate scores/p-values (or only scores for `SelectKBest()` and `SelectPercentile()`):\n\n* For regression: `r_regression`, `f_regression`, `mutual_info_regression`\n* For classification: `chi2`, `f_classif`, `mutual_info_classif`\n\nThe methods based on F-test estimate the degree of linear dependency between two random variables. For example, we create a dataset with two informative features among a hundred. To simplify our example, we do not include either redundant or repeated features. In addition, We will explicitly not shuffle the dataset to ensure that the informative features will correspond to the three two columns of X.\n\nWe will create two machine learning pipelines. \n\n1. The former will be a random forest that will use all available features.\n\n2. The latter will also be a random forest, but we will add a feature selection step to train this classifier. \n\nWe will measure the average time spent to train each pipeline and make it predict. Besides, we will compute the testing score of the model. We will collect these results via cross-validation.\n\nTo analyze the results, we will merge the results from the two pipeline in a single pandas dataframe.\n\nLet’s first analyze the train and score time for each pipeline.\n\nWe can draw the same conclusions for both training and scoring elapsed time: selecting the most informative features speed-up our pipeline. Of course, such speed-up is beneficial only if the generalization performance in terms of metrics remain the same. Let’s check the testing score.\n\nWe can observe that the model’s generalization performance selecting a subset of features decreases compared with the model using all available features. Since we generated the dataset, we can infer that the decrease is because of the selection. **The feature selection algorithm did not choose the two informative features.**\n\nWe see that the feature 1 is always selected while the other feature varies depending on the cross-validation fold.\n\nIf we would like to keep our score with similar generalization performance, **we could choose another metric to perform the test or select more features.** For instance, we could select the number of features based on a specific percentile of the highest scores.\n\n#### Mutual information\n\nThe [*Automobile*](https://www.kaggle.com/toramky/automobile-dataset) dataset consists of 193 cars from the 1985 model year. The goal for this dataset is to predict a car's `price` (the target) from 23 of the car's features, such as `make`, `body_style`, and `horsepower`. In this example, we'll rank the features with mutual information and investigate the results by data visualization. (The original dataset requires data cleaning, you could refer to https://skill-lync.com/student-projects/project-1-1299 for more details)\n\nThe `scikit-learn` algorithm for MI treats discrete features differently from continuous features. Consequently, you need to tell it which are which. As a rule of thumb, anything that have a `float` dtype is *not* discrete. Categoricals (`object` or `categorial` dtype) can be treated as discrete by giving them a label encoding\n\n`Scikit-learn` has two mutual information metrics in its `feature_selection` module: one for real-valued targets (`mutual_info_regression()`) and one for categorical targets (`mutual_info_classif()`). Our target, `price`, is real-valued. The next cell computes the MI scores for our features and wraps them up in a nice dataframe.\n\nThe `fuel_type` feature has a fairly low MI score, but as we can see from the figure below, it clearly separates two `price` populations with different trends within the `horsepower` feature. **This indicates that `fuel_type` contributes an interaction effect and might not be unimportant after all.** Before deciding a feature is unimportant from its MI score, it's good to investigate any possible interaction effects -- domain knowledge can offer a lot of guidance here.\n\n### Sequential feature selection\n\nSequential Feature Selection is available in the `SequentialFeatureSelector` transformer. SFS can be either forward or backward:\n\n* Forward-SFS is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. Concretely, we initially start with zero feature and find the one feature that maximizes a cross-validated score when an estimator is trained on this single feature. Once that first feature is selected, we repeat the procedure by adding a new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the `n_features_to_select` parameter.\n\n* Backward-SFS follows the same idea but works in the opposite direction: instead of starting with no feature and greedily adding features, we start with all the features and greedily remove features from the set. The direction parameter controls whether forward or backward SFS is used.\n\n> In general, forward and backward selection do not yield equivalent results. Also, one may be much faster than the other depending on the requested number of selected features: if we have 10 features and ask for 7 selected features, forward selection would need to perform 7 iterations while backward selection would only need to perform 3.\n\n### Feature selection from model\n\n`SelectFromModel` is a meta-transformer that can be used alongside any estimator that assigns importance to each feature through a specific attribute (such as `coef_`, `feature_importances_`) or via an `importance_getter` callable after fitting. The features are considered unimportant and removed if the corresponding importance of the feature values are below the provided threshold parameter. \n\nApart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. **Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”. In combination with the threshold criteria, one can use the `max_features` parameter to set a limit on the number of features to select.**\n\n### A Concret example\n\nThe following dataset is our old friend which is a record of neighborhoods in California district, predicting the median house value (target) given some information about the neighborhoods, as the average number of rooms, the latitude, the longitude or the median income of people in the neighborhoods (block).\n\nThe feature reads as follow:\n\n* `MedInc`: median income in block\n* `HouseAge`: median house age in block\n* `AveRooms`: average number of rooms\n* `AveBedrms`: average number of bedrooms\n* `Population`: block population\n* `AveOccup`: average house occupancy\n* `Latitude`: house block latitude\n* `Longitude`: house block longitude\n* `MedHouseVal`: Median house value in 100k$ (target)\n\nTo assert the quality of our inspection technique, **let’s add some random feature that won’t help the prediction (un-informative feature)**\n\nIn linear models, the target value is modeled as a linear combination of the features.\n\nOur linear model obtains a score of .60, so it explains a significant part of the target. Its coefficient should be somehow relevant. Let’s look at the coefficient learnt\n\nThe `AveBedrms` have the higher coefficient. However, we can’t compare the magnitude of these coefficients directly, since they are not scaled. Indeed, `Population` is an integer which can be thousands, while `AveBedrms` is around 4 and `Latitude` is in degree.\n\nSo the `Population` coefficient is expressed in `“100k$/habitant”` while the `AveBedrms` is expressed in `“100k$/nb of bedrooms”` and the Latitude coefficient in `“100k$/degree”`. We see that changing population by one does not change the outcome, while as we go south (latitude increase) the price becomes cheaper. Also, adding a bedroom (keeping all other feature constant) shall rise the price of the house by `80k$`.\n\nSo looking at the coefficient plot to gauge feature importance can be misleading as some of them vary on a small scale, while others vary a lot more, several decades. **So before any interpretation, we need to scale each column (removing the mean and scaling the variance to 1).**\n\nNow that the coefficients have been scaled, we can safely compare them. The `MedInc` feature, with `longitude` and `latitude` are the three variables that most influence the model.\n\nThe plot above tells us about dependencies between a specific feature and the target when all other features remain constant, i.e., conditional dependencies. An increase of the `HouseAge` will induce an increase of the price when all other features remain constant. On the contrary, an increase of the `AveRooms` will induce an decrease of the price when all other features remain constant.\n\nWe can check the coefficient variability through cross-validation: it is a form of data perturbation.\n\nNow if we want to select the four features which are the most important according to the coefficients. The `SelectFromModel()` is meant just for that. `SelectFromModel()` accepts a `threshold` parameter and will select the features whose importance (defined by the coefficients) are above this threshold.\n\n#### Linear models with sparse coefficients (Lasso)\n\nIn it important to keep in mind that the associations extracted depend on the model. To illustrate this point we consider a Lasso model, that performs feature selection with a L1 penalty. Let us fit a Lasso model with a strong regularization parameters `alpha`\n\nHere the model score is a bit lower, because of the strong regularization. However, it has zeroed out 3 coefficients, selecting a small number of variables to make its prediction.\n\n#### Randomforest with feature importance\n\nOn some algorithms, there are some feature importance methods, inherently built within the model. It is the case in RandomForest models. Let’s investigate the built-in `feature_importances_` attribute.\n\n`MedInc` is still the most important feature. It also has a small bias toward high cardinality features, such as the noisy feature `rnd_num`, which are here predicted having `0.1` importance, more than `HouseAge` (which has low cardinality).\n\n#### Feature importance by permutation\n\nWe introduce here a new technique to evaluate the feature importance of any given fitted model. It basically shuffles a feature and sees how the model changes its prediction. Thus, the change in prediction will correspond to the feature importance.\n\nWe see again that the feature `MedInc`, `Latitude` and `Longitude` are important for the model. We note that our random variable `rnd_num` is now less important than `Latitude`. Indeed, the feature importance built-in in `RandomForest` has bias for continuous data, such as `AveOccup` and `rnd_num`.\n\n#### Feature rejection using Boruta\n\n## Dimensional reduction\n\nWe now looked at our model-based method for feature engineering: principal component analysis (PCA). You could think of PCA as a partitioning of the variation in the data. PCA is a great tool to help you discover important relationships in the data and can also be used to create more informative features.\n\nThere are two ways you could use PCA for feature engineering.\n\n1. The first way is to use it as a descriptive technique. Since the components tell you about the variation, **you could compute the MI scores for the components and see what kind of variation is most predictive of your target.** That could give you ideas for kinds of features to create -- a product of `'Height'` and `'Diameter'` if `'Size'` is important, say, or a ratio of `'Height'` and `'Diameter'` if `Shape` is important. You could even try clustering on one or more of the high-scoring components. [Biplot](https://blog.bioturing.com/2018/06/18/how-to-read-pca-biplots-and-scree-plots/) will be useful in this case.\n\n2. The second way is to use the components themselves as features. Because the components expose the variational structure of the data directly, **they can often be more informative than the original features.** Here are some use-cases:\n- **Dimensionality reduction**: When your features are highly redundant (*multicollinear*, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.\n- **Anomaly detection**: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.\n- **Noise reduction**: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.\n- **Decorrelation**: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.\n\nPCA basically gives you direct access to the correlational structure of your data. You'll no doubt come up with applications of your own!\n\nWe've selected four features that cover a range of properties. Each of these features also has a high MI score with the target, `price`. We'll standardize the data since these features aren't naturally on the same scale.\n\nNow we can fit scikit-learn's `PCA` estimator and create the principal components. You can see here the first few rows of the transformed dataset.\n\nAfter fitting, the `PCA` instance contains the loadings in its `components_` attribute. We'll wrap the loadings up in a dataframe.\n\nRecall that the signs and magnitudes of a component's loadings tell us what kind of variation it's captured. The first component (`PC1`) shows a contrast between large, powerful vehicles with poor gas milage, and smaller, more economical vehicles with good gas milage. We might call this the \"Luxury/Economy\" axis. The next figure shows that our four chosen features mostly vary along the Luxury/Economy axis.\n\nLet's also look at the MI scores of the components. Not surprisingly, `PC1` is highly informative, though the remaining components, despite their small variance, still have a significant relationship with `price`. Examining those components could be worthwhile to find relationships not captured by the main Luxury/Economy axis.\n\nThe third component shows a contrast between `horsepower` and `curb_weight` -- sports cars vs. wagons, it seems.\n\nTo express this contrast, let's create a new ratio feature:\n\n## Manifold learning\n\n#### t-SNE\n\n#### UMAP\n\nUMAP is useful for generating visualisations, but if you want to make use of UMAP more generally for machine learning tasks it is important to be be able to train a model and then later pass new data to the model and have it transform that data into the learned space. For example if we use UMAP to learn a latent space and then train a classifier on data transformed into the latent space then the classifier is only useful for prediction if we can transform data for which we want a prediction into the latent space the classifier uses. \n\nNow to get a benchmark idea of what we are looking at let’s train a couple of different classifiers and then see how well they score on the test set. For this example let’s try a support vector classifier and a KNN classifier.\n\nThe goal now is to make use of UMAP as a preprocessing step that one could potentially fit into a pipeline. \n\nThis looks very promising! Most of the classes got very cleanly separated, and that gives us some hope that it could help with classifier performance. We can now train some new models (again an SVC and a KNN classifier) on the embedded training data. This looks exactly as before but now we pass it the embedded data. \n\nThe next important question is what the transform did to our test data. In principle we have a new two dimensional representation of the test-set, and ideally this should be based on the existing embedding of the training set\n\nThe results look like what we should expect; the test data has been embedded into two dimensions in exactly the locations we should expect (by class) given the embedding of the training data visualised above. This means we can now try out models that were trained on the embedded training data by handing them the newly transformed test set.\n\nThe results are pretty good. While the accuracy of the KNN classifier did not improve there was not a lot of scope for improvement given the data. On the other hand the SVC has improved to have equal accuracy to the KNN classifier!\n\nFor more interesting datasets the larger dimensional embedding might have been a significant gain – it is certainly worth exploring as one of the parameters in a grid search across a pipeline that includes UMAP.\n\n\n\n## Clustering\n\nWhen used for feature engineering, we could attempt to discover groups of customers representing a market segment, for instance, or geographic areas that share similar weather patterns. Adding a feature of cluster labels can help machine learning models untangle complicated relationships of space or proximity.\n\n### Cluster Labels as a feature\n\nApplied to a single real-valued feature, clustering acts like a traditional \"binning\" or \"discretization\" transform. On multiple features, it's like \"multi-dimensional binning\" (sometimes called *vector quantization*).\n\nIt's important to remember that this Cluster feature is categorical. Here, it's shown with a label encoding (that is, as a sequence of integers) as a typical clustering algorithm would produce; depending on your model, a one-hot encoding may be more appropriate. The motivating idea **for adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks**. Our model can then just learn the simpler chunks one-by-one instead having to learn the complicated whole all at once. It's a \"divide and conquer\" strategy.\n\nAs spatial features, [*California Housing*](https://www.kaggle.com/camnugent/california-housing-prices)'s `'Latitude'` and `'Longitude'` make natural candidates for k-means clustering. In this example we'll cluster these with `'MedInc'` (median income) to create economic segments in different regions of California. Since k-means clustering is sensitive to scale, it can be a good idea rescale or normalize data with extreme values. Our features are already roughly on the same scale, so we'll leave them as-is.\n\nNotice the differnece between `predict()` and `transform()` in the KMeans. `predict()` will predict the closest cluster each sample in `X` belongs to. `transform()` will transform data to a cluster-distance space where each dimension is the distance to the cluster centers.\n\n\nNow let's look at a couple plots to see how effective this was. First, a scatter plot that shows the geographic distribution of the clusters. It seems like the algorithm has created separate segments for higher-income areas on the coasts.\n\nThe target in this dataset is `MedHouseVal` (median house value). These box-plots show the distribution of the target within each cluster. If the clustering is informative, these distributions should, for the most part, separate across `MedHouseVal`, which is indeed what we see.\n\n### Cluster distance as a feature\n\nNow let's fit a Logistic Regression model and evaluate it on the test set:\n\nOkay, that's our baseline: 96.89% accuracy. Let's see if we can do better by using K-Means as a preprocessing step. **We will create a pipeline that will first cluster the training set into 30 clusters and replace the images with their distances to the 30 clusters**, then apply a logistic regression model:\n\nHow much did the error rate drop?\n\nWe reduced the error rate by over 14%! \n\n## Guideline to determine the optimal number of features or threshold?\n\nTo determine the optimal hyperparameter, we can use cross validation. For instance, in the above example, we chose the number of clusters `k` completely arbitrarily. However, we can surely do better. Since K-Means is just a preprocessing step in a classification pipeline, finding a good value for `k` is the best value of `k` is simply the one that results in the best classification performance.\n\nIn the same way, you can also use cross-validation to evaluate model performance with different numbers of top-ranked features or different numbers of features and choose the optimal number based on the performance metric (e.g., highest accuracy or lowest error).\n\n### Using Clustering for Semi-Supervised Learning\n\nAnother use case for clustering is in semi-supervised learning, when we have plenty of unlabeled instances and very few labeled instances.\n\nLet's look at the performance of a logistic regression model when we only have 50 labeled instances:\n\nThe model’s accuracy is just 83.33%. It's much less than earlier of course. Let's see how we can do better. First, let's cluster the training set into 50 clusters, then for each cluster let's **find the image closest to the centroid. We will call these images the representative images:**\n\nNow let's plot these representative images and label them manually:\n\nNow we have a dataset with just 50 labeled instances, but instead of being completely random instances, each of them is a representative image of its cluster. Let's see if the performance is any better:\n\nWe jumped from 83.3% accuracy to 92.2%, although we are still only training the model on 50 instances. Since it's often costly and painful to label instances, especially when it has to be done manually by experts, it's a good idea to make them label representative instances rather than just random instances.\n\nBut perhaps we can go one step further: **what if we propagated the labels to all the other instances in the same cluster?**\n\nWe got a tiny little accuracy boost. Better than nothing, **but we should probably have propagated the labels only to the instances closest to the centroid, because by propagating to the full cluster, we have certainly included some outliers.** Let's only propagate the labels to the 75th percentile closest to the centroid:\n\nA bit better. With just 50 labeled instances (just 5 examples per class on average!), we got 93.5% performance, which is getting closer to the performance of logistic regression on the fully labeled digits dataset.\n\nOur propagated labels are actually pretty good: their accuracy is about 97.5%:\n\nYou could also do a few iterations of active learning:\n\n1. Manually label the instances that the classifier is least sure about, if possible by picking them in distinct clusters.\n2. Train a new model with these additional labels.\n\n### Feature agglomeration\n\n`cluster.FeatureAgglomeration` applies Hierarchical clustering to group together features that behave similarly.\n\n## References\n\n1. [https://www.kaggle.com/learn/feature-engineering](https://www.kaggle.com/learn/feature-engineering)\n2. [https://inria.github.io/scikit-learn-mooc/python_scripts/dev_features_importance.html#](https://inria.github.io/scikit-learn-mooc/python_scripts/dev_features_importance.html#) \n3. [https://scikit-learn.org/stable/modules/feature_selection.html](https://scikit-learn.org/stable/modules/feature_selection.html) \n4. [https://scikit-learn.org/stable/modules/preprocessing.html#](https://scikit-learn.org/stable/modules/preprocessing.html#) \n5. [https://scikit-learn.org/stable/modules/unsupervised_reduction.html](https://scikit-learn.org/stable/modules/unsupervised_reduction.html  )\n6. [https://github.com/ageron/handson-ml3/blob/main/09_unsupervised_learning.ipynb](https://github.com/ageron/handson-ml3/blob/main/09_unsupervised_learning.ipynb)\n\n\n","srcMarkdownNoYaml":"\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/05_Feature_selection_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/05_Feature_selection_extraction.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>\n<br/>\n\n## Setup\n\n## Feature selection\n\nThe classes in the `sklearn.feature_selection` module can be used for feature selection/extraction methods on datasets, either to improve estimators’ accuracy scores or to boost their performance on very high-dimensional datasets.\n\n### Removing low variance features\n\nSuppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is `0.8 * (1 - 0.8)`\n\nAs expected, `VarianceThreshold()` has removed the first column, which has a probability of containing a zero.\n\n### Univariate feature selection\n\n`Scikit-learn` exposes feature selection routines as objects that implement the `transform()` method. For instance, we can perform a $\\chi^2$ test to the samples to retrieve only the two best features as follows:\n\nThese objects take as input a scoring function that returns univariate scores/p-values (or only scores for `SelectKBest()` and `SelectPercentile()`):\n\n* For regression: `r_regression`, `f_regression`, `mutual_info_regression`\n* For classification: `chi2`, `f_classif`, `mutual_info_classif`\n\nThe methods based on F-test estimate the degree of linear dependency between two random variables. For example, we create a dataset with two informative features among a hundred. To simplify our example, we do not include either redundant or repeated features. In addition, We will explicitly not shuffle the dataset to ensure that the informative features will correspond to the three two columns of X.\n\nWe will create two machine learning pipelines. \n\n1. The former will be a random forest that will use all available features.\n\n2. The latter will also be a random forest, but we will add a feature selection step to train this classifier. \n\nWe will measure the average time spent to train each pipeline and make it predict. Besides, we will compute the testing score of the model. We will collect these results via cross-validation.\n\nTo analyze the results, we will merge the results from the two pipeline in a single pandas dataframe.\n\nLet’s first analyze the train and score time for each pipeline.\n\nWe can draw the same conclusions for both training and scoring elapsed time: selecting the most informative features speed-up our pipeline. Of course, such speed-up is beneficial only if the generalization performance in terms of metrics remain the same. Let’s check the testing score.\n\nWe can observe that the model’s generalization performance selecting a subset of features decreases compared with the model using all available features. Since we generated the dataset, we can infer that the decrease is because of the selection. **The feature selection algorithm did not choose the two informative features.**\n\nWe see that the feature 1 is always selected while the other feature varies depending on the cross-validation fold.\n\nIf we would like to keep our score with similar generalization performance, **we could choose another metric to perform the test or select more features.** For instance, we could select the number of features based on a specific percentile of the highest scores.\n\n#### Mutual information\n\nThe [*Automobile*](https://www.kaggle.com/toramky/automobile-dataset) dataset consists of 193 cars from the 1985 model year. The goal for this dataset is to predict a car's `price` (the target) from 23 of the car's features, such as `make`, `body_style`, and `horsepower`. In this example, we'll rank the features with mutual information and investigate the results by data visualization. (The original dataset requires data cleaning, you could refer to https://skill-lync.com/student-projects/project-1-1299 for more details)\n\nThe `scikit-learn` algorithm for MI treats discrete features differently from continuous features. Consequently, you need to tell it which are which. As a rule of thumb, anything that have a `float` dtype is *not* discrete. Categoricals (`object` or `categorial` dtype) can be treated as discrete by giving them a label encoding\n\n`Scikit-learn` has two mutual information metrics in its `feature_selection` module: one for real-valued targets (`mutual_info_regression()`) and one for categorical targets (`mutual_info_classif()`). Our target, `price`, is real-valued. The next cell computes the MI scores for our features and wraps them up in a nice dataframe.\n\nThe `fuel_type` feature has a fairly low MI score, but as we can see from the figure below, it clearly separates two `price` populations with different trends within the `horsepower` feature. **This indicates that `fuel_type` contributes an interaction effect and might not be unimportant after all.** Before deciding a feature is unimportant from its MI score, it's good to investigate any possible interaction effects -- domain knowledge can offer a lot of guidance here.\n\n### Sequential feature selection\n\nSequential Feature Selection is available in the `SequentialFeatureSelector` transformer. SFS can be either forward or backward:\n\n* Forward-SFS is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. Concretely, we initially start with zero feature and find the one feature that maximizes a cross-validated score when an estimator is trained on this single feature. Once that first feature is selected, we repeat the procedure by adding a new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the `n_features_to_select` parameter.\n\n* Backward-SFS follows the same idea but works in the opposite direction: instead of starting with no feature and greedily adding features, we start with all the features and greedily remove features from the set. The direction parameter controls whether forward or backward SFS is used.\n\n> In general, forward and backward selection do not yield equivalent results. Also, one may be much faster than the other depending on the requested number of selected features: if we have 10 features and ask for 7 selected features, forward selection would need to perform 7 iterations while backward selection would only need to perform 3.\n\n### Feature selection from model\n\n`SelectFromModel` is a meta-transformer that can be used alongside any estimator that assigns importance to each feature through a specific attribute (such as `coef_`, `feature_importances_`) or via an `importance_getter` callable after fitting. The features are considered unimportant and removed if the corresponding importance of the feature values are below the provided threshold parameter. \n\nApart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. **Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”. In combination with the threshold criteria, one can use the `max_features` parameter to set a limit on the number of features to select.**\n\n### A Concret example\n\nThe following dataset is our old friend which is a record of neighborhoods in California district, predicting the median house value (target) given some information about the neighborhoods, as the average number of rooms, the latitude, the longitude or the median income of people in the neighborhoods (block).\n\nThe feature reads as follow:\n\n* `MedInc`: median income in block\n* `HouseAge`: median house age in block\n* `AveRooms`: average number of rooms\n* `AveBedrms`: average number of bedrooms\n* `Population`: block population\n* `AveOccup`: average house occupancy\n* `Latitude`: house block latitude\n* `Longitude`: house block longitude\n* `MedHouseVal`: Median house value in 100k$ (target)\n\nTo assert the quality of our inspection technique, **let’s add some random feature that won’t help the prediction (un-informative feature)**\n\nIn linear models, the target value is modeled as a linear combination of the features.\n\nOur linear model obtains a score of .60, so it explains a significant part of the target. Its coefficient should be somehow relevant. Let’s look at the coefficient learnt\n\nThe `AveBedrms` have the higher coefficient. However, we can’t compare the magnitude of these coefficients directly, since they are not scaled. Indeed, `Population` is an integer which can be thousands, while `AveBedrms` is around 4 and `Latitude` is in degree.\n\nSo the `Population` coefficient is expressed in `“100k$/habitant”` while the `AveBedrms` is expressed in `“100k$/nb of bedrooms”` and the Latitude coefficient in `“100k$/degree”`. We see that changing population by one does not change the outcome, while as we go south (latitude increase) the price becomes cheaper. Also, adding a bedroom (keeping all other feature constant) shall rise the price of the house by `80k$`.\n\nSo looking at the coefficient plot to gauge feature importance can be misleading as some of them vary on a small scale, while others vary a lot more, several decades. **So before any interpretation, we need to scale each column (removing the mean and scaling the variance to 1).**\n\nNow that the coefficients have been scaled, we can safely compare them. The `MedInc` feature, with `longitude` and `latitude` are the three variables that most influence the model.\n\nThe plot above tells us about dependencies between a specific feature and the target when all other features remain constant, i.e., conditional dependencies. An increase of the `HouseAge` will induce an increase of the price when all other features remain constant. On the contrary, an increase of the `AveRooms` will induce an decrease of the price when all other features remain constant.\n\nWe can check the coefficient variability through cross-validation: it is a form of data perturbation.\n\nNow if we want to select the four features which are the most important according to the coefficients. The `SelectFromModel()` is meant just for that. `SelectFromModel()` accepts a `threshold` parameter and will select the features whose importance (defined by the coefficients) are above this threshold.\n\n#### Linear models with sparse coefficients (Lasso)\n\nIn it important to keep in mind that the associations extracted depend on the model. To illustrate this point we consider a Lasso model, that performs feature selection with a L1 penalty. Let us fit a Lasso model with a strong regularization parameters `alpha`\n\nHere the model score is a bit lower, because of the strong regularization. However, it has zeroed out 3 coefficients, selecting a small number of variables to make its prediction.\n\n#### Randomforest with feature importance\n\nOn some algorithms, there are some feature importance methods, inherently built within the model. It is the case in RandomForest models. Let’s investigate the built-in `feature_importances_` attribute.\n\n`MedInc` is still the most important feature. It also has a small bias toward high cardinality features, such as the noisy feature `rnd_num`, which are here predicted having `0.1` importance, more than `HouseAge` (which has low cardinality).\n\n#### Feature importance by permutation\n\nWe introduce here a new technique to evaluate the feature importance of any given fitted model. It basically shuffles a feature and sees how the model changes its prediction. Thus, the change in prediction will correspond to the feature importance.\n\nWe see again that the feature `MedInc`, `Latitude` and `Longitude` are important for the model. We note that our random variable `rnd_num` is now less important than `Latitude`. Indeed, the feature importance built-in in `RandomForest` has bias for continuous data, such as `AveOccup` and `rnd_num`.\n\n#### Feature rejection using Boruta\n\n## Dimensional reduction\n\nWe now looked at our model-based method for feature engineering: principal component analysis (PCA). You could think of PCA as a partitioning of the variation in the data. PCA is a great tool to help you discover important relationships in the data and can also be used to create more informative features.\n\nThere are two ways you could use PCA for feature engineering.\n\n1. The first way is to use it as a descriptive technique. Since the components tell you about the variation, **you could compute the MI scores for the components and see what kind of variation is most predictive of your target.** That could give you ideas for kinds of features to create -- a product of `'Height'` and `'Diameter'` if `'Size'` is important, say, or a ratio of `'Height'` and `'Diameter'` if `Shape` is important. You could even try clustering on one or more of the high-scoring components. [Biplot](https://blog.bioturing.com/2018/06/18/how-to-read-pca-biplots-and-scree-plots/) will be useful in this case.\n\n2. The second way is to use the components themselves as features. Because the components expose the variational structure of the data directly, **they can often be more informative than the original features.** Here are some use-cases:\n- **Dimensionality reduction**: When your features are highly redundant (*multicollinear*, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.\n- **Anomaly detection**: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.\n- **Noise reduction**: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.\n- **Decorrelation**: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.\n\nPCA basically gives you direct access to the correlational structure of your data. You'll no doubt come up with applications of your own!\n\nWe've selected four features that cover a range of properties. Each of these features also has a high MI score with the target, `price`. We'll standardize the data since these features aren't naturally on the same scale.\n\nNow we can fit scikit-learn's `PCA` estimator and create the principal components. You can see here the first few rows of the transformed dataset.\n\nAfter fitting, the `PCA` instance contains the loadings in its `components_` attribute. We'll wrap the loadings up in a dataframe.\n\nRecall that the signs and magnitudes of a component's loadings tell us what kind of variation it's captured. The first component (`PC1`) shows a contrast between large, powerful vehicles with poor gas milage, and smaller, more economical vehicles with good gas milage. We might call this the \"Luxury/Economy\" axis. The next figure shows that our four chosen features mostly vary along the Luxury/Economy axis.\n\nLet's also look at the MI scores of the components. Not surprisingly, `PC1` is highly informative, though the remaining components, despite their small variance, still have a significant relationship with `price`. Examining those components could be worthwhile to find relationships not captured by the main Luxury/Economy axis.\n\nThe third component shows a contrast between `horsepower` and `curb_weight` -- sports cars vs. wagons, it seems.\n\nTo express this contrast, let's create a new ratio feature:\n\n## Manifold learning\n\n#### t-SNE\n\n#### UMAP\n\nUMAP is useful for generating visualisations, but if you want to make use of UMAP more generally for machine learning tasks it is important to be be able to train a model and then later pass new data to the model and have it transform that data into the learned space. For example if we use UMAP to learn a latent space and then train a classifier on data transformed into the latent space then the classifier is only useful for prediction if we can transform data for which we want a prediction into the latent space the classifier uses. \n\nNow to get a benchmark idea of what we are looking at let’s train a couple of different classifiers and then see how well they score on the test set. For this example let’s try a support vector classifier and a KNN classifier.\n\nThe goal now is to make use of UMAP as a preprocessing step that one could potentially fit into a pipeline. \n\nThis looks very promising! Most of the classes got very cleanly separated, and that gives us some hope that it could help with classifier performance. We can now train some new models (again an SVC and a KNN classifier) on the embedded training data. This looks exactly as before but now we pass it the embedded data. \n\nThe next important question is what the transform did to our test data. In principle we have a new two dimensional representation of the test-set, and ideally this should be based on the existing embedding of the training set\n\nThe results look like what we should expect; the test data has been embedded into two dimensions in exactly the locations we should expect (by class) given the embedding of the training data visualised above. This means we can now try out models that were trained on the embedded training data by handing them the newly transformed test set.\n\nThe results are pretty good. While the accuracy of the KNN classifier did not improve there was not a lot of scope for improvement given the data. On the other hand the SVC has improved to have equal accuracy to the KNN classifier!\n\nFor more interesting datasets the larger dimensional embedding might have been a significant gain – it is certainly worth exploring as one of the parameters in a grid search across a pipeline that includes UMAP.\n\n\n\n## Clustering\n\nWhen used for feature engineering, we could attempt to discover groups of customers representing a market segment, for instance, or geographic areas that share similar weather patterns. Adding a feature of cluster labels can help machine learning models untangle complicated relationships of space or proximity.\n\n### Cluster Labels as a feature\n\nApplied to a single real-valued feature, clustering acts like a traditional \"binning\" or \"discretization\" transform. On multiple features, it's like \"multi-dimensional binning\" (sometimes called *vector quantization*).\n\nIt's important to remember that this Cluster feature is categorical. Here, it's shown with a label encoding (that is, as a sequence of integers) as a typical clustering algorithm would produce; depending on your model, a one-hot encoding may be more appropriate. The motivating idea **for adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks**. Our model can then just learn the simpler chunks one-by-one instead having to learn the complicated whole all at once. It's a \"divide and conquer\" strategy.\n\nAs spatial features, [*California Housing*](https://www.kaggle.com/camnugent/california-housing-prices)'s `'Latitude'` and `'Longitude'` make natural candidates for k-means clustering. In this example we'll cluster these with `'MedInc'` (median income) to create economic segments in different regions of California. Since k-means clustering is sensitive to scale, it can be a good idea rescale or normalize data with extreme values. Our features are already roughly on the same scale, so we'll leave them as-is.\n\nNotice the differnece between `predict()` and `transform()` in the KMeans. `predict()` will predict the closest cluster each sample in `X` belongs to. `transform()` will transform data to a cluster-distance space where each dimension is the distance to the cluster centers.\n\n\nNow let's look at a couple plots to see how effective this was. First, a scatter plot that shows the geographic distribution of the clusters. It seems like the algorithm has created separate segments for higher-income areas on the coasts.\n\nThe target in this dataset is `MedHouseVal` (median house value). These box-plots show the distribution of the target within each cluster. If the clustering is informative, these distributions should, for the most part, separate across `MedHouseVal`, which is indeed what we see.\n\n### Cluster distance as a feature\n\nNow let's fit a Logistic Regression model and evaluate it on the test set:\n\nOkay, that's our baseline: 96.89% accuracy. Let's see if we can do better by using K-Means as a preprocessing step. **We will create a pipeline that will first cluster the training set into 30 clusters and replace the images with their distances to the 30 clusters**, then apply a logistic regression model:\n\nHow much did the error rate drop?\n\nWe reduced the error rate by over 14%! \n\n## Guideline to determine the optimal number of features or threshold?\n\nTo determine the optimal hyperparameter, we can use cross validation. For instance, in the above example, we chose the number of clusters `k` completely arbitrarily. However, we can surely do better. Since K-Means is just a preprocessing step in a classification pipeline, finding a good value for `k` is the best value of `k` is simply the one that results in the best classification performance.\n\nIn the same way, you can also use cross-validation to evaluate model performance with different numbers of top-ranked features or different numbers of features and choose the optimal number based on the performance metric (e.g., highest accuracy or lowest error).\n\n### Using Clustering for Semi-Supervised Learning\n\nAnother use case for clustering is in semi-supervised learning, when we have plenty of unlabeled instances and very few labeled instances.\n\nLet's look at the performance of a logistic regression model when we only have 50 labeled instances:\n\nThe model’s accuracy is just 83.33%. It's much less than earlier of course. Let's see how we can do better. First, let's cluster the training set into 50 clusters, then for each cluster let's **find the image closest to the centroid. We will call these images the representative images:**\n\nNow let's plot these representative images and label them manually:\n\nNow we have a dataset with just 50 labeled instances, but instead of being completely random instances, each of them is a representative image of its cluster. Let's see if the performance is any better:\n\nWe jumped from 83.3% accuracy to 92.2%, although we are still only training the model on 50 instances. Since it's often costly and painful to label instances, especially when it has to be done manually by experts, it's a good idea to make them label representative instances rather than just random instances.\n\nBut perhaps we can go one step further: **what if we propagated the labels to all the other instances in the same cluster?**\n\nWe got a tiny little accuracy boost. Better than nothing, **but we should probably have propagated the labels only to the instances closest to the centroid, because by propagating to the full cluster, we have certainly included some outliers.** Let's only propagate the labels to the 75th percentile closest to the centroid:\n\nA bit better. With just 50 labeled instances (just 5 examples per class on average!), we got 93.5% performance, which is getting closer to the performance of logistic regression on the fully labeled digits dataset.\n\nOur propagated labels are actually pretty good: their accuracy is about 97.5%:\n\nYou could also do a few iterations of active learning:\n\n1. Manually label the instances that the classifier is least sure about, if possible by picking them in distinct clusters.\n2. Train a new model with these additional labels.\n\n### Feature agglomeration\n\n`cluster.FeatureAgglomeration` applies Hierarchical clustering to group together features that behave similarly.\n\n## References\n\n1. [https://www.kaggle.com/learn/feature-engineering](https://www.kaggle.com/learn/feature-engineering)\n2. [https://inria.github.io/scikit-learn-mooc/python_scripts/dev_features_importance.html#](https://inria.github.io/scikit-learn-mooc/python_scripts/dev_features_importance.html#) \n3. [https://scikit-learn.org/stable/modules/feature_selection.html](https://scikit-learn.org/stable/modules/feature_selection.html) \n4. [https://scikit-learn.org/stable/modules/preprocessing.html#](https://scikit-learn.org/stable/modules/preprocessing.html#) \n5. [https://scikit-learn.org/stable/modules/unsupervised_reduction.html](https://scikit-learn.org/stable/modules/unsupervised_reduction.html  )\n6. [https://github.com/ageron/handson-ml3/blob/main/09_unsupervised_learning.ipynb](https://github.com/ageron/handson-ml3/blob/main/09_unsupervised_learning.ipynb)\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"05_Feature_selection_extraction.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.306","theme":"cosmo","cover-image":"cover.jpg","title":"Feature selection and extraction","author":"phonchi","date":"03/20/2023"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}