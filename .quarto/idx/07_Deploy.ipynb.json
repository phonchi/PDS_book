{"title":"Deploy and monitoring","markdown":{"yaml":{"title":"Deploy and monitoring","author":"phonchi","date":"04/10/2023","format":{"html":{"toc":true,"code-line-numbers":true,"code-fold":true,"code-tools":true}}},"headingText":"Setup","containsRefs":false,"markdown":"\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/07_Deploy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/07_Deploy.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>\n<br/>\n\n\nNotice that **you may need to restart the kernel** after the above installations.\n\nHere are some tips for this notebook [https://amitness.com/2020/06/google-colaboratory-tips/](https://amitness.com/2020/06/google-colaboratory-tips/) and [https://stackoverflow.com/questions/59741453/is-there-a-general-way-to-run-web-applications-on-google-colab](https://stackoverflow.com/questions/59741453/is-there-a-general-way-to-run-web-applications-on-google-colab).\n\n`ngrok` is a reverse proxy tool that opens secure tunnels from public URLs to localhost, perfect for exposing local web servers, building webhook integrations, enabling SSH access, testing chatbots, demoing from your own machine, and more. In this lab, we will use use [https://pyngrok.readthedocs.io/en/latest/integrations.html](https://pyngrok.readthedocs.io/en/latest/integrations.html). However, for production environment, it is recommended to use cloud service such as AWS, GCP or Azure, see [here](https://towardsdatascience.com/the-hierarchy-of-ml-tooling-on-the-public-cloud-ed387cac3c27) or [https://pycaret.gitbook.io/docs/get-started/functions/deploy#deploy_model](https://pycaret.gitbook.io/docs/get-started/functions/deploy#deploy_model) for more details.\n\n## Deploying TensorFlow models to TensorFlow Serving (TFS) on remote server\n\nYou could create your own microservice using any technology you want (e.g., using the Flask library), but why reinvent the wheel when you can just use TF Serving?\n\n### Exporting `SavedModels`\n\nTensorFlow provides a simple `tf.keras.models.save_model()` function to export models to the SavedModel format. All you need to do is give it the model, specifying its name and version number, and the function will save the model’s computation graph and its weights:\n\nIt’s usually a good idea to include all the preprocessing layers in the final model you export so that it can ingest data in its natural form once it is deployed to production. This  avoids  having  to  take  care  of  preprocessing  separately  within  the  application that uses the model. Bundling the preprocessing steps within the model also makes it simpler to update them later on and limits the risk of mismatch between a model and\nthe preprocessing steps it requires!\n\nNow to version the model, you just need to create a subdirectory for each model version:\n\nA SavedModel represents a version of your model. It is stored as a directory containing a `saved_model.pb` file, which defines the computation graph (represented as a **serialized protocol buffer**), and a variables subdirectory\ncontaining the variable values. For models containing a large number of weights, these variable values may be split across multiple files. A SavedModel also includes an `assets` subdirectory that may contain additional data, such as vocabulary files, class names, or some example instances for this model.\n\nAs you might expect, you can load a SavedModel using the `tf.keras.models.load_model()` function. \n\nTensorFlow also comes with a small `saved_model_cli` command-line tool to inspect `SavedModels`:\n\nA SavedModel contains one or more metagraphs. When you pass a `tf.keras` model, by default the function saves a simple SavedModel: it saves a single metagraph tagged \"serve\", which contains two signature definitions, an initialization function (called  `_saved_model_init_op`) and a default serving function (called `serving_default`). When saving a `tf.keras` model, the default serving function corresponds to the model’s `call()` function, which of course makes predictions.\n\n### Serve your model with TensorFlow Serving (Server side)\n\nThere are many ways to install TF Serving: using the system’s package manager, using a Docker image, installing from source, and more. Since Colab/Kaggle runs on Ubuntu, we can use Ubuntu’s  apt  package manager like this:\n\n> The  code  above starts  by  adding  TensorFlow's  package  repository  to  Ubuntu's  list  of package  sources.  Then  it  downloads  TensorFlow's  public  GPG  key  and  adds  it  to the  package  manager’s  key  list  so  it  can  verify  TensorFlow's  package  signatures. Next, it uses  apt  to install the  `tensorflow-model-server`  package. Lastly, it installs the  `tensorflow-serving-api`  library, which we will need to communicate with the\nserver.\n\nIf `tensorflow_model_server` is installed (e.g., if you are running this notebook in Colab/Kaggle), then the following 2 cells will start the server. If your OS is Windows, you may need to run the tensorflow_model_server command in a terminal, and replace `${MODEL_DIR}` with the full path to the `my_mnist_model` directory. This is where we start running TensorFlow Serving and load our model. After it loads we can start making inference requests using REST. There are some important parameters:\n\n* `port`: The port that you'll use for gRPC requests.\n* `rest_api_port`: The port that you'll use for REST requests.\n* `model_name`: You'll use this in the URL of REST requests. It can be anything.\n* `model_base_path`: This is the path to the directory where you've saved your model.\n\nThe  `%%bash  --bg`   magic  command  executes  the  cell  as  a  bash script,  running  it  in  the  background.  The  `>my_server.log  2>&1`   part  redirects  the standard output and standard error to the `server.log` file. And that’s it! TF Serving is now running in the background, and its logs are saved to `server.log`.\n\n### Querying TF Serving through the REST API (client side)\n\nLet’s start by creating the query. It must contain the name of the function signature you want to call, and of course the input data. Since the request must use the JSON format, we have to convert the input images from a `NumPy` array to a Python list:\n\nNote that the JSON format is 100% text-based:\n\nNow let’s send the input data to TF Serving by sending an HTTP POST request. This can be done easily using the `requests` library:\n\nThe response is a dictionary containing a single \"predictions\" key. The corresponding value is the list of predictions. This list is a Python list, so let’s convert it to a `NumPy` array and round the floats it contains to the\nsecond decimal:\n\nFor more information, please refer to [https://github.com/tensorflow/serving](https://github.com/tensorflow/serving) which include the usuage of gRPC.\n\n### Deploying a new model version\n\nNow let’s create a new model version and export a SavedModel to the `my_mnist_model/0002` directory, just like earlier:\n\nAt regular intervals (the delay is configurable), TensorFlow Serving checks for new model versions. If it finds one, it will automatically handle the transition gracefully: by default, it will answer pending requests (if any) with the previous model version, while handling new requests with the new version. As soon as every pending request has been answered, the previous model version is unloaded. You can see this at work in the TF Serving logs:\n\nAs you can see, TF Serving makes it quite simple to deploy new models. Moreover, if you discover that version 2 does not work as well as you expected, then rolling back to version 1 is as simple as removing the `my_mnist_model/0002` directory.\n\nYou can also refer to [https://github.com/microsoft/ML-For-Beginners/blob/main/3-Web-App/1-Web-App/README.md](https://github.com/microsoft/ML-For-Beginners/blob/main/3-Web-App/1-Web-App/README.md) or [https://github.com/rodrigo-arenas/fast-ml-deploy](https://github.com/rodrigo-arenas/fast-ml-deploy) which use [Flask](https://flask.palletsprojects.com/en/2.1.x/) and [FastAPI](https://fastapi.tiangolo.com/) that may have more flexibility. \n\nIf you would like to deploy to GCP vertex AI, checkout [here](https://github.com/ageron/handson-ml3/blob/main/19_training_and_deploying_at_scale.ipynb).\n\n## Deploy a REST API server using BentoML on remote server\n\nTo begin with `BentoML`, you will need to save your trained models with `BentoML` API in its model store (a local directory managed by `BentoML`). The model store is used for managing all your trained models locally as well as accessing them for serving.\n\n### Train a classifier model using the iris dataset\n\nSave the `clf` model with BentoML. We begin by saving a trained model instance to **BentoML’s local model store**. The local model store is used to version your models as well as control which models are packaged with your bento. It is noted that there are a [wide range of models](https://docs.bentoml.org/en/latest/frameworks/index.html#frameworks-page) can be saved via BentoML. \n\n> It is possible to use pre-trained models directly with BentoML or import existing trained model files to BentoML. Learn more about it from [Preparing Models](https://docs.bentoml.org/en/latest/concepts/model.html).\n\nModels saved can be accessed via `bentoml models` CLI command:\n\nTo verify that the saved model can be loaded correctly:\n\nIn BentoML, the recommended way of running ML model inference in serving is via `Runner`:\n\nIn this example, `bentoml.sklearn.get()` creates a reference to the saved model in the model store, and `to_runner()` creates a `Runner` instance from the model. The `Runner` abstraction gives `BentoServer` more flexibility in terms of how to schedule the inference computation, how to dynamically batch inference calls and better take advantage of all hardware resource available.\n\n### Create a BentoML Service for serving the model\n\nServices are the core components of BentoML, where the serving logic is defined. With the model saved in the model store, we can define the service by creating a Python file `service.py` with the following contents:\n\nIn this example, we defined the input and output type to be `numpy.ndarray`. More options, such as `pandas.DataFrame`, `JSON` and `PIL.image` are also supported. The `svc.api` decorator adds a function to the `bentoml.Service` object's APIs list. The input and output parameter takes an IO Descriptor object, which specifies the API function's expected input/output types, and is used for generating HTTP endpoints. Inside the API function, users can define any business logic, feature fetching, and feature transformation code. Model inference calls are made directly through runner objects, that are passed into `bentoml.Service(name=.., runners=[..])` call when creating the service object.\n\n> BentoML Server runs the Service API in an ASGI web serving layer and puts Runners in a separate worker process pool managed by BentoML. **The ASGI web serving layer will expose REST endpoints for inference APIs, such as POST /predict and common infrastructure APIs, such as GET /metrics for monitoring.** You can use other ASGI app like FastAPI or WSGI app like Flask, see [here](https://docs.bentoml.org/en/latest/guides/server.html).\n\nWe now have everything we need to serve our first request. Launch the server in debug mode by running the `bentoml serve` command in the current working directory. Using the `--reload` option allows the server to reflect any changes made to the `service.py` module without restarting:\n\nWe can then send requests to the newly started service with any HTTP client:\n\n### Build and Deploy Bentos 🍱\n\nOnce we are happy with the service definition, we can build the model and service into a `bento`. Bento is the distribution format for a service. It is a self-contained archive that contains all the source code, model files and dependency specifications required to run the service. Checkout [Building Bentos](https://docs.bentoml.org/en/latest/concepts/bento.html) for more details.\n\nTo build a Bento, first create a file named `bentofile.yaml` in your project directory:\n\nNext, use the bentoml build CLI command in the same directory to build a bento.\n\nBentos built will be saved in the local bento store, which you can view using the `bentoml list` CLI command.\n\nWe can serve bentos from the bento store using the `bentoml serve` `--production` CLI command. Using the `--production` option will serve the bento in production mode.\n\nThis is another way to query the server\n\nThe Bento directory contains all code, files, models and configs required for running this service. BentoML standarlizes this file structure which enables serving runtimes and deployment tools to be built on top of it. By default, Bentos are managed under the `~/bentoml/bentos` directory:\n\nFor more information, please refer to [https://docs.bentoml.org/en/latest/index.html](https://docs.bentoml.org/en/latest/index.html).\n\n## Deploy web base application in local computer using streamit\n\nStreamlit's simple and focused API lets you build incredibly rich and powerful tools. It contains a large number of [elements](https://docs.streamlit.io/library/api-reference) and [components](https://streamlit.io/components) that you can use.\n\nThere are a few ways to display data (tables, arrays, data frames) in Streamlit apps. Below, [`st.write()`](https://docs.streamlit.io/library/api-reference/write-magic/magic) can be used to write anything from text, plots to tables. In addition, when you've got the data or model into the state that you want to explore, you can add in widgets like `st.slider()`, `st.button()` or `st.selectbox()`. Finally, Streamlit makes it easy to organize your widgets in a left panel sidebar with `st.sidebar`. Each element that's passed to `st.sidebar` is pinned to the left, allowing users to focus on the content in your app while still having access to UI controls. For example, if you want to add a selectbox and a slider to a sidebar, use `st.sidebar.slider` and `st.sidebar.selectbox` instead of `st.slider` and `st.selectbox`:\n\nAs soon as you run the script as shown above, a local Streamlit server will spin up and your app will open in a new tab in your default web browser. The app is your canvas, where you'll draw charts, text, widgets, tables, and more.\n\nTry to click the above link to access the web app. For more information, please refer to [https://github.com/streamlit/streamlit](https://github.com/streamlit/streamlit).\n\n## Deploy web base application in local computer using Gradio\n\nUI models are perfect to use with Gradio's image input component, so in this section we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this.\n\n### Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from Keras. You can use a different pretrained model or train your own.\n\n### Defining a predict function\n\nNext, we will need to define a function that takes in the user input, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from [this text file](https://raw.githubusercontent.com/gradio-app/mobilenet-example/master/labels.txt).\n\nIn the case of our pretrained model, it will look like this:\n\nLet's break this down. The function takes one parameter:\n* `inp`: the input image as a NumPy array\n\nThen, the function adds a batch dimension, passes it through the model, and returns:\n* `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n### Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it. In this case, the input component is a drag-and-drop image component. To create this input, we can use the `gradio.inputs.Image` class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.\n\nThe output component will be a \"label\", which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images.\n\nFinally, we'll add one more parameter, the examples, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\nGradio automatically produces sharable links with others, but you can also access the web app with our port as follows:\n\nYou can see that there is a `flaaged` directory which collect data from users who try the model. To close the gradio, just call the `close_all()` function.\n\nFor more information, please refer to [https://github.com/gradio-app/gradio](https://github.com/gradio-app/gradio).\n\n## Deploy web base applocation using Tensorflow.js\n\nTensorflow.js is a WebGL accelerated JavaScript library for training and deploying ML models. The TensorFlow.js project includes a `tensorflowjs_converter` tool that can convert a TensorFlow `SavedModel` or a Keras model file to the TensorFlow.js Layers format: this is a directory\ncontaining a set of sharded weight files in binary format and a model.json file that describes the model’s architecture and links to the weight files. This format is optimized to be downloaded efficiently on the web.\n\nUsers can then download the model and run predictions in the browser using the TensorFlow.js library. Here is a code snippet to give you an idea of what the JavaScript API looks like:\n\n\n```\nimport * as tf from '@tensorflow/tfjs';\nconst model = await tf.loadLayersModel('https://example.com/tfjs/model.json');\nconst image = tf.fromPixels(webcamElement);\nconst prediction = model.predict(image);\n```\n\nFor more information, please refer to [https://github.com/tensorflow/tfjs](https://github.com/tensorflow/tfjs).\n\n## Deploy mobile application using Tensorflow Lite\n\nOnce again, doing justice to this topic would require a whole book. If you want to learn more about TensorFlow Lite, check out the O’Reilly book [Practical Deep Learning for Cloud, Mobile, and Edge](https://www.oreilly.com/library/view/practical-deep-learning/9781492034858/) or refer to [https://www.tensorflow.org/lite](https://www.tensorflow.org/lite).\n\n## Monitoring shift with evidently\n\n### The task at hand: bike demand forecasting\n\nWe took a Kaggle dataset on [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand/data). Our goal is to predict the volume of bike rentals on an hourly basis. To do that, we have some data about the season, weather, and day of the week.\n\n### Train a model\n\nWe trained a random forest model using data for the four weeks from January. Let's imagine that in practice, we just started the data collection, and that was all the data available. The performance of the trained model looked acceptable, so we decided to give it a go.\n\nWe further assume that we only learn the ground truth (the actual demand) **at the end of each week.** That is a realistic assumption in real-world machine learning. Integrating and updating different data sources is not always straightforward. Even after the actual event has occurred! Maybe the daily usage data is stored locally and is only sent and merged in the database once per week.\n\nTo run it, we prepare our performance data as a Pandas DataFrame. It should include:\n* **Model application logs**—features that went into the model and corresponding prediction; and\n* **Ground truth data**—the actual number of bikes rented each hour as our \"target.\"\n\nOnce we train the model, we can take our training dataset and generated predictions and specify it as the \"Reference\" data. We can select this period directly from the DataFrame since it has datetime as an index:\n\nWe also map the columns to show `Evidently` what each column contains and perform a correct analysis:\n\nBy default, Evidently uses the index as an x-axis in plots. In this case, it is datetime, so we do not need to add anything else explicitly. Otherwise, we would have to specify it in our column mapping.\n\nNext, we call a corresponding report for regression models.\n\nAnd display the results right in the Jupyter notebook.\n\nWe also save it as a .html file to be able to share it easily.\n\nWe can see that the model has a fine quality given that we only trained on four weeks of data! The error is symmetric and distributed around zero. There is no obvious under- or over-estimation.\n\n**We will continue treating this dataset from model performance in training as our \"reference.\"** It gives us a good feel of the quality we can expect from our model in production use. So, we can contrast the future performance against this benchmark.\n\n### The first week in production\n\nObserving the model in production has straightforward goals. We want to detect if something goes wrong. Ideally, in advance. We also want to diagnose the root cause and get a quick understanding of how to address it. Maybe, the model degrades too fast, and we need to retrain it more often? Perhaps, the error is too high, and we need to adapt the model and rebuild it? Which new patterns are emerging?\n\n**In our case, we simply start by checking how well the model performs outside the training data.** Our first week becomes what would have otherwise been a holdout dataset.\n\nFor demonstration purposes, we generated all predictions for several weeks ahead in a single batch. In reality, we would run the model sequentially as the data comes in.\n\nLet's start by comparing the performance in the first week to what we have seen in training. The first 28 days are our Reference dataset; the next 7 are the Production.\n\nThe error has slightly increased and is leaning towards underestimation. Let's check if there is any statistical change in our target. To do that, we will generate the Target Drift report.\n\nWe can see that the distribution of the actual number of bikes rented remains sufficiently similar. To be more precise, the similarity hypothesis is not rejected. No drift is detected. The distributions of our predictions did not change much either.\n\nDespite this, a rational decision is to update your model by including the new week's data. This way, the model can continue to learn, and we can probably improve the error. For the sake of demonstration, we'll stick to see how fast things go really wrong.\n\n### The second week: failing to keep up\n\nOnce again, we benchmark our new week against the reference dataset.\n\nAt first glance, the model performance in the second week does not differ much. MAE remains almost the same. But, the skew towards under-estimation continues to grow. It seems that the error is not random! To know more, we move to the plots. We can see that the model catches overall daily trends just fine. So it learned something useful! **But, at peak hours, the actual demand tends to be higher than predicted.**\n\nIn the error distribution plot, we can see how it became \"wider,\" as we have more predictions with a high error. The shift to the left is visible, too. In some extreme instances, we have errors between 80 and 40 bikes that were unseen previously.\n\nLet's check our target as well.\n\nThings are getting interesting!\n\n**We can see that the target distribution is now different: the similarity hypothesis is rejected. Literally, people are renting more bikes. And this is a statistically different change from our training period.**\n\nBut, the distribution of our predictions does not keep up! That is an obvious example of **model decay. Something new happens in the world, but it misses the patterns.**\n\nIt is tempting to investigate further. Is there anything in the data that can explain this change? If there is some new signal, retraining would likely help the model to keep up. The Target Drift report has a section to help us explore the relationship between the features and the target (or model predictions).\n‍When browsing through the individual features, we can inspect if we notice any new patterns. We know that predictions did not change, so we only look at the relations with the target. For example, there is a shift towards higher temperatures (measured in Celsius) with a corresponding increase in rented bikes.\n\nMaybe, it would pick up these patterns in retraining. But for now, we simply move on to the next week without any updates.\n\n### Week 3: when things go south\n\nOkay, now things do look bad. On week 3, we face a major quality drop. Both absolute and percentage error grew significantly. If we look at the plots, the model predictions are visibly scattered. We also face a **new data segment with high demand that the model fails to predict.** But even within the known range of target value, the model now makes errors. Things did change since the training. We can see that the model does not extrapolate well. The predicted demand stays within the same known range, while actual values are peaking.\n\nIf we zoom in on specific days, we might suggest that the error is higher on specific (active) hours of the day. We are doing just fine from 10 pm to 6 am!\n\nIn our example, we particularly want to understand the segment where the model underestimates the target function. The `Error Bias table` gives up more details. We sort it by the `\"Range%\"` field. If the values of a specific feature are significantly different in the group where the model under- or over-estimates, this feature will rank high. **In our case, we can see that the extreme errors are dependent on the \"temp\" (temperature) and \"atemp\" (feels-like temperature) features.**\n\nAfter this quick analysis, we have a more specific idea about model performance and its weaknesses. The model faces new, unusually high demand. Given how it was trained, it tends to underestimate it. On top of it, these errors are not at all random. At the very least, they are related to the temperature we observe. The higher it is, the larger the underestimation. **It suggests new patterns that are related to the weather that the model could not learn before. Days got warmer, and the model went rogue.**\n\nIf we run a target drift report, we will also see a relevant change in the linear correlations between the feature and the target. Temperature and humidity stand out.\n\nWe should retrain as soon as possible and do this often until we learn all the patterns. If we are not comfortable with frequent retraining, we might choose an algorithm that is more suitable for time series or is better in extrapolation.\n\n### Data Drift\n\nIn practice, once we receive the ground truth, we can indeed course-correct quickly. Had we retrained the model after week one, it would have likely ended less dramatically. **But what if we do not have the ground truth available? Can we catch such decay in advance?**\n\nIn this case, we can analyze the data drift. We do not need actuals to calculate the error. Instead, our goal is to see if the input data has changed.\n\n**Once again, let's compare the first week of production to our data in training.** We can, of course, look at all our features. But we can also conclude that categorical features (like \"season,\" \"holiday\" and \"workingday\") are not likely to change. Let's look at numerical features only!\n\nWe specify these features so that the tool applies the correct statistical test. It would be Kolmogorov-Smirnov in this case.\n\n> The data drift report compares the distributions of each feature in the two datasets. It [automatically picks an appropriate statistical test](https://docs.evidentlyai.com/reference/data-drift-algorithm) or metric based on the feature type and volume. It then returns p-values or distances and visually plots the distributions. You can also adjust the drift detection method or thresholds, or pass your own.\n\nOnce we show the report, it returns an answer. We can see already during the first week there is a statistical change in feature distributions.\n\nLet's zoom in on our usual suspect—temperature. The report gives us two views on how the feature distributions evolve with time. We can notice how the observed temperature becomes higher day by day. The values clearly drift out of our green corridor (one standard deviation from the mean) that we saw in training. Looking at the steady growth, we can suspect an upward trend.\n\nAs we checked earlier, we did not detect drift in the model predictions after week one. Given that our model is not good at extrapolating, we should not really expect it. Such prediction drift might still happen and signal about things like broken input data. Otherwise, we would observe it if we had a more sensitive model. Regardless of this, the data drift alone provides excellent early monitoring to detect the change and react to it.\n\nFor more information please refer to [https://github.com/evidentlyai/evidently](https://github.com/evidentlyai/evidently), [https://github.com/SeldonIO/alibi-detect](https://github.com/SeldonIO/alibi-detect), [https://github.com/great-expectations/great_expectations](https://github.com/great-expectations/great_expectations) or [https://github.com/whylabs/whylogs](https://github.com/whylabs/whylogs).\n\n## References\n\n1. [https://github.com/ageron/handson-ml2/blob/master/19_training_and_deploying_at_scale.ipynb](https://github.com/ageron/handson-ml2/blob/master/19_training_and_deploying_at_scale.ipynb)\n2. [https://github.com/bentoml/BentoML](https://github.com/bentoml/BentoML)\n3. [https://github.com/streamlit/streamlit](https://github.com/streamlit/streamlit)\n4. [https://raw.githubusercontent.com/dataprofessor/code/master/streamlit/part2/iris-ml-app.py](https://raw.githubusercontent.com/dataprofessor/code/master/streamlit/part2/iris-ml-app.py)\n5. [https://gradio.app/image-classification-in-tensorflow/](https://gradio.app/image-classification-in-tensorflow/)\n6. [https://evidentlyai.com/blog/tutorial-1-model-analytics-in-production](https://evidentlyai.com/blog/tutorial-1-model-analytics-in-production)\n","srcMarkdownNoYaml":"\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/07_Deploy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/07_Deploy.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>\n<br/>\n\n## Setup\n\nNotice that **you may need to restart the kernel** after the above installations.\n\nHere are some tips for this notebook [https://amitness.com/2020/06/google-colaboratory-tips/](https://amitness.com/2020/06/google-colaboratory-tips/) and [https://stackoverflow.com/questions/59741453/is-there-a-general-way-to-run-web-applications-on-google-colab](https://stackoverflow.com/questions/59741453/is-there-a-general-way-to-run-web-applications-on-google-colab).\n\n`ngrok` is a reverse proxy tool that opens secure tunnels from public URLs to localhost, perfect for exposing local web servers, building webhook integrations, enabling SSH access, testing chatbots, demoing from your own machine, and more. In this lab, we will use use [https://pyngrok.readthedocs.io/en/latest/integrations.html](https://pyngrok.readthedocs.io/en/latest/integrations.html). However, for production environment, it is recommended to use cloud service such as AWS, GCP or Azure, see [here](https://towardsdatascience.com/the-hierarchy-of-ml-tooling-on-the-public-cloud-ed387cac3c27) or [https://pycaret.gitbook.io/docs/get-started/functions/deploy#deploy_model](https://pycaret.gitbook.io/docs/get-started/functions/deploy#deploy_model) for more details.\n\n## Deploying TensorFlow models to TensorFlow Serving (TFS) on remote server\n\nYou could create your own microservice using any technology you want (e.g., using the Flask library), but why reinvent the wheel when you can just use TF Serving?\n\n### Exporting `SavedModels`\n\nTensorFlow provides a simple `tf.keras.models.save_model()` function to export models to the SavedModel format. All you need to do is give it the model, specifying its name and version number, and the function will save the model’s computation graph and its weights:\n\nIt’s usually a good idea to include all the preprocessing layers in the final model you export so that it can ingest data in its natural form once it is deployed to production. This  avoids  having  to  take  care  of  preprocessing  separately  within  the  application that uses the model. Bundling the preprocessing steps within the model also makes it simpler to update them later on and limits the risk of mismatch between a model and\nthe preprocessing steps it requires!\n\nNow to version the model, you just need to create a subdirectory for each model version:\n\nA SavedModel represents a version of your model. It is stored as a directory containing a `saved_model.pb` file, which defines the computation graph (represented as a **serialized protocol buffer**), and a variables subdirectory\ncontaining the variable values. For models containing a large number of weights, these variable values may be split across multiple files. A SavedModel also includes an `assets` subdirectory that may contain additional data, such as vocabulary files, class names, or some example instances for this model.\n\nAs you might expect, you can load a SavedModel using the `tf.keras.models.load_model()` function. \n\nTensorFlow also comes with a small `saved_model_cli` command-line tool to inspect `SavedModels`:\n\nA SavedModel contains one or more metagraphs. When you pass a `tf.keras` model, by default the function saves a simple SavedModel: it saves a single metagraph tagged \"serve\", which contains two signature definitions, an initialization function (called  `_saved_model_init_op`) and a default serving function (called `serving_default`). When saving a `tf.keras` model, the default serving function corresponds to the model’s `call()` function, which of course makes predictions.\n\n### Serve your model with TensorFlow Serving (Server side)\n\nThere are many ways to install TF Serving: using the system’s package manager, using a Docker image, installing from source, and more. Since Colab/Kaggle runs on Ubuntu, we can use Ubuntu’s  apt  package manager like this:\n\n> The  code  above starts  by  adding  TensorFlow's  package  repository  to  Ubuntu's  list  of package  sources.  Then  it  downloads  TensorFlow's  public  GPG  key  and  adds  it  to the  package  manager’s  key  list  so  it  can  verify  TensorFlow's  package  signatures. Next, it uses  apt  to install the  `tensorflow-model-server`  package. Lastly, it installs the  `tensorflow-serving-api`  library, which we will need to communicate with the\nserver.\n\nIf `tensorflow_model_server` is installed (e.g., if you are running this notebook in Colab/Kaggle), then the following 2 cells will start the server. If your OS is Windows, you may need to run the tensorflow_model_server command in a terminal, and replace `${MODEL_DIR}` with the full path to the `my_mnist_model` directory. This is where we start running TensorFlow Serving and load our model. After it loads we can start making inference requests using REST. There are some important parameters:\n\n* `port`: The port that you'll use for gRPC requests.\n* `rest_api_port`: The port that you'll use for REST requests.\n* `model_name`: You'll use this in the URL of REST requests. It can be anything.\n* `model_base_path`: This is the path to the directory where you've saved your model.\n\nThe  `%%bash  --bg`   magic  command  executes  the  cell  as  a  bash script,  running  it  in  the  background.  The  `>my_server.log  2>&1`   part  redirects  the standard output and standard error to the `server.log` file. And that’s it! TF Serving is now running in the background, and its logs are saved to `server.log`.\n\n### Querying TF Serving through the REST API (client side)\n\nLet’s start by creating the query. It must contain the name of the function signature you want to call, and of course the input data. Since the request must use the JSON format, we have to convert the input images from a `NumPy` array to a Python list:\n\nNote that the JSON format is 100% text-based:\n\nNow let’s send the input data to TF Serving by sending an HTTP POST request. This can be done easily using the `requests` library:\n\nThe response is a dictionary containing a single \"predictions\" key. The corresponding value is the list of predictions. This list is a Python list, so let’s convert it to a `NumPy` array and round the floats it contains to the\nsecond decimal:\n\nFor more information, please refer to [https://github.com/tensorflow/serving](https://github.com/tensorflow/serving) which include the usuage of gRPC.\n\n### Deploying a new model version\n\nNow let’s create a new model version and export a SavedModel to the `my_mnist_model/0002` directory, just like earlier:\n\nAt regular intervals (the delay is configurable), TensorFlow Serving checks for new model versions. If it finds one, it will automatically handle the transition gracefully: by default, it will answer pending requests (if any) with the previous model version, while handling new requests with the new version. As soon as every pending request has been answered, the previous model version is unloaded. You can see this at work in the TF Serving logs:\n\nAs you can see, TF Serving makes it quite simple to deploy new models. Moreover, if you discover that version 2 does not work as well as you expected, then rolling back to version 1 is as simple as removing the `my_mnist_model/0002` directory.\n\nYou can also refer to [https://github.com/microsoft/ML-For-Beginners/blob/main/3-Web-App/1-Web-App/README.md](https://github.com/microsoft/ML-For-Beginners/blob/main/3-Web-App/1-Web-App/README.md) or [https://github.com/rodrigo-arenas/fast-ml-deploy](https://github.com/rodrigo-arenas/fast-ml-deploy) which use [Flask](https://flask.palletsprojects.com/en/2.1.x/) and [FastAPI](https://fastapi.tiangolo.com/) that may have more flexibility. \n\nIf you would like to deploy to GCP vertex AI, checkout [here](https://github.com/ageron/handson-ml3/blob/main/19_training_and_deploying_at_scale.ipynb).\n\n## Deploy a REST API server using BentoML on remote server\n\nTo begin with `BentoML`, you will need to save your trained models with `BentoML` API in its model store (a local directory managed by `BentoML`). The model store is used for managing all your trained models locally as well as accessing them for serving.\n\n### Train a classifier model using the iris dataset\n\nSave the `clf` model with BentoML. We begin by saving a trained model instance to **BentoML’s local model store**. The local model store is used to version your models as well as control which models are packaged with your bento. It is noted that there are a [wide range of models](https://docs.bentoml.org/en/latest/frameworks/index.html#frameworks-page) can be saved via BentoML. \n\n> It is possible to use pre-trained models directly with BentoML or import existing trained model files to BentoML. Learn more about it from [Preparing Models](https://docs.bentoml.org/en/latest/concepts/model.html).\n\nModels saved can be accessed via `bentoml models` CLI command:\n\nTo verify that the saved model can be loaded correctly:\n\nIn BentoML, the recommended way of running ML model inference in serving is via `Runner`:\n\nIn this example, `bentoml.sklearn.get()` creates a reference to the saved model in the model store, and `to_runner()` creates a `Runner` instance from the model. The `Runner` abstraction gives `BentoServer` more flexibility in terms of how to schedule the inference computation, how to dynamically batch inference calls and better take advantage of all hardware resource available.\n\n### Create a BentoML Service for serving the model\n\nServices are the core components of BentoML, where the serving logic is defined. With the model saved in the model store, we can define the service by creating a Python file `service.py` with the following contents:\n\nIn this example, we defined the input and output type to be `numpy.ndarray`. More options, such as `pandas.DataFrame`, `JSON` and `PIL.image` are also supported. The `svc.api` decorator adds a function to the `bentoml.Service` object's APIs list. The input and output parameter takes an IO Descriptor object, which specifies the API function's expected input/output types, and is used for generating HTTP endpoints. Inside the API function, users can define any business logic, feature fetching, and feature transformation code. Model inference calls are made directly through runner objects, that are passed into `bentoml.Service(name=.., runners=[..])` call when creating the service object.\n\n> BentoML Server runs the Service API in an ASGI web serving layer and puts Runners in a separate worker process pool managed by BentoML. **The ASGI web serving layer will expose REST endpoints for inference APIs, such as POST /predict and common infrastructure APIs, such as GET /metrics for monitoring.** You can use other ASGI app like FastAPI or WSGI app like Flask, see [here](https://docs.bentoml.org/en/latest/guides/server.html).\n\nWe now have everything we need to serve our first request. Launch the server in debug mode by running the `bentoml serve` command in the current working directory. Using the `--reload` option allows the server to reflect any changes made to the `service.py` module without restarting:\n\nWe can then send requests to the newly started service with any HTTP client:\n\n### Build and Deploy Bentos 🍱\n\nOnce we are happy with the service definition, we can build the model and service into a `bento`. Bento is the distribution format for a service. It is a self-contained archive that contains all the source code, model files and dependency specifications required to run the service. Checkout [Building Bentos](https://docs.bentoml.org/en/latest/concepts/bento.html) for more details.\n\nTo build a Bento, first create a file named `bentofile.yaml` in your project directory:\n\nNext, use the bentoml build CLI command in the same directory to build a bento.\n\nBentos built will be saved in the local bento store, which you can view using the `bentoml list` CLI command.\n\nWe can serve bentos from the bento store using the `bentoml serve` `--production` CLI command. Using the `--production` option will serve the bento in production mode.\n\nThis is another way to query the server\n\nThe Bento directory contains all code, files, models and configs required for running this service. BentoML standarlizes this file structure which enables serving runtimes and deployment tools to be built on top of it. By default, Bentos are managed under the `~/bentoml/bentos` directory:\n\nFor more information, please refer to [https://docs.bentoml.org/en/latest/index.html](https://docs.bentoml.org/en/latest/index.html).\n\n## Deploy web base application in local computer using streamit\n\nStreamlit's simple and focused API lets you build incredibly rich and powerful tools. It contains a large number of [elements](https://docs.streamlit.io/library/api-reference) and [components](https://streamlit.io/components) that you can use.\n\nThere are a few ways to display data (tables, arrays, data frames) in Streamlit apps. Below, [`st.write()`](https://docs.streamlit.io/library/api-reference/write-magic/magic) can be used to write anything from text, plots to tables. In addition, when you've got the data or model into the state that you want to explore, you can add in widgets like `st.slider()`, `st.button()` or `st.selectbox()`. Finally, Streamlit makes it easy to organize your widgets in a left panel sidebar with `st.sidebar`. Each element that's passed to `st.sidebar` is pinned to the left, allowing users to focus on the content in your app while still having access to UI controls. For example, if you want to add a selectbox and a slider to a sidebar, use `st.sidebar.slider` and `st.sidebar.selectbox` instead of `st.slider` and `st.selectbox`:\n\nAs soon as you run the script as shown above, a local Streamlit server will spin up and your app will open in a new tab in your default web browser. The app is your canvas, where you'll draw charts, text, widgets, tables, and more.\n\nTry to click the above link to access the web app. For more information, please refer to [https://github.com/streamlit/streamlit](https://github.com/streamlit/streamlit).\n\n## Deploy web base application in local computer using Gradio\n\nUI models are perfect to use with Gradio's image input component, so in this section we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this.\n\n### Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from Keras. You can use a different pretrained model or train your own.\n\n### Defining a predict function\n\nNext, we will need to define a function that takes in the user input, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from [this text file](https://raw.githubusercontent.com/gradio-app/mobilenet-example/master/labels.txt).\n\nIn the case of our pretrained model, it will look like this:\n\nLet's break this down. The function takes one parameter:\n* `inp`: the input image as a NumPy array\n\nThen, the function adds a batch dimension, passes it through the model, and returns:\n* `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n### Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it. In this case, the input component is a drag-and-drop image component. To create this input, we can use the `gradio.inputs.Image` class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.\n\nThe output component will be a \"label\", which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images.\n\nFinally, we'll add one more parameter, the examples, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\nGradio automatically produces sharable links with others, but you can also access the web app with our port as follows:\n\nYou can see that there is a `flaaged` directory which collect data from users who try the model. To close the gradio, just call the `close_all()` function.\n\nFor more information, please refer to [https://github.com/gradio-app/gradio](https://github.com/gradio-app/gradio).\n\n## Deploy web base applocation using Tensorflow.js\n\nTensorflow.js is a WebGL accelerated JavaScript library for training and deploying ML models. The TensorFlow.js project includes a `tensorflowjs_converter` tool that can convert a TensorFlow `SavedModel` or a Keras model file to the TensorFlow.js Layers format: this is a directory\ncontaining a set of sharded weight files in binary format and a model.json file that describes the model’s architecture and links to the weight files. This format is optimized to be downloaded efficiently on the web.\n\nUsers can then download the model and run predictions in the browser using the TensorFlow.js library. Here is a code snippet to give you an idea of what the JavaScript API looks like:\n\n\n```\nimport * as tf from '@tensorflow/tfjs';\nconst model = await tf.loadLayersModel('https://example.com/tfjs/model.json');\nconst image = tf.fromPixels(webcamElement);\nconst prediction = model.predict(image);\n```\n\nFor more information, please refer to [https://github.com/tensorflow/tfjs](https://github.com/tensorflow/tfjs).\n\n## Deploy mobile application using Tensorflow Lite\n\nOnce again, doing justice to this topic would require a whole book. If you want to learn more about TensorFlow Lite, check out the O’Reilly book [Practical Deep Learning for Cloud, Mobile, and Edge](https://www.oreilly.com/library/view/practical-deep-learning/9781492034858/) or refer to [https://www.tensorflow.org/lite](https://www.tensorflow.org/lite).\n\n## Monitoring shift with evidently\n\n### The task at hand: bike demand forecasting\n\nWe took a Kaggle dataset on [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand/data). Our goal is to predict the volume of bike rentals on an hourly basis. To do that, we have some data about the season, weather, and day of the week.\n\n### Train a model\n\nWe trained a random forest model using data for the four weeks from January. Let's imagine that in practice, we just started the data collection, and that was all the data available. The performance of the trained model looked acceptable, so we decided to give it a go.\n\nWe further assume that we only learn the ground truth (the actual demand) **at the end of each week.** That is a realistic assumption in real-world machine learning. Integrating and updating different data sources is not always straightforward. Even after the actual event has occurred! Maybe the daily usage data is stored locally and is only sent and merged in the database once per week.\n\nTo run it, we prepare our performance data as a Pandas DataFrame. It should include:\n* **Model application logs**—features that went into the model and corresponding prediction; and\n* **Ground truth data**—the actual number of bikes rented each hour as our \"target.\"\n\nOnce we train the model, we can take our training dataset and generated predictions and specify it as the \"Reference\" data. We can select this period directly from the DataFrame since it has datetime as an index:\n\nWe also map the columns to show `Evidently` what each column contains and perform a correct analysis:\n\nBy default, Evidently uses the index as an x-axis in plots. In this case, it is datetime, so we do not need to add anything else explicitly. Otherwise, we would have to specify it in our column mapping.\n\nNext, we call a corresponding report for regression models.\n\nAnd display the results right in the Jupyter notebook.\n\nWe also save it as a .html file to be able to share it easily.\n\nWe can see that the model has a fine quality given that we only trained on four weeks of data! The error is symmetric and distributed around zero. There is no obvious under- or over-estimation.\n\n**We will continue treating this dataset from model performance in training as our \"reference.\"** It gives us a good feel of the quality we can expect from our model in production use. So, we can contrast the future performance against this benchmark.\n\n### The first week in production\n\nObserving the model in production has straightforward goals. We want to detect if something goes wrong. Ideally, in advance. We also want to diagnose the root cause and get a quick understanding of how to address it. Maybe, the model degrades too fast, and we need to retrain it more often? Perhaps, the error is too high, and we need to adapt the model and rebuild it? Which new patterns are emerging?\n\n**In our case, we simply start by checking how well the model performs outside the training data.** Our first week becomes what would have otherwise been a holdout dataset.\n\nFor demonstration purposes, we generated all predictions for several weeks ahead in a single batch. In reality, we would run the model sequentially as the data comes in.\n\nLet's start by comparing the performance in the first week to what we have seen in training. The first 28 days are our Reference dataset; the next 7 are the Production.\n\nThe error has slightly increased and is leaning towards underestimation. Let's check if there is any statistical change in our target. To do that, we will generate the Target Drift report.\n\nWe can see that the distribution of the actual number of bikes rented remains sufficiently similar. To be more precise, the similarity hypothesis is not rejected. No drift is detected. The distributions of our predictions did not change much either.\n\nDespite this, a rational decision is to update your model by including the new week's data. This way, the model can continue to learn, and we can probably improve the error. For the sake of demonstration, we'll stick to see how fast things go really wrong.\n\n### The second week: failing to keep up\n\nOnce again, we benchmark our new week against the reference dataset.\n\nAt first glance, the model performance in the second week does not differ much. MAE remains almost the same. But, the skew towards under-estimation continues to grow. It seems that the error is not random! To know more, we move to the plots. We can see that the model catches overall daily trends just fine. So it learned something useful! **But, at peak hours, the actual demand tends to be higher than predicted.**\n\nIn the error distribution plot, we can see how it became \"wider,\" as we have more predictions with a high error. The shift to the left is visible, too. In some extreme instances, we have errors between 80 and 40 bikes that were unseen previously.\n\nLet's check our target as well.\n\nThings are getting interesting!\n\n**We can see that the target distribution is now different: the similarity hypothesis is rejected. Literally, people are renting more bikes. And this is a statistically different change from our training period.**\n\nBut, the distribution of our predictions does not keep up! That is an obvious example of **model decay. Something new happens in the world, but it misses the patterns.**\n\nIt is tempting to investigate further. Is there anything in the data that can explain this change? If there is some new signal, retraining would likely help the model to keep up. The Target Drift report has a section to help us explore the relationship between the features and the target (or model predictions).\n‍When browsing through the individual features, we can inspect if we notice any new patterns. We know that predictions did not change, so we only look at the relations with the target. For example, there is a shift towards higher temperatures (measured in Celsius) with a corresponding increase in rented bikes.\n\nMaybe, it would pick up these patterns in retraining. But for now, we simply move on to the next week without any updates.\n\n### Week 3: when things go south\n\nOkay, now things do look bad. On week 3, we face a major quality drop. Both absolute and percentage error grew significantly. If we look at the plots, the model predictions are visibly scattered. We also face a **new data segment with high demand that the model fails to predict.** But even within the known range of target value, the model now makes errors. Things did change since the training. We can see that the model does not extrapolate well. The predicted demand stays within the same known range, while actual values are peaking.\n\nIf we zoom in on specific days, we might suggest that the error is higher on specific (active) hours of the day. We are doing just fine from 10 pm to 6 am!\n\nIn our example, we particularly want to understand the segment where the model underestimates the target function. The `Error Bias table` gives up more details. We sort it by the `\"Range%\"` field. If the values of a specific feature are significantly different in the group where the model under- or over-estimates, this feature will rank high. **In our case, we can see that the extreme errors are dependent on the \"temp\" (temperature) and \"atemp\" (feels-like temperature) features.**\n\nAfter this quick analysis, we have a more specific idea about model performance and its weaknesses. The model faces new, unusually high demand. Given how it was trained, it tends to underestimate it. On top of it, these errors are not at all random. At the very least, they are related to the temperature we observe. The higher it is, the larger the underestimation. **It suggests new patterns that are related to the weather that the model could not learn before. Days got warmer, and the model went rogue.**\n\nIf we run a target drift report, we will also see a relevant change in the linear correlations between the feature and the target. Temperature and humidity stand out.\n\nWe should retrain as soon as possible and do this often until we learn all the patterns. If we are not comfortable with frequent retraining, we might choose an algorithm that is more suitable for time series or is better in extrapolation.\n\n### Data Drift\n\nIn practice, once we receive the ground truth, we can indeed course-correct quickly. Had we retrained the model after week one, it would have likely ended less dramatically. **But what if we do not have the ground truth available? Can we catch such decay in advance?**\n\nIn this case, we can analyze the data drift. We do not need actuals to calculate the error. Instead, our goal is to see if the input data has changed.\n\n**Once again, let's compare the first week of production to our data in training.** We can, of course, look at all our features. But we can also conclude that categorical features (like \"season,\" \"holiday\" and \"workingday\") are not likely to change. Let's look at numerical features only!\n\nWe specify these features so that the tool applies the correct statistical test. It would be Kolmogorov-Smirnov in this case.\n\n> The data drift report compares the distributions of each feature in the two datasets. It [automatically picks an appropriate statistical test](https://docs.evidentlyai.com/reference/data-drift-algorithm) or metric based on the feature type and volume. It then returns p-values or distances and visually plots the distributions. You can also adjust the drift detection method or thresholds, or pass your own.\n\nOnce we show the report, it returns an answer. We can see already during the first week there is a statistical change in feature distributions.\n\nLet's zoom in on our usual suspect—temperature. The report gives us two views on how the feature distributions evolve with time. We can notice how the observed temperature becomes higher day by day. The values clearly drift out of our green corridor (one standard deviation from the mean) that we saw in training. Looking at the steady growth, we can suspect an upward trend.\n\nAs we checked earlier, we did not detect drift in the model predictions after week one. Given that our model is not good at extrapolating, we should not really expect it. Such prediction drift might still happen and signal about things like broken input data. Otherwise, we would observe it if we had a more sensitive model. Regardless of this, the data drift alone provides excellent early monitoring to detect the change and react to it.\n\nFor more information please refer to [https://github.com/evidentlyai/evidently](https://github.com/evidentlyai/evidently), [https://github.com/SeldonIO/alibi-detect](https://github.com/SeldonIO/alibi-detect), [https://github.com/great-expectations/great_expectations](https://github.com/great-expectations/great_expectations) or [https://github.com/whylabs/whylogs](https://github.com/whylabs/whylogs).\n\n## References\n\n1. [https://github.com/ageron/handson-ml2/blob/master/19_training_and_deploying_at_scale.ipynb](https://github.com/ageron/handson-ml2/blob/master/19_training_and_deploying_at_scale.ipynb)\n2. [https://github.com/bentoml/BentoML](https://github.com/bentoml/BentoML)\n3. [https://github.com/streamlit/streamlit](https://github.com/streamlit/streamlit)\n4. [https://raw.githubusercontent.com/dataprofessor/code/master/streamlit/part2/iris-ml-app.py](https://raw.githubusercontent.com/dataprofessor/code/master/streamlit/part2/iris-ml-app.py)\n5. [https://gradio.app/image-classification-in-tensorflow/](https://gradio.app/image-classification-in-tensorflow/)\n6. [https://evidentlyai.com/blog/tutorial-1-model-analytics-in-production](https://evidentlyai.com/blog/tutorial-1-model-analytics-in-production)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"07_Deploy.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.306","theme":"cosmo","cover-image":"cover.jpg","title":"Deploy and monitoring","author":"phonchi","date":"04/10/2023"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}