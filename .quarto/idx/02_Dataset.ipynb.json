{"title":"Framing the problem and constructing the dataset","markdown":{"yaml":{"title":"Framing the problem and constructing the dataset","author":"phonchi","date":"02/20/2023","format":{"html":{"toc":true,"code-line-numbers":true,"code-fold":true,"code-tools":true}}},"headingText":"Setup","containsRefs":false,"markdown":"\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/02_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/02_Dataset.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>\n<br/>\n\n\n## Scraping data using BeautifulSoup\n\nSometimes we have to build our dataset using crawler. See\n1. [https://github.com/marvelje/weather_forecasting](https://github.com/marvelje/weather_forecasting) (Constructing weather dataset)\n2. [https://www.kaggle.com/code/pandula/a-simple-tutorial-on-beautifulsoup/notebook](https://www.kaggle.com/code/pandula/a-simple-tutorial-on-beautifulsoup/notebook) (Constructing IMDB dataset)\n3. [https://github.com/sharmasapna/Web-scraping-for-images](https://github.com/sharmasapna/Web-scraping-for-images)\n(Constructing image dataset)\n\n\n## Manual Labeling with pigeonXT\n\nIn many data science projects, the first step at which the data science team gets involved is in labeling the image data. Even if the labeling will be automated, the first few images in a proof of concept are almost always hand-labeled. The form and organization will differ based on the problem type (image classification or object detection) and whether an image can have multiple labels or only one. To hand-label images, a rater views the image, determines the label(s), and records the label(s). There are two typical approaches to doing this recording: using a **folder structure** and a **metadata table.**\n\nIn a folder organization, raters simply move images to different folders depending on what their label is. All flowers that are daisies are stored in a folder named *daisy*, for example. Raters can do this quickly because most operating systems provide previews of images and handy ways to select groups of images and move them into folders.\n\nThe problem with the folder approach is that it leads to duplication if an image can have multiple labels—for example, if an image contains both *roses* and *daisies*. The alternative, and recommended, approach is to record the label(s) in a metadata table (such as in a spreadsheet or a CSV file) that has at least two columns - one column is the filename of the image file, and the other is the list of labels that are valid for the image.\n\nA labeling tool should have a facility to display the image, and enable the rater to quickly select valid categories and save the rating to a database. We will use `pigeonXT` which is a wrapper of Jupyter widget next.\n\n\n\n### Image classification\n\n### Binary or multi-class text classification\n\nThe output can be save to a CSV or JSON file.\n\n### Multi-label text classification\n\nAnything that can be displayed on Jupyter (text, images, audio, graphs, etc.)can be displayed by pigeonXT by providing the appropriate `display_fn` argument.\n\nAdditionally, custom hooks can be attached to each row update (`example_process_fn`), or when the annotating task is complete(`final_process_fn`). See https://github.com/dennisbakhuis/pigeonXT for more details.\n\n## Improve Consensus Labels for Multiannotator Data with Cleanlab\n\nThis example contains classification data that has been labeled by multiple annotators (where each example has been labeled by at least one annotator, but not every annotator has labeled every example)\n\nFor this part, we will generate a toy dataset that has 50 annotators and 300 examples. There are three possible classes, 0, 1 and 2.\n\nEach annotator annotates approximately 10% of the examples. We also synthetically made the last 5 annotators in our toy dataset have much noisier labels than the rest of the annotators. To generate our multiannotator data, we define a `make_data()` method \n\nLet’s view the first few rows of the data. Here are the labels selected by each annotator for the first few examples:\n\n`multiannotator_labels` contains the class labels that each annotator chose for each example, with examples that a particular annotator did not label represented using `np.nan`. `X` contains the features for each example.\n\n### Get majority vote labels and compute out-of-sample predicted probabilites\n\nBefore training a machine learning model, we must first obtain the consensus labels from the annotators that labeled the data. The simplest way to obtain an initial set of consensus labels is to select it using majority vote.\n\nNext, we will train a model on the consensus labels obtained using majority vote to compute out-of-sample predicted probabilities. Here, we use a simple logistic regression model.\n\n### Use cleanlab to get better consensus labels and other statistics\n\nUsing the annotators’ labels and the out-of-sample predicted probabilites from the model, cleanlab can help us obtain improved consensus labels for our data.\n\nHere, we use the `multiannotator.get_label_quality_multiannotator()` function which returns a dictionary containing three items:\n\n1. `label_quality` which gives us the improved consensus labels using information from each of the annotators and the model. The DataFrame also contains information about the number of annotations, annotator agreement and consensus quality score for each example.\n\n2. `detailed_label_quality` which returns the label quality score for each label given by every annotator\n\n3. `annotator_stats` which gives us the annotator quality score for each annotator, alongisde other information such as the number of examples each annotator labeled, their agreement with the consensus labels and the class they perform the worst at.\n\nWe can get the improved consensus labels from the label_quality DataFrame shown above. We can compare the accuracy of the consensus labels obtained using majority vote, and the improved consensus labels obtained using cleanlab.\n\nWe can see that the accuracy of the consensus labels improved as a result of using cleanlab!\n\nAfter obtaining the improved consensus labels, we can now retrain a better version of our machine learning model using these newly obtained labels. You can also repeatedly iterate this process of getting better consensus labels using the model’s out-of-sample predicted probabilites and then retraining the model with the improved labels to get even better predicted probabilities!\n\n## Active Learning with modAL\n\nIn this example, the active learning workflow of modAL is demonstrated - with you in the loop! By running this notebook, you’ll be queried to label digits using the [DIGITS](http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits) dataset.\n\nHere we use the pool-based sampling. In this setting, we assume a small set of labeled data `L` and a large set of unlabeled data `U` such that `|L|≪|U|`.\n\nThe high level description about this strategy is as follows: Queries are selectively drawn from the pool, which is usually assumed to be closed (i.e., static or non-changing), although this is not strictly necessary. Typically, instances are queried in a greedy fashion, according to an informativeness measure used to evaluate all instances in the pool (or, perhaps if `U` is very large, some subsample thereof).\n\nNow we set up the initial training set for our classifier. If you would like to play around, you can try to modifiy the value `n_initial` below and see if it impacts the algorithm!\n\n### Initializing the learner\n\nAlong with our pool-based sampling strategy, `modAL`’s modular design allows you to vary parameters surrounding the active learning process, **including the core estimator and query strategy.**\n\nNow we initialize the active learner. Feel free to change the underlying `RandomForestClassifier` or the `uncertainty_sampling`!\n\n### The active learning loop\n\nBy querying hard example to the rater, the performance increase from 83% to 86%!\n\n### Iris example\n\nIn this example, we use `scikit-learn`’s `k-nearest neighbors classifier` as our estimator and default to `modAL`’s uncertainty sampling query strategy.\n\nNow we partition our iris dataset into a training set `L` and `U`. We first specify our training set `L` consisting of 3 random examples. The remaining examples go to our “unlabeled” pool `U`.\n\nFor the classification, we are going to use a simple `k-nearest neighbors classifier`. In this step, we are also going to initialize the `ActiveLearner`.\n\nLet’s see how our classifier performs on the initial training set!\n\n#### Update our model by pool-based sampling our “unlabeled” dataset U\n\n\nAs we can see, our model is unable to properly learn the underlying data distribution. All of its predictions are for the third class label, and as such it is only as competitive as defaulting its predictions to a single class – if only we had more data!\n\nBelow, we tune our classifier by allowing it to query 20 instances it hasn’t seen before. Using uncertainty sampling, **our classifier aims to reduce the amount of uncertainty in its predictions using a variety of measures** — see https://modal-python.readthedocs.io/en/latest/index.html for more on specific classification uncertainty measures. With each requested query, we remove that record from our pool `U` and record our model’s accuracy on the raw dataset.\n\nHere, we first plot the query iteration index against model accuracy. To visualize the performance of our classifier, we also plot the correct and incorrect predictions on the full dataset.\n\n## Weak Supervison using Snorkel\n\nWe will walk through the process of using `Snorkel` to build a training set for classifying YouTube comments as spam or not spam. The goal of this tutorial is to illustrate the basic components and concepts of `Snorkel` in a simple way, but also to dive into the actual process of iteratively developing real applications in `Snorkel`.\n\nOur goal is to train a classifier over the comment data that can predict whether a comment is spam or not spam. We have access to a large amount of unlabeled data in the form of YouTube comments with some metadata. In order to train a classifier, we need to label our data, **but doing so by hand for real world applications can often be prohibitively slow and expensive.**\n\nIn these cases, we can turn to a weak supervision approach, using **labeling functions (LFs)** in Snorkel: noisy, programmatic rules and heuristics that assign labels to unlabeled training data. \n\nWe’ll dive into the Snorkel API and how we write labeling functions later in this tutorial, but as an example, we can write a LF that labels data points with \"http\" in the comment text as spam since many spam comments contain links.\n\nWe use a [YouTube comments dataset](https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection) that consists of YouTube comments from 5 videos. The task is to classify each comment as being\n\nHAM: comments relevant to the video (even very simple ones), or SPAM: irrelevant (often trying to advertise something) or inappropriate messages\n\nWe split our data into two sets:\n\n* **Training Set**: The largest split of the dataset, and the one without any ground truth (“gold”) labels. We will generate labels for these data points with weak supervision.\n\n* **Test Set**: A small, standard held-out blind hand-labeled set for final evaluation of our classifier. This set should only be used for final evaluation, not error analysis.\n\n> Note that in more advanced production settings, we will often further split up the available hand-labeled data into a development split, for getting ideas to write labeling functions, and a validation split for e.g. checking our performance without looking at test set scores, hyperparameter tuning, etc. These splits are omitted for simplicity here.\n\n### Loading Data\n\nWe load the YouTube comments dataset and create Pandas DataFrame objects for the train and test sets. Snorkel provides native support for several DataFrame-like data structures, including `Pandas`, `Dask`, and `PySpark`.\n\nEach DataFrame consists of the following fields:\n\n* author: Username of the comment author\n* data: Date and time the comment was posted\n* text: Raw text content of the comment\n* label: Whether the comment is SPAM (1), HAM (0), or UNKNOWN/ABSTAIN (-1)\n* video: Video the comment is associated with\n\nWe start by loading our data. The `load_spam_dataset()` method downloads the raw CSV files from the internet, divides them into splits, converts them into `DataFrames`, and shuffles them. As mentioned above, the dataset contains comments from 5 of the most popular YouTube videos during a period between 2014 and 2015. **The first four videos’ comments are combined to form the train set. This set has no gold labels. The fifth video is part of the test set.**\n\n### Writing Labeling Functions (LFs)\n\nLabeling functions (LFs) help users encode domain knowledge and other supervision sources programmatically.\n\nLFs are heuristics that take as input a data point and either assign a label to it (in this case, HAM or SPAM) or abstain (don’t assign any label). **Labeling functions can be noisy: they don’t have perfect accuracy and don’t have to label every data point. Moreover, different labeling functions can overlap (label the same data point) and even conflict (assign different labels to the same data point).** This is expected, and we demonstrate how we deal with this later.\n\nBecause their only requirement is that they map a data point a label (or abstain), they can wrap a wide variety of forms of supervision.\n\nTypical LF development cycles include multiple iterations of ideation, refining, evaluation, and debugging. A typical cycle consists of the following steps:\n\n1. Look at examples to generate ideas for LFs\n2. Write an initial version of an LF\n3. Spot check its performance by looking at its output on data points in the training set (or development set if available)\n4. Refine and debug to improve coverage or accuracy as necessary\n\nOur goal for LF development is to create a high quality set of training labels for our unlabeled dataset, not to label everything or directly create a model for inference using the LFs. The training labels are used to train a separate discriminative model (in this case, one which just uses the comment text) in order to generalize to new, unseen data points. Using this model, we can make predictions for data points that our LFs don’t cover.\n\na) Pattern-matching LFs (regular expressions)\n\nLabeling functions in `Snorkel` are created with the `@labeling_function` decorator. **The decorator can be applied to any Python function that returns a label for a single data point.**\n\n> See https://realpython.com/primer-on-python-decorators/ for more details about decorators.\n\nLet’s start developing an LF to catch instances of commenters trying to get people to “check out” their channel, video, or website. We’ll start by just looking for the exact string \"check out\" in the text, and see how that compares to looking for just \"check\" in the text. For the two versions of our rule, we’ll write a Python function over a single data point that express it, then add the decorator.\n\nOne dominant pattern in the comments that look like spam (which we might know from prior domain experience, or from inspection of a few training data points) is the use of the phrase **“check out”** (e.g. “check out my channel”). Let’s start with that.\n\nb) Keyword LFs\n\nFor text applications, some of the simplest LFs to write are often just keyword lookups. These will often follow the same execution pattern, so we can create a template and use the resources parameter to pass in LF-specific keywords. Similar to the `labeling_function` decorator, the `LabelingFunction` class wraps a Python function (the `f` parameter), and we can use the resources parameter to pass in keyword arguments (here, our keywords to lookup) to said function.\n\nc) Heuristic LFs\nThere may other heuristics or “rules of thumb” that you come up with as you look at the data. So long as you can express it in a function, it’s a viable LF!\n\nTo apply one or more LFs that we’ve written to a collection of data points, we use an `LFApplier`. Because our data points are represented with a `Pandas` `DataFrame` in this tutorial, we use the `PandasLFApplier`. Correspondingly, a single data point `x` that’s passed into our LFs will be a `Pandas` `Series` object.\n\nIt’s important to note that these LFs will work for any object with an attribute named text, not just `Pandas` objects. `Snorkel` has several other appliers for different data point collection types which you can browse in the https://snorkel.readthedocs.io/en/master/packages/labeling.html.\n\nThe output of the `apply(...)` method is a label matrix, a fundamental concept in `Snorkel`. It’s a `NumPy` array `L` with one column for each LF and one row for each data point, where `L[i, j]` is the label that the jth labeling function output for the ith data point. We’ll create a label matrix for the train set.\n\nWe can easily calculate the coverage of these LFs (i.e., the percentage of the dataset that they label) as follows:\n\nLots of statistics about labeling functions - like coverage - are useful when building any `Snorkel` application. So Snorkel provides tooling for common LF analyses using the `LFAnalysis` utility. \n\nCheckout the statistics here [https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.LFAnalysis.html](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.LFAnalysis.html)\n\nOur goal is now to convert the labels from our LFs into a single noise-aware probabilistic (or confidence-weighted) label per data point. A simple baseline for doing this is to take the majority vote on a per-data point basis: **if more LFs voted SPAM than HAM, label it SPAM (and vice versa)**. We can test this with the `MajorityLabelVoter` baseline model.\n\nThe LFs may have varying properties and should not be treated identically. In addition to having varied accuracies and coverages, LFs may be correlated, resulting in certain signals being overrepresented in a majority-vote-based model. \n\nTo handle these issues appropriately, we will instead use a more sophisticated Snorkel `LabelModel` to combine the outputs of the LFs. **This model will ultimately produce a single set of noise-aware training labels, which are probabilistic or confidence-weighted labels. We will then use these labels to train a classifier for our task.**\n\nNote that no gold labels are used during the training process. The only information we need is the label matrix, which contains the output of the LFs on our training set. The `LabelModel` is able to learn weights for the labeling functions using only the label matrix as input. We also specify the **cardinality, or number of classes.**\n\nThe majority vote model or more sophisticated `LabelModel` could in principle be used directly as a classifier if the outputs of our labeling functions were made available at test time. However, these models (i.e. these re-weighted combinations of our labeling function’s votes) will abstain on the data points that our labeling functions don’t cover.\n\nWe will instead use the outputs of the `LabelModel` as training labels to train a discriminative classifier which can generalize beyond the labeling function outputs to see if we can improve performance further. This classifier will also only need the text of the comment to make predictions, making it much more suitable for inference over unseen comments.\n\nLet's briefly confirm that the labels the `LabelModel` produces are indeed probabilistic in nature. The following histogram shows the confidences we have that each data point has the label SPAM. The points we are least certain about will have labels close to 0.5.\n\nAs we saw earlier, some of the data points in our train set received no labels from any of our LFs. These data points convey no supervision signal and tend to hurt performance, so we filter them out before training using a built-in utility.\n\n### Training a Classifier\n\nIn this final section of the tutorial, we’ll use the probabilistic training labels we generated in the last section to train a classifier for our task. The output of the `Snorkel` `LabelModel` is just a set of labels which can be used with most popular libraries for performing supervised learning. Note that typically, `Snorkel` is used (and really shines!) with much more complex, training data-hungry models, but we will use Logistic Regression here for simplicity of exposition.\n\nFor simplicity and speed, we use a simple “bag of n-grams” feature representation: each data point is represented by a one-hot vector marking which words or 2-word combinations are present in the comment text.\n\nIf we want to use a library or model that doesn’t accept probabilistic labels (such as `Scikit-Learn`), we can instead replace each label distribution with the label of the class that has the maximum probability. This can easily be done using the `probs_to_preds` helper method. **We do note, however, that this transformation is lossy**, as we no longer have values for our confidence in each label.\n\nWe then use these labels to train a classifier as usual.\n\nWe observe an additional boost in accuracy over the `LabelModel` by multiple points! This is in part because the discriminative model generalizes beyond the labeling function’s labels and makes good predictions on all data points, not just the ones covered by labeling functions. By using the label model to transfer the domain knowledge encoded in our LFs to the discriminative model, we were able to generalize beyond the noisy labeling heuristics.\n\n## Data augmentation using Albumentations\n\n`Scikit-image` reads an image in RGB format which is consistent with `Albumentations`.\n\nWe fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, **you shouldn't fix the random seed** before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time\n\nFor more information, please refer to [https://albumentations.ai/docs/#examples](https://albumentations.ai/docs/#examples).\n\n## Reference\n\nThis notebook contains the sample from \n\n1. [https://github.com/dennisbakhuis/pigeonXT/blob/master/pigeonXT_Examples.ipynb](https://github.com/dennisbakhuis/pigeonXT/blob/master/pigeonXT_Examples.ipynb)\n2. [https://docs.cleanlab.ai/stable/tutorials/multiannotator.html](https://docs.cleanlab.ai/stable/tutorials/multiannotator.html)\n3. [https://modal-python.readthedocs.io/en/latest/content/examples/interactive_labeling.html_](https://modal-python.readthedocs.io/en/latest/content/examples/interactive_labeling.html_)\n4. [https://modal-python.readthedocs.io/en/latest/content/examples/pool-based_sampling.html](https://modal-python.readthedocs.io/en/latest/content/examples/pool-based_sampling.html)\n5. [https://www.snorkel.org/use-cases/01-spam-tutorial](https://www.snorkel.org/use-cases/01-spam-tutorial)\n6. [https://albumentations.ai/docs/#examples](https://albumentations.ai/docs/#examples)\n\n\n","srcMarkdownNoYaml":"\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/02_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/02_Dataset.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>\n<br/>\n\n## Setup\n\n## Scraping data using BeautifulSoup\n\nSometimes we have to build our dataset using crawler. See\n1. [https://github.com/marvelje/weather_forecasting](https://github.com/marvelje/weather_forecasting) (Constructing weather dataset)\n2. [https://www.kaggle.com/code/pandula/a-simple-tutorial-on-beautifulsoup/notebook](https://www.kaggle.com/code/pandula/a-simple-tutorial-on-beautifulsoup/notebook) (Constructing IMDB dataset)\n3. [https://github.com/sharmasapna/Web-scraping-for-images](https://github.com/sharmasapna/Web-scraping-for-images)\n(Constructing image dataset)\n\n\n## Manual Labeling with pigeonXT\n\nIn many data science projects, the first step at which the data science team gets involved is in labeling the image data. Even if the labeling will be automated, the first few images in a proof of concept are almost always hand-labeled. The form and organization will differ based on the problem type (image classification or object detection) and whether an image can have multiple labels or only one. To hand-label images, a rater views the image, determines the label(s), and records the label(s). There are two typical approaches to doing this recording: using a **folder structure** and a **metadata table.**\n\nIn a folder organization, raters simply move images to different folders depending on what their label is. All flowers that are daisies are stored in a folder named *daisy*, for example. Raters can do this quickly because most operating systems provide previews of images and handy ways to select groups of images and move them into folders.\n\nThe problem with the folder approach is that it leads to duplication if an image can have multiple labels—for example, if an image contains both *roses* and *daisies*. The alternative, and recommended, approach is to record the label(s) in a metadata table (such as in a spreadsheet or a CSV file) that has at least two columns - one column is the filename of the image file, and the other is the list of labels that are valid for the image.\n\nA labeling tool should have a facility to display the image, and enable the rater to quickly select valid categories and save the rating to a database. We will use `pigeonXT` which is a wrapper of Jupyter widget next.\n\n\n\n### Image classification\n\n### Binary or multi-class text classification\n\nThe output can be save to a CSV or JSON file.\n\n### Multi-label text classification\n\nAnything that can be displayed on Jupyter (text, images, audio, graphs, etc.)can be displayed by pigeonXT by providing the appropriate `display_fn` argument.\n\nAdditionally, custom hooks can be attached to each row update (`example_process_fn`), or when the annotating task is complete(`final_process_fn`). See https://github.com/dennisbakhuis/pigeonXT for more details.\n\n## Improve Consensus Labels for Multiannotator Data with Cleanlab\n\nThis example contains classification data that has been labeled by multiple annotators (where each example has been labeled by at least one annotator, but not every annotator has labeled every example)\n\nFor this part, we will generate a toy dataset that has 50 annotators and 300 examples. There are three possible classes, 0, 1 and 2.\n\nEach annotator annotates approximately 10% of the examples. We also synthetically made the last 5 annotators in our toy dataset have much noisier labels than the rest of the annotators. To generate our multiannotator data, we define a `make_data()` method \n\nLet’s view the first few rows of the data. Here are the labels selected by each annotator for the first few examples:\n\n`multiannotator_labels` contains the class labels that each annotator chose for each example, with examples that a particular annotator did not label represented using `np.nan`. `X` contains the features for each example.\n\n### Get majority vote labels and compute out-of-sample predicted probabilites\n\nBefore training a machine learning model, we must first obtain the consensus labels from the annotators that labeled the data. The simplest way to obtain an initial set of consensus labels is to select it using majority vote.\n\nNext, we will train a model on the consensus labels obtained using majority vote to compute out-of-sample predicted probabilities. Here, we use a simple logistic regression model.\n\n### Use cleanlab to get better consensus labels and other statistics\n\nUsing the annotators’ labels and the out-of-sample predicted probabilites from the model, cleanlab can help us obtain improved consensus labels for our data.\n\nHere, we use the `multiannotator.get_label_quality_multiannotator()` function which returns a dictionary containing three items:\n\n1. `label_quality` which gives us the improved consensus labels using information from each of the annotators and the model. The DataFrame also contains information about the number of annotations, annotator agreement and consensus quality score for each example.\n\n2. `detailed_label_quality` which returns the label quality score for each label given by every annotator\n\n3. `annotator_stats` which gives us the annotator quality score for each annotator, alongisde other information such as the number of examples each annotator labeled, their agreement with the consensus labels and the class they perform the worst at.\n\nWe can get the improved consensus labels from the label_quality DataFrame shown above. We can compare the accuracy of the consensus labels obtained using majority vote, and the improved consensus labels obtained using cleanlab.\n\nWe can see that the accuracy of the consensus labels improved as a result of using cleanlab!\n\nAfter obtaining the improved consensus labels, we can now retrain a better version of our machine learning model using these newly obtained labels. You can also repeatedly iterate this process of getting better consensus labels using the model’s out-of-sample predicted probabilites and then retraining the model with the improved labels to get even better predicted probabilities!\n\n## Active Learning with modAL\n\nIn this example, the active learning workflow of modAL is demonstrated - with you in the loop! By running this notebook, you’ll be queried to label digits using the [DIGITS](http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits) dataset.\n\nHere we use the pool-based sampling. In this setting, we assume a small set of labeled data `L` and a large set of unlabeled data `U` such that `|L|≪|U|`.\n\nThe high level description about this strategy is as follows: Queries are selectively drawn from the pool, which is usually assumed to be closed (i.e., static or non-changing), although this is not strictly necessary. Typically, instances are queried in a greedy fashion, according to an informativeness measure used to evaluate all instances in the pool (or, perhaps if `U` is very large, some subsample thereof).\n\nNow we set up the initial training set for our classifier. If you would like to play around, you can try to modifiy the value `n_initial` below and see if it impacts the algorithm!\n\n### Initializing the learner\n\nAlong with our pool-based sampling strategy, `modAL`’s modular design allows you to vary parameters surrounding the active learning process, **including the core estimator and query strategy.**\n\nNow we initialize the active learner. Feel free to change the underlying `RandomForestClassifier` or the `uncertainty_sampling`!\n\n### The active learning loop\n\nBy querying hard example to the rater, the performance increase from 83% to 86%!\n\n### Iris example\n\nIn this example, we use `scikit-learn`’s `k-nearest neighbors classifier` as our estimator and default to `modAL`’s uncertainty sampling query strategy.\n\nNow we partition our iris dataset into a training set `L` and `U`. We first specify our training set `L` consisting of 3 random examples. The remaining examples go to our “unlabeled” pool `U`.\n\nFor the classification, we are going to use a simple `k-nearest neighbors classifier`. In this step, we are also going to initialize the `ActiveLearner`.\n\nLet’s see how our classifier performs on the initial training set!\n\n#### Update our model by pool-based sampling our “unlabeled” dataset U\n\n\nAs we can see, our model is unable to properly learn the underlying data distribution. All of its predictions are for the third class label, and as such it is only as competitive as defaulting its predictions to a single class – if only we had more data!\n\nBelow, we tune our classifier by allowing it to query 20 instances it hasn’t seen before. Using uncertainty sampling, **our classifier aims to reduce the amount of uncertainty in its predictions using a variety of measures** — see https://modal-python.readthedocs.io/en/latest/index.html for more on specific classification uncertainty measures. With each requested query, we remove that record from our pool `U` and record our model’s accuracy on the raw dataset.\n\nHere, we first plot the query iteration index against model accuracy. To visualize the performance of our classifier, we also plot the correct and incorrect predictions on the full dataset.\n\n## Weak Supervison using Snorkel\n\nWe will walk through the process of using `Snorkel` to build a training set for classifying YouTube comments as spam or not spam. The goal of this tutorial is to illustrate the basic components and concepts of `Snorkel` in a simple way, but also to dive into the actual process of iteratively developing real applications in `Snorkel`.\n\nOur goal is to train a classifier over the comment data that can predict whether a comment is spam or not spam. We have access to a large amount of unlabeled data in the form of YouTube comments with some metadata. In order to train a classifier, we need to label our data, **but doing so by hand for real world applications can often be prohibitively slow and expensive.**\n\nIn these cases, we can turn to a weak supervision approach, using **labeling functions (LFs)** in Snorkel: noisy, programmatic rules and heuristics that assign labels to unlabeled training data. \n\nWe’ll dive into the Snorkel API and how we write labeling functions later in this tutorial, but as an example, we can write a LF that labels data points with \"http\" in the comment text as spam since many spam comments contain links.\n\nWe use a [YouTube comments dataset](https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection) that consists of YouTube comments from 5 videos. The task is to classify each comment as being\n\nHAM: comments relevant to the video (even very simple ones), or SPAM: irrelevant (often trying to advertise something) or inappropriate messages\n\nWe split our data into two sets:\n\n* **Training Set**: The largest split of the dataset, and the one without any ground truth (“gold”) labels. We will generate labels for these data points with weak supervision.\n\n* **Test Set**: A small, standard held-out blind hand-labeled set for final evaluation of our classifier. This set should only be used for final evaluation, not error analysis.\n\n> Note that in more advanced production settings, we will often further split up the available hand-labeled data into a development split, for getting ideas to write labeling functions, and a validation split for e.g. checking our performance without looking at test set scores, hyperparameter tuning, etc. These splits are omitted for simplicity here.\n\n### Loading Data\n\nWe load the YouTube comments dataset and create Pandas DataFrame objects for the train and test sets. Snorkel provides native support for several DataFrame-like data structures, including `Pandas`, `Dask`, and `PySpark`.\n\nEach DataFrame consists of the following fields:\n\n* author: Username of the comment author\n* data: Date and time the comment was posted\n* text: Raw text content of the comment\n* label: Whether the comment is SPAM (1), HAM (0), or UNKNOWN/ABSTAIN (-1)\n* video: Video the comment is associated with\n\nWe start by loading our data. The `load_spam_dataset()` method downloads the raw CSV files from the internet, divides them into splits, converts them into `DataFrames`, and shuffles them. As mentioned above, the dataset contains comments from 5 of the most popular YouTube videos during a period between 2014 and 2015. **The first four videos’ comments are combined to form the train set. This set has no gold labels. The fifth video is part of the test set.**\n\n### Writing Labeling Functions (LFs)\n\nLabeling functions (LFs) help users encode domain knowledge and other supervision sources programmatically.\n\nLFs are heuristics that take as input a data point and either assign a label to it (in this case, HAM or SPAM) or abstain (don’t assign any label). **Labeling functions can be noisy: they don’t have perfect accuracy and don’t have to label every data point. Moreover, different labeling functions can overlap (label the same data point) and even conflict (assign different labels to the same data point).** This is expected, and we demonstrate how we deal with this later.\n\nBecause their only requirement is that they map a data point a label (or abstain), they can wrap a wide variety of forms of supervision.\n\nTypical LF development cycles include multiple iterations of ideation, refining, evaluation, and debugging. A typical cycle consists of the following steps:\n\n1. Look at examples to generate ideas for LFs\n2. Write an initial version of an LF\n3. Spot check its performance by looking at its output on data points in the training set (or development set if available)\n4. Refine and debug to improve coverage or accuracy as necessary\n\nOur goal for LF development is to create a high quality set of training labels for our unlabeled dataset, not to label everything or directly create a model for inference using the LFs. The training labels are used to train a separate discriminative model (in this case, one which just uses the comment text) in order to generalize to new, unseen data points. Using this model, we can make predictions for data points that our LFs don’t cover.\n\na) Pattern-matching LFs (regular expressions)\n\nLabeling functions in `Snorkel` are created with the `@labeling_function` decorator. **The decorator can be applied to any Python function that returns a label for a single data point.**\n\n> See https://realpython.com/primer-on-python-decorators/ for more details about decorators.\n\nLet’s start developing an LF to catch instances of commenters trying to get people to “check out” their channel, video, or website. We’ll start by just looking for the exact string \"check out\" in the text, and see how that compares to looking for just \"check\" in the text. For the two versions of our rule, we’ll write a Python function over a single data point that express it, then add the decorator.\n\nOne dominant pattern in the comments that look like spam (which we might know from prior domain experience, or from inspection of a few training data points) is the use of the phrase **“check out”** (e.g. “check out my channel”). Let’s start with that.\n\nb) Keyword LFs\n\nFor text applications, some of the simplest LFs to write are often just keyword lookups. These will often follow the same execution pattern, so we can create a template and use the resources parameter to pass in LF-specific keywords. Similar to the `labeling_function` decorator, the `LabelingFunction` class wraps a Python function (the `f` parameter), and we can use the resources parameter to pass in keyword arguments (here, our keywords to lookup) to said function.\n\nc) Heuristic LFs\nThere may other heuristics or “rules of thumb” that you come up with as you look at the data. So long as you can express it in a function, it’s a viable LF!\n\nTo apply one or more LFs that we’ve written to a collection of data points, we use an `LFApplier`. Because our data points are represented with a `Pandas` `DataFrame` in this tutorial, we use the `PandasLFApplier`. Correspondingly, a single data point `x` that’s passed into our LFs will be a `Pandas` `Series` object.\n\nIt’s important to note that these LFs will work for any object with an attribute named text, not just `Pandas` objects. `Snorkel` has several other appliers for different data point collection types which you can browse in the https://snorkel.readthedocs.io/en/master/packages/labeling.html.\n\nThe output of the `apply(...)` method is a label matrix, a fundamental concept in `Snorkel`. It’s a `NumPy` array `L` with one column for each LF and one row for each data point, where `L[i, j]` is the label that the jth labeling function output for the ith data point. We’ll create a label matrix for the train set.\n\nWe can easily calculate the coverage of these LFs (i.e., the percentage of the dataset that they label) as follows:\n\nLots of statistics about labeling functions - like coverage - are useful when building any `Snorkel` application. So Snorkel provides tooling for common LF analyses using the `LFAnalysis` utility. \n\nCheckout the statistics here [https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.LFAnalysis.html](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.LFAnalysis.html)\n\nOur goal is now to convert the labels from our LFs into a single noise-aware probabilistic (or confidence-weighted) label per data point. A simple baseline for doing this is to take the majority vote on a per-data point basis: **if more LFs voted SPAM than HAM, label it SPAM (and vice versa)**. We can test this with the `MajorityLabelVoter` baseline model.\n\nThe LFs may have varying properties and should not be treated identically. In addition to having varied accuracies and coverages, LFs may be correlated, resulting in certain signals being overrepresented in a majority-vote-based model. \n\nTo handle these issues appropriately, we will instead use a more sophisticated Snorkel `LabelModel` to combine the outputs of the LFs. **This model will ultimately produce a single set of noise-aware training labels, which are probabilistic or confidence-weighted labels. We will then use these labels to train a classifier for our task.**\n\nNote that no gold labels are used during the training process. The only information we need is the label matrix, which contains the output of the LFs on our training set. The `LabelModel` is able to learn weights for the labeling functions using only the label matrix as input. We also specify the **cardinality, or number of classes.**\n\nThe majority vote model or more sophisticated `LabelModel` could in principle be used directly as a classifier if the outputs of our labeling functions were made available at test time. However, these models (i.e. these re-weighted combinations of our labeling function’s votes) will abstain on the data points that our labeling functions don’t cover.\n\nWe will instead use the outputs of the `LabelModel` as training labels to train a discriminative classifier which can generalize beyond the labeling function outputs to see if we can improve performance further. This classifier will also only need the text of the comment to make predictions, making it much more suitable for inference over unseen comments.\n\nLet's briefly confirm that the labels the `LabelModel` produces are indeed probabilistic in nature. The following histogram shows the confidences we have that each data point has the label SPAM. The points we are least certain about will have labels close to 0.5.\n\nAs we saw earlier, some of the data points in our train set received no labels from any of our LFs. These data points convey no supervision signal and tend to hurt performance, so we filter them out before training using a built-in utility.\n\n### Training a Classifier\n\nIn this final section of the tutorial, we’ll use the probabilistic training labels we generated in the last section to train a classifier for our task. The output of the `Snorkel` `LabelModel` is just a set of labels which can be used with most popular libraries for performing supervised learning. Note that typically, `Snorkel` is used (and really shines!) with much more complex, training data-hungry models, but we will use Logistic Regression here for simplicity of exposition.\n\nFor simplicity and speed, we use a simple “bag of n-grams” feature representation: each data point is represented by a one-hot vector marking which words or 2-word combinations are present in the comment text.\n\nIf we want to use a library or model that doesn’t accept probabilistic labels (such as `Scikit-Learn`), we can instead replace each label distribution with the label of the class that has the maximum probability. This can easily be done using the `probs_to_preds` helper method. **We do note, however, that this transformation is lossy**, as we no longer have values for our confidence in each label.\n\nWe then use these labels to train a classifier as usual.\n\nWe observe an additional boost in accuracy over the `LabelModel` by multiple points! This is in part because the discriminative model generalizes beyond the labeling function’s labels and makes good predictions on all data points, not just the ones covered by labeling functions. By using the label model to transfer the domain knowledge encoded in our LFs to the discriminative model, we were able to generalize beyond the noisy labeling heuristics.\n\n## Data augmentation using Albumentations\n\n`Scikit-image` reads an image in RGB format which is consistent with `Albumentations`.\n\nWe fix the random seed for visualization purposes, so the augmentation will always produce the same result. In a real computer vision pipeline, **you shouldn't fix the random seed** before applying a transform to the image because, in that case, the pipeline will always output the same image. The purpose of image augmentation is to use different transformations each time\n\nFor more information, please refer to [https://albumentations.ai/docs/#examples](https://albumentations.ai/docs/#examples).\n\n## Reference\n\nThis notebook contains the sample from \n\n1. [https://github.com/dennisbakhuis/pigeonXT/blob/master/pigeonXT_Examples.ipynb](https://github.com/dennisbakhuis/pigeonXT/blob/master/pigeonXT_Examples.ipynb)\n2. [https://docs.cleanlab.ai/stable/tutorials/multiannotator.html](https://docs.cleanlab.ai/stable/tutorials/multiannotator.html)\n3. [https://modal-python.readthedocs.io/en/latest/content/examples/interactive_labeling.html_](https://modal-python.readthedocs.io/en/latest/content/examples/interactive_labeling.html_)\n4. [https://modal-python.readthedocs.io/en/latest/content/examples/pool-based_sampling.html](https://modal-python.readthedocs.io/en/latest/content/examples/pool-based_sampling.html)\n5. [https://www.snorkel.org/use-cases/01-spam-tutorial](https://www.snorkel.org/use-cases/01-spam-tutorial)\n6. [https://albumentations.ai/docs/#examples](https://albumentations.ai/docs/#examples)\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"02_Dataset.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.306","theme":"cosmo","cover-image":"cover.jpg","title":"Framing the problem and constructing the dataset","author":"phonchi","date":"02/20/2023"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}