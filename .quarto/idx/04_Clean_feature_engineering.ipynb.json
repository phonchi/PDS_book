{"title":"Data cleaning and feature engineering","markdown":{"yaml":{"title":"Data cleaning and feature engineering","author":"phonchi","date":"03/13/2023","format":{"html":{"toc":true,"code-line-numbers":true,"code-fold":true,"code-tools":true}}},"headingText":"Setup","containsRefs":false,"markdown":"\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/04_Clean_feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/04_Clean_feature_engineering.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>\n<br/>\n\n\n## Exploratory Data Analysis\n\nYou can checkout some of useful EDA tools [pandas-profiling](https://github.com/ydataai/ydata-profiling), [dataprep](https://github.com/sfu-db/dataprep), [lux](https://github.com/lux-org/lux) or [dtale](https://github.com/man-group/dtale)\n\n## Handling missing value\n\nIn this section, you'll learn why you've run into the data cleaning problems and, more importantly, how to fix them! Specifically, you’ll learn how to tackle some of the most common data cleaning problems so you can get to actually analyzing your data faster. \n\n### Take a first look at the data\n\nFor demonstration, we'll use a dataset of events that occured in American Football games. You'll apply your new skills to a dataset of building permits issued in San Francisco. The dataset that we will use was made available by Kaggle. You can download the original dataset from https://www.kaggle.com/datasets/maxhorowitz/nflplaybyplay2009to2016.\n\nBut you can also use Kaggle API. First, you need to create a Kaggle API key and download it to your local machine. Just navigate to the Kaggle website in a web browser, log in, and go to the My Account page. In your account settings, you’ll find an API section. Clicking the Create New API Token button will generate a kaggle.json key file and will download it to your machine.\n\n<p align=\"center\">\n<img src=\"https://drive.google.com/uc?id=1Yo0bW4A59Se1EbE50JZMiWNueErB2eJa\" alt=\"drawing\" width=\"600\"/>\n</p>\n\n> In some dataset, when the first time you try to download the data, you may get a “403 Forbidden” error. That’s because you need to accept the terms associated with the dataset before you download it and click the \"I Understand and Accept button\". You only need to do this once.\n\nThe first thing to do when you get a new dataset is take a look at some of it. This lets you see that it all read in correctly and gives an idea of what's going on with the data. In this case, let's see if there are any missing values, which will be reprsented with `NaN` or `None`.\n\n### How many missing data points do we have?\n\nOk, now we know that we do have some missing values. Let's see how many we have in each column. \n\nAlmost a quarter of the cells in this dataset are empty! In the next step, we're going to take a closer look at some of the columns with missing values and try to figure out what might be going on with them.\n\nLooking at the number of missing values in the `nfl_data` dataframe, we notice that the column `TimesSecs` has missing values in it. By looking at [the documentation](https://www.kaggle.com/maxhorowitz/nflplaybyplay2009to2016), we can see that this column has information on the number of seconds left in the game when the play was made. This means that these values are probably missing because **they were not recorded**, rather than because they don't exist. So, it would make sense for us to try and guess what they should be rather than just leaving them as NA's.\n\nOn the other hand, there are other fields, like `PenalizedTeam` that also have lot of missing fields. In this case, though, the field is missing because if there was no penalty then it doesn't make sense to say *which* team was penalized. For this column, it would make more sense to either leave it empty or to add a third value like \"neither\" and use that to replace the NA's.\n\nWe'll cover some \"quick and dirty\" techniques that can help you with missing values but will probably also end up removing some useful information or adding some noise to your data.\n\n### Drop missing values\n\nIf you're sure you want to drop rows with missing values, pandas does have a handy function, `dropna()` to help you do this. Let's try it out on our NFL dataset!\n\nNotice that this can drop a lot of data that you might want to keep, particularly in smaller datasets. What if you just want to drop rows or columns that contain several or even just all null values? You specify those setting in dropna with the `how` and `thresh` parameters.\n\nBy default, `how='any'`. You could alternatively specify `how='all'` so as to **drop only rows or columns that contain all null values**. The `thresh `parameter gives you finer-grained control: you set the number of non-null values that a row or column needs to have in order to be kept.\n\nHere, the first and last row have been dropped, because they contain only two non-null values.\n\n### Filling in missing values automatically\n\nDepending on your dataset, it can sometimes make more sense to fill null values with valid ones rather than drop them. Pandas provides `fillna()`, which returns a copy of the `Series` or `DataFrame` with the missing values replaced with one of your choosing.\n\nWe could also replace missing values with whatever value comes directly after/before it in the same column. (This makes a lot of sense for datasets where the observations have some sort of logical order to them.)\n\nYou can forward-fill null values, which is to use the last valid value to fill a null:\n\nBackward-fill to propagate the next valid value backward to fill a null:\n\nNotice that when a previous/next value is not available for forward/backward-filling, the null value remains.\n\n### Imputation of missing value\n\n#### Univariate feature imputation\n\nThe `SimpleImputer` class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.\n\nThe following snippet demonstrates how to replace missing values, encoded as `np.nan`, using the mean value of the columns that contain the missing values:\n\nThe `SimpleImputer` class also supports categorical data represented as string values or pandas categoricals when using the `most_frequent` or `constant` strategy:\n\n#### Multivariate feature imputation\n\nA more sophisticated approach is to use the `IterativeImputer` class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. \n\nThe `KNNImputer` class provides imputation for filling in missing values using the k-Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values, `nan_euclidean_distances`, is used to find the nearest neighbors. Each missing feature is imputed using values from `n_neighbors` nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor.\n\nThe following snippet demonstrates how to replace missing values, encoded as `np.nan`, using the mean feature value of the two nearest neighbors of samples with missing values:\n\nif you wishes to apply matrix completion to your data, you can use functions from [`fancyimpute`](https://github.com/iskandr/fancyimpute)\n\nFor more information, please refer to https://github.com/iskandr/fancyimpute or https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute.\n\n## Other data cleaning problem\n\n### Duplicate data entry\n\nIn addition to missing data, you will often encounter duplicated data in real-world datasets. Fortunately, `pandas` provides an easy means of detecting and removing duplicate entries.\n\nYou can easily spot duplicate values using the `duplicated()` method in `pandas`, which returns a Boolean mask indicating whether an entry in a DataFrame is a duplicate of an earlier one. Let's create another example `DataFrame` to see this in action.\n\n`drop_duplicates()` will simply returns a copy of the data for which all of the duplicated values are False:\n\nBoth `duplicated()` and `drop_duplicates()` default to consider all columns but you can specify that they examine only a subset of columns in your DataFrame:\n\n### Inconsistent data entry\n\nWe will use the dataset that is modified from https://www.kaggle.com/datasets/zusmani/pakistanintellectualcapitalcs.\n\nSay we're interested in cleaning up the `Country` column to make sure there's no data entry inconsistencies in it. We could go through and check each row by hand, of course, and hand-correct inconsistencies when we find them. There's a more efficient way to do this, though!\n\nJust looking at this, we can see some problems due to inconsistent data entry: ' Germany', and 'germany', for example, or ' New Zealand' (start wirh whitespace) and 'New Zealand'.\n\nThe first thing we are going to do is make everything lower case (we can change it back at the end if we like) and remove any white spaces at the beginning and end of cells. **Inconsistencies in capitalizations and trailing white spaces are very common in text data and you can fix a good 80% of your text data entry inconsistencies by doing this.**\n\nNote that `.str()` provide vectorized method for columns. See https://realpython.com/python-data-cleaning-numpy-pandas/#tidying-up-fields-in-the-data for more details.\n\n#### Use fuzzy matching to correct inconsistent data entry\n\nAlright, let's take another look at the `Country` column and see if there's any more data cleaning we need to do\n\nIt does look like there is another inconsistency: 'southkorea' and 'south korea' should be the same. We're going to use the [`thefuzz`](https://github.com/seatgeek/thefuzz) package to help identify which strings are closest to each other. This dataset is small enough that we could probably correct errors by hand, but that approach doesn't scale well. (Would you want to correct a thousand errors by hand? What about ten thousand? Automating things as early as possible is generally a good idea!)\n\n`thefuzz` returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. Here, we're going to get the ten strings from our list of cities that have the closest distance to \"south korea\"\n\nWe can see that two of the items in the cities are very close to \"south korea\": \"south korea\" and \"southkorea\". Let's replace all rows in our \"Country\" column that have a score > 90 with \"south korea\".\n\nTo do this, we are going to write a function.\n\nNow that we have a function, we can put it to the test!\n\nAnd now let's check the unique values in our \"Country\" column again and make sure we've tidied up \"south korea\" correctly.\n\nNow we only have \"south korea\" in our dataframe and we didn't have to change anything by hand.\n\n### Character encoding\n\nThere are two main data types you'll encounter when working with text in Python 3. One is is the `string`, which is what text is by default.\n\nThe other data is the [bytes](https://docs.python.org/3/library/stdtypes.html#binary-sequence-types-bytes-bytearray-memoryview) data type, which is a sequence of integers. You can convert a `string` into `bytes` by specifying which encoding it's in:\n\nIf you look at a `bytes` object, you'll see that it has a `b` in front of it, and then maybe some text after. **That's because bytes are printed out as if they were characters encoded in UTF-8**. Here you can see that our euro symbol  has been replaced with some mojibake that looks like \"\\xe2\\x82\\xac\" when it's printed as if it were an UTF-8 string\n\nWhen we convert our `bytes` back to a `string` with the correct encoding, we can see that our text is all there correctly, which is great! :)\n\nHowever, when we try to use a different encoding to map our bytes into a string, we get an error. This is because the encoding we're trying to use doesn't know what to do with the `bytes` we're trying to pass it. You need to tell Python the encoding that the byte string is actually supposed to be in\n\n**The best time to convert non UTF-8 input into UTF-8 is when you read in files**, which we'll talk about next.\n\n### Reading in files with encoding problems\n\nMost files you'll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won't run into problems. However, sometimes you'll get an error like this:\n\nNotice that we get the same `UnicodeDecodeError` we got when we tried to decode UTF-8 bytes! **This tells us that this file isn't actually UTF-8**. We don't know what encoding it actually *is* though. One way to figure it out is to try and test a bunch of different character encodings and see if any of them work. A better way, though, is to use the [`chardet`](https://github.com/chardet/chardet) module to try and automatically guess what the right encoding is. It's not 100% guaranteed to be right, but it's usually faster than just trying to guess.\n\nWe are going to just look at the first ten thousand bytes of this file. This is usually enough for a good guess about what the encoding is and is much faster than trying to look at the whole file. (Especially with a  large file this can be very slow.)\n\nSo chardet is 73%  confidence that the right encoding is \"Windows-1252\". Let's see if that's correct:\n\nYep, looks like `chardet` was right! The file reads in with no problem (although we do get a warning about datatypes) and when we look at the first few rows it seems to be fine. \n\nWhat if the encoding `chardet` guesses isn't right? Since `chardet` is basically just a fancy guesser, sometimes it will guess the wrong encoding. **One thing you can try is looking at more or less of the file and seeing if you get a different result and then try that.**\n\n### Saving your files with UTF-8 encoding\n\n\nFinally, once you've gone through all the trouble of getting your file into UTF-8, you'll probably want to keep it that way. The easiest way to do that is to save your files with UTF-8 encoding. The good news is, since UTF-8 is the standard encoding in Python, when you save a file it will be saved as UTF-8 by default:\n\n## Scaling and normalization\n\n### Standardization\n\nBy scaling your variables, you can help compare different variables on equal footing. The [`preprocessing`](https://scikit-learn.org/stable/modules/preprocessing.html) module provides the `StandardScaler` utility class, which is a quick and easy way to perform the following operation on an array-like dataset.\n\nNotice that **you should apply the same transfrom** on both training and testing dataset.\n\nScaled data has zero mean and unit variance:\n\nNote that it is possible to disable either centering or scaling by either passing `with_mean=False` or `with_std=False` to the constructor of `StandardScaler`.\n\n### Scaling\n\nAn alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one.\n\nNote that you can scale to any range by specifying `feature_range=(min, max)`.\n\n### Logarithm transform and binning\n\nBinning allows you to transform numerical variable to categorical variable\n\n### Power transfrom\n\nBox-cox transform only works for postive data\n\n### Quantile transfrom\n\n## Encoding\n\n### One-hot encoding\n\nOne possibility to convert categorical features to features that can be used with `scikit-learn` estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the `OneHotEncoder`, which transforms each categorical feature with `n_categories` possible values into `n_categories` binary features, with one of them 1, and all others 0.\n\nWhen an unknown category is encountered during `transform()`, the resulting one-hot encoded columns for this feature will be all zeros:\n\n### Ordinal encoding\n\n### Target encoding\n\nThe [*MovieLens1M*](https://www.kaggle.com/datasets/odedgolden/movielens-1m-dataset) dataset contains one-million movie ratings by users of the MovieLens website, with features describing each user and movie. This hidden cell sets everything up:\n\nWith over 3000 categories, the `Zipcode` feature makes a good candidate for target encoding, and the size of this dataset (over one-million rows) means we can spare some data to create the encoding.\n\nWe'll start by creating a 25% split to train the target encoder.\n\nThe `category_encoders` package in [`scikit-learn-contrib`](https://github.com/scikit-learn-contrib/category_encoders) implements an `MEstimateEncoder`, which we'll use to encode our `Zipcode` feature.\n\nLet's compare the encoded values to the target to see how informative our encoding might be.\n\nThe distribution of the encoded `Zipcode` feature roughly follows the distribution of the actual ratings, meaning that movie-watchers differed enough in their ratings from zipcode to zipcode that our target encoding was able to capture useful information.\n\nIf you would like to use KFold encoding, take a look at http://contrib.scikit-learn.org/category_encoders/wrapper.html#category_encoders.wrapper.NestedCVWrapper\n\n## Feature Engineering\n\nWe'll see how adding a few synthetic features to a dataset can improve the predictive performance of a random forest model.\n\nThe [*Concrete*](https://www.kaggle.com/sinamhd9/concrete-comprehensive-strength) dataset contains a variety of concrete formulations and the resulting product's *compressive strength*, which is a measure of how much load that kind of concrete can bear. The task for this dataset is to predict a concrete's compressive strength given its formulation.\n\nYou can see here the various ingredients going into each variety of concrete. We'll see in a moment how adding some additional synthetic features derived from these can help a model to learn important relationships among them. We'll first establish a baseline by training the model on the un-augmented dataset. This will help us determine whether our new features are actually useful.\n\nEstablishing baselines like this is good practice at the start of the feature engineering process. A baseline score can help you decide whether your new features are worth keeping, or whether you should discard them and possibly try something else.\n\nIf you ever cook at home, you might know that the ***ratio*** of ingredients in a recipe is usually a better predictor of how the recipe turns out than their absolute amounts. We might reason then that ratios of the features above would be a good predictor of `CompressiveStrength`. The cell below adds three new ratio features to the dataset.\n\nAnd sure enough, performance improved! This is evidence that these new ratio features exposed important information to the model that it wasn't detecting before.\n\n### Mathematical Transforms\n\nWe'll use four datasets that having a range of feature types: [*US Traffic Accidents*](https://www.kaggle.com/sobhanmoosavi/us-accidents), [*1985 Automobiles*](https://www.kaggle.com/toramky/automobile-dataset), and [*Customer Lifetime Value*](https://www.kaggle.com/pankajjsh06/ibm-watson-marketing-customer-value-data). The following hidden cell loads them up.\n\nRelationships among numerical features are often expressed through mathematical formulas, which you'll frequently come across as part of your domain research. In `Pandas`, you can apply arithmetic operations to columns just as if they were ordinary numbers.\n\nIn the *Automobile* dataset are features describing a car's engine. Research yields a variety of formulas for creating potentially useful new features. The \"stroke ratio\", for instance, is a measure of how efficient an engine is versus how performant:\n\nData visualization can suggest transformations, often a \"reshaping\" of a feature through powers or logarithms. The distribution of `WindSpeed` in *US Accidents* is highly skewed, for instance. In this case the logarithm is effective at normalizing it:\n\n### Counts\n\nFeatures describing the presence or absence of something often come in sets, the set of risk factors for a disease, say. You can aggregate such features by creating a **count**. These features will be *binary* (`1` for Present, `0` for Absent) or *boolean* (`True` or `False`). In Python, booleans can be added up just as if they were integers.\n\nIn *Traffic Accidents* are several features indicating whether some roadway object was near the accident. This will create a count of the total number of roadway features nearby using the `sum()` method:\n\n### Group Transforms\n\nFinally we have **Group transforms**, which aggregate information across multiple rows grouped by some category. With a group transform you can create features like: \"the average income of a person's state of residence,\" or \"the proportion of movies released on a weekday, by genre.\" If you had discovered a category interaction, a group transform over that categry could be something good to investigate.\n\nUsing an aggregation function, a group transform combines two features: a categorical feature that provides the grouping and another feature whose values you wish to aggregate. For an \"average income by state\", you would choose `State` for the grouping feature, `mean()` for the aggregation function, and `Income` for the aggregated feature. To compute this in Pandas, we use the `groupby()` and `transform()` methods:\n\nThe `mean()` function is a built-in `dataframe` method, which means we can pass it as a string to `transform()`. Other handy methods include `max()`, `min()`, `median()`, `var()`, `std()`, and `count()`. Here's how you could calculate the frequency with which each state occurs in the dataset:\n\n\n## Data-centric AI with CleanLab\n\n`cleanlab` automatically finds and fixes errors in any ML dataset. This data-centric AI package facilitates machine learning with messy, real-world data by providing clean labels during training.\n\nThe figure above represents a toy dataset we’ll use to demonstrate various `cleanlab` functionality. In this data, the features `X` are 2-dimensional and examples are colored according to their given label above. The given label happens to be incorrect for some of the examples (circled in red) in this dataset!\n\n### Use `CleanLearning()` for everything\n\nNote! Accuracy refers to the accuracy with respect to the true error-free labels of a test set., i.e. what we actually care about in practice because that’s what real-world model performance is based on!\n\n### Use `CleanLearning()` to `find_label_issues()` in one line of code\n\nVisualize the twenty examples with lowest label quality to see if Cleanlab works.\n\nAbove, the top 20 label issues circled in red are found automatically using `cleanlab` (no true labels given).\n\n### Use `cleanlab` to find dataset-level and class-level issues\n\n- Did you notice that the yellow and seafoam green class above are overlapping?\n- How can a model ever know (or learn) what’s ground truth inside the yellow distribution?\n- If these two classes were merged, the model can learn more accurately from 3 classes (versus 4).\n\n`cleanlab` automatically finds data-set level issues like this, in one line of code. Check this out!\n\nThere are two things being happening here:\n\n- Distribution Overlap: The green distribution has huge variance and overlaps with other distributions. `Cleanlab` handles this for you: read the theory behind cleanlab for overlapping classes [here](https://arxiv.org/abs/1705.01936).\n- Label Issues: A ton of examples (which actually belong to the purple class) have been mislabeled as “green” in our dataset.\n\nNow, let’s see what happens if we merge classes “seafoam green” and “yellow”\n\nWhile on one hand that’s a huge improvement, it’s important to remember that choosing among three classes is an easier task than choosing among four classes, so it’s not fair to directly compare these numbers. Instead, the big takeaway is… if you get to choose your classes, combining overlapping classes can make the learning task easier for your model!\n\n### Clean your test set too if you’re doing ML with noisy labels!\n\nIf your test and training data were randomly split, then be aware that your test labels are likely noisy too! It is thus important to fix label issues in them before we can trust measures like test accuracy.\n\n### One score to rule them all – use cleanlab’s overall dataset health score\n\nThis score can be fairly compared across datasets or across versions of a dataset to track overall dataset quality (a.k.a. dataset health) over time.\n\nBecause we know the true labels (we created this toy dataset), we can compare with ground truth.\n\n`cleanlab` seems to be overestimating. Since data points that fall in between two overlapping distributions are often impossible to label and are counted as issues.\n\nFor more details, see [https://docs.cleanlab.ai/stable/tutorials/indepth_overview.html#](https://docs.cleanlab.ai/stable/tutorials/indepth_overview.html#)\n\n# References\n\n1. [https://www.kaggle.com/learn/feature-engineering](https://www.kaggle.com/learn/feature-engineering)\n2. [https://www.kaggle.com/learn/data-cleaning](https://www.kaggle.com/learn/data-cleaning)\n3. [https://madewithml.com/courses/mlops/preprocessing/] (https://madewithml.com/courses/mlops/preprocessing/)\n4. [https://github.com/microsoft/Data-Science-For-Beginners/blob/main/2-Working-With-Data/08-data-preparation/README.md](https://github.com/microsoft/Data-Science-For-Beginners/blob/main/2-Working-With-Data/08-data-preparation/README.md) \n5. [https://scikit-learn.org/stable/modules/preprocessing.html#](https://scikit-learn.org/stable/modules/preprocessing.html#)\n6. [https://www.books.com.tw/products/0010883417](https://www.books.com.tw/products/0010883417 )\n7. [https://docs.cleanlab.ai/stable/tutorials/indepth_overview.html#](https://docs.cleanlab.ai/stable/tutorials/indepth_overview.html#)\n","srcMarkdownNoYaml":"\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/04_Clean_feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/04_Clean_feature_engineering.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>\n<br/>\n\n## Setup\n\n## Exploratory Data Analysis\n\nYou can checkout some of useful EDA tools [pandas-profiling](https://github.com/ydataai/ydata-profiling), [dataprep](https://github.com/sfu-db/dataprep), [lux](https://github.com/lux-org/lux) or [dtale](https://github.com/man-group/dtale)\n\n## Handling missing value\n\nIn this section, you'll learn why you've run into the data cleaning problems and, more importantly, how to fix them! Specifically, you’ll learn how to tackle some of the most common data cleaning problems so you can get to actually analyzing your data faster. \n\n### Take a first look at the data\n\nFor demonstration, we'll use a dataset of events that occured in American Football games. You'll apply your new skills to a dataset of building permits issued in San Francisco. The dataset that we will use was made available by Kaggle. You can download the original dataset from https://www.kaggle.com/datasets/maxhorowitz/nflplaybyplay2009to2016.\n\nBut you can also use Kaggle API. First, you need to create a Kaggle API key and download it to your local machine. Just navigate to the Kaggle website in a web browser, log in, and go to the My Account page. In your account settings, you’ll find an API section. Clicking the Create New API Token button will generate a kaggle.json key file and will download it to your machine.\n\n<p align=\"center\">\n<img src=\"https://drive.google.com/uc?id=1Yo0bW4A59Se1EbE50JZMiWNueErB2eJa\" alt=\"drawing\" width=\"600\"/>\n</p>\n\n> In some dataset, when the first time you try to download the data, you may get a “403 Forbidden” error. That’s because you need to accept the terms associated with the dataset before you download it and click the \"I Understand and Accept button\". You only need to do this once.\n\nThe first thing to do when you get a new dataset is take a look at some of it. This lets you see that it all read in correctly and gives an idea of what's going on with the data. In this case, let's see if there are any missing values, which will be reprsented with `NaN` or `None`.\n\n### How many missing data points do we have?\n\nOk, now we know that we do have some missing values. Let's see how many we have in each column. \n\nAlmost a quarter of the cells in this dataset are empty! In the next step, we're going to take a closer look at some of the columns with missing values and try to figure out what might be going on with them.\n\nLooking at the number of missing values in the `nfl_data` dataframe, we notice that the column `TimesSecs` has missing values in it. By looking at [the documentation](https://www.kaggle.com/maxhorowitz/nflplaybyplay2009to2016), we can see that this column has information on the number of seconds left in the game when the play was made. This means that these values are probably missing because **they were not recorded**, rather than because they don't exist. So, it would make sense for us to try and guess what they should be rather than just leaving them as NA's.\n\nOn the other hand, there are other fields, like `PenalizedTeam` that also have lot of missing fields. In this case, though, the field is missing because if there was no penalty then it doesn't make sense to say *which* team was penalized. For this column, it would make more sense to either leave it empty or to add a third value like \"neither\" and use that to replace the NA's.\n\nWe'll cover some \"quick and dirty\" techniques that can help you with missing values but will probably also end up removing some useful information or adding some noise to your data.\n\n### Drop missing values\n\nIf you're sure you want to drop rows with missing values, pandas does have a handy function, `dropna()` to help you do this. Let's try it out on our NFL dataset!\n\nNotice that this can drop a lot of data that you might want to keep, particularly in smaller datasets. What if you just want to drop rows or columns that contain several or even just all null values? You specify those setting in dropna with the `how` and `thresh` parameters.\n\nBy default, `how='any'`. You could alternatively specify `how='all'` so as to **drop only rows or columns that contain all null values**. The `thresh `parameter gives you finer-grained control: you set the number of non-null values that a row or column needs to have in order to be kept.\n\nHere, the first and last row have been dropped, because they contain only two non-null values.\n\n### Filling in missing values automatically\n\nDepending on your dataset, it can sometimes make more sense to fill null values with valid ones rather than drop them. Pandas provides `fillna()`, which returns a copy of the `Series` or `DataFrame` with the missing values replaced with one of your choosing.\n\nWe could also replace missing values with whatever value comes directly after/before it in the same column. (This makes a lot of sense for datasets where the observations have some sort of logical order to them.)\n\nYou can forward-fill null values, which is to use the last valid value to fill a null:\n\nBackward-fill to propagate the next valid value backward to fill a null:\n\nNotice that when a previous/next value is not available for forward/backward-filling, the null value remains.\n\n### Imputation of missing value\n\n#### Univariate feature imputation\n\nThe `SimpleImputer` class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.\n\nThe following snippet demonstrates how to replace missing values, encoded as `np.nan`, using the mean value of the columns that contain the missing values:\n\nThe `SimpleImputer` class also supports categorical data represented as string values or pandas categoricals when using the `most_frequent` or `constant` strategy:\n\n#### Multivariate feature imputation\n\nA more sophisticated approach is to use the `IterativeImputer` class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. \n\nThe `KNNImputer` class provides imputation for filling in missing values using the k-Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values, `nan_euclidean_distances`, is used to find the nearest neighbors. Each missing feature is imputed using values from `n_neighbors` nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor.\n\nThe following snippet demonstrates how to replace missing values, encoded as `np.nan`, using the mean feature value of the two nearest neighbors of samples with missing values:\n\nif you wishes to apply matrix completion to your data, you can use functions from [`fancyimpute`](https://github.com/iskandr/fancyimpute)\n\nFor more information, please refer to https://github.com/iskandr/fancyimpute or https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute.\n\n## Other data cleaning problem\n\n### Duplicate data entry\n\nIn addition to missing data, you will often encounter duplicated data in real-world datasets. Fortunately, `pandas` provides an easy means of detecting and removing duplicate entries.\n\nYou can easily spot duplicate values using the `duplicated()` method in `pandas`, which returns a Boolean mask indicating whether an entry in a DataFrame is a duplicate of an earlier one. Let's create another example `DataFrame` to see this in action.\n\n`drop_duplicates()` will simply returns a copy of the data for which all of the duplicated values are False:\n\nBoth `duplicated()` and `drop_duplicates()` default to consider all columns but you can specify that they examine only a subset of columns in your DataFrame:\n\n### Inconsistent data entry\n\nWe will use the dataset that is modified from https://www.kaggle.com/datasets/zusmani/pakistanintellectualcapitalcs.\n\nSay we're interested in cleaning up the `Country` column to make sure there's no data entry inconsistencies in it. We could go through and check each row by hand, of course, and hand-correct inconsistencies when we find them. There's a more efficient way to do this, though!\n\nJust looking at this, we can see some problems due to inconsistent data entry: ' Germany', and 'germany', for example, or ' New Zealand' (start wirh whitespace) and 'New Zealand'.\n\nThe first thing we are going to do is make everything lower case (we can change it back at the end if we like) and remove any white spaces at the beginning and end of cells. **Inconsistencies in capitalizations and trailing white spaces are very common in text data and you can fix a good 80% of your text data entry inconsistencies by doing this.**\n\nNote that `.str()` provide vectorized method for columns. See https://realpython.com/python-data-cleaning-numpy-pandas/#tidying-up-fields-in-the-data for more details.\n\n#### Use fuzzy matching to correct inconsistent data entry\n\nAlright, let's take another look at the `Country` column and see if there's any more data cleaning we need to do\n\nIt does look like there is another inconsistency: 'southkorea' and 'south korea' should be the same. We're going to use the [`thefuzz`](https://github.com/seatgeek/thefuzz) package to help identify which strings are closest to each other. This dataset is small enough that we could probably correct errors by hand, but that approach doesn't scale well. (Would you want to correct a thousand errors by hand? What about ten thousand? Automating things as early as possible is generally a good idea!)\n\n`thefuzz` returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. Here, we're going to get the ten strings from our list of cities that have the closest distance to \"south korea\"\n\nWe can see that two of the items in the cities are very close to \"south korea\": \"south korea\" and \"southkorea\". Let's replace all rows in our \"Country\" column that have a score > 90 with \"south korea\".\n\nTo do this, we are going to write a function.\n\nNow that we have a function, we can put it to the test!\n\nAnd now let's check the unique values in our \"Country\" column again and make sure we've tidied up \"south korea\" correctly.\n\nNow we only have \"south korea\" in our dataframe and we didn't have to change anything by hand.\n\n### Character encoding\n\nThere are two main data types you'll encounter when working with text in Python 3. One is is the `string`, which is what text is by default.\n\nThe other data is the [bytes](https://docs.python.org/3/library/stdtypes.html#binary-sequence-types-bytes-bytearray-memoryview) data type, which is a sequence of integers. You can convert a `string` into `bytes` by specifying which encoding it's in:\n\nIf you look at a `bytes` object, you'll see that it has a `b` in front of it, and then maybe some text after. **That's because bytes are printed out as if they were characters encoded in UTF-8**. Here you can see that our euro symbol  has been replaced with some mojibake that looks like \"\\xe2\\x82\\xac\" when it's printed as if it were an UTF-8 string\n\nWhen we convert our `bytes` back to a `string` with the correct encoding, we can see that our text is all there correctly, which is great! :)\n\nHowever, when we try to use a different encoding to map our bytes into a string, we get an error. This is because the encoding we're trying to use doesn't know what to do with the `bytes` we're trying to pass it. You need to tell Python the encoding that the byte string is actually supposed to be in\n\n**The best time to convert non UTF-8 input into UTF-8 is when you read in files**, which we'll talk about next.\n\n### Reading in files with encoding problems\n\nMost files you'll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won't run into problems. However, sometimes you'll get an error like this:\n\nNotice that we get the same `UnicodeDecodeError` we got when we tried to decode UTF-8 bytes! **This tells us that this file isn't actually UTF-8**. We don't know what encoding it actually *is* though. One way to figure it out is to try and test a bunch of different character encodings and see if any of them work. A better way, though, is to use the [`chardet`](https://github.com/chardet/chardet) module to try and automatically guess what the right encoding is. It's not 100% guaranteed to be right, but it's usually faster than just trying to guess.\n\nWe are going to just look at the first ten thousand bytes of this file. This is usually enough for a good guess about what the encoding is and is much faster than trying to look at the whole file. (Especially with a  large file this can be very slow.)\n\nSo chardet is 73%  confidence that the right encoding is \"Windows-1252\". Let's see if that's correct:\n\nYep, looks like `chardet` was right! The file reads in with no problem (although we do get a warning about datatypes) and when we look at the first few rows it seems to be fine. \n\nWhat if the encoding `chardet` guesses isn't right? Since `chardet` is basically just a fancy guesser, sometimes it will guess the wrong encoding. **One thing you can try is looking at more or less of the file and seeing if you get a different result and then try that.**\n\n### Saving your files with UTF-8 encoding\n\n\nFinally, once you've gone through all the trouble of getting your file into UTF-8, you'll probably want to keep it that way. The easiest way to do that is to save your files with UTF-8 encoding. The good news is, since UTF-8 is the standard encoding in Python, when you save a file it will be saved as UTF-8 by default:\n\n## Scaling and normalization\n\n### Standardization\n\nBy scaling your variables, you can help compare different variables on equal footing. The [`preprocessing`](https://scikit-learn.org/stable/modules/preprocessing.html) module provides the `StandardScaler` utility class, which is a quick and easy way to perform the following operation on an array-like dataset.\n\nNotice that **you should apply the same transfrom** on both training and testing dataset.\n\nScaled data has zero mean and unit variance:\n\nNote that it is possible to disable either centering or scaling by either passing `with_mean=False` or `with_std=False` to the constructor of `StandardScaler`.\n\n### Scaling\n\nAn alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one.\n\nNote that you can scale to any range by specifying `feature_range=(min, max)`.\n\n### Logarithm transform and binning\n\nBinning allows you to transform numerical variable to categorical variable\n\n### Power transfrom\n\nBox-cox transform only works for postive data\n\n### Quantile transfrom\n\n## Encoding\n\n### One-hot encoding\n\nOne possibility to convert categorical features to features that can be used with `scikit-learn` estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the `OneHotEncoder`, which transforms each categorical feature with `n_categories` possible values into `n_categories` binary features, with one of them 1, and all others 0.\n\nWhen an unknown category is encountered during `transform()`, the resulting one-hot encoded columns for this feature will be all zeros:\n\n### Ordinal encoding\n\n### Target encoding\n\nThe [*MovieLens1M*](https://www.kaggle.com/datasets/odedgolden/movielens-1m-dataset) dataset contains one-million movie ratings by users of the MovieLens website, with features describing each user and movie. This hidden cell sets everything up:\n\nWith over 3000 categories, the `Zipcode` feature makes a good candidate for target encoding, and the size of this dataset (over one-million rows) means we can spare some data to create the encoding.\n\nWe'll start by creating a 25% split to train the target encoder.\n\nThe `category_encoders` package in [`scikit-learn-contrib`](https://github.com/scikit-learn-contrib/category_encoders) implements an `MEstimateEncoder`, which we'll use to encode our `Zipcode` feature.\n\nLet's compare the encoded values to the target to see how informative our encoding might be.\n\nThe distribution of the encoded `Zipcode` feature roughly follows the distribution of the actual ratings, meaning that movie-watchers differed enough in their ratings from zipcode to zipcode that our target encoding was able to capture useful information.\n\nIf you would like to use KFold encoding, take a look at http://contrib.scikit-learn.org/category_encoders/wrapper.html#category_encoders.wrapper.NestedCVWrapper\n\n## Feature Engineering\n\nWe'll see how adding a few synthetic features to a dataset can improve the predictive performance of a random forest model.\n\nThe [*Concrete*](https://www.kaggle.com/sinamhd9/concrete-comprehensive-strength) dataset contains a variety of concrete formulations and the resulting product's *compressive strength*, which is a measure of how much load that kind of concrete can bear. The task for this dataset is to predict a concrete's compressive strength given its formulation.\n\nYou can see here the various ingredients going into each variety of concrete. We'll see in a moment how adding some additional synthetic features derived from these can help a model to learn important relationships among them. We'll first establish a baseline by training the model on the un-augmented dataset. This will help us determine whether our new features are actually useful.\n\nEstablishing baselines like this is good practice at the start of the feature engineering process. A baseline score can help you decide whether your new features are worth keeping, or whether you should discard them and possibly try something else.\n\nIf you ever cook at home, you might know that the ***ratio*** of ingredients in a recipe is usually a better predictor of how the recipe turns out than their absolute amounts. We might reason then that ratios of the features above would be a good predictor of `CompressiveStrength`. The cell below adds three new ratio features to the dataset.\n\nAnd sure enough, performance improved! This is evidence that these new ratio features exposed important information to the model that it wasn't detecting before.\n\n### Mathematical Transforms\n\nWe'll use four datasets that having a range of feature types: [*US Traffic Accidents*](https://www.kaggle.com/sobhanmoosavi/us-accidents), [*1985 Automobiles*](https://www.kaggle.com/toramky/automobile-dataset), and [*Customer Lifetime Value*](https://www.kaggle.com/pankajjsh06/ibm-watson-marketing-customer-value-data). The following hidden cell loads them up.\n\nRelationships among numerical features are often expressed through mathematical formulas, which you'll frequently come across as part of your domain research. In `Pandas`, you can apply arithmetic operations to columns just as if they were ordinary numbers.\n\nIn the *Automobile* dataset are features describing a car's engine. Research yields a variety of formulas for creating potentially useful new features. The \"stroke ratio\", for instance, is a measure of how efficient an engine is versus how performant:\n\nData visualization can suggest transformations, often a \"reshaping\" of a feature through powers or logarithms. The distribution of `WindSpeed` in *US Accidents* is highly skewed, for instance. In this case the logarithm is effective at normalizing it:\n\n### Counts\n\nFeatures describing the presence or absence of something often come in sets, the set of risk factors for a disease, say. You can aggregate such features by creating a **count**. These features will be *binary* (`1` for Present, `0` for Absent) or *boolean* (`True` or `False`). In Python, booleans can be added up just as if they were integers.\n\nIn *Traffic Accidents* are several features indicating whether some roadway object was near the accident. This will create a count of the total number of roadway features nearby using the `sum()` method:\n\n### Group Transforms\n\nFinally we have **Group transforms**, which aggregate information across multiple rows grouped by some category. With a group transform you can create features like: \"the average income of a person's state of residence,\" or \"the proportion of movies released on a weekday, by genre.\" If you had discovered a category interaction, a group transform over that categry could be something good to investigate.\n\nUsing an aggregation function, a group transform combines two features: a categorical feature that provides the grouping and another feature whose values you wish to aggregate. For an \"average income by state\", you would choose `State` for the grouping feature, `mean()` for the aggregation function, and `Income` for the aggregated feature. To compute this in Pandas, we use the `groupby()` and `transform()` methods:\n\nThe `mean()` function is a built-in `dataframe` method, which means we can pass it as a string to `transform()`. Other handy methods include `max()`, `min()`, `median()`, `var()`, `std()`, and `count()`. Here's how you could calculate the frequency with which each state occurs in the dataset:\n\n\n## Data-centric AI with CleanLab\n\n`cleanlab` automatically finds and fixes errors in any ML dataset. This data-centric AI package facilitates machine learning with messy, real-world data by providing clean labels during training.\n\nThe figure above represents a toy dataset we’ll use to demonstrate various `cleanlab` functionality. In this data, the features `X` are 2-dimensional and examples are colored according to their given label above. The given label happens to be incorrect for some of the examples (circled in red) in this dataset!\n\n### Use `CleanLearning()` for everything\n\nNote! Accuracy refers to the accuracy with respect to the true error-free labels of a test set., i.e. what we actually care about in practice because that’s what real-world model performance is based on!\n\n### Use `CleanLearning()` to `find_label_issues()` in one line of code\n\nVisualize the twenty examples with lowest label quality to see if Cleanlab works.\n\nAbove, the top 20 label issues circled in red are found automatically using `cleanlab` (no true labels given).\n\n### Use `cleanlab` to find dataset-level and class-level issues\n\n- Did you notice that the yellow and seafoam green class above are overlapping?\n- How can a model ever know (or learn) what’s ground truth inside the yellow distribution?\n- If these two classes were merged, the model can learn more accurately from 3 classes (versus 4).\n\n`cleanlab` automatically finds data-set level issues like this, in one line of code. Check this out!\n\nThere are two things being happening here:\n\n- Distribution Overlap: The green distribution has huge variance and overlaps with other distributions. `Cleanlab` handles this for you: read the theory behind cleanlab for overlapping classes [here](https://arxiv.org/abs/1705.01936).\n- Label Issues: A ton of examples (which actually belong to the purple class) have been mislabeled as “green” in our dataset.\n\nNow, let’s see what happens if we merge classes “seafoam green” and “yellow”\n\nWhile on one hand that’s a huge improvement, it’s important to remember that choosing among three classes is an easier task than choosing among four classes, so it’s not fair to directly compare these numbers. Instead, the big takeaway is… if you get to choose your classes, combining overlapping classes can make the learning task easier for your model!\n\n### Clean your test set too if you’re doing ML with noisy labels!\n\nIf your test and training data were randomly split, then be aware that your test labels are likely noisy too! It is thus important to fix label issues in them before we can trust measures like test accuracy.\n\n### One score to rule them all – use cleanlab’s overall dataset health score\n\nThis score can be fairly compared across datasets or across versions of a dataset to track overall dataset quality (a.k.a. dataset health) over time.\n\nBecause we know the true labels (we created this toy dataset), we can compare with ground truth.\n\n`cleanlab` seems to be overestimating. Since data points that fall in between two overlapping distributions are often impossible to label and are counted as issues.\n\nFor more details, see [https://docs.cleanlab.ai/stable/tutorials/indepth_overview.html#](https://docs.cleanlab.ai/stable/tutorials/indepth_overview.html#)\n\n# References\n\n1. [https://www.kaggle.com/learn/feature-engineering](https://www.kaggle.com/learn/feature-engineering)\n2. [https://www.kaggle.com/learn/data-cleaning](https://www.kaggle.com/learn/data-cleaning)\n3. [https://madewithml.com/courses/mlops/preprocessing/] (https://madewithml.com/courses/mlops/preprocessing/)\n4. [https://github.com/microsoft/Data-Science-For-Beginners/blob/main/2-Working-With-Data/08-data-preparation/README.md](https://github.com/microsoft/Data-Science-For-Beginners/blob/main/2-Working-With-Data/08-data-preparation/README.md) \n5. [https://scikit-learn.org/stable/modules/preprocessing.html#](https://scikit-learn.org/stable/modules/preprocessing.html#)\n6. [https://www.books.com.tw/products/0010883417](https://www.books.com.tw/products/0010883417 )\n7. [https://docs.cleanlab.ai/stable/tutorials/indepth_overview.html#](https://docs.cleanlab.ai/stable/tutorials/indepth_overview.html#)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"04_Clean_feature_engineering.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.306","theme":"cosmo","cover-image":"cover.jpg","title":"Data cleaning and feature engineering","author":"phonchi","date":"03/13/2023"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}