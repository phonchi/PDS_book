{"title":"End-to-end Machine Learning project","markdown":{"yaml":{"title":"End-to-end Machine Learning project","author":"phonchi","date":"02/13/2023","format":{"html":{"toc":true,"code-line-numbers":true,"code-fold":true,"code-tools":true}}},"headingText":"Setup","containsRefs":false,"markdown":"\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/01_end_to_end_machine_learning_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/01_end_to_end_machine_learning_project.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>\n<br/>\n\n\nFirst, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20.\n\nRemember to restart the notebook to update the modules\n\n## Get the Data\n\n### Download the Data\n\nIt is preferable to create a small function to do that. It is useful in particular \n\n1. If data changes regularly, as it allows you to write a small script that you can run whenever you need to fetch the latest data (or you can set up a scheduled job to do that automatically at regular intervals).\n2. Automating the process of fetching the data is also useful if you need to install the dataset on multiple machines.\n\n\n \n\n\n\nHere is the function to fetch and load the data:\n\n### Take a Quick Look at the Data Structure\n\nNow let’s load the data using `Pandas`. Once again you should write a small function to load the data:\n\nhttps://www.kaggle.com/camnugent/california-housing-prices\n\nEach row represents one district. There are 10 attributes\n\nNotice that the `total_bedrooms` attribute has only 20,433 non-null values, meaning that 207 districts are missing this feature. We will need to take care of this later.\n\nAll attributes are numerical, except for `ocean_proximity`. Its type is object, so it could hold any kind of Python object. But since you loaded this data from a CSV file, you know that it must be a text attribute. When you looked at the top five rows, you probably noticed that the values in the `ocean_proximity` column were repetitive, which means that it is probably a categorical attribute. You can find out what categories exist and how many districts belong to each category by using the `value_counts()` method.\n\nNote that the null values are ignored below (so, for example, count of `total_bedrooms` is 20,433, not 20,640).\n\nAnother quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis).\n\nYou can either plot this one attribute at a time, or you can call the `hist()` method on the whole dataset (dataframe), and it will plot a histogram\nfor each numerical attribute:\n\nNotice a few things in these histograms:\n1. First, the median income attribute does not look like it is expressed in US dollars (USD). After checking with the team that collected the data, you are told that the data has been scaled and capped at 15 (actually 15.0001) for higher median incomes, and at 0.5 (actually 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about $30,000). Working with preprocessed attributes is common in Machine Learning, and it is not necessarily a problem, but you should try to understand how the data was computed.\n\n2. The housing median age and the median house value were also capped. The latter may be a serious problem since it is your target attribute (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond $500,000, then you have mainly two options:\n\n* Collect proper labels for the districts whose labels were capped.\n* Remove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond\n$500,000).\n\n3. These attributes have very different scales. We will discuss this later in this chapter when we explore feature scaling.\n\n4. Finally, many histograms are heavy tail and skewed right: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes later on to have more bell-shaped distributions.\n\n### Create a Test Set\n\nTo avoid the **data snooping bias**. Creating a test set! This is theoretically quite simple: just pick some instances randomly, typically 20% of the dataset (or less if your dataset is very large), and set them aside\n\n#### Optional\n\nSo far we have considered purely random sampling methods. This is generally fine if your dataset is large enough (especially relative to the number of attributes), but if it is not, you run the risk of introducing a significant sampling bias. When a survey company decides to call 1,000 people to ask them a few questions, they don’t just pick 1,000 people randomly in a phone book. They try to ensure that these 1,000 people are **representative** of the whole population. \n\nFor example, the US population is composed of 51.3% female and 48.7% male, so a well-conducted survey in the US would try to maintain this ratio in the sample: 513 female and 487 male. This is called stratified sampling: the population is divided into homogeneous subgroups called strata, and the right number of instances is sampled from each stratum to guarantee that the **test set is representative of the overall population**. If they used purely random sampling, the survey results may be significantly biased.\n\n**Suppose you chatted with experts who told you that the median income is a very important attribute to predict median housing prices**. You may want to ensure that the test set is representative of the various categories of incomes in the whole dataset. \n\nSince the median income is a continuous numerical attribute, you first need to create an income category attribute. Let’s look at the median income histogram more closely. \n\nMost median income values are clustered around 1.5 to 6 (i.e., $15,000 – $60,000), but some median incomes go far beyond 6. It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of the stratum’s importance may be biased. This means that you **should not have too many strata**, and each stratum should be large enough. The following code uses the `pd.cut()` function to create an income category attribute with 5 categories (labeled from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from 1.5 to 3, and so on:\n\nNow you are ready to do stratified sampling based on the income category. For this you can use Scikit-Learn’s `StratifiedShuffleSplit` class:\n\nhttps://scikit-learn.org/stable/modules/cross_validation.html#stratified-shuffle-split\n\nLet’s see if this worked as expected. You can start by looking at the income category proportions in the test set:\n\nWith similar code you can measure the income category proportions in the full dataset. Below we compare the income category proportions in the overall dataset, in the test set generated with stratified sampling, and in a test set generated using purely random sampling. As you can see, the test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is quite skewed.\n\nYou won’t use the income_cat column again, so you might as well drop it, reverting the data back to its original state:\n\n## Discover and Visualize the Data to Gain Insights\n\nLet’s create a copy so you can play with it without harming the training set\n\n### Visualizing Geographical Data\n\nSince there is geographical information (latitude and longitude), it is a good idea to create a scatterplot of all districts to visualize the data\n\nThis looks like California all right, but other than that it is hard to see any particular pattern. Setting the alpha option to 0.2 makes it much easier to visualize the places where there is a **high density of data points**\n\nNow that’s much better: you can clearly see the high-density areas, namely the Bay Area and around Los Angeles and San Diego, plus a long line of fairly high-density areas in the Central Valley.\n\nNow let’s look at the housing prices (Figure 2-13). The radius of each circle represents the district’s population (option `s`), and the color represents the price (option `c`). We will use a predefined color map (option `cmap`) called `jet`, which ranges from blue (low values) to red (high prices):\n\nThe argument `sharex=False` fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https://github.com/pandas-dev/pandas/issues/10611 ).\n\nThis image tells you that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density, as you probably knew already.\n\nIt will probably be useful to use a clustering algorithm to detect the main clusters, and add new features that measure the **proximity to the cluster centers**. The `ocean proximity` attribute may be useful as well, although in Northern California the housing prices in coastal districts are not too high, so it is not a simple rule.\n\n### Looking for Correlations\n\nSince the dataset is not too large, you can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes\n\nNow let’s look at how much each attribute correlates with the median house value:\n\nThe correlation coefficient ranges from –1 to 1. When it is close to 1, it means that there is a strong positive correlation; For example, the median house value tends to go up when the median income goes up. \n\nWhen the coefficient is close to –1, it means that there is a strong negative correlation; you can see a small negative correlation between the latitude and the median house value (i.e., prices have a slight tendency to go down when you go north). \n\nFinally, coefficients close to zero mean that there is no linear correlation. It may completely miss out on nonlinear relationships (e.g., “if x is close to zero then y generally goes up”)\n\nAnother way to check for correlation between attributes is to use Pandas’ `scatter_matrix` function, which plots every numerical attribute against every other numerical attribute. Since there are now 11 numerical attributes, you would get 121 plots (including index), which would not fit on a page, so let’s just focus on a few promising attributes that seem most correlated with the median housing value\n\nThe most promising attribute to predict the median house value is the median income, so let’s zoom in on their correlation scatterplot\n\nThis plot reveals a few things. \n\n1. First, the correlation is indeed very strong; you can clearly see the upward \ntrend and the points are not too dispersed. \n\n2. Second, the price cap that we noticed earlier is clearly visible as a horizontal line at `$500,000`. But this plot reveals other less obvious straight lines: a horizontal line around `$450,000`, another around `$350,000`, perhaps one around `$280,000`, and a few more below that. You may want to try removing the corresponding districts to prevent your algorithms from learning to reproduce these data quirks.\n\n### Experimenting with Attribute Combinations\n\nHopefully the previous sections gave you an idea of a few ways you can explore the data and gain insights. \n\n* We identified a few data quirks that you may want to clean up before feeding the data to a machine learning algorithm\n* We found interesting correlations between attributes, in particular with the target attribute\n* We also noticed that some attributes have a skewed-right distribution, so you may want to transform them (e.g., by computing their logarithm or square root). \n\nOf course, your mileage will vary considerably with each project, but the general ideas are similar.\n\nOne last thing you may want to do before actually preparing the data for Machine Learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you really want is the number of rooms per household. \n\nSimilarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. \n\nLet’s create these new attributes:\n\nAnd now let’s look at the correlation matrix again:\n\nHey, not bad! The new `bedrooms_ratio` attribute is much more correlated with the median house value than the total number of rooms or bedrooms. Apparently houses with a lower bedroom/room ratio tend to be more expensive. **The number of rooms per household is also more informative than the total number of rooms in a district—obviously the larger the houses, the more expensive they are.**\n\nThis round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first reasonably good prototype. But this is an **iterative process**: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step.\n\n## Prepare the Data for Machine Learning Algorithms\n\nLet’s separate the predictors and the labels since we don’t necessarily want to apply the same transformations to the predictors and the target values (note that `drop()` creates a copy of the data and does not affect `strat_train_set`):\n\n### Data Cleaning\n\nMost Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. You noticed earlier that the `total_bedrooms` attribute has some missing values, so let’s fix this. You have three options:\n\n* Get rid of the corresponding districts.\n* Get rid of the whole attribute.\n* Set the values to some value (zero, the mean, the median, etc.).\n\nIf you choose option 3, you should compute the median value on the training set, and use it to fill the missing values in the training set, but also don’t forget to **save the median value** that you have computed. You will need it later to replace missing values in the test set when you want to evaluate your system, and also once the system goes live to replace missing values in new data.\n\n`Scikit-Learn` provides a handy class to take care of missing values: `SimpleImputer`.\n\nSince the median can only be computed on numerical attributes, you then need to\ncreate a copy of the data with only the numerical attributes (this will exclude the text attribute `ocean_proximity`)\n\n> All objects in SKlearn share a consistent and simple interface:\n> 1. **Estimators**: Any object that can estimate some parameters based on a dataset is called an estimator (e.g., a `SimpleImputer` is an estimator). The estimation itself is performed by the `fit()` method, and it takes a dataset as a parameter, or two for supervised learning algorithms—the second dataset contains the labels. Any other parameter needed to guide the estimation process is considered a hyperparameter (such as a `SimpleImputer`’s strategy), and it must be set as an instance variable (generally via a constructor parameter).\n> 2. **Transformers**: Some estimators (such as a `SimpleImputer`) can also transform a dataset; these are called transformers. Once again, the API is simple: the transformation is performed by the `transform()` method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters, as is the case for a `SimpleImputer`. All transformers also have a convenience method called `fit_transform()`, which is equivalent to calling `fit()` and then `transform()` (but sometimes `fit_transform()` is optimized and runs much faster).\n> 3. **Predictors**: Finally, some estimators, given a dataset, are capable of making predictions; they are called predictors. For example, the `LinearRegression` model was a predictor. A predictor has a `predict()` method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a `score()` method that measures the quality of the predictions, given a test set (and the corresponding labels, in the case of supervised learning algorithms).\n\n> As with all estimators, it is important to fit the scalers to the training\ndata only: never use `fit()` or `fit_transform()` for anything else than the training set. Once you have a trained scaler, you can then use it to `transform()` any other set, including the validation set, the test set, and new data.\n\nThe imputer has simply computed the median of each attribute and stored the result in its `statistics_` instance variable. Only the `total_bedrooms` attribute had missing values, **but we cannot be sure that there won’t be any missing values in new data after the system goes live**, so it is safer to apply the imputer to all the numerical attributes:\n\nNow you can use this “trained” imputer to transform the training set by replacing missing values with the learned medians:\n\nThe result is a plain NumPy array containing the transformed features. If you want to put it back into a Pandas DataFrame, it’s simple:\n\nSee https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py\n\n### Dealing with outlier (Optional)\n\nIf you wanted to drop outliers, you would run the following code:\n\n### Handling Text and Categorical Attributes\n\nSo far we have only dealt with numerical attributes, but your data may also contain text attributes. In this dataset, there is just one: the `ocean_proximity` attribute. Let’s look at its value for the first few instances:\n\nMost Machine Learning algorithms prefer to work with numbers anyway, so let’s convert these categories from text to numbers.\n\nYou can get the list of categories using the `categories_` instance variable. It is a list containing a 1D array of categories for each categorical attribute (in this case, a list containing a single array since there is just one categorical attribute):\n\nOne issue with this representation is that ML algorithms will **assume that two nearby values are more similar than two distant values**. This may be fine in some cases (e.g., for ordered categories such as “bad”, “average”, “good”, “excellent”), but it is obviously not the case for the `ocean_proximity` column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). \n\nTo fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is “<1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is called **one-hot encoding**, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called dummy attributes. Scikit-Learn provides a `OneHotEncoder` class to convert categorical values into one-hot vectors\n\nBy default, the `OneHotEncoder` class returns a sparse array, but we can convert it to a dense array if needed by calling the `toarray()` method:\n\nThe advantage of `OneHotEncoder` over `get_dummies()` is that \n\n1. It remembers which categories it was trained on. This is very important because once your model is in production, it should be fed exactly the same features as during training: no more, no less. \n\n2. `OneHotEncoder` is smarter: it will detect the unknown category and raise\nan exception. If you prefer, you can set the `handle_unknown` hyperparameter to\n`\"ignore\"`, in which case it will just represent the unknown category with zeros\n\nIf a categorical attribute has a large number of possible categories (e.g., country code, profession, species, etc.), then one-hot encoding will result in a large number of input features. **This may slow down training and degrade performance**. \n\nIf this happens, you may want to replace the categorical input with useful numerical features related to the categories: for example, you could replace the\n`ocean_proximity` feature with the distance to the ocean (similarly, a country code could be replaced with the country’s population and GDP per capita). \n\nAlternatively, you could replace each category with a learnable low dimensional vector called an embedding. Each category’s representation would be learned during training: this is an example of **representation learning**.\n\nWhen you fit any Scikit-Learn estimator using a DataFrame, the estimator stores the column names in the `feature_names_in_` attribute. Scikit-Learn then ensures that any DataFrame fed to this estimator after that (e.g., to `transform()` or `predict()`) has the same column names. Transformers also provide a `get_feature_names_out()` method that you can use to build a DataFrame around the transformer’s output:\n\n### Feature Scaling and Transformation\n\nOne of the most important transformations you need to apply to your data is feature scaling. With few exceptions, machine learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Without any scaling, most models will be biased toward ignoring the median income and focusing more on the number of rooms.\n\n#### Min-max scaling\n\nMin-max scaling (many people call this **normalization**) is the simplest: for each attribute, the values are shifted and rescaled so that they end up ranging from 0 to 1. This is performed by subtracting the min value and dividing by the difference between the min and the max. Scikit-Learn provides a transformer called `MinMaxScaler` for this. It has a `feature_range` hyperparameter that lets you change the range if, for some reason, you don’t want 0–1\n\n#### Standardization\n\nStandardization is different: first it subtracts the mean value (so standardized values have a zero mean), then it divides the result by the standard deviation (so standardized values have a standard deviation equal to 1). Unlike min-max scaling, standardization does not restrict values to a specific range. **However, standardization is much less affected by outliers**.\n\n#### Other scaling (Optional)\n\nWhen a feature’s distribution has a heavy tail (i.e., when values far from the mean are not exponentially rare), both min-max scaling and standardization will squash most values into a small range. Machine learning models generally don’t like this at all. So before you scale the feature, you should first transform it to shrink the heavy tail, and if possible to make the distribution roughly\nsymmetrical. For example, a common way to do this for positive features with a heavy tail to the right is to replace the feature with its **square root** (or raise the feature to a power between 0 and 1). If the feature has a really long and heavy tail, such as a power law distribution, then replacing the feature with its **logarithm** may help.\n\nFor example, the population feature roughly follows a power law\n\nAnother approach to handle heavy-tailed features consists in **bucketizing** the feature. This means chopping its distribution into roughly equal-sized buckets, and replacing each feature value with the index of the bucket it belongs to. For example, you could replace each value with its percentile. Bucketizing with equal-sized buckets results in a feature with an almost uniform distribution, so there’s no need for further scaling, or you can just divide by the number of buckets to force the values to the 0–1 range.\n\n> When a feature has a multimodal distribution (i.e., with two or more clear peaks, called modes), such as the `housing_median_age` feature, it can also be helpful to bucketize it, **but this time treating the bucket IDs as categories**, rather than as numerical values. This means that the bucket indices must be encoded, for example using a `OneHotEncoder` (so you usually don’t want to use too many buckets). This approach will allow the regression model to more easily learn different rules for different ranges of this feature value. \n\nFor example, perhaps houses built around 35 years ago have a peculiar style that fell out of fashion, and therefore they’re cheaper than their age\nalone would suggest.\n\nAnother approach to transforming multimodal distributions is to add a feature for each of the modes (at least the main ones), representing the similarity between the housing median age and that particular mode. The similarity measure is typically computed using a radial basis function (RBF). Using Scikit-Learn’s `rbf_kernel()` function, you can create a new Gaussian RBF feature measuring the similarity between the housing median age and 35:\n\nAs the chart shows, the new age similarity feature peaks at 35, right around the spike in the housing median age distribution: if this particular age group is well correlated with lower prices, there’s a good chance that this new feature will help.\n\n### Custom Transformers\n\nAlthough `Scikit-Learn` provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific\nattributes.\n\n#### Function with no training parameter (Optional)\n\nFor transformations that don’t require any training, you can just write a function that takes a NumPy array as input and outputs the transformed. Let’s create a log-transformer and apply it to the `population` feature:\n\n#### Transform with training parameter\n\n`FunctionTransformer` is very handy, but what if you would like your transformer to be trainable, learning some parameters in the `fit()` method and using them later in the `transform()` method? You can get `fit_transform()` for free by simply adding `TransformerMixin` as a base class: the default implementation will just call `fit()` and then `transform()`. If you\nadd `BaseEstimator` as a base class (and avoid using `*args` and `**kwargs` in your constructor), you will also get two extra methods: `get_params()` and `set_params()`. These will be useful for automatic hyperparameter tuning.\n\nFor example, the following code demonstrates custom transformer that uses a\nKMeans clusterer in the `fit()` method to identify the main clusters in the training data, and then uses `rbf_kernel()` in the `transform()` method to measure how similar each sample is to each cluster center:\n\nK-means is a clustering algorithm that locates clusters in the data. How many it searches for is controlled by the `n_clusters` hyperparameter. After training, the cluster centers are available via the `cluster_centers_` attribute. The `fit()` method of KMeans supports an optional argument `sample_weight`, which lets the user specify the relative weights of the samples. k-means is a stochastic algorithm, meaning that it relies on randomness to locate the clusters, so if you want reproducible results, you must set the `random_state` parameter. As you can see, despite the complexity of the task, the code is fairly straightforward. Now let’s use this\ncustom transformer:\n\nThis code creates a `ClusterSimilarity` transformer, setting the number of clusters to 10. Then it calls `fit_transform()` with the latitude and longitude of every district in the training set, weighting each district by its median house value. The transformer uses k-means to locate the clusters, then measures the Gaussian RBF similarity between each district and all 10 cluster centers. The result is a matrix with one row per district, and one column per cluster.\n\n> See https://www.kaggle.com/code/ryanholbrook/clustering-with-k-means for the rationale of using clustering results as features.\n\nThe figure shows the 10 cluster centers found by k-means. The districts are colored according to their geographic similarity to their closest cluster center. As you can see, most clusters are located in highly populated and expensive areas.\n\n### Transformation Pipelines\n\nAs you can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the `Pipeline` class to help with such sequences of transformations. Here is a small pipeline for numerical attributes, which will first impute then scale the input features:\n\nThe `Pipeline` constructor takes a list of name/estimator pairs (2-tuples) defining a sequence of steps. The names can be anything you like, as long as they are unique and don’t contain double underscores `(__)`. They will be useful later, when we discuss hyperparameter tuning. The estimators must all be transformers (i.e., they must have a `fit_transform()` method), except for the last one, which can be anything: a transformer, a predictor, or any other type of estimator.\n\nWhen you call the pipeline’s `fit()` method, it calls `fit_transform()` sequentially on all the transformers, passing the output of each call as the parameter to the next call until it reaches the final estimator, for which it just calls the `fit()` method. The pipeline exposes the same methods as the final estimator. In this example the last estimator is a `StandardScaler`, which is a transformer, so the pipeline also acts like a transformer. \n\nIf you call the pipeline’s `transform()` method, it will sequentially apply all the transformations to the data. If the last estimator were a predictor instead\nof a transformer, then the pipeline would have a `predict()` method rather than a `transform()` method. Calling it would sequentially apply all the transformations to the data and pass the result to the predictor’s `predict()` method.\n\nLet’s call the pipeline’s `fit_transform()` method and look at the output’s first two rows, rounded to two decimal places:\n\nSo far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer capable of handling all columns, applying the appropriate transformations to each column. For this, you can use a `ColumnTransformer`. For example, the following `ColumnTransformer` will apply `num_pipeline` (the one we just defined) to the numerical attributes and `cat_pipeline` to the categorical attribute:\n\nWe construct a `ColumnTransformer`. Its constructor requires a list of triplets (3-tuples), each containing a name (which must be unique and not contain double underscores), a transformer, and a list of names (or indices) of columns that\nthe transformer should be applied to.\n\n> Instead of using a transformer, you can specify the string \"drop\" if you want the columns to be dropped, or you can specify \"passthrough\" if you want the columns to be left untouched. By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but you can set the remainder hyperparameter to any transformer (or to \"passthrough\") if you want these columns to be handled differently.\n\nSince listing all the column names is not very convenient, Scikit-Learn provides\na `make_column_selector()` function that returns a selector function you can use\nto automatically select all the features of a given type, such as numerical or categorical.\n\nYou can pass this selector function to the `ColumnTransformer` instead of column names or indices. Moreover, if you don’t care about naming the transformers, you can use `make_column_transformer()`, which chooses the names for you.\n\nNow we’re ready to apply this `ColumnTransformer` to the housing data:\n\nOnce again this returns a NumPy array, but you can get the column names using\n`preprocessing.get_feature_names_out()` and wrap the data in a nice DataFrame\nas we did before.\n\nThe code that builds the pipeline to do all of this should look familiar to you by now:\n\n* Missing values in numerical features will be imputed by replacing them with the median, as most ML algorithms don’t expect missing values. In categorical features, missing values will be replaced by the most frequent category.\n\n* The categorical feature will be one-hot encoded, as most ML algorithms only\naccept numerical inputs.\n\n* A few ratio features will be computed and added: `bedrooms_ratio`,`rooms_per_house`, and `people_per_house`. Hopefully these will better correlate\nwith the median house value, and thereby help the ML models.\n\n* A few cluster similarity features will also be added. These will likely be more useful to the model than latitude and longitude.\n\n* Features with a long tail will be replaced by their logarithm, as most models\nprefer features with roughly uniform or Gaussian distributions.\n\n* All numerical features will be standardized, as most ML algorithms prefer when\nall features have roughly the same scale.\n\n> If you don’t want to name the transformers, you can use the `make_pipeline()` function instead `Pipeline`\n\nIf you run this `ColumnTransformer`, it performs all the transformations and outputs a NumPy array with 24 features:\n\n## Select and Train a Model\n\nAt last! You framed the problem, you got the data and explored it, you sampled a\ntraining set and a test set, and you wrote a preprocessing pipeline to automatically clean up and prepare your data for machine learning algorithms. You are now ready to select and train a machine learning model.\n\n### Training and Evaluating on the Training Set\n\nLet’s first train a Linear Regression model\n\nYou now have a working linear regression model. You try it out on the training\nset, looking at the first five predictions and comparing them to the labels:\n\nRemember that you chose to use the RMSE as your performance\nmeasure, so you want to measure this regression model’s RMSE on the whole training set using Scikit-Learn’s `mean_squared_error()` function, with the `squared` argument set to `False`:\n\nThis is better than nothing, but clearly not a great score: the `median_housing_values` of most districts range between `$120,000` and `$265,000`, so a typical prediction error of `$68,628` is really not very satisfying. This is an example of a model **underfitting** the training data. When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough. \n\nThe main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. This model is not regularized, which rules out the last option. You could try to add more features, but first you want to try a more complex\nmodel to see how it does. We decide to try a `DecisionTreeRegressor`, as this is a fairly powerful model capable of finding complex nonlinear relationships in the data.\n\nIt is much more likely that the model has badly **overfit** the data. How can you be sure? As we saw earlier, you don’t want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training, and part for model validation.\n\n### Better Evaluation Using Cross-Validation\n\nOne way to evaluate the Decision Tree model would be to use the `train_test_split` function to split the training set into a smaller training set and a **validation set**, then train your models against the smaller training set and evaluate them against the validation set. \n\nA great alternative is to use `Scikit-Learn`’s K-fold cross-validation feature. The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores:\n\nLet’s look at the results:\n\nNow the decision tree doesn’t look as good as it did earlier. In fact, it seems to perform almost as poorly as the linear regression model! Notice that cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The decision tree has an RMSE of about 66,868, with a standard deviation of about 2,061. You would not have this information if you just used one validation set! We know there’s an overfitting problem because\nthe training error is low (actually zero) while the validation error is high.\n\nLet’s compute the same scores for the Linear Regression model just to be sure\n\nLet’s try one last model now: the `RandomForestRegressor`.Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. \n\nBuilding a model on top of many other models is called **Ensemble Learning**, and it is often a great way to push ML algorithms even further.\n\nWow, this is much better: random forests really look very promising for this task! However, if you train a `RandomForest` and measure the RMSE on the training set, you will find roughly 17,474: that’s much lower, meaning that there’s still quite a lot of overfitting going on. \n\nPossible solutions are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data. Before you dive much deeper into random forests, however, you should try out many other models from various categories of machine learning algorithms (e.g., several support vector machines with different kernels, and possibly a neural network), without spending too much time tweaking the hyperparameters. **The goal is to shortlist a few (two to five) promising models.**\n\n## Fine-Tune Your Model\n\nLet’s assume that you now have a shortlist of promising models. You now need to fine-tune them. Let’s look at a few ways you can do that.\n\n### Grid Search\n\nOne way to do that would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.\n\nInstead you should get `Scikit-Learn`’s `GridSearchCV` to search for you. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation. For example, the following code searches for the best combination of hyperparameter values for the `RandomForestRegressor`:\n\nNotice that you can refer to any hyperparameter of any estimator in a pipeline,\neven if this estimator is nested deep inside several pipelines and `column transformers`. For example, when Scikit-Learn sees `\"preprocessing__geo__n_clusters\"`, **it splits this string at the double underscores**, then it looks for an estimator named `\"preprocessing\"` in the pipeline and finds the preprocessing `ColumnTransformer`. Next, it looks for a transformer named `\"geo\"` inside this `ColumnTransformer` and finds the `ClusterSimilarity` transformer we used on the latitude and longitude attributes. Then it finds this transformer’s `n_clusters` hyperparameter. Similarly, `random_forest__max_features` refers to the `max_features` hyperparameter of the estimator named `\"random_forest\"`, which is of course the `RandomForest` model\n\nThere are two dictionaries in this `param_grid`, so `GridSearchCV` will first evaluate all 3 × 3 = 9 combinations of `n_clusters` and `max_features` hyperparameter values specified in the first dict, then it will try all 2 × 3 = 6 combinations of hyperparameter values in the second dict. So in total the grid search will explore 9 + 6 = 15 combinations of hyperparameter values, and it will train the pipeline 3 times per combination, since we are using 3-fold cross validation. This means there will be a grand total of 15 × 3 = 45 rounds of training!\n\nYou can access the best estimator using `grid_search.best_estimator_`. If\n`GridSearchCV` is initialized with `refit=True` (which is the default), then once it finds the best estimator using cross-validation, it retrains it on the whole training set. This is usually a good idea, since feeding it more data will likely improve its performance.\n\nThe evaluation scores are available using `grid_search.cv_results_`. This is a dictionary, but if you wrap it in a DataFrame you get a nice list of all the test scores for each combination of hyperparameters and for each cross-validation split, as well as the mean test score across all splits:\n\nLet's look at the score of each hyperparameter combination tested during the grid search:\n\nThe mean test RMSE score for the best model is 44,042, which is better than the\nscore you got earlier using the default hyperparameter values (which was 47,019). Congratulations, you have successfully fine-tuned your best model!\n\n### Randomized Search\n\nThe grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use `RandomizedSearchCV` instead. This class can be used in much the same way as the `GridSearchCV` class, but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration.\n\nFor each hyperparameter, you must provide either a list of possible values, or a\nprobability distribution:\n\nTry 30 (`n_iter` × `cv`) random combinations of hyperparameters:\n\nAnother way to fine-tune your system is to try to combine the models that perform best. This is especially true if the individual models make very different types of errors.\n\n### Analyze the Best Models and Their Errors\n\nYou will often gain good insights on the problem by inspecting the best models. For example, the `RandomForestRegressor` can indicate the relative importance of each attribute for making accurate predictions\n\nWith this information, you may want to try dropping some of the less useful features (e.g., apparently only one ocean_proximity category is really useful, so you could try dropping the others)\n\nYou should also look at the specific errors that your system makes, then try to understand why it makes them and what could fix the problem (adding extra features or, on the contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\n\n> Now is also a good time to ensure that your model not only works well on average, but also on all categories of districts, whether they’re rural or urban, rich or poor, northern or southern, minority or not, etc. Creating subsets of your validation set for each category takes a bit of work, but it’s important: if your model performs poorly on a whole category of districts, then it should probably not be deployed until the issue is solved, or at least it should not be used to make predictions for that category, as it may do more harm than good!\n\n### Evaluate Your System on the Test Set\n\nAfter tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set. Just get the predictors and the labels from your test set, and run your `final_model` to transform the data and make predictions, then evaluate these predictions:\n\nIn some cases, such a point estimate of the generalization error will not be quite enough to convince you to launch: what if it is just 0.1% better than the model currently in production? You might want to have an idea of how precise this estimate is.\n\nFor this, you can compute a 95% confidence interval for the generalization error using `scipy.stats.t.interval()`:\n\nWe could compute the interval manually like this:\n\nAlternatively, we could use a z-scores rather than t-scores:\n\nThe performance will usually be slightly worse than what you measured using cross-validation if you did a lot of hyperparameter tuning (**because your system ends up fine-tuned to perform well on the validation data, and will likely not perform as well on unknown datasets**). It is not the case in this example, but when this happens you must resist the temptation to tweak the hyperparameters to make the numbers look good on the test set; the improvements would be unlikely to generalize to new data.\n\nNow comes the project prelaunch phase: you need to present your solution highlighting what you have learned, what worked and what did not, what assumptions were made, and what your system’s limitations are), document everything, and create nice presentations with clear visualizations and easy-to-remember statements (e.g., “the median income is the number one predictor of housing prices”). \n\nIn this California housing example, the final performance of the system is not much better than the experts’, but it may still be a good idea to launch it, especially if this frees up some time for the experts so they can work on more interesting and productive tasks.\n\n## Launch, Monitor, and Maintain Your System\n\nPerfect, you got approval to launch! You now need to get your solution ready for production (e.g., polish the code, write documentation and tests, and so on). Then you can deploy your model to your production environment. \n\nThe most basic way to do this is just to save the best model you trained, transfer the file to your production environment, and load it. To save the model, you can use the `joblib` library like this:\n\nOnce your model is transferred to production, you can load it and use it. For this you must first import any custom classes and functions the model relies on (which means transferring the code to production), then load the model using `joblib` and use it to make predictions:\n\nFor example, perhaps the model will be used within a website: the user will type in some data about a new district and click the Estimate Price button. This will send a query containing the data to the web server, which will forward it to your web application, and finally your code will simply call the model’s `predict()` method (**you want to load the model upon server startup**, rather than every time the model is used). \n\nAlternatively, you can wrap the model within a dedicated web service that your web application can query through a REST API. This makes it easier to upgrade your model to new versions without interrupting the main application. It also simplifies scaling, since you can start as many web services as needed and load-balance the requests coming from your web application across these web\nservices. Moreover, it allows your web application to use any language, not just Python.\n\nAnother popular strategy is to deploy your model to the cloud, for example on\nGoogle’s Vertex AI (formerly known as Google Cloud AI Platform and Google Cloud\nML Engine): just save your model using `joblib` and upload it to Google Cloud\nStorage (GCS), then head over to Vertex AI and create a new model version, pointing it to the GCS file. That’s it! This gives you a simple web service that takes care of load balancing and scaling for you.\n\nIt take JSON requests containing the input data (e.g., of a district) and returns JSON responses containing the predictions. You can then use this web service in your website (or whatever production environment you are using). As we will see in model serving lesson that we will use it on AI Platform is not much different from deploying Scikit-Learn models.\n\nBut deployment is not the end of the story. You also need to write **monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops**. This could be a steep drop, likely due to a broken component in your infrastructure, but be aware that it could also be a gentle decay that could easily go unnoticed for a long time. This is quite common because models tend to “rot” over time: indeed, the world changes, so if the model was trained with last year’s data, it may not be adapted to today’s data.\n\n### Deployment\n\nSo you need to monitor your model’s live performance. But how do you that? Well, it depends. In some cases, the model’s performance can be inferred from **downstream metrics**. For example, if your model is part of a recommender system and it suggests products that the users may be interested in, then it’s easy to monitor the number of recommended products sold each day. If this number drops (compared to non-recommended products), then the prime suspect is the model. This may be because the data pipeline is broken, or perhaps the\nmodel needs to be retrained on fresh data. \n\nHowever, it’s not always possible to determine the model’s performance without any human analysis. For example, suppose you trained an image classification model to detect several product defects on a production line. How can you get an alert if the model’s performance drops, before thousands of defective products get shipped to your clients? One solution is to send to human raters a sample of all the pictures that the model classified (especially pictures that the model wasn’t so sure about). Depending on the task, the raters may need to be experts, or they could be nonspecialists, such as workers on a crowdsourcing platform (e.g., Amazon Mechanical Turk). In some applications they could even be the users themselves, responding for example via surveys or repurposed captchas. Either way, you need to put in place a monitoring system (with or without human raters to evaluate the live model), as well as all the relevant processes to define what to do in case of failures and how to prepare for them.\n\nUnfortunately, this can be a lot of work. In fact, **it is often much more work than building and training a model**. If the data keeps evolving, you will need to update your datasets and retrain your model regularly. You should probably automate the whole process as much as possible. Here are a few things you can automate:\n\n* Collect fresh data regularly and label it (e.g., using human raters).\n* Write a script to train the model and fine-tune the hyperparameters automatically. This script could run automatically, for example every day or every week, depending on your needs.\n* Write another script that will evaluate both the new model and the previous model on the updated test set, and deploy the model to production if the performance has not decreased (if it did, make sure you investigate why). The script should probably test the performance of your model on various subsets of the test set, such as poor or rich districts, rural or urban districts, etc.\n\nYou should also make sure **you evaluate the model’s input data quality**. Sometimes performance will degrade slightly because of a poor-quality signal (e.g., a malfunctioning sensor sending random values, or another team’s output becoming stale), but it may take a while before your system’s performance degrades enough to trigger an alert. If you monitor your model’s inputs, you may catch this earlier. For example, you could trigger an alert if more and more inputs are missing a feature, or if its mean or standard deviation drifts too far from the training set, or a categorical feature starts containing new categories.\n\nFinally, make sure you keep **backups of every model** you create and have the process and tools in place to roll back to a previous model quickly, in case the new model starts failing badly for some reason. Having backups also makes it possible to easily compare new models with previous ones. Similarly, you should keep backups of every version of your datasets so that you can roll back to a previous dataset if the new one ever gets corrupted (e.g., if the fresh data that gets added to it turns out to be full of outliers). Having backups of your datasets also allows you to evaluate any model against any previous dataset.\n\nAs you can see, Machine Learning involves quite a lot of infrastructure, so don’t be surprised if your first ML project takes a lot of effort and time to build and deploy to production. Fortunately, once all the infrastructure is in place, going from idea to production will be much faster.\n\n## Low-code ML using `PyCaret`\n\n`PyCaret` is a high-level, low-code Python library that makes it easy to compare, train, evaluate, tune, and deploy machine learning models with only a few lines of code. \n\nAt its core, `PyCaret` is basically just a large wrapper over many data science libraries such as `Scikit-learn`, `Yellowbrick`, `SHAP`, `Optuna`, and `Spacy`. Yes, you could use these libraries for the same tasks, but if you don’t want to write a lot of code, PyCaret could save you a lot of time.\n\n[https://pycaret.readthedocs.io/en/latest/api/regression.html](https://pycaret.readthedocs.io/en/latest/api/regression.html)\n\nRemember to restart the notebook to update the modules\n\n### Get the data\n\nNow that we have the data, we can initialize a `PyCaret` experiment, which will preprocess the data and enable logging for all of the models that we will train on this dataset.\n\n### Explore the data\n\n`ydata-profiling` primary goal is to provide a one-line Exploratory Data Analysis (EDA) experience in a consistent and fast solution. Like pandas `df.describe()` function, that is so handy, `ydata-profiling` delivers an extended analysis of a DataFrame while allowing the data analysis to be exported in different formats such as html and json.\n\n[https://github.com/ydataai/ydata-profiling](https://github.com/ydataai/ydata-profiling)\n\nThe `setup()` function initializes the environment in pycaret and creates the transformation pipeline to prepare the data for modeling and deployment. `setup()` must be called before executing any other function in pycaret. It takes two mandatory parameters: a pandas dataframe and the name of the target column. All other parameters are optional and are used to customize the pre-processing pipeline.\n\nWhen `setup()` is executed, PyCaret's inference algorithm will automatically infer the data types for all features based on certain properties. The data type should be inferred correctly but this is not always the case. To account for this, PyCaret displays a table containing the features and their inferred data types after `setup()` is executed. \n\n> Ensuring that the data types are correct is of fundamental importance in PyCaret as it automatically performs a few pre-processing tasks which are imperative to any machine learning experiment. These tasks are performed differently for each data type which means it is very important for them to be correctly configured.\n\n[https://pycaret.gitbook.io/docs/get-started/functions/initialize](https://pycaret.gitbook.io/docs/get-started/functions/initialize)\n\nThe `eda()` function generates automated Exploratory Data Analysis (EDA) using the `AutoViz` library.\n\n[https://pycaret.gitbook.io/docs/get-started/functions/analyze#eda](https://pycaret.gitbook.io/docs/get-started/functions/analyze#eda)\n\n### Prepare the data\n\nIn order to demonstrate the `predict_model()` function on unseen data, a sample of 10% records has been withheld from the original dataset to be used for predictions. This should not be confused with a train/test split as this particular split is performed to simulate a real life scenario. Another way to think about this is that these records are not available at the time when the machine learning experiment was performed.\n\n[https://pycaret.gitbook.io/docs/get-started/preprocessing](https://pycaret.gitbook.io/docs/get-started/preprocessing)\n\nNote that starting from PyCaret 3.0, it supports OOP API.\n\n[https://www.kaggle.com/code/uyeanil/titanic-custom-transformer-pipeline-pycaret](https://www.kaggle.com/code/uyeanil/titanic-custom-transformer-pipeline-pycaret) as AutoML\n\n### Select and train a model\n\nComparing all models to evaluate performance is the recommended starting point for modeling once the setup is completed (unless you exactly know what kind of model you need, which is often not the case). This function trains all models in the model library and scores them using k-fold cross validation for metric evaluation. The output prints a score grid that shows average MAE, MSE, RMSE,R2, RMSLE and MAPE accross the folds (10 by default) along with training time.\n\n[https://pycaret.gitbook.io/docs/get-started/functions/train](https://pycaret.gitbook.io/docs/get-started/functions/train)\n\nThe score grid printed above highlights the highest performing metric for comparison purposes only. By passing sort parameter, `compare_models(sort = 'RMSE')` will sort the grid by RMSE (lower to higher since lower is better). We also change the fold parameter from the default value of 10 to a different value by using `compare_models(fold = 5)` which will compare all models on 5 fold cross validation. Reducing the number of folds will improve the training time. By default, `compare_models` return the best performing model based on default sort order but can be used to return a list of top N models by using `n_select` parameter. Notice that you can also use exclude parameter to block certain models.\n\nThere are 25 regressors available in the model library of PyCaret. To see list of all regressors either check the docstring or use `models()` function to see the library.\n\n`create_model()` is the most granular function in PyCaret and is often the foundation behind most of the PyCaret functionalities. As the name suggests this function trains and evaluates a model using cross validation that can be set with fold parameter. The output prints a score grid that shows MAE, MSE, RMSE, R2, RMSLE and MAPE by fold.\n\n### Fine-Tune your model\n\n[https://pycaret.gitbook.io/docs/get-started/functions/optimize](https://pycaret.gitbook.io/docs/get-started/functions/optimize)\n\n[https://pycaret.gitbook.io/docs/get-started/functions/others#automl](https://pycaret.gitbook.io/docs/get-started/functions/others#automl)\n\n[https://pycaret.gitbook.io/docs/get-started/functions/optimize#ensemble_model](https://pycaret.gitbook.io/docs/get-started/functions/optimize#ensemble_model)\n\nWhen a model is created using the create_model function it uses the default hyperparameters to train the model. In order to tune hyperparameters, the tune_model function is used. This function automatically tunes the hyperparameters of a model using Random Grid Search on a pre-defined search space. The output prints a score grid that shows MAE, MSE, RMSE, R2, RMSLE and MAPE by fold. To use the custom search grid, you can pass `custom_grid` parameter in the tune_model function.\n\n### Analyze the model\n\nBefore model finalization, the `plot_model()` function can be used to analyze the performance across different aspects such as Residuals Plot, Prediction Error, Feature Importance etc. This function takes a trained model object and returns a plot based on the test / hold-out set.\n\nThe `evaluate_model()` displays a user interface for analyzing the performance of a trained model. It calls the `plot_model()` function internally.\n\n[https://pycaret.gitbook.io/docs/get-started/functions/analyze](https://pycaret.gitbook.io/docs/get-started/functions/analyze)\n\nThe `interpret_model()` analyzes the predictions generated from a trained model. Most plots in this function are implemented based on the SHAP (Shapley Additive exPlanations)\n\n### Predict on test set\n\nBefore finalizing the model, it is advisable to perform one final check by predicting the test/hold-out set and reviewing the evaluation metrics.\n\n### Finalize model\n\nModel finalization is the last step in the experiment. A normal machine learning workflow in PyCaret starts with `setup()`, followed by comparing all models using `compare_models()` and shortlisting a few candidate models (based on the metric of interest) to perform several modeling techniques such as hyperparameter tuning, ensembling, stacking etc. This workflow will eventually lead you to the best model for use in making predictions on new and unseen data. The `finalize_model()` function fits the model onto the complete dataset including the test/hold-out sample (20% in this case). The purpose of this function is to train the model on the complete dataset before it is deployed in production.\n\nThe `predict_model()` function is also used to predict on the unseen dataset.  `data_unseen` is the variable created at the beginning of the tutorial and contains 10% of the original dataset which was never exposed to PyCaret\n\n### Save and load the model\n\nWe have now finished the experiment by finalizing the `tuned_lightgbm` model which is now stored in `final_lightgbm` variable. We have also used the model stored in `final_lightgbm` to predict `data_unseen`. This brings us to the end of our experiment, but one question is still to be asked: What happens when you have more new data to predict? Do you have to go through the entire experiment again? The answer is no, PyCaret's inbuilt function `save_model()` allows you to save the model along with entire transformation pipeline for later use.\n\nTo load a saved model at a future date in the same or an alternative environment, we would use PyCaret's `load_model()` function and then easily apply the saved model on new unseen data for prediction.\n\n### Deplpy\n\n[https://pycaret.gitbook.io/docs/get-started/functions/deploy](https://pycaret.gitbook.io/docs/get-started/functions/deploy)\n\n[https://pycaret.gitbook.io/docs/get-started/functions/others#get_config](https://pycaret.gitbook.io/docs/get-started/functions/others#get_config)\n\n## Reference\n\n1. [https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb](https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb)\n2. [https://pycaret.org/](https://pycaret.org/)\n3. [https://pycaret.readthedocs.io/en/latest/](https://pycaret.readthedocs.io/en/latest/)\n\n","srcMarkdownNoYaml":"\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/01_end_to_end_machine_learning_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/01_end_to_end_machine_learning_project.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>\n<br/>\n\n## Setup\n\nFirst, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20.\n\nRemember to restart the notebook to update the modules\n\n## Get the Data\n\n### Download the Data\n\nIt is preferable to create a small function to do that. It is useful in particular \n\n1. If data changes regularly, as it allows you to write a small script that you can run whenever you need to fetch the latest data (or you can set up a scheduled job to do that automatically at regular intervals).\n2. Automating the process of fetching the data is also useful if you need to install the dataset on multiple machines.\n\n\n \n\n\n\nHere is the function to fetch and load the data:\n\n### Take a Quick Look at the Data Structure\n\nNow let’s load the data using `Pandas`. Once again you should write a small function to load the data:\n\nhttps://www.kaggle.com/camnugent/california-housing-prices\n\nEach row represents one district. There are 10 attributes\n\nNotice that the `total_bedrooms` attribute has only 20,433 non-null values, meaning that 207 districts are missing this feature. We will need to take care of this later.\n\nAll attributes are numerical, except for `ocean_proximity`. Its type is object, so it could hold any kind of Python object. But since you loaded this data from a CSV file, you know that it must be a text attribute. When you looked at the top five rows, you probably noticed that the values in the `ocean_proximity` column were repetitive, which means that it is probably a categorical attribute. You can find out what categories exist and how many districts belong to each category by using the `value_counts()` method.\n\nNote that the null values are ignored below (so, for example, count of `total_bedrooms` is 20,433, not 20,640).\n\nAnother quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis).\n\nYou can either plot this one attribute at a time, or you can call the `hist()` method on the whole dataset (dataframe), and it will plot a histogram\nfor each numerical attribute:\n\nNotice a few things in these histograms:\n1. First, the median income attribute does not look like it is expressed in US dollars (USD). After checking with the team that collected the data, you are told that the data has been scaled and capped at 15 (actually 15.0001) for higher median incomes, and at 0.5 (actually 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about $30,000). Working with preprocessed attributes is common in Machine Learning, and it is not necessarily a problem, but you should try to understand how the data was computed.\n\n2. The housing median age and the median house value were also capped. The latter may be a serious problem since it is your target attribute (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond $500,000, then you have mainly two options:\n\n* Collect proper labels for the districts whose labels were capped.\n* Remove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond\n$500,000).\n\n3. These attributes have very different scales. We will discuss this later in this chapter when we explore feature scaling.\n\n4. Finally, many histograms are heavy tail and skewed right: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes later on to have more bell-shaped distributions.\n\n### Create a Test Set\n\nTo avoid the **data snooping bias**. Creating a test set! This is theoretically quite simple: just pick some instances randomly, typically 20% of the dataset (or less if your dataset is very large), and set them aside\n\n#### Optional\n\nSo far we have considered purely random sampling methods. This is generally fine if your dataset is large enough (especially relative to the number of attributes), but if it is not, you run the risk of introducing a significant sampling bias. When a survey company decides to call 1,000 people to ask them a few questions, they don’t just pick 1,000 people randomly in a phone book. They try to ensure that these 1,000 people are **representative** of the whole population. \n\nFor example, the US population is composed of 51.3% female and 48.7% male, so a well-conducted survey in the US would try to maintain this ratio in the sample: 513 female and 487 male. This is called stratified sampling: the population is divided into homogeneous subgroups called strata, and the right number of instances is sampled from each stratum to guarantee that the **test set is representative of the overall population**. If they used purely random sampling, the survey results may be significantly biased.\n\n**Suppose you chatted with experts who told you that the median income is a very important attribute to predict median housing prices**. You may want to ensure that the test set is representative of the various categories of incomes in the whole dataset. \n\nSince the median income is a continuous numerical attribute, you first need to create an income category attribute. Let’s look at the median income histogram more closely. \n\nMost median income values are clustered around 1.5 to 6 (i.e., $15,000 – $60,000), but some median incomes go far beyond 6. It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of the stratum’s importance may be biased. This means that you **should not have too many strata**, and each stratum should be large enough. The following code uses the `pd.cut()` function to create an income category attribute with 5 categories (labeled from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from 1.5 to 3, and so on:\n\nNow you are ready to do stratified sampling based on the income category. For this you can use Scikit-Learn’s `StratifiedShuffleSplit` class:\n\nhttps://scikit-learn.org/stable/modules/cross_validation.html#stratified-shuffle-split\n\nLet’s see if this worked as expected. You can start by looking at the income category proportions in the test set:\n\nWith similar code you can measure the income category proportions in the full dataset. Below we compare the income category proportions in the overall dataset, in the test set generated with stratified sampling, and in a test set generated using purely random sampling. As you can see, the test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is quite skewed.\n\nYou won’t use the income_cat column again, so you might as well drop it, reverting the data back to its original state:\n\n## Discover and Visualize the Data to Gain Insights\n\nLet’s create a copy so you can play with it without harming the training set\n\n### Visualizing Geographical Data\n\nSince there is geographical information (latitude and longitude), it is a good idea to create a scatterplot of all districts to visualize the data\n\nThis looks like California all right, but other than that it is hard to see any particular pattern. Setting the alpha option to 0.2 makes it much easier to visualize the places where there is a **high density of data points**\n\nNow that’s much better: you can clearly see the high-density areas, namely the Bay Area and around Los Angeles and San Diego, plus a long line of fairly high-density areas in the Central Valley.\n\nNow let’s look at the housing prices (Figure 2-13). The radius of each circle represents the district’s population (option `s`), and the color represents the price (option `c`). We will use a predefined color map (option `cmap`) called `jet`, which ranges from blue (low values) to red (high prices):\n\nThe argument `sharex=False` fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https://github.com/pandas-dev/pandas/issues/10611 ).\n\nThis image tells you that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density, as you probably knew already.\n\nIt will probably be useful to use a clustering algorithm to detect the main clusters, and add new features that measure the **proximity to the cluster centers**. The `ocean proximity` attribute may be useful as well, although in Northern California the housing prices in coastal districts are not too high, so it is not a simple rule.\n\n### Looking for Correlations\n\nSince the dataset is not too large, you can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes\n\nNow let’s look at how much each attribute correlates with the median house value:\n\nThe correlation coefficient ranges from –1 to 1. When it is close to 1, it means that there is a strong positive correlation; For example, the median house value tends to go up when the median income goes up. \n\nWhen the coefficient is close to –1, it means that there is a strong negative correlation; you can see a small negative correlation between the latitude and the median house value (i.e., prices have a slight tendency to go down when you go north). \n\nFinally, coefficients close to zero mean that there is no linear correlation. It may completely miss out on nonlinear relationships (e.g., “if x is close to zero then y generally goes up”)\n\nAnother way to check for correlation between attributes is to use Pandas’ `scatter_matrix` function, which plots every numerical attribute against every other numerical attribute. Since there are now 11 numerical attributes, you would get 121 plots (including index), which would not fit on a page, so let’s just focus on a few promising attributes that seem most correlated with the median housing value\n\nThe most promising attribute to predict the median house value is the median income, so let’s zoom in on their correlation scatterplot\n\nThis plot reveals a few things. \n\n1. First, the correlation is indeed very strong; you can clearly see the upward \ntrend and the points are not too dispersed. \n\n2. Second, the price cap that we noticed earlier is clearly visible as a horizontal line at `$500,000`. But this plot reveals other less obvious straight lines: a horizontal line around `$450,000`, another around `$350,000`, perhaps one around `$280,000`, and a few more below that. You may want to try removing the corresponding districts to prevent your algorithms from learning to reproduce these data quirks.\n\n### Experimenting with Attribute Combinations\n\nHopefully the previous sections gave you an idea of a few ways you can explore the data and gain insights. \n\n* We identified a few data quirks that you may want to clean up before feeding the data to a machine learning algorithm\n* We found interesting correlations between attributes, in particular with the target attribute\n* We also noticed that some attributes have a skewed-right distribution, so you may want to transform them (e.g., by computing their logarithm or square root). \n\nOf course, your mileage will vary considerably with each project, but the general ideas are similar.\n\nOne last thing you may want to do before actually preparing the data for Machine Learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you really want is the number of rooms per household. \n\nSimilarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. \n\nLet’s create these new attributes:\n\nAnd now let’s look at the correlation matrix again:\n\nHey, not bad! The new `bedrooms_ratio` attribute is much more correlated with the median house value than the total number of rooms or bedrooms. Apparently houses with a lower bedroom/room ratio tend to be more expensive. **The number of rooms per household is also more informative than the total number of rooms in a district—obviously the larger the houses, the more expensive they are.**\n\nThis round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first reasonably good prototype. But this is an **iterative process**: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step.\n\n## Prepare the Data for Machine Learning Algorithms\n\nLet’s separate the predictors and the labels since we don’t necessarily want to apply the same transformations to the predictors and the target values (note that `drop()` creates a copy of the data and does not affect `strat_train_set`):\n\n### Data Cleaning\n\nMost Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. You noticed earlier that the `total_bedrooms` attribute has some missing values, so let’s fix this. You have three options:\n\n* Get rid of the corresponding districts.\n* Get rid of the whole attribute.\n* Set the values to some value (zero, the mean, the median, etc.).\n\nIf you choose option 3, you should compute the median value on the training set, and use it to fill the missing values in the training set, but also don’t forget to **save the median value** that you have computed. You will need it later to replace missing values in the test set when you want to evaluate your system, and also once the system goes live to replace missing values in new data.\n\n`Scikit-Learn` provides a handy class to take care of missing values: `SimpleImputer`.\n\nSince the median can only be computed on numerical attributes, you then need to\ncreate a copy of the data with only the numerical attributes (this will exclude the text attribute `ocean_proximity`)\n\n> All objects in SKlearn share a consistent and simple interface:\n> 1. **Estimators**: Any object that can estimate some parameters based on a dataset is called an estimator (e.g., a `SimpleImputer` is an estimator). The estimation itself is performed by the `fit()` method, and it takes a dataset as a parameter, or two for supervised learning algorithms—the second dataset contains the labels. Any other parameter needed to guide the estimation process is considered a hyperparameter (such as a `SimpleImputer`’s strategy), and it must be set as an instance variable (generally via a constructor parameter).\n> 2. **Transformers**: Some estimators (such as a `SimpleImputer`) can also transform a dataset; these are called transformers. Once again, the API is simple: the transformation is performed by the `transform()` method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters, as is the case for a `SimpleImputer`. All transformers also have a convenience method called `fit_transform()`, which is equivalent to calling `fit()` and then `transform()` (but sometimes `fit_transform()` is optimized and runs much faster).\n> 3. **Predictors**: Finally, some estimators, given a dataset, are capable of making predictions; they are called predictors. For example, the `LinearRegression` model was a predictor. A predictor has a `predict()` method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a `score()` method that measures the quality of the predictions, given a test set (and the corresponding labels, in the case of supervised learning algorithms).\n\n> As with all estimators, it is important to fit the scalers to the training\ndata only: never use `fit()` or `fit_transform()` for anything else than the training set. Once you have a trained scaler, you can then use it to `transform()` any other set, including the validation set, the test set, and new data.\n\nThe imputer has simply computed the median of each attribute and stored the result in its `statistics_` instance variable. Only the `total_bedrooms` attribute had missing values, **but we cannot be sure that there won’t be any missing values in new data after the system goes live**, so it is safer to apply the imputer to all the numerical attributes:\n\nNow you can use this “trained” imputer to transform the training set by replacing missing values with the learned medians:\n\nThe result is a plain NumPy array containing the transformed features. If you want to put it back into a Pandas DataFrame, it’s simple:\n\nSee https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py\n\n### Dealing with outlier (Optional)\n\nIf you wanted to drop outliers, you would run the following code:\n\n### Handling Text and Categorical Attributes\n\nSo far we have only dealt with numerical attributes, but your data may also contain text attributes. In this dataset, there is just one: the `ocean_proximity` attribute. Let’s look at its value for the first few instances:\n\nMost Machine Learning algorithms prefer to work with numbers anyway, so let’s convert these categories from text to numbers.\n\nYou can get the list of categories using the `categories_` instance variable. It is a list containing a 1D array of categories for each categorical attribute (in this case, a list containing a single array since there is just one categorical attribute):\n\nOne issue with this representation is that ML algorithms will **assume that two nearby values are more similar than two distant values**. This may be fine in some cases (e.g., for ordered categories such as “bad”, “average”, “good”, “excellent”), but it is obviously not the case for the `ocean_proximity` column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). \n\nTo fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is “<1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is called **one-hot encoding**, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called dummy attributes. Scikit-Learn provides a `OneHotEncoder` class to convert categorical values into one-hot vectors\n\nBy default, the `OneHotEncoder` class returns a sparse array, but we can convert it to a dense array if needed by calling the `toarray()` method:\n\nThe advantage of `OneHotEncoder` over `get_dummies()` is that \n\n1. It remembers which categories it was trained on. This is very important because once your model is in production, it should be fed exactly the same features as during training: no more, no less. \n\n2. `OneHotEncoder` is smarter: it will detect the unknown category and raise\nan exception. If you prefer, you can set the `handle_unknown` hyperparameter to\n`\"ignore\"`, in which case it will just represent the unknown category with zeros\n\nIf a categorical attribute has a large number of possible categories (e.g., country code, profession, species, etc.), then one-hot encoding will result in a large number of input features. **This may slow down training and degrade performance**. \n\nIf this happens, you may want to replace the categorical input with useful numerical features related to the categories: for example, you could replace the\n`ocean_proximity` feature with the distance to the ocean (similarly, a country code could be replaced with the country’s population and GDP per capita). \n\nAlternatively, you could replace each category with a learnable low dimensional vector called an embedding. Each category’s representation would be learned during training: this is an example of **representation learning**.\n\nWhen you fit any Scikit-Learn estimator using a DataFrame, the estimator stores the column names in the `feature_names_in_` attribute. Scikit-Learn then ensures that any DataFrame fed to this estimator after that (e.g., to `transform()` or `predict()`) has the same column names. Transformers also provide a `get_feature_names_out()` method that you can use to build a DataFrame around the transformer’s output:\n\n### Feature Scaling and Transformation\n\nOne of the most important transformations you need to apply to your data is feature scaling. With few exceptions, machine learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Without any scaling, most models will be biased toward ignoring the median income and focusing more on the number of rooms.\n\n#### Min-max scaling\n\nMin-max scaling (many people call this **normalization**) is the simplest: for each attribute, the values are shifted and rescaled so that they end up ranging from 0 to 1. This is performed by subtracting the min value and dividing by the difference between the min and the max. Scikit-Learn provides a transformer called `MinMaxScaler` for this. It has a `feature_range` hyperparameter that lets you change the range if, for some reason, you don’t want 0–1\n\n#### Standardization\n\nStandardization is different: first it subtracts the mean value (so standardized values have a zero mean), then it divides the result by the standard deviation (so standardized values have a standard deviation equal to 1). Unlike min-max scaling, standardization does not restrict values to a specific range. **However, standardization is much less affected by outliers**.\n\n#### Other scaling (Optional)\n\nWhen a feature’s distribution has a heavy tail (i.e., when values far from the mean are not exponentially rare), both min-max scaling and standardization will squash most values into a small range. Machine learning models generally don’t like this at all. So before you scale the feature, you should first transform it to shrink the heavy tail, and if possible to make the distribution roughly\nsymmetrical. For example, a common way to do this for positive features with a heavy tail to the right is to replace the feature with its **square root** (or raise the feature to a power between 0 and 1). If the feature has a really long and heavy tail, such as a power law distribution, then replacing the feature with its **logarithm** may help.\n\nFor example, the population feature roughly follows a power law\n\nAnother approach to handle heavy-tailed features consists in **bucketizing** the feature. This means chopping its distribution into roughly equal-sized buckets, and replacing each feature value with the index of the bucket it belongs to. For example, you could replace each value with its percentile. Bucketizing with equal-sized buckets results in a feature with an almost uniform distribution, so there’s no need for further scaling, or you can just divide by the number of buckets to force the values to the 0–1 range.\n\n> When a feature has a multimodal distribution (i.e., with two or more clear peaks, called modes), such as the `housing_median_age` feature, it can also be helpful to bucketize it, **but this time treating the bucket IDs as categories**, rather than as numerical values. This means that the bucket indices must be encoded, for example using a `OneHotEncoder` (so you usually don’t want to use too many buckets). This approach will allow the regression model to more easily learn different rules for different ranges of this feature value. \n\nFor example, perhaps houses built around 35 years ago have a peculiar style that fell out of fashion, and therefore they’re cheaper than their age\nalone would suggest.\n\nAnother approach to transforming multimodal distributions is to add a feature for each of the modes (at least the main ones), representing the similarity between the housing median age and that particular mode. The similarity measure is typically computed using a radial basis function (RBF). Using Scikit-Learn’s `rbf_kernel()` function, you can create a new Gaussian RBF feature measuring the similarity between the housing median age and 35:\n\nAs the chart shows, the new age similarity feature peaks at 35, right around the spike in the housing median age distribution: if this particular age group is well correlated with lower prices, there’s a good chance that this new feature will help.\n\n### Custom Transformers\n\nAlthough `Scikit-Learn` provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific\nattributes.\n\n#### Function with no training parameter (Optional)\n\nFor transformations that don’t require any training, you can just write a function that takes a NumPy array as input and outputs the transformed. Let’s create a log-transformer and apply it to the `population` feature:\n\n#### Transform with training parameter\n\n`FunctionTransformer` is very handy, but what if you would like your transformer to be trainable, learning some parameters in the `fit()` method and using them later in the `transform()` method? You can get `fit_transform()` for free by simply adding `TransformerMixin` as a base class: the default implementation will just call `fit()` and then `transform()`. If you\nadd `BaseEstimator` as a base class (and avoid using `*args` and `**kwargs` in your constructor), you will also get two extra methods: `get_params()` and `set_params()`. These will be useful for automatic hyperparameter tuning.\n\nFor example, the following code demonstrates custom transformer that uses a\nKMeans clusterer in the `fit()` method to identify the main clusters in the training data, and then uses `rbf_kernel()` in the `transform()` method to measure how similar each sample is to each cluster center:\n\nK-means is a clustering algorithm that locates clusters in the data. How many it searches for is controlled by the `n_clusters` hyperparameter. After training, the cluster centers are available via the `cluster_centers_` attribute. The `fit()` method of KMeans supports an optional argument `sample_weight`, which lets the user specify the relative weights of the samples. k-means is a stochastic algorithm, meaning that it relies on randomness to locate the clusters, so if you want reproducible results, you must set the `random_state` parameter. As you can see, despite the complexity of the task, the code is fairly straightforward. Now let’s use this\ncustom transformer:\n\nThis code creates a `ClusterSimilarity` transformer, setting the number of clusters to 10. Then it calls `fit_transform()` with the latitude and longitude of every district in the training set, weighting each district by its median house value. The transformer uses k-means to locate the clusters, then measures the Gaussian RBF similarity between each district and all 10 cluster centers. The result is a matrix with one row per district, and one column per cluster.\n\n> See https://www.kaggle.com/code/ryanholbrook/clustering-with-k-means for the rationale of using clustering results as features.\n\nThe figure shows the 10 cluster centers found by k-means. The districts are colored according to their geographic similarity to their closest cluster center. As you can see, most clusters are located in highly populated and expensive areas.\n\n### Transformation Pipelines\n\nAs you can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the `Pipeline` class to help with such sequences of transformations. Here is a small pipeline for numerical attributes, which will first impute then scale the input features:\n\nThe `Pipeline` constructor takes a list of name/estimator pairs (2-tuples) defining a sequence of steps. The names can be anything you like, as long as they are unique and don’t contain double underscores `(__)`. They will be useful later, when we discuss hyperparameter tuning. The estimators must all be transformers (i.e., they must have a `fit_transform()` method), except for the last one, which can be anything: a transformer, a predictor, or any other type of estimator.\n\nWhen you call the pipeline’s `fit()` method, it calls `fit_transform()` sequentially on all the transformers, passing the output of each call as the parameter to the next call until it reaches the final estimator, for which it just calls the `fit()` method. The pipeline exposes the same methods as the final estimator. In this example the last estimator is a `StandardScaler`, which is a transformer, so the pipeline also acts like a transformer. \n\nIf you call the pipeline’s `transform()` method, it will sequentially apply all the transformations to the data. If the last estimator were a predictor instead\nof a transformer, then the pipeline would have a `predict()` method rather than a `transform()` method. Calling it would sequentially apply all the transformations to the data and pass the result to the predictor’s `predict()` method.\n\nLet’s call the pipeline’s `fit_transform()` method and look at the output’s first two rows, rounded to two decimal places:\n\nSo far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer capable of handling all columns, applying the appropriate transformations to each column. For this, you can use a `ColumnTransformer`. For example, the following `ColumnTransformer` will apply `num_pipeline` (the one we just defined) to the numerical attributes and `cat_pipeline` to the categorical attribute:\n\nWe construct a `ColumnTransformer`. Its constructor requires a list of triplets (3-tuples), each containing a name (which must be unique and not contain double underscores), a transformer, and a list of names (or indices) of columns that\nthe transformer should be applied to.\n\n> Instead of using a transformer, you can specify the string \"drop\" if you want the columns to be dropped, or you can specify \"passthrough\" if you want the columns to be left untouched. By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but you can set the remainder hyperparameter to any transformer (or to \"passthrough\") if you want these columns to be handled differently.\n\nSince listing all the column names is not very convenient, Scikit-Learn provides\na `make_column_selector()` function that returns a selector function you can use\nto automatically select all the features of a given type, such as numerical or categorical.\n\nYou can pass this selector function to the `ColumnTransformer` instead of column names or indices. Moreover, if you don’t care about naming the transformers, you can use `make_column_transformer()`, which chooses the names for you.\n\nNow we’re ready to apply this `ColumnTransformer` to the housing data:\n\nOnce again this returns a NumPy array, but you can get the column names using\n`preprocessing.get_feature_names_out()` and wrap the data in a nice DataFrame\nas we did before.\n\nThe code that builds the pipeline to do all of this should look familiar to you by now:\n\n* Missing values in numerical features will be imputed by replacing them with the median, as most ML algorithms don’t expect missing values. In categorical features, missing values will be replaced by the most frequent category.\n\n* The categorical feature will be one-hot encoded, as most ML algorithms only\naccept numerical inputs.\n\n* A few ratio features will be computed and added: `bedrooms_ratio`,`rooms_per_house`, and `people_per_house`. Hopefully these will better correlate\nwith the median house value, and thereby help the ML models.\n\n* A few cluster similarity features will also be added. These will likely be more useful to the model than latitude and longitude.\n\n* Features with a long tail will be replaced by their logarithm, as most models\nprefer features with roughly uniform or Gaussian distributions.\n\n* All numerical features will be standardized, as most ML algorithms prefer when\nall features have roughly the same scale.\n\n> If you don’t want to name the transformers, you can use the `make_pipeline()` function instead `Pipeline`\n\nIf you run this `ColumnTransformer`, it performs all the transformations and outputs a NumPy array with 24 features:\n\n## Select and Train a Model\n\nAt last! You framed the problem, you got the data and explored it, you sampled a\ntraining set and a test set, and you wrote a preprocessing pipeline to automatically clean up and prepare your data for machine learning algorithms. You are now ready to select and train a machine learning model.\n\n### Training and Evaluating on the Training Set\n\nLet’s first train a Linear Regression model\n\nYou now have a working linear regression model. You try it out on the training\nset, looking at the first five predictions and comparing them to the labels:\n\nRemember that you chose to use the RMSE as your performance\nmeasure, so you want to measure this regression model’s RMSE on the whole training set using Scikit-Learn’s `mean_squared_error()` function, with the `squared` argument set to `False`:\n\nThis is better than nothing, but clearly not a great score: the `median_housing_values` of most districts range between `$120,000` and `$265,000`, so a typical prediction error of `$68,628` is really not very satisfying. This is an example of a model **underfitting** the training data. When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough. \n\nThe main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. This model is not regularized, which rules out the last option. You could try to add more features, but first you want to try a more complex\nmodel to see how it does. We decide to try a `DecisionTreeRegressor`, as this is a fairly powerful model capable of finding complex nonlinear relationships in the data.\n\nIt is much more likely that the model has badly **overfit** the data. How can you be sure? As we saw earlier, you don’t want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training, and part for model validation.\n\n### Better Evaluation Using Cross-Validation\n\nOne way to evaluate the Decision Tree model would be to use the `train_test_split` function to split the training set into a smaller training set and a **validation set**, then train your models against the smaller training set and evaluate them against the validation set. \n\nA great alternative is to use `Scikit-Learn`’s K-fold cross-validation feature. The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores:\n\nLet’s look at the results:\n\nNow the decision tree doesn’t look as good as it did earlier. In fact, it seems to perform almost as poorly as the linear regression model! Notice that cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The decision tree has an RMSE of about 66,868, with a standard deviation of about 2,061. You would not have this information if you just used one validation set! We know there’s an overfitting problem because\nthe training error is low (actually zero) while the validation error is high.\n\nLet’s compute the same scores for the Linear Regression model just to be sure\n\nLet’s try one last model now: the `RandomForestRegressor`.Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. \n\nBuilding a model on top of many other models is called **Ensemble Learning**, and it is often a great way to push ML algorithms even further.\n\nWow, this is much better: random forests really look very promising for this task! However, if you train a `RandomForest` and measure the RMSE on the training set, you will find roughly 17,474: that’s much lower, meaning that there’s still quite a lot of overfitting going on. \n\nPossible solutions are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data. Before you dive much deeper into random forests, however, you should try out many other models from various categories of machine learning algorithms (e.g., several support vector machines with different kernels, and possibly a neural network), without spending too much time tweaking the hyperparameters. **The goal is to shortlist a few (two to five) promising models.**\n\n## Fine-Tune Your Model\n\nLet’s assume that you now have a shortlist of promising models. You now need to fine-tune them. Let’s look at a few ways you can do that.\n\n### Grid Search\n\nOne way to do that would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.\n\nInstead you should get `Scikit-Learn`’s `GridSearchCV` to search for you. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation. For example, the following code searches for the best combination of hyperparameter values for the `RandomForestRegressor`:\n\nNotice that you can refer to any hyperparameter of any estimator in a pipeline,\neven if this estimator is nested deep inside several pipelines and `column transformers`. For example, when Scikit-Learn sees `\"preprocessing__geo__n_clusters\"`, **it splits this string at the double underscores**, then it looks for an estimator named `\"preprocessing\"` in the pipeline and finds the preprocessing `ColumnTransformer`. Next, it looks for a transformer named `\"geo\"` inside this `ColumnTransformer` and finds the `ClusterSimilarity` transformer we used on the latitude and longitude attributes. Then it finds this transformer’s `n_clusters` hyperparameter. Similarly, `random_forest__max_features` refers to the `max_features` hyperparameter of the estimator named `\"random_forest\"`, which is of course the `RandomForest` model\n\nThere are two dictionaries in this `param_grid`, so `GridSearchCV` will first evaluate all 3 × 3 = 9 combinations of `n_clusters` and `max_features` hyperparameter values specified in the first dict, then it will try all 2 × 3 = 6 combinations of hyperparameter values in the second dict. So in total the grid search will explore 9 + 6 = 15 combinations of hyperparameter values, and it will train the pipeline 3 times per combination, since we are using 3-fold cross validation. This means there will be a grand total of 15 × 3 = 45 rounds of training!\n\nYou can access the best estimator using `grid_search.best_estimator_`. If\n`GridSearchCV` is initialized with `refit=True` (which is the default), then once it finds the best estimator using cross-validation, it retrains it on the whole training set. This is usually a good idea, since feeding it more data will likely improve its performance.\n\nThe evaluation scores are available using `grid_search.cv_results_`. This is a dictionary, but if you wrap it in a DataFrame you get a nice list of all the test scores for each combination of hyperparameters and for each cross-validation split, as well as the mean test score across all splits:\n\nLet's look at the score of each hyperparameter combination tested during the grid search:\n\nThe mean test RMSE score for the best model is 44,042, which is better than the\nscore you got earlier using the default hyperparameter values (which was 47,019). Congratulations, you have successfully fine-tuned your best model!\n\n### Randomized Search\n\nThe grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use `RandomizedSearchCV` instead. This class can be used in much the same way as the `GridSearchCV` class, but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration.\n\nFor each hyperparameter, you must provide either a list of possible values, or a\nprobability distribution:\n\nTry 30 (`n_iter` × `cv`) random combinations of hyperparameters:\n\nAnother way to fine-tune your system is to try to combine the models that perform best. This is especially true if the individual models make very different types of errors.\n\n### Analyze the Best Models and Their Errors\n\nYou will often gain good insights on the problem by inspecting the best models. For example, the `RandomForestRegressor` can indicate the relative importance of each attribute for making accurate predictions\n\nWith this information, you may want to try dropping some of the less useful features (e.g., apparently only one ocean_proximity category is really useful, so you could try dropping the others)\n\nYou should also look at the specific errors that your system makes, then try to understand why it makes them and what could fix the problem (adding extra features or, on the contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\n\n> Now is also a good time to ensure that your model not only works well on average, but also on all categories of districts, whether they’re rural or urban, rich or poor, northern or southern, minority or not, etc. Creating subsets of your validation set for each category takes a bit of work, but it’s important: if your model performs poorly on a whole category of districts, then it should probably not be deployed until the issue is solved, or at least it should not be used to make predictions for that category, as it may do more harm than good!\n\n### Evaluate Your System on the Test Set\n\nAfter tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set. Just get the predictors and the labels from your test set, and run your `final_model` to transform the data and make predictions, then evaluate these predictions:\n\nIn some cases, such a point estimate of the generalization error will not be quite enough to convince you to launch: what if it is just 0.1% better than the model currently in production? You might want to have an idea of how precise this estimate is.\n\nFor this, you can compute a 95% confidence interval for the generalization error using `scipy.stats.t.interval()`:\n\nWe could compute the interval manually like this:\n\nAlternatively, we could use a z-scores rather than t-scores:\n\nThe performance will usually be slightly worse than what you measured using cross-validation if you did a lot of hyperparameter tuning (**because your system ends up fine-tuned to perform well on the validation data, and will likely not perform as well on unknown datasets**). It is not the case in this example, but when this happens you must resist the temptation to tweak the hyperparameters to make the numbers look good on the test set; the improvements would be unlikely to generalize to new data.\n\nNow comes the project prelaunch phase: you need to present your solution highlighting what you have learned, what worked and what did not, what assumptions were made, and what your system’s limitations are), document everything, and create nice presentations with clear visualizations and easy-to-remember statements (e.g., “the median income is the number one predictor of housing prices”). \n\nIn this California housing example, the final performance of the system is not much better than the experts’, but it may still be a good idea to launch it, especially if this frees up some time for the experts so they can work on more interesting and productive tasks.\n\n## Launch, Monitor, and Maintain Your System\n\nPerfect, you got approval to launch! You now need to get your solution ready for production (e.g., polish the code, write documentation and tests, and so on). Then you can deploy your model to your production environment. \n\nThe most basic way to do this is just to save the best model you trained, transfer the file to your production environment, and load it. To save the model, you can use the `joblib` library like this:\n\nOnce your model is transferred to production, you can load it and use it. For this you must first import any custom classes and functions the model relies on (which means transferring the code to production), then load the model using `joblib` and use it to make predictions:\n\nFor example, perhaps the model will be used within a website: the user will type in some data about a new district and click the Estimate Price button. This will send a query containing the data to the web server, which will forward it to your web application, and finally your code will simply call the model’s `predict()` method (**you want to load the model upon server startup**, rather than every time the model is used). \n\nAlternatively, you can wrap the model within a dedicated web service that your web application can query through a REST API. This makes it easier to upgrade your model to new versions without interrupting the main application. It also simplifies scaling, since you can start as many web services as needed and load-balance the requests coming from your web application across these web\nservices. Moreover, it allows your web application to use any language, not just Python.\n\nAnother popular strategy is to deploy your model to the cloud, for example on\nGoogle’s Vertex AI (formerly known as Google Cloud AI Platform and Google Cloud\nML Engine): just save your model using `joblib` and upload it to Google Cloud\nStorage (GCS), then head over to Vertex AI and create a new model version, pointing it to the GCS file. That’s it! This gives you a simple web service that takes care of load balancing and scaling for you.\n\nIt take JSON requests containing the input data (e.g., of a district) and returns JSON responses containing the predictions. You can then use this web service in your website (or whatever production environment you are using). As we will see in model serving lesson that we will use it on AI Platform is not much different from deploying Scikit-Learn models.\n\nBut deployment is not the end of the story. You also need to write **monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops**. This could be a steep drop, likely due to a broken component in your infrastructure, but be aware that it could also be a gentle decay that could easily go unnoticed for a long time. This is quite common because models tend to “rot” over time: indeed, the world changes, so if the model was trained with last year’s data, it may not be adapted to today’s data.\n\n### Deployment\n\nSo you need to monitor your model’s live performance. But how do you that? Well, it depends. In some cases, the model’s performance can be inferred from **downstream metrics**. For example, if your model is part of a recommender system and it suggests products that the users may be interested in, then it’s easy to monitor the number of recommended products sold each day. If this number drops (compared to non-recommended products), then the prime suspect is the model. This may be because the data pipeline is broken, or perhaps the\nmodel needs to be retrained on fresh data. \n\nHowever, it’s not always possible to determine the model’s performance without any human analysis. For example, suppose you trained an image classification model to detect several product defects on a production line. How can you get an alert if the model’s performance drops, before thousands of defective products get shipped to your clients? One solution is to send to human raters a sample of all the pictures that the model classified (especially pictures that the model wasn’t so sure about). Depending on the task, the raters may need to be experts, or they could be nonspecialists, such as workers on a crowdsourcing platform (e.g., Amazon Mechanical Turk). In some applications they could even be the users themselves, responding for example via surveys or repurposed captchas. Either way, you need to put in place a monitoring system (with or without human raters to evaluate the live model), as well as all the relevant processes to define what to do in case of failures and how to prepare for them.\n\nUnfortunately, this can be a lot of work. In fact, **it is often much more work than building and training a model**. If the data keeps evolving, you will need to update your datasets and retrain your model regularly. You should probably automate the whole process as much as possible. Here are a few things you can automate:\n\n* Collect fresh data regularly and label it (e.g., using human raters).\n* Write a script to train the model and fine-tune the hyperparameters automatically. This script could run automatically, for example every day or every week, depending on your needs.\n* Write another script that will evaluate both the new model and the previous model on the updated test set, and deploy the model to production if the performance has not decreased (if it did, make sure you investigate why). The script should probably test the performance of your model on various subsets of the test set, such as poor or rich districts, rural or urban districts, etc.\n\nYou should also make sure **you evaluate the model’s input data quality**. Sometimes performance will degrade slightly because of a poor-quality signal (e.g., a malfunctioning sensor sending random values, or another team’s output becoming stale), but it may take a while before your system’s performance degrades enough to trigger an alert. If you monitor your model’s inputs, you may catch this earlier. For example, you could trigger an alert if more and more inputs are missing a feature, or if its mean or standard deviation drifts too far from the training set, or a categorical feature starts containing new categories.\n\nFinally, make sure you keep **backups of every model** you create and have the process and tools in place to roll back to a previous model quickly, in case the new model starts failing badly for some reason. Having backups also makes it possible to easily compare new models with previous ones. Similarly, you should keep backups of every version of your datasets so that you can roll back to a previous dataset if the new one ever gets corrupted (e.g., if the fresh data that gets added to it turns out to be full of outliers). Having backups of your datasets also allows you to evaluate any model against any previous dataset.\n\nAs you can see, Machine Learning involves quite a lot of infrastructure, so don’t be surprised if your first ML project takes a lot of effort and time to build and deploy to production. Fortunately, once all the infrastructure is in place, going from idea to production will be much faster.\n\n## Low-code ML using `PyCaret`\n\n`PyCaret` is a high-level, low-code Python library that makes it easy to compare, train, evaluate, tune, and deploy machine learning models with only a few lines of code. \n\nAt its core, `PyCaret` is basically just a large wrapper over many data science libraries such as `Scikit-learn`, `Yellowbrick`, `SHAP`, `Optuna`, and `Spacy`. Yes, you could use these libraries for the same tasks, but if you don’t want to write a lot of code, PyCaret could save you a lot of time.\n\n[https://pycaret.readthedocs.io/en/latest/api/regression.html](https://pycaret.readthedocs.io/en/latest/api/regression.html)\n\nRemember to restart the notebook to update the modules\n\n### Get the data\n\nNow that we have the data, we can initialize a `PyCaret` experiment, which will preprocess the data and enable logging for all of the models that we will train on this dataset.\n\n### Explore the data\n\n`ydata-profiling` primary goal is to provide a one-line Exploratory Data Analysis (EDA) experience in a consistent and fast solution. Like pandas `df.describe()` function, that is so handy, `ydata-profiling` delivers an extended analysis of a DataFrame while allowing the data analysis to be exported in different formats such as html and json.\n\n[https://github.com/ydataai/ydata-profiling](https://github.com/ydataai/ydata-profiling)\n\nThe `setup()` function initializes the environment in pycaret and creates the transformation pipeline to prepare the data for modeling and deployment. `setup()` must be called before executing any other function in pycaret. It takes two mandatory parameters: a pandas dataframe and the name of the target column. All other parameters are optional and are used to customize the pre-processing pipeline.\n\nWhen `setup()` is executed, PyCaret's inference algorithm will automatically infer the data types for all features based on certain properties. The data type should be inferred correctly but this is not always the case. To account for this, PyCaret displays a table containing the features and their inferred data types after `setup()` is executed. \n\n> Ensuring that the data types are correct is of fundamental importance in PyCaret as it automatically performs a few pre-processing tasks which are imperative to any machine learning experiment. These tasks are performed differently for each data type which means it is very important for them to be correctly configured.\n\n[https://pycaret.gitbook.io/docs/get-started/functions/initialize](https://pycaret.gitbook.io/docs/get-started/functions/initialize)\n\nThe `eda()` function generates automated Exploratory Data Analysis (EDA) using the `AutoViz` library.\n\n[https://pycaret.gitbook.io/docs/get-started/functions/analyze#eda](https://pycaret.gitbook.io/docs/get-started/functions/analyze#eda)\n\n### Prepare the data\n\nIn order to demonstrate the `predict_model()` function on unseen data, a sample of 10% records has been withheld from the original dataset to be used for predictions. This should not be confused with a train/test split as this particular split is performed to simulate a real life scenario. Another way to think about this is that these records are not available at the time when the machine learning experiment was performed.\n\n[https://pycaret.gitbook.io/docs/get-started/preprocessing](https://pycaret.gitbook.io/docs/get-started/preprocessing)\n\nNote that starting from PyCaret 3.0, it supports OOP API.\n\n[https://www.kaggle.com/code/uyeanil/titanic-custom-transformer-pipeline-pycaret](https://www.kaggle.com/code/uyeanil/titanic-custom-transformer-pipeline-pycaret) as AutoML\n\n### Select and train a model\n\nComparing all models to evaluate performance is the recommended starting point for modeling once the setup is completed (unless you exactly know what kind of model you need, which is often not the case). This function trains all models in the model library and scores them using k-fold cross validation for metric evaluation. The output prints a score grid that shows average MAE, MSE, RMSE,R2, RMSLE and MAPE accross the folds (10 by default) along with training time.\n\n[https://pycaret.gitbook.io/docs/get-started/functions/train](https://pycaret.gitbook.io/docs/get-started/functions/train)\n\nThe score grid printed above highlights the highest performing metric for comparison purposes only. By passing sort parameter, `compare_models(sort = 'RMSE')` will sort the grid by RMSE (lower to higher since lower is better). We also change the fold parameter from the default value of 10 to a different value by using `compare_models(fold = 5)` which will compare all models on 5 fold cross validation. Reducing the number of folds will improve the training time. By default, `compare_models` return the best performing model based on default sort order but can be used to return a list of top N models by using `n_select` parameter. Notice that you can also use exclude parameter to block certain models.\n\nThere are 25 regressors available in the model library of PyCaret. To see list of all regressors either check the docstring or use `models()` function to see the library.\n\n`create_model()` is the most granular function in PyCaret and is often the foundation behind most of the PyCaret functionalities. As the name suggests this function trains and evaluates a model using cross validation that can be set with fold parameter. The output prints a score grid that shows MAE, MSE, RMSE, R2, RMSLE and MAPE by fold.\n\n### Fine-Tune your model\n\n[https://pycaret.gitbook.io/docs/get-started/functions/optimize](https://pycaret.gitbook.io/docs/get-started/functions/optimize)\n\n[https://pycaret.gitbook.io/docs/get-started/functions/others#automl](https://pycaret.gitbook.io/docs/get-started/functions/others#automl)\n\n[https://pycaret.gitbook.io/docs/get-started/functions/optimize#ensemble_model](https://pycaret.gitbook.io/docs/get-started/functions/optimize#ensemble_model)\n\nWhen a model is created using the create_model function it uses the default hyperparameters to train the model. In order to tune hyperparameters, the tune_model function is used. This function automatically tunes the hyperparameters of a model using Random Grid Search on a pre-defined search space. The output prints a score grid that shows MAE, MSE, RMSE, R2, RMSLE and MAPE by fold. To use the custom search grid, you can pass `custom_grid` parameter in the tune_model function.\n\n### Analyze the model\n\nBefore model finalization, the `plot_model()` function can be used to analyze the performance across different aspects such as Residuals Plot, Prediction Error, Feature Importance etc. This function takes a trained model object and returns a plot based on the test / hold-out set.\n\nThe `evaluate_model()` displays a user interface for analyzing the performance of a trained model. It calls the `plot_model()` function internally.\n\n[https://pycaret.gitbook.io/docs/get-started/functions/analyze](https://pycaret.gitbook.io/docs/get-started/functions/analyze)\n\nThe `interpret_model()` analyzes the predictions generated from a trained model. Most plots in this function are implemented based on the SHAP (Shapley Additive exPlanations)\n\n### Predict on test set\n\nBefore finalizing the model, it is advisable to perform one final check by predicting the test/hold-out set and reviewing the evaluation metrics.\n\n### Finalize model\n\nModel finalization is the last step in the experiment. A normal machine learning workflow in PyCaret starts with `setup()`, followed by comparing all models using `compare_models()` and shortlisting a few candidate models (based on the metric of interest) to perform several modeling techniques such as hyperparameter tuning, ensembling, stacking etc. This workflow will eventually lead you to the best model for use in making predictions on new and unseen data. The `finalize_model()` function fits the model onto the complete dataset including the test/hold-out sample (20% in this case). The purpose of this function is to train the model on the complete dataset before it is deployed in production.\n\nThe `predict_model()` function is also used to predict on the unseen dataset.  `data_unseen` is the variable created at the beginning of the tutorial and contains 10% of the original dataset which was never exposed to PyCaret\n\n### Save and load the model\n\nWe have now finished the experiment by finalizing the `tuned_lightgbm` model which is now stored in `final_lightgbm` variable. We have also used the model stored in `final_lightgbm` to predict `data_unseen`. This brings us to the end of our experiment, but one question is still to be asked: What happens when you have more new data to predict? Do you have to go through the entire experiment again? The answer is no, PyCaret's inbuilt function `save_model()` allows you to save the model along with entire transformation pipeline for later use.\n\nTo load a saved model at a future date in the same or an alternative environment, we would use PyCaret's `load_model()` function and then easily apply the saved model on new unseen data for prediction.\n\n### Deplpy\n\n[https://pycaret.gitbook.io/docs/get-started/functions/deploy](https://pycaret.gitbook.io/docs/get-started/functions/deploy)\n\n[https://pycaret.gitbook.io/docs/get-started/functions/others#get_config](https://pycaret.gitbook.io/docs/get-started/functions/others#get_config)\n\n## Reference\n\n1. [https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb](https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb)\n2. [https://pycaret.org/](https://pycaret.org/)\n3. [https://pycaret.readthedocs.io/en/latest/](https://pycaret.readthedocs.io/en/latest/)\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"01_end_to_end_machine_learning_project.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.306","theme":"cosmo","cover-image":"cover.jpg","title":"End-to-end Machine Learning project","author":"phonchi","date":"02/13/2023"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}