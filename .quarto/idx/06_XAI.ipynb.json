{"title":"Explainable AI","markdown":{"yaml":{"title":"Explainable AI","author":"phonchi","date":"03/27/2023","format":{"html":{"toc":true,"code-line-numbers":true,"code-tools":true}}},"headingText":"Setup","containsRefs":false,"markdown":"\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/06_XAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/06_XAI.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>\n<br/>\n\n\n## Decsion-Rule based modeld by `imodels`\n\n`imodels` provides a simple interface for fitting and using state-of-the-art interpretable models, all compatible with `scikit-learn`. These models can often replace black-box models (e.g. random forests) with simpler models (e.g. rule lists) while improving interpretability and computational efficiency, all without sacrificing predictive accuracy!\n\nThe Ames dataset is a housing dataset that use seveal conditions to predict the housing price. The diabetes dataset has a binary-valued variable. We would like to investigated whether the patient shows signs of diabetes according to World Health Organization criteria.\n\nWe will now show how to fit different models. All models support the `fit()` and `predict()` method (classifiers also support `predict_proba()`).\n\nThe simplest way to visualize a fitted model `m` is usually just to call `str(m)` or `print(m)`. Some models have custom methods that allow you to visualize them further. To pass feature names into a model for visualization, you can usually (i) pass in the `feature_names` argument to the `fit()` function or (ii) pass in a pandas dataframe with the feature names as column names.\n\n### Rule lists\n\nRule list is nonoverlapping\n\n#### oneR\nFits a rule list restricted to use only one feature\n\n#### Bayesian rule lists\n\n### Rule sets\n\n\nRule sets are models that create a set of (potentially overlapping) rules.\n\n#### Rulefit \n\nIt fits a sparse linear model on rules extracted from decision trees\n\n### Ruletree\n\n#### FIGSClassifier\n\nSee https://github.com/parrt/dtreeviz/blob/master/notebooks/dtreeviz_sklearn_visualisations.ipynb for more information.\n\n#### HSTreeClassifier\n\n## Partial Depedency Plot and Individual Conditional Expectation plots\n\nPartial dependence plots (PDP) and individual conditional expectation (ICE) plots can be used to visualize and analyze interaction between the target response 1 and a set of input features of interest.\n\nBoth PDPs and ICEs **assume that the input features of interest are independent from the complement features**, and this assumption is often violated in practice. Thus, in the case of correlated features, we will create absurd data points to compute the PDP/ICE.\n\n### Partial dependence plots\n\nPartial dependence plots (PDP) show the dependence between the target response and a set of input features of interest, **marginalizing over the values of all other input features (the 'complement' features).** Intuitively, we can interpret the partial dependence as the expected target response as a function of the input features of interest.\n\nDue to the limits of human perception the size of the set of input feature of interest must be small (usually, one or two) **thus the input features of interest are usually chosen among the most important features.**\n\n\n\n#### 1-way partial dependence with different models\n\nNote that it is important to **check that the model is accurate enough on a test set before plotting the partial dependence** since there would be little use in explaining the impact of a given feature on the prediction function of a poor model.\n\nThe `sklearn.inspection` module provides a convenience function `from_estimator` to create one-way and two-way partial dependence plots.\n\nWe can clearly see on the PDPs (dashed orange line) that the median house price shows a linear relationship with the median income (left) and that the house price drops when the average occupants per household increases (middle). The right plots show that the house age in a district does not have a strong influence on the (median) house price; so does the average rooms per household.\n\nOne-way PDPs tell us about the interaction between the target response and an input feature of interest feature (e.g. linear, non-linear). \n\n#### 2D Partial Dependence Plots\n\nPDPs with two features of interest enable us to visualize interactions among them. Another consideration is linked to the performance to compute the PDPs. With the tree-based algorithm, when only PDPs are requested, they can be computed on an efficient way using the 'recursion' method.\n\nThe left plot in the above figure shows the effect of the average occupancy on the median house price; we can clearly see a linear relationship among them when the average occupancy is inferior to 3 persons. Similarly, we could analyze the effect of the house age on the median house price (middle plot). Thus, these interpretations are marginal, considering a feature at a time.\n\nThe two-way partial dependence plot shows the dependence of median house price on joint values of house age and average occupants per household. We can clearly see an interaction between the two features: **for an average occupancy greater than two, the house price is nearly independent of the house age, whereas for values less than two there is a strong dependence on age.**\n\nIf you need the raw values of the partial dependence function rather than the plots, you can use the `sklearn.inspection.partial_dependence()` function.\n\n#### Another example\n\nLike permutation importance, partial dependence plots are calculated **after a model has been fit.** The model is fit on real data that has not been artificially manipulated in any way. Our example will use a model that predicts whether a soccer/football team will have the \"Man of the Game\" winner based on the team's statistics. The \"Man of the Game\" award is given to the best player in the game. \n\nTeams may differ in many ways. How many passes they made, shots they took, goals they scored, etc. At first glance, it seems difficult to disentangle the effect of these features. To see how partial plots separate out the effect of each feature, we start by considering a single row of data. For example, that row of data might represent a team that had the ball 50% of the time, made 100 passes, took 10 shots and scored 1 goal.\n\nWe will use the fitted model to predict our outcome (probability their player won \"man of the match\"). **But we repeatedly alter the value for one variable to make a series of predictions.** We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time. Then predict again for 60%. And so on. We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis).\n\nIn this description, we used only a single row of data. Interactions between features may cause the plot for a single row to be atypical. So, we repeat that mental experiment with multiple rows from the original dataset, and we plot the average predicted outcome on the vertical axis.\n\nOur first example uses a decision tree, which you can see below. In practice, you'll use more sophistated models for real-world applications.\n\nA few items are worth pointing out as you interpret this plot\n\n* The y axis is interpreted as **change in the prediction** from what it would be predicted at the baseline or leftmost value.\n* A blue shaded area indicates level of confidence\n\nFrom this particular graph, we see that scoring a goal substantially increases your chances of winning \"Man of The Match.\"  But extra goals beyond that appear to have little impact on predictions.\n\nHere is another example plot:\n\nThis graph seems too simple to represent reality. But that's because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model's structure.\n\nYou can easily compare the structure or implications of different models. Here is the same plot with a Random Forest model.\n\nThis model thinks you are more likely to win *Man of the Match* if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.\n\nIn general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model.  Though this dataset is small enough that we would be careful in how we interpret any model.\n\nIf you are curious about interactions between features, 2D partial dependence plots are also useful. An example may clarify this.  \n\nWe will again use the Decision Tree model for this graph.  It will create an extremely simple plot, but you should be able to match what you see in the plot to the tree itself.\n\nThis graph shows predictions for any combination of Goals Scored and Distance covered.\n\nFor example, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km. If they score 0 goals, distance covered doesn't matter. Can you see this by tracing through the decision tree with 0 goals?\n\nBut distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot. Can you see this pattern in the decision tree too?\n\n###  Individual conditional expectation (ICE) plot\n\nDue to the limits of human perception, **only one input feature** of interest is supported for ICE plots.\n\nWhile the PDPs are good at showing the average effect of the target features, they can obscure a **heterogeneous relationship** created by interactions. When interactions are present the ICE plot will provide many more insights. For example, we could observe a linear relationship between the median income and the house price in the PD line. However, the ICE lines show that there are some exceptions, where the house price remains constant in some ranges of the median income. We will plot the partial dependence, both individual (ICE) and averaged one (PDP). We limit to only 50 ICE curves to not overcrowd the plot.\n\nThe sklearn.inspection module’s `PartialDependenceDisplay.from_estimator `convenience function can be used to create ICE plots by setting `kind='individual'`. But in ICE plots it might not be easy to see the average effect of the input feature of interest. Hence, it is recommended to use ICE plots alongside PDPs. They can be plotted together with `kind='both'`.\n\nThe ICE curves (light blue lines) complement the analysis: we can see that there are some exceptions, where the house price remain constant with median income and average occupants. On the other hand, while the house age (top right) does not have a strong influence on the median house price on average, **there seems to be a number of exceptions where the house price increase when between the ages 15-25.** Similar exceptions can be observed for the average number of rooms (bottom left). Therefore, ICE plots show some individual effect which are attenuated by taking the averages.\n\nCheckout more information at https://scikit-learn.org/stable/modules/partial_dependence.html# or https://github.com/SauceCat/PDPbox\n\n## LIME\n\nWe'll use the Iris dataset, and we'll train a random forest.\n\n### Tabular data\n\n#### Create the explainer\n\nTabular explainers need a training set. The reason for this is because we compute statistics on each feature (column). If the feature is numerical, we compute the mean and std, and discretize it into quartiles. If the feature is categorical, we compute the frequency of each value. For this part, we'll only look at numerical features.\n\nWe use these computed statistics for two things:\n\n1. To scale the data, so that we can meaningfully compute distances when the attributes are not on the same scale\n2. To sample perturbed instances - which we do by sampling from a `Normal(0,1)`, multiplying by the std and adding back the mean.\n\n#### Explaining an instance\n\nSince this is a multi-class classification problem, we set the `top_labels` parameter, so that we only explain the top class.\n\nWe now explain a single instance:\n\nNow, there is a lot going on here. First, note that the row we are explained is displayed on the right side, in table format. Since we had the `show_all` parameter set to false, only the features used in the explanation are displayed.\nThe value column displays the original value for each feature.\n\nNote that LIME has discretized the features in the explanation. This is because we let `discretize_continuous=True` in the constructor (this is the default). Discretized features make for more intuitive explanations.\n\n### Image data\n\n#### Explanation\n\nNow let's get an explanation\n\n#### Now let's see the explanation for the classes\n\nWe can also see the 'pros and cons' (pros in green, cons in red)\n\nAlternatively, we can also plot explanation weights onto a heatmap visualization. The colorbar shows the values of the weights.\n\nLet's see the explanation for the wombat\n\nFor more information, please refer to https://github.com/marcotcr/lime\n\n## SHAP\n\n### The force plot\n\nAn example is helpful, and we'll continue the soccer/football example from the partial dependence plots. In these part, we predicted whether **a team would have a player win the Man of the Match award.**\n\nWe could ask:\n* How much was a prediction driven by the fact that the team scored 3 goals?\n\nBut it's easier to give a concrete, numeric answer if we restate this as:\n* How much was a prediction driven by the fact that the team scored 3 goals, instead of some baseline number of goals.\n\nOf course, each team has many features. So if we answer this question for number of goals, we could repeat the process for all other features.\n\nWe will look at SHAP values for a single row of the dataset (we arbitrarily chose row 5). For context, we'll look at the raw predictions before looking at the SHAP values.\n\nThe team is 71% likely to have a player win the award. \nNow, we'll move onto the code to get SHAP values for that single prediction.\n\nThe `shap_values` object above is a list with two arrays. The first array is the SHAP values for a negative outcome (don't win the award), and the second array is the list of SHAP values for the positive outcome (wins the award). We typically think about predictions in terms of the prediction of a positive outcome, so we'll pull out SHAP values for positive outcomes (pulling out `shap_values[1]`).\n\nIt's cumbersome to review raw arrays, but the shap package has a nice way to visualize the results. \n\nIf you look carefully at the code where we created the SHAP values, you'll notice we reference Trees in `shap.TreeExplainer(my_model)`.  But the SHAP package has explainers for every type of model.\n\n* `shap.DeepExplainer` works with Deep Learning models. \n* `shap.KernelExplainer` works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.\n\n### Summary Plots\n\nIn addition to this nice breakdown for each prediction, the [Shap library](https://github.com/slundberg/shap) offers great visualizations of groups of Shap values. We will focus on two of these visualizations. These visualizations have conceptual similarities to permutation importance and partial dependence plots.\n\nThe code isn't too complex. But there are a few caveats.\n\n* When plotting, we call `shap_values[1]`.  For classification problems, there is a separate array of SHAP values for each possible outcome. In this case, **we index in to get the SHAP values for the prediction of \"True\".**\n* Calculating SHAP values can be slow. It isn't a problem here, because this dataset is small.  But you'll want to be careful when running these to plot with reasonably sized datasets.  The exception is when using an `xgboost` model, which SHAP has some optimizations for and which is thus much faster.\n\nThis provides a great overview of the model, but we might want to delve into a single feature. That's where SHAP dependence contribution plots come into play.\n\n### Dependence Contribution Plots\n\n### Image data\n\nDeep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with DeepLIFT. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. \n\nThe plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model's output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the 'zero' image the blank middle is important, while for the 'four' image the lack of a connection on top makes it a four instead of a nine.\n\n## Protodash using AXI360\n\nYou can find more examples [here](https://github.com/Trusted-AI/AIX360/tree/master/examples/protodash)\n\n## Counterfactual instances\n\nYou can find more informations [here](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html) and [here](https://github.com/interpretml/DiCE)\n\n## Using Interpretable Features for Model Debugging\n\nYou can find more informations [here](https://github.com/dcai-course/dcai-lab/blob/master/interpretable_features/Solution%20-%20Interpretable%20Features.ipynb)\n\n## References\n\n1. [https://www.kaggle.com/learn/machine-learning-explainability](https://www.kaggle.com/learn/machine-learning-explainability)\n2. [https://scikit-learn.org/stable/modules/partial_dependence.html#](https://scikit-learn.org/stable/modules/partial_dependence.html#) \n3. [https://github.com/csinva/imodels](https://github.com/csinva/imodels)\n4. [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)\n5. [https://github.com/slundberg/shap](https://github.com/slundberg/shap)\n\n\n","srcMarkdownNoYaml":"\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/06_XAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/06_XAI.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>\n<br/>\n\n## Setup\n\n## Decsion-Rule based modeld by `imodels`\n\n`imodels` provides a simple interface for fitting and using state-of-the-art interpretable models, all compatible with `scikit-learn`. These models can often replace black-box models (e.g. random forests) with simpler models (e.g. rule lists) while improving interpretability and computational efficiency, all without sacrificing predictive accuracy!\n\nThe Ames dataset is a housing dataset that use seveal conditions to predict the housing price. The diabetes dataset has a binary-valued variable. We would like to investigated whether the patient shows signs of diabetes according to World Health Organization criteria.\n\nWe will now show how to fit different models. All models support the `fit()` and `predict()` method (classifiers also support `predict_proba()`).\n\nThe simplest way to visualize a fitted model `m` is usually just to call `str(m)` or `print(m)`. Some models have custom methods that allow you to visualize them further. To pass feature names into a model for visualization, you can usually (i) pass in the `feature_names` argument to the `fit()` function or (ii) pass in a pandas dataframe with the feature names as column names.\n\n### Rule lists\n\nRule list is nonoverlapping\n\n#### oneR\nFits a rule list restricted to use only one feature\n\n#### Bayesian rule lists\n\n### Rule sets\n\n\nRule sets are models that create a set of (potentially overlapping) rules.\n\n#### Rulefit \n\nIt fits a sparse linear model on rules extracted from decision trees\n\n### Ruletree\n\n#### FIGSClassifier\n\nSee https://github.com/parrt/dtreeviz/blob/master/notebooks/dtreeviz_sklearn_visualisations.ipynb for more information.\n\n#### HSTreeClassifier\n\n## Partial Depedency Plot and Individual Conditional Expectation plots\n\nPartial dependence plots (PDP) and individual conditional expectation (ICE) plots can be used to visualize and analyze interaction between the target response 1 and a set of input features of interest.\n\nBoth PDPs and ICEs **assume that the input features of interest are independent from the complement features**, and this assumption is often violated in practice. Thus, in the case of correlated features, we will create absurd data points to compute the PDP/ICE.\n\n### Partial dependence plots\n\nPartial dependence plots (PDP) show the dependence between the target response and a set of input features of interest, **marginalizing over the values of all other input features (the 'complement' features).** Intuitively, we can interpret the partial dependence as the expected target response as a function of the input features of interest.\n\nDue to the limits of human perception the size of the set of input feature of interest must be small (usually, one or two) **thus the input features of interest are usually chosen among the most important features.**\n\n\n\n#### 1-way partial dependence with different models\n\nNote that it is important to **check that the model is accurate enough on a test set before plotting the partial dependence** since there would be little use in explaining the impact of a given feature on the prediction function of a poor model.\n\nThe `sklearn.inspection` module provides a convenience function `from_estimator` to create one-way and two-way partial dependence plots.\n\nWe can clearly see on the PDPs (dashed orange line) that the median house price shows a linear relationship with the median income (left) and that the house price drops when the average occupants per household increases (middle). The right plots show that the house age in a district does not have a strong influence on the (median) house price; so does the average rooms per household.\n\nOne-way PDPs tell us about the interaction between the target response and an input feature of interest feature (e.g. linear, non-linear). \n\n#### 2D Partial Dependence Plots\n\nPDPs with two features of interest enable us to visualize interactions among them. Another consideration is linked to the performance to compute the PDPs. With the tree-based algorithm, when only PDPs are requested, they can be computed on an efficient way using the 'recursion' method.\n\nThe left plot in the above figure shows the effect of the average occupancy on the median house price; we can clearly see a linear relationship among them when the average occupancy is inferior to 3 persons. Similarly, we could analyze the effect of the house age on the median house price (middle plot). Thus, these interpretations are marginal, considering a feature at a time.\n\nThe two-way partial dependence plot shows the dependence of median house price on joint values of house age and average occupants per household. We can clearly see an interaction between the two features: **for an average occupancy greater than two, the house price is nearly independent of the house age, whereas for values less than two there is a strong dependence on age.**\n\nIf you need the raw values of the partial dependence function rather than the plots, you can use the `sklearn.inspection.partial_dependence()` function.\n\n#### Another example\n\nLike permutation importance, partial dependence plots are calculated **after a model has been fit.** The model is fit on real data that has not been artificially manipulated in any way. Our example will use a model that predicts whether a soccer/football team will have the \"Man of the Game\" winner based on the team's statistics. The \"Man of the Game\" award is given to the best player in the game. \n\nTeams may differ in many ways. How many passes they made, shots they took, goals they scored, etc. At first glance, it seems difficult to disentangle the effect of these features. To see how partial plots separate out the effect of each feature, we start by considering a single row of data. For example, that row of data might represent a team that had the ball 50% of the time, made 100 passes, took 10 shots and scored 1 goal.\n\nWe will use the fitted model to predict our outcome (probability their player won \"man of the match\"). **But we repeatedly alter the value for one variable to make a series of predictions.** We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time. Then predict again for 60%. And so on. We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis).\n\nIn this description, we used only a single row of data. Interactions between features may cause the plot for a single row to be atypical. So, we repeat that mental experiment with multiple rows from the original dataset, and we plot the average predicted outcome on the vertical axis.\n\nOur first example uses a decision tree, which you can see below. In practice, you'll use more sophistated models for real-world applications.\n\nA few items are worth pointing out as you interpret this plot\n\n* The y axis is interpreted as **change in the prediction** from what it would be predicted at the baseline or leftmost value.\n* A blue shaded area indicates level of confidence\n\nFrom this particular graph, we see that scoring a goal substantially increases your chances of winning \"Man of The Match.\"  But extra goals beyond that appear to have little impact on predictions.\n\nHere is another example plot:\n\nThis graph seems too simple to represent reality. But that's because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model's structure.\n\nYou can easily compare the structure or implications of different models. Here is the same plot with a Random Forest model.\n\nThis model thinks you are more likely to win *Man of the Match* if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.\n\nIn general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model.  Though this dataset is small enough that we would be careful in how we interpret any model.\n\nIf you are curious about interactions between features, 2D partial dependence plots are also useful. An example may clarify this.  \n\nWe will again use the Decision Tree model for this graph.  It will create an extremely simple plot, but you should be able to match what you see in the plot to the tree itself.\n\nThis graph shows predictions for any combination of Goals Scored and Distance covered.\n\nFor example, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km. If they score 0 goals, distance covered doesn't matter. Can you see this by tracing through the decision tree with 0 goals?\n\nBut distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot. Can you see this pattern in the decision tree too?\n\n###  Individual conditional expectation (ICE) plot\n\nDue to the limits of human perception, **only one input feature** of interest is supported for ICE plots.\n\nWhile the PDPs are good at showing the average effect of the target features, they can obscure a **heterogeneous relationship** created by interactions. When interactions are present the ICE plot will provide many more insights. For example, we could observe a linear relationship between the median income and the house price in the PD line. However, the ICE lines show that there are some exceptions, where the house price remains constant in some ranges of the median income. We will plot the partial dependence, both individual (ICE) and averaged one (PDP). We limit to only 50 ICE curves to not overcrowd the plot.\n\nThe sklearn.inspection module’s `PartialDependenceDisplay.from_estimator `convenience function can be used to create ICE plots by setting `kind='individual'`. But in ICE plots it might not be easy to see the average effect of the input feature of interest. Hence, it is recommended to use ICE plots alongside PDPs. They can be plotted together with `kind='both'`.\n\nThe ICE curves (light blue lines) complement the analysis: we can see that there are some exceptions, where the house price remain constant with median income and average occupants. On the other hand, while the house age (top right) does not have a strong influence on the median house price on average, **there seems to be a number of exceptions where the house price increase when between the ages 15-25.** Similar exceptions can be observed for the average number of rooms (bottom left). Therefore, ICE plots show some individual effect which are attenuated by taking the averages.\n\nCheckout more information at https://scikit-learn.org/stable/modules/partial_dependence.html# or https://github.com/SauceCat/PDPbox\n\n## LIME\n\nWe'll use the Iris dataset, and we'll train a random forest.\n\n### Tabular data\n\n#### Create the explainer\n\nTabular explainers need a training set. The reason for this is because we compute statistics on each feature (column). If the feature is numerical, we compute the mean and std, and discretize it into quartiles. If the feature is categorical, we compute the frequency of each value. For this part, we'll only look at numerical features.\n\nWe use these computed statistics for two things:\n\n1. To scale the data, so that we can meaningfully compute distances when the attributes are not on the same scale\n2. To sample perturbed instances - which we do by sampling from a `Normal(0,1)`, multiplying by the std and adding back the mean.\n\n#### Explaining an instance\n\nSince this is a multi-class classification problem, we set the `top_labels` parameter, so that we only explain the top class.\n\nWe now explain a single instance:\n\nNow, there is a lot going on here. First, note that the row we are explained is displayed on the right side, in table format. Since we had the `show_all` parameter set to false, only the features used in the explanation are displayed.\nThe value column displays the original value for each feature.\n\nNote that LIME has discretized the features in the explanation. This is because we let `discretize_continuous=True` in the constructor (this is the default). Discretized features make for more intuitive explanations.\n\n### Image data\n\n#### Explanation\n\nNow let's get an explanation\n\n#### Now let's see the explanation for the classes\n\nWe can also see the 'pros and cons' (pros in green, cons in red)\n\nAlternatively, we can also plot explanation weights onto a heatmap visualization. The colorbar shows the values of the weights.\n\nLet's see the explanation for the wombat\n\nFor more information, please refer to https://github.com/marcotcr/lime\n\n## SHAP\n\n### The force plot\n\nAn example is helpful, and we'll continue the soccer/football example from the partial dependence plots. In these part, we predicted whether **a team would have a player win the Man of the Match award.**\n\nWe could ask:\n* How much was a prediction driven by the fact that the team scored 3 goals?\n\nBut it's easier to give a concrete, numeric answer if we restate this as:\n* How much was a prediction driven by the fact that the team scored 3 goals, instead of some baseline number of goals.\n\nOf course, each team has many features. So if we answer this question for number of goals, we could repeat the process for all other features.\n\nWe will look at SHAP values for a single row of the dataset (we arbitrarily chose row 5). For context, we'll look at the raw predictions before looking at the SHAP values.\n\nThe team is 71% likely to have a player win the award. \nNow, we'll move onto the code to get SHAP values for that single prediction.\n\nThe `shap_values` object above is a list with two arrays. The first array is the SHAP values for a negative outcome (don't win the award), and the second array is the list of SHAP values for the positive outcome (wins the award). We typically think about predictions in terms of the prediction of a positive outcome, so we'll pull out SHAP values for positive outcomes (pulling out `shap_values[1]`).\n\nIt's cumbersome to review raw arrays, but the shap package has a nice way to visualize the results. \n\nIf you look carefully at the code where we created the SHAP values, you'll notice we reference Trees in `shap.TreeExplainer(my_model)`.  But the SHAP package has explainers for every type of model.\n\n* `shap.DeepExplainer` works with Deep Learning models. \n* `shap.KernelExplainer` works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.\n\n### Summary Plots\n\nIn addition to this nice breakdown for each prediction, the [Shap library](https://github.com/slundberg/shap) offers great visualizations of groups of Shap values. We will focus on two of these visualizations. These visualizations have conceptual similarities to permutation importance and partial dependence plots.\n\nThe code isn't too complex. But there are a few caveats.\n\n* When plotting, we call `shap_values[1]`.  For classification problems, there is a separate array of SHAP values for each possible outcome. In this case, **we index in to get the SHAP values for the prediction of \"True\".**\n* Calculating SHAP values can be slow. It isn't a problem here, because this dataset is small.  But you'll want to be careful when running these to plot with reasonably sized datasets.  The exception is when using an `xgboost` model, which SHAP has some optimizations for and which is thus much faster.\n\nThis provides a great overview of the model, but we might want to delve into a single feature. That's where SHAP dependence contribution plots come into play.\n\n### Dependence Contribution Plots\n\n### Image data\n\nDeep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with DeepLIFT. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. \n\nThe plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model's output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the 'zero' image the blank middle is important, while for the 'four' image the lack of a connection on top makes it a four instead of a nine.\n\n## Protodash using AXI360\n\nYou can find more examples [here](https://github.com/Trusted-AI/AIX360/tree/master/examples/protodash)\n\n## Counterfactual instances\n\nYou can find more informations [here](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html) and [here](https://github.com/interpretml/DiCE)\n\n## Using Interpretable Features for Model Debugging\n\nYou can find more informations [here](https://github.com/dcai-course/dcai-lab/blob/master/interpretable_features/Solution%20-%20Interpretable%20Features.ipynb)\n\n## References\n\n1. [https://www.kaggle.com/learn/machine-learning-explainability](https://www.kaggle.com/learn/machine-learning-explainability)\n2. [https://scikit-learn.org/stable/modules/partial_dependence.html#](https://scikit-learn.org/stable/modules/partial_dependence.html#) \n3. [https://github.com/csinva/imodels](https://github.com/csinva/imodels)\n4. [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)\n5. [https://github.com/slundberg/shap](https://github.com/slundberg/shap)\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"06_XAI.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.306","theme":"cosmo","cover-image":"cover.jpg","title":"Explainable AI","author":"phonchi","date":"03/27/2023"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}