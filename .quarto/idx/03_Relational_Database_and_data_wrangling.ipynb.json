{"title":"Relational Database and data wrangling","markdown":{"yaml":{"title":"Relational Database and data wrangling","author":"phonchi","date":"03/06/2023","format":{"html":{"toc":true,"code-line-numbers":true,"code-tools":true}}},"headingText":"Queringing data with BigQuery","containsRefs":false,"markdown":"\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/03_Relational_Database_and_data_wrangling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/03_Relational_Database_and_data_wrangling.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>\n<br/>\n\n\nStructured Query Language, or SQL, is the programming language used with databases, and it is an important skill for any data scientist. In this example, you'll build your SQL skills using BigQuery, a web service work as database management system that lets you apply SQL to huge datasets.\n\n### Preliminaries for google colab (optional)\n\nWe want to start exploring the Google BiqQuery [public datasets](https://cloud.google.com/bigquery/public-data/). Let's start by walking through the required setup steps, and then we can load and explore some data.\n\nIf you are using colab. Follow [this quickstart guide](https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries), which will explain how to:\n1. Create a [Cloud Platform project](https://console.cloud.google.com/cloud-resource-manager) if you don't have one already.\n2. [Enable billing](https://support.google.com/cloud/answer/6293499#enable-billing) for the project (If you apply the free trial, you already satisfy this condition.)\n3. [Enable the BigQuery API](https://console.cloud.google.com/flows/enableapi?apiid=bigquery)\n4. [Enabling the Service account](https://cloud.google.com/docs/authentication/getting-started)\n\nNow we need to authenticate to gain access to the BigQuery API. We will create a client, specifying the service account key file (replace 'lunar-pact-378812-7a28b789bde2.json' with your key file).\n\nNow that we're authenticated, we need to load the BigQuery package, and the `google.colab.data_table` package that can be used to display large pandas dataframes as an interactive data. Loading `data_table` is optional, but it will be useful for working with data in pandas.\n\n### Create the reference\n\n\nYou can also work with Kaggle, which provide bigquery integration that you do not need to setup a google account. **Each Kaggle user can scan 5TB every 30 days for free.  Once you hit that limit, you'll have to wait for it to reset.** See https://www.kaggle.com/product-feedback/48573 for more details.\n\n\nThe first step in the workflow is to create a [`Client`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client) object.  As you'll soon see, this `Client` object will play a central role in retrieving information from BigQuery datasets.\n\nWe'll work with a dataset of posts on Hacker News, a website focusing on computer science and cybersecurity news. In BigQuery, each dataset is contained in a corresponding project. In this case, our `hacker_news` dataset is contained in the `bigquery-public-data project`. \n\nTo access the dataset, We begin by constructing a reference to the dataset with the [`dataset()`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.dataset.Dataset) method. Next, we use the [`get_dataset()`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client#google_cloud_bigquery_client_Client_get_dataset) method, along with the reference we just constructed, to fetch the dataset.\n\n[See the full list of public datasets](https://console.cloud.google.com/marketplace/browse?filter=solution-type:dataset) or the [kaggle bigquery dataset](https://www.kaggle.com/datasets?search=bigquery) if you want to explore others.\n\nEvery dataset is just a collection of tables. You can think of a dataset as a spreadsheet file containing multiple tables, all composed of rows and columns.We use the [`list_tables()`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client#google_cloud_bigquery_client_Client_list_tables) method to list the tables in the dataset.\n\nSimilar to how we fetched a dataset, we can fetch a table. In the code cell below, we fetch the `full` table in the hacker_news dataset\n\nIn the next section, you'll explore the contents of this table in more detail.  For now, take the time to use the image below to consolidate what you've learned so far.\n\n![first_commands](https://i.imgur.com/biYqbUB.png)\n\n### Table schema\n\nThe structure of a table is called its **schema**.  **We need to understand a table's schema to effectively pull out the data we want.** \n\nIn this example, we'll investigate the `full` table that we fetched above.\n\nEach [`SchemaField`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.schema.SchemaField) tells us about a specific column (which we also refer to as a **field**). In order, the information is:\n\n* The **name** of the column\n* The **field type** (or datatype) in the column\n* The **mode** of the column (`'NULLABLE'` means that a column allows NULL values, and is the default)\n* A **description** of the data in that column\n\nFor instance, the field has the SchemaField:\n\n`SchemaField('by', 'string', 'NULLABLE', \"The username of the item's author.\",())`\n\nThis tells us:\n- the field (or column) is called `by`,\n- the data in this field is strings, \n- NULL values are allowed, and\n- it contains the usernames corresponding to each item's author.\n\nWe can use the [`list_rows()`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client#google_cloud_bigquery_client_Client_list_rows) method to check just the first five lines of of the `full` table to make sure this is right.  This returns a BigQuery [`RowIterator`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.table.RowIterator) object that can quickly be converted to a pandas DataFrame with the [`to_dataframe()`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.table.RowIterator#google_cloud_bigquery_table_RowIterator_to_dataframe) method.\n\nThe `list_rows()` method will also let us look at just the information in a specific column. If we want to see the first five entries in the `by` column, for example, we can do that!\n\n### Select, From & Where\n\nNow that you know how to access and examine a dataset, you're ready to write your first SQL query!  As you'll soon see, **SQL queries will help you sort through a massive dataset, to retrieve only the information that you need.** We'll begin by using the keywords **SELECT**, **FROM**, and **WHERE** to get data from specific columns based on conditions you specify. \n\nWe'll use an [OpenAQ](https://openaq.org) dataset about air quality. First, we'll set up everything we need to run queries and take a quick peek at what tables are in our database.\n\nThe dataset contains only one table, called `global_air_quality`.  We'll fetch the table and take a peek at the first few rows to see what sort of data it contains.\n\nlet's put together a query. Say we want to select all the values from the `city` column that are in rows where the `country` column is `'US'` (for \"United States\").\n\nNotice also that SQL statements requires single quotes for its strings inside python string (we use triple quotation mark here). We begin by setting up the query with the [`query()`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client#google_cloud_bigquery_client_Client_query) method.\n\nNow we've got a pandas DataFrame called `us_cities`, which we can use like any other DataFrame.\n\nIf you want multiple columns, you can select them with a comma between the names:\n\nYou can select all columns with a `*` like this:\n\n### Querying big dataset\n\nYou can estimate the size of any query before running it. Here is an example using the Hacker News dataset. To see how much data a query will scan, we create a [`QueryJobConfig`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.job.QueryJobConfig) object and set the `dry_run` parameter to `True`.\n\nYou can also specify a parameter when running the query to limit how much data you are willing to scan. Here's an example with a low limit.\n\nIn this case, the query was cancelled, because the limit of 1 MB was exceeded.  However, we can also increase the limit to run the query successfully!\n\n### Group By, Having & Count\n\nNow that you can select raw data, you're ready to learn how to group your data and count things within those groups.\n\nThe Hacker News dataset contains information on stories and comments from the Hacker News social networking site. We'll work with the `comments` table and begin by printing the first few rows\n\nLet's use the table to see which comments generated the most replies.  Since:\n- the `parent` column indicates the comment that was replied to, and \n- the `id` column has the unique ID used to identify each comment, \n\nwe can **GROUP BY** the `parent` column and **COUNT()** the `id` column in order to figure out the number of comments that were made as responses to a specific comment.\n\nFurthermore, since we're only interested in popular comments, we'll look at comments with more than ten replies.  So, we'll only return groups **HAVING** more than ten ID's.\n\nEach row in the `popular_comments` DataFrame corresponds to a comment that received more than ten replies.\n\nA couple hints to make your queries even better:\n- The column resulting from `COUNT(id)` was called `f0__`. That's not a very descriptive name. You can change the name by adding `AS NumPosts` after you specify the aggregation. This is called **aliasing**.\n- If you are ever unsure what to put inside the **COUNT()** function, you can do `COUNT(1)` to count the rows in each group. Most people find it especially readable, because we know it's not focusing on other columns. It also scans less data than if supplied column names (making it faster and using less of your data access quota).\n\nUsing these tricks, we can rewrite our query:\n\nNow you have the data you want, and it has descriptive names. \n\n#### Note on using **GROUP BY**\n\nNote that because it tells SQL how to apply aggregate functions (like **COUNT()**), it doesn't make sense to use **GROUP BY** without an aggregate function.  Similarly, if you have any **GROUP BY** clause, then all variables must be passed to either a\n1. **GROUP BY** command, or\n2. an aggregation function.\n\nConsider the query below:\n\n\n\nNote that there are two variables: `parent` and `id`. \n- `parent` was passed to a **GROUP BY** command (in `GROUP BY parent`), and \n- `id` was passed to an aggregate function (in `COUNT(id)`).\n\nAnd the query below won't work, because the `author` column isn't passed to an aggregate function or a **GROUP BY** clause:\n\n### Order By\n\nFrequently, you’ll want to sort your results. **Let's use the US Traffic Fatality Records database, which contains information on traffic accidents in the US where at least one person died.**\n\nWe'll investigate the `accident_2015` table. Here is a view of the first few rows. \n\nLet's use the table to determine how the number of accidents varies with the day of the week.  Since:\n- the `consecutive_number` column contains a unique ID for each accident, and\n- the `timestamp_of_crash` column contains the date of the accident in [DATETIME](https://cloud.google.com/bigquery/docs/reference/standard-sql/date_functions) format,\n\nwe can:\n- **EXTRACT** the day of the week (as `day_of_week` in the query below) from the `timestamp_of_crash` column, and\n- **GROUP BY** the day of the week, before we **COUNT** the `consecutive_number` column to determine the number of accidents for each day of the week.\n\nThen we sort the table with an **ORDER BY** clause, so the days with the most accidents are returned first.\n\nNotice that the data is sorted by the `num_accidents` column, where the days with more traffic accidents appear first.\n\nTo map the numbers returned for the `day_of_week` column to the actual day, you might consult [the BigQuery documentation](https://cloud.google.com/bigquery/docs/reference/standard-sql/date_functions) on the DAYOFWEEK function. It says that it returns \"an integer between 1 (Sunday) and 7 (Saturday), inclusively\". So, in 2015, most fatal motor accidents in the US occured on Sunday and Saturday, while the fewest happened on Tuesday.\n\n### As and With\n\nOn its own, `AS` is a convenient way to clean up the data returned by your query. **We're going to use a common table expression (CTE)** to find out **how many Bitcoin transactions were made each day for the entire timespan of a bitcoin transaction dataset.**\n\nWe'll investigate the transactions table. Here is a view of the first few rows.\n\nSince the `block_timestamp` column contains the date of each transaction in DATETIME format, we'll convert these into DATE format using the **DATE()** command.\n\nWe do that using a CTE, and then the next part of the query counts the number of transactions for each date and sorts the table so that earlier dates appear first. \n\nSince they're returned sorted, we can easily plot the raw results to show us the number of Bitcoin transactions per day over the whole timespan of this dataset.\n\nAs you can see, common table expressions (CTEs) let you shift a lot of your data cleaning into SQL. **That's an especially good thing in the case of BigQuery, because it is vastly faster than doing the work in Pandas.**\n\n### Joining data\n\nWhen our data lives across different tables, how do we analyze it? By\nJOINing the tables together. A `JOIN` combines rows in the left table with\ncorresponding rows in the right table, where the meaning of “corresponding” is based on how we specify the join.\n\nGitHub is the most popular place to collaborate on software projects. A GitHub **repository** (or **repo**) is a collection of files associated with a specific project. Most repos on GitHub are shared under a specific legal license, which determines the legal restrictions on how they are used.  **For our example, we're going to look at how many different files have been released under each license.** \n\nWe'll work with two tables in the database.  The first table is the `licenses` table, which provides the name of each GitHub repo (in the `repo_name` column) and its corresponding license.  Here's a view of the first five rows.\n\nThe second table is the `sample_files` table, which provides, among other information, the GitHub repo that each file belongs to (in the `repo_name` column).  The first several rows of this table are printed below.\n\nNext, we write a query that uses information in both tables to determine how many files are released in each license.\n\nIt's a big query, and so we'll investigate each piece separately.\n\n![](https://i.imgur.com/QeufD01.png)\n    \nWe'll begin with the **JOIN** (highlighted in blue above).  This specifies the sources of data and how to join them. We use **ON** to specify that we combine the tables by matching the values in the `repo_name` columns in the tables.\n\nNext, we'll talk about **SELECT** and **GROUP BY** (highlighted in yellow).  The **GROUP BY** breaks the data into a different group for each license, before we **COUNT** the number of rows in the `sample_files` table that corresponds to each license.  (Remember that you can count the number of rows with `COUNT(1)`.) \n\nFinally, the **ORDER BY** (highlighted in purple) sorts the results so that licenses with more files appear first.\n\nIt was a big query, but it gave us a nice table summarizing how many files have been committed under each license:  \n\nThere are a few more types of JOIN, along with how to use UNIONs to pull information from multiple tables. We'll work with the [Hacker News](https://www.kaggle.com/hacker-news/hacker-news) dataset. We begin by reviewing the first several rows of the `comments` table.\n\nThe query below pulls information from the `stories` and `comments` tables to create a table showing all stories posted on January 1, 2012, along with the corresponding number of comments.  We use a **LEFT JOIN** so that the results include stories that didn't receive any comments.\n\nSince the results are ordered by the `num_comments` column, stories without comments appear at the end of the DataFrame.  (Remember that **NaN** stands for \"not a number\".)\n\nAs you've seen, JOINs horizontally combine results from different tables. If you instead would like to vertically concatenate columns, you can do so with a `UNION`. \n\nNext, we write a query to select all usernames corresponding to users who wrote stories or comments on January 1, 2014.  We use **UNION DISTINCT** (instead of **UNION ALL**) to ensure that each user appears in the table at most once.\n\nTo get the number of users who posted on January 1, 2014, we need only take the length of the DataFrame.\n\n### Nested and Repeated data\n\nSo far, you've worked with many types of data, including numeric types (integers, floating point values), strings, and the DATETIME type. In this tutorial, you'll learn how to query nested and repeated data. These are the most complex data types that you can find in BigQuery datasets!\n\nWe'll work with the Google Analytics Sample dataset. It contains information tracking the behavior of visitors to the Google Merchandise store, an e-commerce website that sells Google branded items.\n\nWe begin by printing the first few rows of the `ga_sessions_20170801` table. This table tracks visits to the website on August 1, 2017. The table has many nested fields from table preview:\n\nNow we'll work with the `hits` column as an example of data that is both nested and repeated. Since:\n\n- `hits` is a STRUCT (contains nested data) and is repeated,\n- `hitNumber`, `page`, and `type` are all nested inside the `hits` column, and\n- `pagePath` is nested inside the `page` field,\n\nwe can query these fields with the following syntax:\n\n### Analytic Function (Optional)\n\nYou can also define analytic functions, which also operate on a set of rows like aggregation function. However, unlike aggregate functions, analytic functions return a (potentially different) value for each row in the original table. Analytic functions allow us to perform complex calculations with relatively straightforward syntax. For instance, we can quickly calculate moving averages and running totals, among other quantities.\n\nWe'll work with the [San Francisco Open Data](https://www.kaggle.com/datasf/san-francisco) dataset.\n\nEach row of the table corresponds to a different bike trip, and we can use an analytic function to **calculate the cumulative number of trips for each date in 2015.**\n\nThe query uses a [common table expression (CTE)](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#with_clause) to first calculate the daily number of trips.  Then, we use **SUM()** as an aggregate function.\n- Since there is no **PARTITION BY** clause, the entire table is treated as a single partition.\n- The **ORDER BY** clause orders the rows by date, where earlier dates appear first. \n- By setting the **window frame** clause to `ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW`, we ensure that all rows up to and including the current date are used to calculate the (cumulative) sum. See https://cloud.google.com/bigquery/docs/reference/standard-sql/analytic-function-concepts#def_window_frame for more details.\n\nThe next query **tracks the stations where each bike began (in `start_station_id`) and ended (in `end_station_id`) the day on October 25, 2015.**\n\nThe query uses both **FIRST_VALUE()** and **LAST_VALUE()** as analytic functions.\n- The **PARTITION BY** clause breaks the data into partitions based on the `bike_number` column.  Since this column holds unique identifiers for the bikes, this ensures the calculations are performed separately for each bike.\n- The **ORDER BY** clause puts the rows within each partition in chronological order.\n- Since the **window frame** clause is `ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING`, for each row, its entire partition is used to perform the calculation.  (_This ensures the calculated values for rows in the same partition are identical._)\n\nYou can check https://cloud.google.com/bigquery/docs/reference/standard-sql/introduction and https://googleapis.dev/python/bigquery/latest/index.html for more details.\n\n## Data Wrangling with Pandas\n\n### `Series` objects\nThe `Pandas` library contains these useful data structures:\n* `Series` objects, that we will discuss now. A `Series` object is 1D array, similar to a column in a spreadsheet (with a column name and row labels).\n* `DataFrame` objects. This is a 2D table, similar to a spreadsheet (with column names and row labels).\n\n#### Creating a `Series`\n\nLet's start by creating our first `Series` object!\n\nArithmetic operations on `Series` are also possible, and they apply *elementwise*, just like for `ndarray`s in NumPy:\n\n#### Index labels\n\nEach item in a `Series` object has a unique identifier called the *index label*. By default, it is simply the rank of the item in the `Series` (starting at `0`) but you can also set the index labels manually:\n\nYou can then use the `Series` just like a `dict`:\n\nYou can still access the items by integer location, like in a regular array:\n\nTo make it clear when you are accessing, **it is recommended to always use the `loc` attribute when accessing by label, and the `iloc` attribute when accessing by integer location**:\n\nSlicing a `Series` also slices the index labels:\n\n#### Initialize from `dict`\n\nYou can create a `Series` object from a `dict`. The keys will be used as index labels:\n\nWhen an operation involves multiple `Series` objects, `pandas` automatically aligns items by matching index labels.\n\nThe resulting `Series` contains the union of index labels from `s2` and `s3`. Since `\"colin\"` is missing from `s2` and `\"charles\"` is missing from `s3`, these items have a `NaN` result value. (ie. Not-a-Number means *missing*).\n\nAutomatic alignment is very handy when working with data that may come from various sources with varying structure and missing items\n\n#### Initialize with a scalar\n\nYou can also initialize a `Series` object using a scalar and a list of index labels: all items will be set to the scalar.\n\nPandas makes it easy to plot `Series` data using matplotlib:\n\nYou can easily convert it to NumPy array by dicarding the index. \n\nThere are *many* options for plotting your data. It is not necessary to list them all here: if you need a particular type of plot (histograms, pie charts, etc.), just look for it in the excellent [Visualization](http://pandas.pydata.org/pandas-docs/stable/visualization.html) section of pandas' documentation, and look at the example code.\n\n### Handling time\n\n\nMany datasets have timestamps, and pandas is awesome at manipulating such data:\n* it can represent periods (such as 2016Q3) and frequencies (such as \"monthly\")\n* it can convert periods to actual timestamps, and *vice versa*\n* it can resample data and aggregate values any way you like\n* it can handle timezones.\n\n#### Time range\n\nLet's start by creating a time series using `pd.date_range()`. This returns a `DatetimeIndex` containing one datetime per hour for 12 hours starting on March 6th 2023 at 5:30pm.\n\nThis `DatetimeIndex` may be used as an index in a `Series`:\n\nLet's plot this series:\n\n### Periods\n\nThe `pd.period_range()` function returns a `PeriodIndex` instead of a `DatetimeIndex`. For example, let's get all quarters in 2022 and 2023:\n\nAdding a number `N` to a `PeriodIndex` shifts the periods by `N` times the `PeriodIndex`'s frequency:\n\nPandas also provides many other time-related functions that we recommend you check out in the [documentation](http://pandas.pydata.org/pandas-docs/stable/timeseries.html)\n\n### `DataFrame` objects\n\nA DataFrame object represents a spreadsheet, with cell values, column names and row index labels. You can define expressions to compute columns based on other columns, create pivot-tables, group rows, draw graphs, etc. You can see `DataFrame` as dictionaries of `Series`.\n\n#### Creating a `DataFrame`\n\nYou can create a DataFrame by passing a dictionary of `Series` objects:\n\nA few things to note:\n* the `Series` were automatically aligned based on their index,\n* missing values are represented as `NaN`,\n* `Series` names are ignored (the name `\"year\"` was dropped),\n* `DataFrame`s are displayed nicely in Jupyter notebooks!\n\n#### Subsets - Accessing columns\n\nYou can access columns by using the column name or fancy indexing. They are returned as `Series` objects:\n\nYou can also get multiple columns at once:\n\nAnother convenient way to create a `DataFrame` is to pass all the values to the constructor as an `ndarray`, or a list of lists, and specify the column names and row index labels separately:\n\n#### Multi-index (optional)\n\nYou can also create multi-index datafram as follows:\n\nIf all columns are tuples of the same size, then they are understood as a multi-index. The same goes for row index labels. For example:\n\nYou can now get a DataFrame containing all the \"public\" columns very simply:\n\nIt is noted that most methods return modified copies in pandas.\n\n#### Subsets - Accessing rows\n\n\nLet's go back to the `people` `DataFrame`:\n\n**The `loc` attribute lets you access rows instead of columns.** The result is a `Series` object in which the `DataFrame`'s column names are mapped to row index labels:\n\nYou can also access rows by integer location using the `iloc` attribute:\n\nYou can also get a slice of rows, and this returns a `DataFrame` object:\n\nFinally, you can pass a boolean array to get the matching rows. This is most useful when combined with boolean expressions:\n\nYou can also accessing columns by specifiying the second axis:\n\n#### Adding and removing columns\n\nYou can generally treat `DataFrame` objects like dictionaries of `Series`, so the following work fine:\n\nWhen you add a new column, it must have the same number of rows. Missing rows are filled with NaN, and extra rows are ignored:\n\nWhen adding a new column, it is added at the end (on the right) by default. You can also insert a column anywhere else using the `insert()` method:\n\nYou can also create new columns by calling the `assign()` method. Note that this returns a new `DataFrame` object, the original is not modified\n\nYou can also rename the column name:\n\n#### Querying a `DataFrame`\n\nThe `query()` method lets you **filter a `DataFrame` based on a query expression**:\n\n#### Sorting a `DataFrame`\n\nYou can sort a `DataFrame` by calling its `sort_index` method. By default it sorts the rows by their index label, in ascending order, but let's reverse the order:\n\nNote that `sort_index` returned a sorted *copy* of the `DataFrame`. To modify `people` directly, we can set the `inplace` argument to `True`. Also, we can sort the columns instead of the rows by setting `axis=1`:\n\nTo sort the `DataFrame` by the values instead of the labels, we can use `sort_values` and specify the column to sort by:\n\n#### Plotting a `DataFrame`\n\nJust like for `Series`, pandas makes it easy to draw nice graphs based on a `DataFrame`.\n\nFor example, it is trivial to create a line plot from a `DataFrame`'s data by calling its `plot` method:\n\nAgain, there are way too many options to list here: the best option is to scroll through the [Visualization](http://pandas.pydata.org/pandas-docs/stable/visualization.html) page in pandas' documentation, find the plot you are interested in and look at the example code.\n\n#### Operations on `DataFrame`s\n\nAlthough `DataFrame`s do not try to mimick NumPy arrays, there are a few similarities. Let's create a `DataFrame` to demonstrate this:\n\nYou can apply NumPy mathematical functions on a `DataFrame`: the function is applied to all values:\n\nAggregation operations, such as computing the `max()`, the `sum()` or the `mean()` of a `DataFrame`, apply to each column, and you get back a `Series` object:\n\nMost of these functions take an optional `axis` parameter which lets you specify along which axis of the `DataFrame` you want the operation executed. The default is `axis=0`, meaning that the operation is executed vertically (on each column). You can set `axis=1` to execute the operation horizontally (on each row). For example, let's find out which students had all grades greater than `5`:\n\nIf you add a `Series` object to a `DataFrame` (or execute any other binary operation), Pandas attempts to broadcast the operation to all *rows* in the `DataFrame`. This only works if the `Series` has the same size as the `DataFrame`s rows. For example, let's subtract the `mean` of the `DataFrame` (a `Series` object) from the `DataFrame`:\n\nIf you want to subtract the global mean from every grade, here is one way to do it:\n\nThe following shows the behavior of `nan`\n\n#### Handling missing data\n\nDealing with missing data is a frequent task when working with real life data. Pandas offers a few tools to handle missing data.\n \nLet's try to fix the problem above. For example, we can decide that missing data should result in a zero, instead of `NaN`. We can replace all `NaN` values by a any value using the `fillna()` method:\n\nWe can call the `dropna()` method to get rid of rows that are full of `NaN`s:\n\nNow let's remove columns that are full of `NaN`s by setting the `axis` argument to `1`:\n\n#### Aggregating with `groupby`\n\nSimilar to the SQL language, pandas allows grouping your data into groups to run calculations over each group.\n\nFirst, let's add some extra data about each person so we can group them, and let's go back to the `final_grades` `DataFrame` so we can see how `NaN` values are handled:\n\nNow let's group data in this `DataFrame` by hobby:\n\nWe are ready to compute the average grade per hobby:\n\nThat was easy! Note that the `NaN` values have simply been skipped when computing the means.\n\n#### Pivot tables (Optional)\n\nPandas supports spreadsheet-like [pivot tables](https://en.wikipedia.org/wiki/Pivot_table) that allow quick data summarization.\n\n#### Overview functions\n\nWhen dealing with large `DataFrames`, it is useful to get a quick overview of its content. Pandas offers a few functions for this. First, let's create a large `DataFrame` with a mix of numeric values, missing values and text values. Notice how Jupyter displays only the corners of the `DataFrame`:\n\nThe `head()` method returns the top 5 rows:\n\nOf course there's also a `tail()` function to view the bottom 5 rows. You can pass the number of rows you want:\n\nThe `info()` method prints out a summary of each columns contents:\n\nFinally, the `describe()` method gives a nice overview of the main aggregated values over each column:\n* `count`: number of non-null (not NaN) values\n* `mean`: mean of non-null values\n* `std`: [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation) of non-null values\n* `min`: minimum of non-null values\n* `25%`, `50%`, `75%`: 25th, 50th and 75th [percentile](https://en.wikipedia.org/wiki/Percentile) of non-null values\n* `max`: maximum of non-null values\n\n#### Saving & loading\n\nPandas can save `DataFrame`s to various backends, including file formats such as CSV, Excel, JSON, HTML and HDF5, or to a SQL database. Let's create a `DataFrame` to demonstrate this:\n\nLet's save it to CSV, HTML and JSON:\n\nDone! Let's take a peek at what was saved:\n\nNote that the index is saved as the first column (with no name) in a CSV file, as `<th>` tags in HTML and as keys in JSON.\n\nSaving to other formats works very similarly, but some formats require extra libraries to be installed. For example, saving to Excel requires the `openpyxl` library:\n\nNow let's load our CSV file back into a `DataFrame`:\n\nAs you might guess, there are similar `read_json`, `read_html`, `read_excel` functions as well.  We can also read data straight from the Internet. For example, let's load the top 1,000 U.S. cities from github:\n\nThere are more options available, in particular regarding datetime format. Check out the [documentation](http://pandas.pydata.org/pandas-docs/stable/io.html) for more details.\n\n#### Combining `DataFrame`s\n\nOne powerful feature of Pandas is it's ability to perform SQL-like joins on `DataFrame`s. Various types of joins are supported: inner joins, left/right outer joins and full joins. To illustrate this, let's start by creating a couple simple `DataFrame`s:\n\nNow let's join these `DataFrame`s using the `merge()` function:\n\nNote that both `DataFrame`s have a column named `state`, so in the result they got renamed to `state_x` and `state_y`.\n\nAlso, note that Cleveland, Salt Lake City and Houston were dropped because they don't exist in *both* `DataFrame`s. This is the equivalent of a SQL `INNER JOIN`. If you want a `FULL OUTER JOIN`, where no city gets dropped and `NaN` values are added, you must specify `how=\"outer\"`:\n\nOf course `LEFT OUTER JOIN` is also available by setting `how=\"left\"`: only the cities present in the left `DataFrame` end up in the result. Similarly, with `how=\"right\"` only cities in the right `DataFrame` appear in the result. For example:\n\n#### Concatenation\n\nRather than joining `DataFrame`s, we may just want to concatenate them. That's what `concat()` is for:\n\nNote that this operation aligned the data horizontally (by columns) but not vertically (by rows). In this example, we end up with multiple rows having the same index (eg. 3). Pandas handles this rather gracefully:\n\nOr you can tell Pandas to just ignore the index:\n\nNotice that when a column does not exist in a `DataFrame`, it acts as if it was filled with `NaN` values. If we set `join=\"inner\"`, then only columns that exist in *both* `DataFrame`s are returned:\n\n#### Categories\n\nIt is quite frequent to have values that represent categories, for example `1` for female and `2` for male, or `\"A\"` for Good, `\"B\"` for Average, `\"C\"` for Bad. These categorical values can be hard to read and cumbersome to handle, but fortunately pandas makes it easy. To illustrate this, let's take the `city_pop` `DataFrame` we created earlier, and add a column that represents a category:\n\nRight now the `eco_code` column is full of apparently meaningless codes. Let's fix that. First, we will create a new categorical column based on the `eco_code`s:\n\nNow we can give each category a meaningful name:\n\nNote that categorical values are sorted according to their categorical order, *not* their alphabetical order:\n\n## What next?\n\nAs you probably noticed by now, pandas is quite a large library with *many* features. Although we went through the most important features, there is still a lot to discover. Probably the best way to learn more is to get your hands dirty with some real-life data. It is also a good idea to go through pandas' excellent [documentation](http://pandas.pydata.org/pandas-docs/stable/index.html), in particular the [Cookbook](http://pandas.pydata.org/pandas-docs/stable/cookbook.html).\n\nYou can also work with BigQuery in Pandas. Check out [https://pandas.pydata.org/docs/reference/api/pandas.read_gbq.html](https://pandas.pydata.org/docs/reference/api/pandas.read_gbq.html) and [https://pandas-gbq.readthedocs.io/en/latest/](https://pandas-gbq.readthedocs.io/en/latest/) for more details.\n\n## References\n\n1. [https://www.kaggle.com/learn/](https://www.kaggle.com/learn/)\n2. [https://github.com/ageron/handson-ml3](https://github.com/ageron/handson-ml3)\n3. [https://github.com/ageron/handson-ml3](https://github.com/ageron/handson-ml3)\n","srcMarkdownNoYaml":"\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/phonchi/nsysu-math608/blob/master/static_files/presentations/03_Relational_Database_and_data_wrangling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/phonchi/nsysu-math608/blob/master/static_files/presentations/03_Relational_Database_and_data_wrangling.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>\n<br/>\n\n## Queringing data with BigQuery\n\nStructured Query Language, or SQL, is the programming language used with databases, and it is an important skill for any data scientist. In this example, you'll build your SQL skills using BigQuery, a web service work as database management system that lets you apply SQL to huge datasets.\n\n### Preliminaries for google colab (optional)\n\nWe want to start exploring the Google BiqQuery [public datasets](https://cloud.google.com/bigquery/public-data/). Let's start by walking through the required setup steps, and then we can load and explore some data.\n\nIf you are using colab. Follow [this quickstart guide](https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries), which will explain how to:\n1. Create a [Cloud Platform project](https://console.cloud.google.com/cloud-resource-manager) if you don't have one already.\n2. [Enable billing](https://support.google.com/cloud/answer/6293499#enable-billing) for the project (If you apply the free trial, you already satisfy this condition.)\n3. [Enable the BigQuery API](https://console.cloud.google.com/flows/enableapi?apiid=bigquery)\n4. [Enabling the Service account](https://cloud.google.com/docs/authentication/getting-started)\n\nNow we need to authenticate to gain access to the BigQuery API. We will create a client, specifying the service account key file (replace 'lunar-pact-378812-7a28b789bde2.json' with your key file).\n\nNow that we're authenticated, we need to load the BigQuery package, and the `google.colab.data_table` package that can be used to display large pandas dataframes as an interactive data. Loading `data_table` is optional, but it will be useful for working with data in pandas.\n\n### Create the reference\n\n\nYou can also work with Kaggle, which provide bigquery integration that you do not need to setup a google account. **Each Kaggle user can scan 5TB every 30 days for free.  Once you hit that limit, you'll have to wait for it to reset.** See https://www.kaggle.com/product-feedback/48573 for more details.\n\n\nThe first step in the workflow is to create a [`Client`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client) object.  As you'll soon see, this `Client` object will play a central role in retrieving information from BigQuery datasets.\n\nWe'll work with a dataset of posts on Hacker News, a website focusing on computer science and cybersecurity news. In BigQuery, each dataset is contained in a corresponding project. In this case, our `hacker_news` dataset is contained in the `bigquery-public-data project`. \n\nTo access the dataset, We begin by constructing a reference to the dataset with the [`dataset()`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.dataset.Dataset) method. Next, we use the [`get_dataset()`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client#google_cloud_bigquery_client_Client_get_dataset) method, along with the reference we just constructed, to fetch the dataset.\n\n[See the full list of public datasets](https://console.cloud.google.com/marketplace/browse?filter=solution-type:dataset) or the [kaggle bigquery dataset](https://www.kaggle.com/datasets?search=bigquery) if you want to explore others.\n\nEvery dataset is just a collection of tables. You can think of a dataset as a spreadsheet file containing multiple tables, all composed of rows and columns.We use the [`list_tables()`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client#google_cloud_bigquery_client_Client_list_tables) method to list the tables in the dataset.\n\nSimilar to how we fetched a dataset, we can fetch a table. In the code cell below, we fetch the `full` table in the hacker_news dataset\n\nIn the next section, you'll explore the contents of this table in more detail.  For now, take the time to use the image below to consolidate what you've learned so far.\n\n![first_commands](https://i.imgur.com/biYqbUB.png)\n\n### Table schema\n\nThe structure of a table is called its **schema**.  **We need to understand a table's schema to effectively pull out the data we want.** \n\nIn this example, we'll investigate the `full` table that we fetched above.\n\nEach [`SchemaField`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.schema.SchemaField) tells us about a specific column (which we also refer to as a **field**). In order, the information is:\n\n* The **name** of the column\n* The **field type** (or datatype) in the column\n* The **mode** of the column (`'NULLABLE'` means that a column allows NULL values, and is the default)\n* A **description** of the data in that column\n\nFor instance, the field has the SchemaField:\n\n`SchemaField('by', 'string', 'NULLABLE', \"The username of the item's author.\",())`\n\nThis tells us:\n- the field (or column) is called `by`,\n- the data in this field is strings, \n- NULL values are allowed, and\n- it contains the usernames corresponding to each item's author.\n\nWe can use the [`list_rows()`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client#google_cloud_bigquery_client_Client_list_rows) method to check just the first five lines of of the `full` table to make sure this is right.  This returns a BigQuery [`RowIterator`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.table.RowIterator) object that can quickly be converted to a pandas DataFrame with the [`to_dataframe()`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.table.RowIterator#google_cloud_bigquery_table_RowIterator_to_dataframe) method.\n\nThe `list_rows()` method will also let us look at just the information in a specific column. If we want to see the first five entries in the `by` column, for example, we can do that!\n\n### Select, From & Where\n\nNow that you know how to access and examine a dataset, you're ready to write your first SQL query!  As you'll soon see, **SQL queries will help you sort through a massive dataset, to retrieve only the information that you need.** We'll begin by using the keywords **SELECT**, **FROM**, and **WHERE** to get data from specific columns based on conditions you specify. \n\nWe'll use an [OpenAQ](https://openaq.org) dataset about air quality. First, we'll set up everything we need to run queries and take a quick peek at what tables are in our database.\n\nThe dataset contains only one table, called `global_air_quality`.  We'll fetch the table and take a peek at the first few rows to see what sort of data it contains.\n\nlet's put together a query. Say we want to select all the values from the `city` column that are in rows where the `country` column is `'US'` (for \"United States\").\n\nNotice also that SQL statements requires single quotes for its strings inside python string (we use triple quotation mark here). We begin by setting up the query with the [`query()`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client#google_cloud_bigquery_client_Client_query) method.\n\nNow we've got a pandas DataFrame called `us_cities`, which we can use like any other DataFrame.\n\nIf you want multiple columns, you can select them with a comma between the names:\n\nYou can select all columns with a `*` like this:\n\n### Querying big dataset\n\nYou can estimate the size of any query before running it. Here is an example using the Hacker News dataset. To see how much data a query will scan, we create a [`QueryJobConfig`](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.job.QueryJobConfig) object and set the `dry_run` parameter to `True`.\n\nYou can also specify a parameter when running the query to limit how much data you are willing to scan. Here's an example with a low limit.\n\nIn this case, the query was cancelled, because the limit of 1 MB was exceeded.  However, we can also increase the limit to run the query successfully!\n\n### Group By, Having & Count\n\nNow that you can select raw data, you're ready to learn how to group your data and count things within those groups.\n\nThe Hacker News dataset contains information on stories and comments from the Hacker News social networking site. We'll work with the `comments` table and begin by printing the first few rows\n\nLet's use the table to see which comments generated the most replies.  Since:\n- the `parent` column indicates the comment that was replied to, and \n- the `id` column has the unique ID used to identify each comment, \n\nwe can **GROUP BY** the `parent` column and **COUNT()** the `id` column in order to figure out the number of comments that were made as responses to a specific comment.\n\nFurthermore, since we're only interested in popular comments, we'll look at comments with more than ten replies.  So, we'll only return groups **HAVING** more than ten ID's.\n\nEach row in the `popular_comments` DataFrame corresponds to a comment that received more than ten replies.\n\nA couple hints to make your queries even better:\n- The column resulting from `COUNT(id)` was called `f0__`. That's not a very descriptive name. You can change the name by adding `AS NumPosts` after you specify the aggregation. This is called **aliasing**.\n- If you are ever unsure what to put inside the **COUNT()** function, you can do `COUNT(1)` to count the rows in each group. Most people find it especially readable, because we know it's not focusing on other columns. It also scans less data than if supplied column names (making it faster and using less of your data access quota).\n\nUsing these tricks, we can rewrite our query:\n\nNow you have the data you want, and it has descriptive names. \n\n#### Note on using **GROUP BY**\n\nNote that because it tells SQL how to apply aggregate functions (like **COUNT()**), it doesn't make sense to use **GROUP BY** without an aggregate function.  Similarly, if you have any **GROUP BY** clause, then all variables must be passed to either a\n1. **GROUP BY** command, or\n2. an aggregation function.\n\nConsider the query below:\n\n\n\nNote that there are two variables: `parent` and `id`. \n- `parent` was passed to a **GROUP BY** command (in `GROUP BY parent`), and \n- `id` was passed to an aggregate function (in `COUNT(id)`).\n\nAnd the query below won't work, because the `author` column isn't passed to an aggregate function or a **GROUP BY** clause:\n\n### Order By\n\nFrequently, you’ll want to sort your results. **Let's use the US Traffic Fatality Records database, which contains information on traffic accidents in the US where at least one person died.**\n\nWe'll investigate the `accident_2015` table. Here is a view of the first few rows. \n\nLet's use the table to determine how the number of accidents varies with the day of the week.  Since:\n- the `consecutive_number` column contains a unique ID for each accident, and\n- the `timestamp_of_crash` column contains the date of the accident in [DATETIME](https://cloud.google.com/bigquery/docs/reference/standard-sql/date_functions) format,\n\nwe can:\n- **EXTRACT** the day of the week (as `day_of_week` in the query below) from the `timestamp_of_crash` column, and\n- **GROUP BY** the day of the week, before we **COUNT** the `consecutive_number` column to determine the number of accidents for each day of the week.\n\nThen we sort the table with an **ORDER BY** clause, so the days with the most accidents are returned first.\n\nNotice that the data is sorted by the `num_accidents` column, where the days with more traffic accidents appear first.\n\nTo map the numbers returned for the `day_of_week` column to the actual day, you might consult [the BigQuery documentation](https://cloud.google.com/bigquery/docs/reference/standard-sql/date_functions) on the DAYOFWEEK function. It says that it returns \"an integer between 1 (Sunday) and 7 (Saturday), inclusively\". So, in 2015, most fatal motor accidents in the US occured on Sunday and Saturday, while the fewest happened on Tuesday.\n\n### As and With\n\nOn its own, `AS` is a convenient way to clean up the data returned by your query. **We're going to use a common table expression (CTE)** to find out **how many Bitcoin transactions were made each day for the entire timespan of a bitcoin transaction dataset.**\n\nWe'll investigate the transactions table. Here is a view of the first few rows.\n\nSince the `block_timestamp` column contains the date of each transaction in DATETIME format, we'll convert these into DATE format using the **DATE()** command.\n\nWe do that using a CTE, and then the next part of the query counts the number of transactions for each date and sorts the table so that earlier dates appear first. \n\nSince they're returned sorted, we can easily plot the raw results to show us the number of Bitcoin transactions per day over the whole timespan of this dataset.\n\nAs you can see, common table expressions (CTEs) let you shift a lot of your data cleaning into SQL. **That's an especially good thing in the case of BigQuery, because it is vastly faster than doing the work in Pandas.**\n\n### Joining data\n\nWhen our data lives across different tables, how do we analyze it? By\nJOINing the tables together. A `JOIN` combines rows in the left table with\ncorresponding rows in the right table, where the meaning of “corresponding” is based on how we specify the join.\n\nGitHub is the most popular place to collaborate on software projects. A GitHub **repository** (or **repo**) is a collection of files associated with a specific project. Most repos on GitHub are shared under a specific legal license, which determines the legal restrictions on how they are used.  **For our example, we're going to look at how many different files have been released under each license.** \n\nWe'll work with two tables in the database.  The first table is the `licenses` table, which provides the name of each GitHub repo (in the `repo_name` column) and its corresponding license.  Here's a view of the first five rows.\n\nThe second table is the `sample_files` table, which provides, among other information, the GitHub repo that each file belongs to (in the `repo_name` column).  The first several rows of this table are printed below.\n\nNext, we write a query that uses information in both tables to determine how many files are released in each license.\n\nIt's a big query, and so we'll investigate each piece separately.\n\n![](https://i.imgur.com/QeufD01.png)\n    \nWe'll begin with the **JOIN** (highlighted in blue above).  This specifies the sources of data and how to join them. We use **ON** to specify that we combine the tables by matching the values in the `repo_name` columns in the tables.\n\nNext, we'll talk about **SELECT** and **GROUP BY** (highlighted in yellow).  The **GROUP BY** breaks the data into a different group for each license, before we **COUNT** the number of rows in the `sample_files` table that corresponds to each license.  (Remember that you can count the number of rows with `COUNT(1)`.) \n\nFinally, the **ORDER BY** (highlighted in purple) sorts the results so that licenses with more files appear first.\n\nIt was a big query, but it gave us a nice table summarizing how many files have been committed under each license:  \n\nThere are a few more types of JOIN, along with how to use UNIONs to pull information from multiple tables. We'll work with the [Hacker News](https://www.kaggle.com/hacker-news/hacker-news) dataset. We begin by reviewing the first several rows of the `comments` table.\n\nThe query below pulls information from the `stories` and `comments` tables to create a table showing all stories posted on January 1, 2012, along with the corresponding number of comments.  We use a **LEFT JOIN** so that the results include stories that didn't receive any comments.\n\nSince the results are ordered by the `num_comments` column, stories without comments appear at the end of the DataFrame.  (Remember that **NaN** stands for \"not a number\".)\n\nAs you've seen, JOINs horizontally combine results from different tables. If you instead would like to vertically concatenate columns, you can do so with a `UNION`. \n\nNext, we write a query to select all usernames corresponding to users who wrote stories or comments on January 1, 2014.  We use **UNION DISTINCT** (instead of **UNION ALL**) to ensure that each user appears in the table at most once.\n\nTo get the number of users who posted on January 1, 2014, we need only take the length of the DataFrame.\n\n### Nested and Repeated data\n\nSo far, you've worked with many types of data, including numeric types (integers, floating point values), strings, and the DATETIME type. In this tutorial, you'll learn how to query nested and repeated data. These are the most complex data types that you can find in BigQuery datasets!\n\nWe'll work with the Google Analytics Sample dataset. It contains information tracking the behavior of visitors to the Google Merchandise store, an e-commerce website that sells Google branded items.\n\nWe begin by printing the first few rows of the `ga_sessions_20170801` table. This table tracks visits to the website on August 1, 2017. The table has many nested fields from table preview:\n\nNow we'll work with the `hits` column as an example of data that is both nested and repeated. Since:\n\n- `hits` is a STRUCT (contains nested data) and is repeated,\n- `hitNumber`, `page`, and `type` are all nested inside the `hits` column, and\n- `pagePath` is nested inside the `page` field,\n\nwe can query these fields with the following syntax:\n\n### Analytic Function (Optional)\n\nYou can also define analytic functions, which also operate on a set of rows like aggregation function. However, unlike aggregate functions, analytic functions return a (potentially different) value for each row in the original table. Analytic functions allow us to perform complex calculations with relatively straightforward syntax. For instance, we can quickly calculate moving averages and running totals, among other quantities.\n\nWe'll work with the [San Francisco Open Data](https://www.kaggle.com/datasf/san-francisco) dataset.\n\nEach row of the table corresponds to a different bike trip, and we can use an analytic function to **calculate the cumulative number of trips for each date in 2015.**\n\nThe query uses a [common table expression (CTE)](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#with_clause) to first calculate the daily number of trips.  Then, we use **SUM()** as an aggregate function.\n- Since there is no **PARTITION BY** clause, the entire table is treated as a single partition.\n- The **ORDER BY** clause orders the rows by date, where earlier dates appear first. \n- By setting the **window frame** clause to `ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW`, we ensure that all rows up to and including the current date are used to calculate the (cumulative) sum. See https://cloud.google.com/bigquery/docs/reference/standard-sql/analytic-function-concepts#def_window_frame for more details.\n\nThe next query **tracks the stations where each bike began (in `start_station_id`) and ended (in `end_station_id`) the day on October 25, 2015.**\n\nThe query uses both **FIRST_VALUE()** and **LAST_VALUE()** as analytic functions.\n- The **PARTITION BY** clause breaks the data into partitions based on the `bike_number` column.  Since this column holds unique identifiers for the bikes, this ensures the calculations are performed separately for each bike.\n- The **ORDER BY** clause puts the rows within each partition in chronological order.\n- Since the **window frame** clause is `ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING`, for each row, its entire partition is used to perform the calculation.  (_This ensures the calculated values for rows in the same partition are identical._)\n\nYou can check https://cloud.google.com/bigquery/docs/reference/standard-sql/introduction and https://googleapis.dev/python/bigquery/latest/index.html for more details.\n\n## Data Wrangling with Pandas\n\n### `Series` objects\nThe `Pandas` library contains these useful data structures:\n* `Series` objects, that we will discuss now. A `Series` object is 1D array, similar to a column in a spreadsheet (with a column name and row labels).\n* `DataFrame` objects. This is a 2D table, similar to a spreadsheet (with column names and row labels).\n\n#### Creating a `Series`\n\nLet's start by creating our first `Series` object!\n\nArithmetic operations on `Series` are also possible, and they apply *elementwise*, just like for `ndarray`s in NumPy:\n\n#### Index labels\n\nEach item in a `Series` object has a unique identifier called the *index label*. By default, it is simply the rank of the item in the `Series` (starting at `0`) but you can also set the index labels manually:\n\nYou can then use the `Series` just like a `dict`:\n\nYou can still access the items by integer location, like in a regular array:\n\nTo make it clear when you are accessing, **it is recommended to always use the `loc` attribute when accessing by label, and the `iloc` attribute when accessing by integer location**:\n\nSlicing a `Series` also slices the index labels:\n\n#### Initialize from `dict`\n\nYou can create a `Series` object from a `dict`. The keys will be used as index labels:\n\nWhen an operation involves multiple `Series` objects, `pandas` automatically aligns items by matching index labels.\n\nThe resulting `Series` contains the union of index labels from `s2` and `s3`. Since `\"colin\"` is missing from `s2` and `\"charles\"` is missing from `s3`, these items have a `NaN` result value. (ie. Not-a-Number means *missing*).\n\nAutomatic alignment is very handy when working with data that may come from various sources with varying structure and missing items\n\n#### Initialize with a scalar\n\nYou can also initialize a `Series` object using a scalar and a list of index labels: all items will be set to the scalar.\n\nPandas makes it easy to plot `Series` data using matplotlib:\n\nYou can easily convert it to NumPy array by dicarding the index. \n\nThere are *many* options for plotting your data. It is not necessary to list them all here: if you need a particular type of plot (histograms, pie charts, etc.), just look for it in the excellent [Visualization](http://pandas.pydata.org/pandas-docs/stable/visualization.html) section of pandas' documentation, and look at the example code.\n\n### Handling time\n\n\nMany datasets have timestamps, and pandas is awesome at manipulating such data:\n* it can represent periods (such as 2016Q3) and frequencies (such as \"monthly\")\n* it can convert periods to actual timestamps, and *vice versa*\n* it can resample data and aggregate values any way you like\n* it can handle timezones.\n\n#### Time range\n\nLet's start by creating a time series using `pd.date_range()`. This returns a `DatetimeIndex` containing one datetime per hour for 12 hours starting on March 6th 2023 at 5:30pm.\n\nThis `DatetimeIndex` may be used as an index in a `Series`:\n\nLet's plot this series:\n\n### Periods\n\nThe `pd.period_range()` function returns a `PeriodIndex` instead of a `DatetimeIndex`. For example, let's get all quarters in 2022 and 2023:\n\nAdding a number `N` to a `PeriodIndex` shifts the periods by `N` times the `PeriodIndex`'s frequency:\n\nPandas also provides many other time-related functions that we recommend you check out in the [documentation](http://pandas.pydata.org/pandas-docs/stable/timeseries.html)\n\n### `DataFrame` objects\n\nA DataFrame object represents a spreadsheet, with cell values, column names and row index labels. You can define expressions to compute columns based on other columns, create pivot-tables, group rows, draw graphs, etc. You can see `DataFrame` as dictionaries of `Series`.\n\n#### Creating a `DataFrame`\n\nYou can create a DataFrame by passing a dictionary of `Series` objects:\n\nA few things to note:\n* the `Series` were automatically aligned based on their index,\n* missing values are represented as `NaN`,\n* `Series` names are ignored (the name `\"year\"` was dropped),\n* `DataFrame`s are displayed nicely in Jupyter notebooks!\n\n#### Subsets - Accessing columns\n\nYou can access columns by using the column name or fancy indexing. They are returned as `Series` objects:\n\nYou can also get multiple columns at once:\n\nAnother convenient way to create a `DataFrame` is to pass all the values to the constructor as an `ndarray`, or a list of lists, and specify the column names and row index labels separately:\n\n#### Multi-index (optional)\n\nYou can also create multi-index datafram as follows:\n\nIf all columns are tuples of the same size, then they are understood as a multi-index. The same goes for row index labels. For example:\n\nYou can now get a DataFrame containing all the \"public\" columns very simply:\n\nIt is noted that most methods return modified copies in pandas.\n\n#### Subsets - Accessing rows\n\n\nLet's go back to the `people` `DataFrame`:\n\n**The `loc` attribute lets you access rows instead of columns.** The result is a `Series` object in which the `DataFrame`'s column names are mapped to row index labels:\n\nYou can also access rows by integer location using the `iloc` attribute:\n\nYou can also get a slice of rows, and this returns a `DataFrame` object:\n\nFinally, you can pass a boolean array to get the matching rows. This is most useful when combined with boolean expressions:\n\nYou can also accessing columns by specifiying the second axis:\n\n#### Adding and removing columns\n\nYou can generally treat `DataFrame` objects like dictionaries of `Series`, so the following work fine:\n\nWhen you add a new column, it must have the same number of rows. Missing rows are filled with NaN, and extra rows are ignored:\n\nWhen adding a new column, it is added at the end (on the right) by default. You can also insert a column anywhere else using the `insert()` method:\n\nYou can also create new columns by calling the `assign()` method. Note that this returns a new `DataFrame` object, the original is not modified\n\nYou can also rename the column name:\n\n#### Querying a `DataFrame`\n\nThe `query()` method lets you **filter a `DataFrame` based on a query expression**:\n\n#### Sorting a `DataFrame`\n\nYou can sort a `DataFrame` by calling its `sort_index` method. By default it sorts the rows by their index label, in ascending order, but let's reverse the order:\n\nNote that `sort_index` returned a sorted *copy* of the `DataFrame`. To modify `people` directly, we can set the `inplace` argument to `True`. Also, we can sort the columns instead of the rows by setting `axis=1`:\n\nTo sort the `DataFrame` by the values instead of the labels, we can use `sort_values` and specify the column to sort by:\n\n#### Plotting a `DataFrame`\n\nJust like for `Series`, pandas makes it easy to draw nice graphs based on a `DataFrame`.\n\nFor example, it is trivial to create a line plot from a `DataFrame`'s data by calling its `plot` method:\n\nAgain, there are way too many options to list here: the best option is to scroll through the [Visualization](http://pandas.pydata.org/pandas-docs/stable/visualization.html) page in pandas' documentation, find the plot you are interested in and look at the example code.\n\n#### Operations on `DataFrame`s\n\nAlthough `DataFrame`s do not try to mimick NumPy arrays, there are a few similarities. Let's create a `DataFrame` to demonstrate this:\n\nYou can apply NumPy mathematical functions on a `DataFrame`: the function is applied to all values:\n\nAggregation operations, such as computing the `max()`, the `sum()` or the `mean()` of a `DataFrame`, apply to each column, and you get back a `Series` object:\n\nMost of these functions take an optional `axis` parameter which lets you specify along which axis of the `DataFrame` you want the operation executed. The default is `axis=0`, meaning that the operation is executed vertically (on each column). You can set `axis=1` to execute the operation horizontally (on each row). For example, let's find out which students had all grades greater than `5`:\n\nIf you add a `Series` object to a `DataFrame` (or execute any other binary operation), Pandas attempts to broadcast the operation to all *rows* in the `DataFrame`. This only works if the `Series` has the same size as the `DataFrame`s rows. For example, let's subtract the `mean` of the `DataFrame` (a `Series` object) from the `DataFrame`:\n\nIf you want to subtract the global mean from every grade, here is one way to do it:\n\nThe following shows the behavior of `nan`\n\n#### Handling missing data\n\nDealing with missing data is a frequent task when working with real life data. Pandas offers a few tools to handle missing data.\n \nLet's try to fix the problem above. For example, we can decide that missing data should result in a zero, instead of `NaN`. We can replace all `NaN` values by a any value using the `fillna()` method:\n\nWe can call the `dropna()` method to get rid of rows that are full of `NaN`s:\n\nNow let's remove columns that are full of `NaN`s by setting the `axis` argument to `1`:\n\n#### Aggregating with `groupby`\n\nSimilar to the SQL language, pandas allows grouping your data into groups to run calculations over each group.\n\nFirst, let's add some extra data about each person so we can group them, and let's go back to the `final_grades` `DataFrame` so we can see how `NaN` values are handled:\n\nNow let's group data in this `DataFrame` by hobby:\n\nWe are ready to compute the average grade per hobby:\n\nThat was easy! Note that the `NaN` values have simply been skipped when computing the means.\n\n#### Pivot tables (Optional)\n\nPandas supports spreadsheet-like [pivot tables](https://en.wikipedia.org/wiki/Pivot_table) that allow quick data summarization.\n\n#### Overview functions\n\nWhen dealing with large `DataFrames`, it is useful to get a quick overview of its content. Pandas offers a few functions for this. First, let's create a large `DataFrame` with a mix of numeric values, missing values and text values. Notice how Jupyter displays only the corners of the `DataFrame`:\n\nThe `head()` method returns the top 5 rows:\n\nOf course there's also a `tail()` function to view the bottom 5 rows. You can pass the number of rows you want:\n\nThe `info()` method prints out a summary of each columns contents:\n\nFinally, the `describe()` method gives a nice overview of the main aggregated values over each column:\n* `count`: number of non-null (not NaN) values\n* `mean`: mean of non-null values\n* `std`: [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation) of non-null values\n* `min`: minimum of non-null values\n* `25%`, `50%`, `75%`: 25th, 50th and 75th [percentile](https://en.wikipedia.org/wiki/Percentile) of non-null values\n* `max`: maximum of non-null values\n\n#### Saving & loading\n\nPandas can save `DataFrame`s to various backends, including file formats such as CSV, Excel, JSON, HTML and HDF5, or to a SQL database. Let's create a `DataFrame` to demonstrate this:\n\nLet's save it to CSV, HTML and JSON:\n\nDone! Let's take a peek at what was saved:\n\nNote that the index is saved as the first column (with no name) in a CSV file, as `<th>` tags in HTML and as keys in JSON.\n\nSaving to other formats works very similarly, but some formats require extra libraries to be installed. For example, saving to Excel requires the `openpyxl` library:\n\nNow let's load our CSV file back into a `DataFrame`:\n\nAs you might guess, there are similar `read_json`, `read_html`, `read_excel` functions as well.  We can also read data straight from the Internet. For example, let's load the top 1,000 U.S. cities from github:\n\nThere are more options available, in particular regarding datetime format. Check out the [documentation](http://pandas.pydata.org/pandas-docs/stable/io.html) for more details.\n\n#### Combining `DataFrame`s\n\nOne powerful feature of Pandas is it's ability to perform SQL-like joins on `DataFrame`s. Various types of joins are supported: inner joins, left/right outer joins and full joins. To illustrate this, let's start by creating a couple simple `DataFrame`s:\n\nNow let's join these `DataFrame`s using the `merge()` function:\n\nNote that both `DataFrame`s have a column named `state`, so in the result they got renamed to `state_x` and `state_y`.\n\nAlso, note that Cleveland, Salt Lake City and Houston were dropped because they don't exist in *both* `DataFrame`s. This is the equivalent of a SQL `INNER JOIN`. If you want a `FULL OUTER JOIN`, where no city gets dropped and `NaN` values are added, you must specify `how=\"outer\"`:\n\nOf course `LEFT OUTER JOIN` is also available by setting `how=\"left\"`: only the cities present in the left `DataFrame` end up in the result. Similarly, with `how=\"right\"` only cities in the right `DataFrame` appear in the result. For example:\n\n#### Concatenation\n\nRather than joining `DataFrame`s, we may just want to concatenate them. That's what `concat()` is for:\n\nNote that this operation aligned the data horizontally (by columns) but not vertically (by rows). In this example, we end up with multiple rows having the same index (eg. 3). Pandas handles this rather gracefully:\n\nOr you can tell Pandas to just ignore the index:\n\nNotice that when a column does not exist in a `DataFrame`, it acts as if it was filled with `NaN` values. If we set `join=\"inner\"`, then only columns that exist in *both* `DataFrame`s are returned:\n\n#### Categories\n\nIt is quite frequent to have values that represent categories, for example `1` for female and `2` for male, or `\"A\"` for Good, `\"B\"` for Average, `\"C\"` for Bad. These categorical values can be hard to read and cumbersome to handle, but fortunately pandas makes it easy. To illustrate this, let's take the `city_pop` `DataFrame` we created earlier, and add a column that represents a category:\n\nRight now the `eco_code` column is full of apparently meaningless codes. Let's fix that. First, we will create a new categorical column based on the `eco_code`s:\n\nNow we can give each category a meaningful name:\n\nNote that categorical values are sorted according to their categorical order, *not* their alphabetical order:\n\n## What next?\n\nAs you probably noticed by now, pandas is quite a large library with *many* features. Although we went through the most important features, there is still a lot to discover. Probably the best way to learn more is to get your hands dirty with some real-life data. It is also a good idea to go through pandas' excellent [documentation](http://pandas.pydata.org/pandas-docs/stable/index.html), in particular the [Cookbook](http://pandas.pydata.org/pandas-docs/stable/cookbook.html).\n\nYou can also work with BigQuery in Pandas. Check out [https://pandas.pydata.org/docs/reference/api/pandas.read_gbq.html](https://pandas.pydata.org/docs/reference/api/pandas.read_gbq.html) and [https://pandas-gbq.readthedocs.io/en/latest/](https://pandas-gbq.readthedocs.io/en/latest/) for more details.\n\n## References\n\n1. [https://www.kaggle.com/learn/](https://www.kaggle.com/learn/)\n2. [https://github.com/ageron/handson-ml3](https://github.com/ageron/handson-ml3)\n3. [https://github.com/ageron/handson-ml3](https://github.com/ageron/handson-ml3)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"03_Relational_Database_and_data_wrangling.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.306","theme":"cosmo","cover-image":"cover.jpg","title":"Relational Database and data wrangling","author":"phonchi","date":"03/06/2023"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}